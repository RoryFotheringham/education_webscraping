<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/</course_url>
    <course_title>Automatic Speech Recognition</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Humanities </list>
      <list>Electrical Engineering </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>20
21</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture20/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>27</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  28N-best List Modifications (cont.)
N-best list with optional rejection:
what_is  6.13   the  5.48   forecast  6.88   for  5.43 paris -0.03 park  4.41    new_jersey  4. 35
what_is  6.13   the  5.48   forecast  6.88   for  5.43 *reject*   0.00 park  4.41   new_jersey  4. 35
what_is  6.13   the  5.48   forecast  6.88   for  4.47 hyann is -0.6 1park  4.41   new_jersey  4. 35
what_is  6.13   the  5.48   forecast  6.88   for  4.47 *reject*  0.00 park  4.41   new_jersey  4. 35
what_is  6.13   the  5.48   forecast  6.88   for  5.12 venice   -0.89 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  5.12 *reject*  0.00 park  4.41   new_jersey  4. 35
what_is  6.13   the  5.48   forecast  6.88   for  4.28 france   -1.12 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  4.28 *reject*   0.00 park  4.41   new_jersey  4. 35
Words w ith poor confidence
scores compete w ith 
rejected w ords during 
natural language 
understanding search</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  8Cepstral Mean Normalization (cont)
M
1
M
m1c[n] c[n,m]
==During recognition, speech is processed in frames
Let c[n,m] be the nth cepstral coefficient of the mth frame:
=f[n,m]f[n]  c[n,m]s[n,m]f[n,m] =+
Because the channel filter is linear time in variant:
  c[n,m]s[n,m]f[n] = +
Goal: Remove the effect of the filter!
Start by averaging cepstrum over all frames:
M
1
M
m1  f[n] s[n,m]
==+</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  25Word Confidence Results
00.10.20.30.40.50.60.70.80.91
00.10.20.30.40.50.60.70.80.91
False Acceptance Rat eCorrect Accep tance Rate
Full Set of 
10 Confidence
FeaturesMean Normalized
Likelihood Score
Only
N-best Purity
Score Onl y</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  4
Noises and Non-Speech Artifacts
Non-speech artifacts can be extremel y varied
Background noises (music , dog bark, door s lam, etc.)
Microphone and channel noises (cli cks, beeps, static, etc.)
Non-lexical speaker noises (cough, laugh, lip smack, etc.)
Noises can be simultaneous with speech</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  24
Word Confidence Experiment
Want to reject hypothesized words for which recognizer has lo w 
confidence
Train confidence model on independent development data
Test on independent test set of JUPITER data
Evaluate using ROC curve
Examines correct acceptances vs. false acceptances
Want to reject incorrectl y hypothesized words and accept correctl y 
hypothesized w ords
Results show n for two indi vidual feature and for full feature vector 
with 10 features
Reference: Hazen, et al, 2002</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  36
References
C. Pao, P. Schmid, and J. Gla ss, Confidence scoring for speech 
understanding, ICSLP, 1998.
S. Kamppari and T. Hazen, Word and phone le vel acoustic confidence 
scoring, ICASSP, 2000.
R. Lippman, E . Martin, and D. Paul , Multi-st yle training for robust 
isolated- word speech recognition, ICASS P, 1987.
C. Ma , M. Randolph, and J . Drish, A support  vector m achine-based 
rejection technique for speec h recognition, ICASSP, 2001.
W. Ward, Modelling non- verbal sounds  for speech recognition, DARP A 
Speech and Natural Language Workshop, October, 1989.
A. Wendemuth, G. Rose, and J. Dolfing, Ad vances in confidence 
measures for large vocabular y, ICASS P, 1999.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  19
Word Confidence Features
Want to extract information from  recognition computation which 
is correlated with correctness
Possible word level confidence fe atures extracted from acoustic 
scores:
Mean normalized acoustic score o ver word
Minim um norma lized acoustic  score o ver word
Mean normalization model score
Other sources of information:
N-best purity  scores
Language model scores
Number of competing h ypotheses
Relativ e score differences betw een hypotheses
Reference: Chase, 1997</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  16Acoustic Likelihood Scores
An acoustic likelihood score is computed as:
r(| ) pxu
Acoustic likelihood scores are good for comparing different 
hypotheses
Score are re lative density  likelihoods, not probabilities
Likelihood scores do not provide good estimate of 
correctness or reliability</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  2Typical Digital Speech Recording
Unique Vocal Tract
Nyquist
FilterQuantization
NoiseDigitization

Digital Signal+Environmental Effects
Background
NoiseRoom
Reverberation+ +
Line
NoiseChannel
FilterChannel Effects
 +</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  26
Using Confidence Scores
To be useful, confidence scores must be integrated with 
language understanding and dialogue modeling
Confidence scores a re often quantized into t wo or three decision
regions:
Accept or reject (tw o regions)
Accept, reject, or uncertain (three regions)
Language understanding componen t can be adapted to handle 
rejected w ords
Dialogue management component can perform  different actions 
based on confidence score
Perform normal action w hen everything is accepted
Ask for confirmation when uncertain
Ask user to repeat or rephrase when rejected
Reference: Hazen, et al, 2002</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  15
Confidence Scoring Overview
Question: Ho w do we assess if a recognizers hypothesis is 
correct or not?
Goal: Generate confidence scor es which estimate the likelihood 
that a hypothesis is correct
Scores can be computed at multiple levels:
Phonetic  scores
Word scores
Utterance scores
One approach:
Find features correlated with correctness
Construct feature v ector from good features
Build correct/incorrect classifier for feature vector</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  27N-best List Modifications
What is the forecast for Paramus Park, New Jersey?
Standard N-best list with confidence scores:
what_is  6.13   the  5.48   forecast  6.88   for  5.43 paris     -0.03 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  4.47 hyannis -0.61 park  4.41    new_je rsey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  5.12 venice  -0.89 park  4.41    new_je rsey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  4.28 france    -1.12 park  4.41    new_je rsey  4.35
N-best list w ith hard rejection of lo w scoring words:
what_is  6.13   the  5.48   forecast  6.88   for  5.43  *reject*  0.00 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  4.47 *reject*  0.00 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  5.12 *reject*  0.00 park  4.41   new_jersey  4.35
what_is  6.13   the  5.48   forecast  6.88   for  4.28 *reject*  0.00 park  4.41   new_jersey  4.35</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  14Non-Speech Modeling Experiment
Added 5 non-speech models to JUPITER
&lt;cough&gt;, &lt; laugh&gt;, &lt;noise&gt;, &lt; background&gt; , &lt;hangup&gt;
Reference: Hazen, Hetherington and Park, 2001
Word error rate results:
All Data
Data w/ Noise
IV Data w/ Noise
IV Data w/ No NoiseTest Set Data Baseline + Noise Models
IV = In-vocabular y data on ly18.9%
64.0%
46.4%
9.4% 9.6%17.1%
45.1%
28.2%</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  29Example Understanding Parse Tree
WHA T      IS    THE  FOREC AST FOR *REJEC T*          PARK       NEW JERS EYWeather
State Unknown_CityQuestion
What Link
Post_City Rej_WordWeather_Property
In PlaceIn_PlaceSemantic concepts extracted
expressed as key-value pairsWEATHER:             FORECA ST
UNKNOWN_CITY:  *REJECT* P ARK
STATE: NEW JERSEY</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  20The N-best Purity Measure
N-best purity is the fraction of N-best hypotheses in w hich a 
word hypothesis appears
(1)what   is   the  weather  in  new york
(2)what   is   the  weather  in   newark
(3)what   is &lt;uh&gt; weather  in  new york
(4)what   is &lt;uh&gt; weather  in   newark
(5)what was the   weather  in  new york1.0
1.0
1.0
1.0
1.00.8
0.8
0.8
0.8
0.20.6
0.6
0.4
0.4
0.61.0
1.0
1.0
1.0
1.01.0
1.0
1.0
1.0
1.00.6
0.4
0.6
0.4
0.6newark is in 
2 of 5 h ypotheses
Cpurity = 2/5 = 
0.4</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  18Normalized Acoustic Scores
Theoretically normalization model is:
=r r() (|)()
upxp xupu
In practice normalization is pe rformed w ith an approximate 
model of r()px
Approximation of          using bottom-up clustering:
Similar Gaussian components merged 
Merged model is  ML approximation of m ixture components to be 
merged
Merging continues until d esired size is reached
Normalization model t ypically  has bet ween 50 and 100 mixture 
components in SLS recognizersr()px</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  21
Confidence Classification
Given a confidence feature vector we want to classify the vector
as correct or incorrect
This is a standard t wo class classification problem
Possible approaches:
Linear discriminant projection (Pao, et al, 1998)
Neural net work classifier (Wendemuth, et al,1999)
Mixture Gaussian classifier (Kampp ari &amp; Hazen, 2000)
Support v ector machines (Ma, et al, 2001)</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  33
More Dialogue Modeling Examples
Guiding the user to wards a query that can be handled:
USER: what is the forecast for *reject* *reject*
KEY_VALUES: WEATHER: forecast
UNKNOWN_CITY: *reject* *reject*
RESPONSE: I may not know the city you are as king about.  
Try asking for the state or country. For example: What 
cities do you know about in Massachusetts?</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  34
More Dialogue Modeling Examples
Asking for confirmation:
USER: what is the forecast for *boston*
KEY_VALUES: WEATHER: forecast
CONFIRM_CITY: boston
RESPONSE: Did you say Boston?</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  23Probabilistic Confidence Classifier
MAP-based classifier trained for raw  score:
t) incorrect(P) incorrect|r(p) correct(P) correct|r(plogc 

=
Probabilistic model:
Trained on independent set of de velopment data
Gaussian m odels can be us ed for likelihood densitie s
Priors based on recognizer hy pothesis e rror ra te
Threshold can be varied to adjust balance of false acceptances 
vs. false rejections</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  13Handling Foreground Noises
Build explicit models for differen t noises and non-speech artifacts
Reference: Ward, 1989
WNOISEW1
WN...
N1
NN...CNOISE
One possible approach:
Build acoustic  model net work for 
each noise model
Noise net work contains multiple 
states to model d ynamic noises
Add noise net works to w ord 
network as ne w words
Contr ol noise detection rate w ith 
cost, CNOISE</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  10
Handling Background Noise
Multi-style training
Train w ith data from a variety of nois y environments
Problem: Poor estimates for new or unexpected en vironments
Reference: Lippmann, et al, 1987
Spectral-subtraction
Estim ate static  spectral components during silence
Subtract static  spectral components from d ynamic spectra
Problem: Poor estimates of speec h in regions w ith lo w signal-to-
noise ratio
Reference: Boll, 1979
Sub-band recognition
Run paralle l sub-band recognizers
Sub-band re cognizers  operate on different spectral bands
Weight sub-bands based on their s ignal-to-noise ratio
Problem: Using multiple recogniz ers is computationall y expen sive
Reference: Bourlard and Dupont, 1996</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  31Understanding Results
Experimental understanding results on JUPITER
Original sy stem did not use conf idence scores during hypothesis 
selection in understanding component
Original System
New System/No Rej.
+ optional rejection
+ hard rejectionExperimental
ConditionsSubConcept Error Rates (CER) (%)
Ins Del Total
Sub = Substitution Del =  Deletion Ins = Insertion2.2 19.9 6.3 28.5
2.1 18.1 6.1 26.2
1.3 8.9 8.5 18.7
1.0 7.0 10.6 18.635%
reduction
in CER</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  32
Dialogue Modeling Examples
Aiding the user w ith additional information:
USER: what is the forecast for *reject* park new jersey
KEY_VALUES: WEATHER: forecast
UNKNOWN_CITY: *reject* park
STATE: new jersey
RESPONSE: I may not know the city you are as king about.  
I know the following places in New Jersey: Atlantic City, 
Camden, Newark, Piscataway, Princeton, and Trenton. 
Please select one.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  30Understanding Evaluation
Confidence scoring rejection evaluated using understanding 
concept error rate (CER)
CER computed from s ubstitutions , insertions, and deletions of 
concept ke y-value pairs
Reference Key -Values:
WEATHER: forecast
COUNTRY: bosnia 
DATE: sunda y
Hypothesis Ke y-Values:
WEATHER: forecast
DATE: monda y
CITY: bostonDeletion Error
Substitution Error
Insertion Error</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  12Parallel Model Combination
Given estimates of the mean spectr al values of clean speech and 
noise, do combination:
( )( )=+=+ rr r11
uu n FS()N()FF()F()
r r
PMC u u P( a|u)N(,)
Issues:
Must be able to reverse estimate spectrum from model mean
Must have a reliable estimate of curre nt noise conditions 
Reference: Gales, 1996</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  9Cepstral Mean Normalization (cont)
c[n,m] c[n,m]-c[n] =Cepstral mean normalization is:
Useful w hen filter variation is larger than speaker variation
Reference: Furui, 1981( )M
1
M
m1 s[n,m]f[n]-f[n] s[n,m]
==+ +
M
1
M
m1s[n,m] s[n,m]
==Filter pr operties
are remo ved
Average cepstrum
of speech is
also remo ved</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  6
Difficult Channel and Noise Conditions
Variable sy stem functions
From different channels (e.g ., land line, cellular, etc.)
Different m icrophones
Constant background noise
Channel static
Car e ngine noise
Air conditioning hiss
Intermittent foreground or background noises
Cough
Laugh
Door slam
Handset taps or c licks
Phone ringing
Dog barking</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  7Cepstral Mean Normalization
The channel of a speech recording can be modeled as a linear-
time invariant filter:
y[n] s[n]f[n] =
recorded
speechoriginal
speechchannel
filter
In the frequenc y domain this becomes:
In the log-frequency domain this becomes:
In the cepstral domain this becomes:Y()S()F() =
  c[n] s[n]f[n] =+logY()logS()logF() = +</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  11
Parallel Model Combination
Parallel Model Combination (PMC) for background noise 
compensation
Train speech acoustic models on clean speech
Estim ate noise  model for current c onditions
Combine clean speech  models with estimated noise model
Method assumes mean spectrum of signal can be reverse 
estimated from mean vector of model
Clean speech model for phonetic  unit u:
rr
uu P(s|u)N(,)=r1
u S()F()
rr
nn P(n)N(,)=r1
n N()F()Noise model estimated from non-speec h region of current conditions:</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  22Linear Discriminant Classifier
Discriminative linear projection applied to confidence feature 
vector:
fprtrr=
raw score
 feature vector
projection vector
Projection vector:
Trained on independent de velopment set
Minimum Classification Error (MCE) training
MCE performs gradient descent training on error rate</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  1
Noise Robustness and Confidence ScoringLecture # 20
Session 2003
Lecture r: T. J. Hazen
Handling variability in acoustic conditions
Channel compen sation
Background noise compensation
Foreground noises and non-speech a rtifacts
Computing and apply ing confidence scores
Recognition confidence scoring
Language understanding issues
Dialogue modeling issues</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  3
Motivation
Recognizers make errors
Some reasons for errors:
Presence of pre viously unseen words or e vents
Diffic ult acoustic  conditions or background noises
Presence of highly  confusable words
Insufficient amou nt of training data
Mismatch bet ween training and testing data
Models too rigid to handle variabilit y
Methods to handling erro r-full data 
Adjust  or adapt to current conditions 
Identify  when errors occur and perform action to reco ver</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  35
References
S. Boll, Suppression of acoustic  noise in speech using spectral
subtraction, IEEE Trans. on Acoustic, Speech, and Signal Processing , 
April 1979.
H. Bourlard and S. Dupont, A  new ASR approach based on 
indep endent processing and recombinat ion of partial frequenc y bands, 
ICSLP, 1996.
L. Chase, Word and acoustic confidence annotation for large 
vocabular y speech recognition, Eurospeech, 1997.
S. Furui, Cepstral analy sis techniques for automatic speaker 
verifi cation, IEEE Trans. on Acoustic, Speech, and Signal Processing , 
April 1981.
M. Ga les and S. Young, Robust co ntinuous speech re cognition using 
parallel model combination, IEEE Trans. on Acoustic, Speech, and 
Signal Processing , September 1996.
T. Hazen, L. Hetherington, and A. Park , FST-based re cognit ion 
techniques for multi-lingual and mu lti-domain spontaneous speech, 
Eurospeech , 2001.
T. Hazen, J. Polifroni and S. Sene ff, Recognition confidence scoring for 
use in speech understanding s ystems, Computer Speech  and 
Language , January , 2002.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.345 Automatic Speech Recognition Noise Robustness and Co nfidence Scoring  17Normalized Acoustic Scores
The a posteriori probability expression is:
=rr
r(| )(| ) ()
()px upux pu
px
normalized acoustic likelihood score
In probabilistic framew ork          is usuall y ignoredr()px
Recognition is unaffected by normalization
normalization model is  independent of phone identit y
normalized scores can be viewed as confidence scores</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>14
15</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture14/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>39</slideno>
          <text>Depth First Search
time
states
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 40</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Decodi ng to classify bet ween Odd and Even
 Approximate to tal prob ability  of all paths with pr obability  of best path 
 Computations can be done in the log domain. Only additions and co mparisons ar e required 
 Cheaper than full-forward where m ultiplic ations and additions are require d 
Score(X|Odd)
P(Odd)
Score(X|Even ) 
P(Even ) 
6.345 Automatic Speech Recognition Designing HMM-based speech recognition systems 13</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Breadth First Search
time
states
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 39</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Statistical Classifi cation of Isolat ed Words
 Classes are words 
 Data are instances of isolated  spoken words 
	Sequence of fea ture vec tors d erived f rom spee ch signa l, typically 1 ve ctor from a 
25ms frame of spe ech, wi th fra mes shifted by 10 ms. 
 Bayesian classification: 
Recogn ized_ Word = argmaxword P(word )P(X| word ) 
 P(word ) is a priori probab ility of word 
 Obtained f rom our expe ctation of th e relative frequenc y of occu rrence  of the word 
 P(X|word ) is the probability of X computed on  the probab ility distribution  function  of word 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 4</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Computing Best Path Scores for Viterbi Decoding
 The forward  recursion is termed Viterbi decod ing 
 The terminology is derived fr om deco ding of err or correction codes 
	The scor e for the word is the scor e of the path  that wins through  to the 
terminal state 
 We use the term score to distinguish it fr om the total p robability of the wo rd 
Score ( X | word ) = Psterminal (T ) 
s3 
s2 
s1 stermina l 
Time  =1 2 3 4 5 6 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 16</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Computing P( X|word )
 The forward recu rsion
 (s3,4| word )
) |5,( 3 word s
   
  
  
stermina l ) |4,( 2 word s

s3
s2
s1
Time  =1 2 3 4 5 6
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 8</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Computing Best Path Scores for Viterbi Decoding
 The forwa rd recursi on
Ps3 (5) = max{ Ps3(4)P(s3| s3)P( X 5| s3), Ps2(4)P(s3| s2)P( X 5| s3)} 
)4(
3s P 
stermina l )4(
2s P  
s3
s2
s1
Time  =1 2 3 4 5 6
6.345 Automatic Speech Recognition Designing HMM-based speech recognition systems 15</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Decodi ng word sequences</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Decodi ng word sequences
SET 2 and its best path
Rock Star Dog Star 
dogstar2 The best path throug h 
Dog Star lies w ithin the 
dotted portio ns of the trellis 
There are four transition 
points from Do g to Star in 
this trellis 
There are four different sets 
paths throug h the dotted 
trellis, each w ith its own 
best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 26</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star 
Similar ly, for Rock Star
the best path throug h 
the trellis is the  best of 
the four transiti on-spec ific 
best paths 
max(ro ckstar) = 
max ( rockstar1, rocks tar2, 
rockstar3, rocks tar4 ) 
6.345 Automatic Speech Recognition (M IT, 2003) Designing HM M-based speech recognition sy stems 30</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Language HMMs for  arbitrarily long word sequences
	Constrained set of word sequences with constrained 
vocabu lary are realistic 
 Typically i n comm and-and-con trol situations 
 Example: operating TV r emote 
 Simple dialog system s 
 When the se t of perm itted respon ses to a  quer y is restrict ed 
 Unconstrained length word sequences : NATURAL LANGUAGE 
 State-of-art large voca bulary deco ders 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 53</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate bigram represen tations 
bigram loop
bigram initialization Wa 
Wb 
Wc 
Wd termination 
P(Wd|Wa) P(Wc|Wa) P(Wb|Wa) P(Wa|Wa) 
P(Wa|START) 
P(END|Wa) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 59</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Decodi ng word sequences
Rock Dog Star Similar l ogic ca n be app lied 
at other entry points to Star 
This copy of the trellis 
for STAR is completely 
removed 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 35</text>
        </slide>
        <slide>
          <slideno>62</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
Designing HM M-based speech recognition sy stems 63 6.345 Automatic Speech Recognition the 
START rock 
star the 
rock 
star 
the 
rock 
star 
the 
rock 
star This always re presents a part ial 
seque nce en ding with rock star  Expla nation wi th example: t hree words voca b the, rock, star 
 The gr aph in itially  begins with  bigrams of START, sin ce nothing 
precedes  START 
 Each  word in  the second level r epresents a specif ic set of two term inal 
words in a partial word sequen ce 
P(star | star rock) P(sta r | rock rock) P(sta r | the  rock) Each word is an HMM</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star The best path throug h 
Dog Star is the best of 
the four transiti on-spec ific 
best paths
max(do gstar) = 
max ( dogs tar1, dogst ar2, 
dogstar3, dogstar 4 )
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 29</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Language HMMs for  arbitrarily long word sequences
Each word is an HMM X 
	Arbitrar y word s equences  can be 
modeled with  loops under some 
assum ptions. E.g .: 
 A bang can  be followed  by another 
bang with pro bability P(bang). X 
 P(bang) = X; P(Termination) = 1-X; 
	Bangs can  occur  only in p airs with 
probability  X 
 A more complex  graph allows m ore X 
complicat ed patterns 
 You can extend  this logi c to other 
vocabular ies wh ere the spe aker says 
more things th an bang 1-Y 
 e.g. bang bang y oure dead  1-X 
 1-X 
 
 1-X 
 Y 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 52</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star We select the high er scorin g 
of the two inco ming ed ges 
here 
t1 
This portion of the 
trellis is no w deleted 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 34</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Language HMMs for  fixed-length word sequences
Rock Dog Star  
 

Rock Dog Star 
Rock Dog 
Star= 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 44</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Const ructing HM Ms for word sequences
 Conacate nate HMMs for eac h of t he words in t he sequence 
HMM for word 1 HMM for word2 
Combi ned HM M for the sequence wor d 1 word 2 
 In fact, w ord HMMs t hemselves are frequ ently con structed by 
concaten ating  HMMs fo r phon emes 
	Phonemes are far fewer in numb er than words, and occur  more fr equently 
in training  data 
	Words that were never  seen  in the training  data can be constructed from 
phoneme HMMs 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 20</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Decodi ng word sequences
SET 1 and its best path
Rock Star Dog Star 
dogstar1 The best path throug h 
Dog Star lies w ithin the 
dotted portio ns of the trellis 
There are four transition 
points from Do g to Star in 
this trellis 
There are four different sets 
paths throug h the dotted 
trellis, each w ith its own 
best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 25</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Languag e HMMs for fixed-leng th word sequen ces: based on  a 
gramm ar for Dr. Seuss 
freez y breeze 
made 
these trees 
freeze three trees 
trees cheese Each word is an HMM 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 49</text>
        </slide>
        <slide>
          <slideno>66</slideno>
          <text>Languag e HMMs for Na tural language: Generic N-gram 
represen tations 
 The log ic can be ex tended 
 A trigram  decoding s tructure for a vo cabu lary of D words 
needs D word instances at the first lev el and D2 word 
instances at the second level 
 Total of D(D +1) word models must be instantiated 
 Othe r, more expensi ve struct ures are  also possible 
 An N-gram  decod ing structu re will n eed 
 D + D2 +D3 DN-1 word instances 
	Arcs must be incorporated such that the exit from a word instance 
in the (N-1)th level always represents a word sequence with the 
same trailing sequence of N-1 words 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 67</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Languag e HMMs for Na tural language: b igram representatio ns
 A unigram model is only usefu l when no statistical  dependency 
between adjacent words can be assumed 
 Or, al terna tely, when the  data used to  learn these dependenc ies are too 
small to learn  them reliably 
 Learning word dep endencies : Later in the p rogram 
	In natural  language, the prob ability of a w ord occurri ng depends on 
past words. 
	Bigram langu age model: the probability of a w ord depends on  the 
previous wo rd 
 P(Star | A Rock) = P(Star | The Rock) = P(Star | Rock) 
 P(Star | Ro ck) is n ot requ ired to be equal to P(Star | Do g) 
 In fact th e two terms are assumed to b e unrelated. 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 56</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Decodi ng isolated words</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Bayesian Classification bet ween word sequences
 Classifying an utterance as either Rock Star or Dog Star 
 Must c ompare P(Roc k,Star)P(X|Rock S tar) with P(Dog,Star)P(X|Dog Star) Rock P(Star|Rock ) P(R k,Star)P(X|RockStar)Star oc 
Dog Star P(Star|Dog ) P(Dog,Star)P(X|DogStar) 
P(Rock ) P(Dog) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 21</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Languag e HMMs for Na tural language: exam ple of a graph which 
uses unigram represen tations Each word is an HMM  Vocabulary: fox, sox, knox, box 
	Arbitrary length sequ ences of arb itrary arrang ements of 
these four words. 
 Actual probabilities: 
 P(fox), P(sox), P(kn ox), P(box), P(END) 
 P(fox) + P(sox) + P(kno x) + P(b ox) + P(END ) = 1.0 
P(END)
begin end Fox 
Sox 
Knox 
Box P(Fox) 
P(Sox) 
P(Knox) 
P(Box) P(END) 
The black dots are non- emitti ng states that are not associ ated with observ ations
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 55</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Designing opti mal graph st ructures for the 
language HMM</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Computing P( X|word )
 To compute P(X|word ), there must be a stati stical di stributi on for X 
correspon ding to word 
 Each  word must be repr esented  by some statistical model. 
 We rep resent each  word b y an H MM 
	An HMM is really  a graphical fo rm of probability density functio n for 
time-var ying data 
non-em itting a bsorbi ng 
state 
 Each  state has  a probability  distribution fun ction 
 Transitions  betw een states are go verned b y transition probab ilities 
	At ea ch time instant the m odel is in som e state, and it emits on e observation 
vector  from the distribution  asso ciated with th at state 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 5</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Classifying between two words: Odd and Even
HMM for Odd HMM for Even 
P(Odd)P(X|Odd) P(Even )P(X|Even ) 
P(Odd) P(Even ) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 11</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Table of contents
 Isolated word  reco gnition 
 Bayes ian Class ification 
	Decoding based on best state 
sequences 
 Word  sequ ence recog nition 
 Bayes ian Class ification 
	Decoding based on best state sequences 
	Best path de coding and collaps ing 
graphs 
 Search str ategies 
 Depth first 
 Breadth f irst  Optimal graph  structures 
 Constr ained language decoding 
 Natural language decoding 
 HMMs  for the language 
 Unigram 
 Bigram 
 trigram 
6.345 Automatic Speech Recognition    Designing HMM-based speech recognition systems 2</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Decodi ng to classify bet ween Odd and Even
 Compare scores (b est state seq uence pro babilities) of all  com peting words
Score(X|Odd)
P(Odd)
Score(X|Even ) 
P(Even ) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 17</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Statistical Classifi cation of Word Sequences
 Classes a re word se quence s
 Data are spoken rec ordings of word sequences
 Bayesian cla ssificati on: 
word1, word2,..., wordN = 
argmaxwd1,wd2,...,wdN {P( X | wd1, wd2,..., wdN )P(wd1, wd2,..., wdN )} 
	P(wd1,wd2,wd3..) is a priori pro bability of w ord sequ ence 
wd1,wd2,wd3.. 
 Obtained  from a model of th e language 
	P(X| wd1,wd2,wd3..) is the pro bability of X computed on the probability 
distribution function of th e wo rd sequence wd1,wd2,wd3.. 
 HMMs now represent prob ability distribu tions of word sequences 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 19</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star Then we d com pare the 
best paths thro ugh 
Dog Star and Rock Star 
max(do gstar) = 
max ( dogs tar1, dogst ar2, 
dogstar3, dogstar 4 ) 
max(ro ckstar) = 
max ( rockstar1, rocks tar2, 
rockstar3, rocks tar4 ) 
Viterbi= 
max(max(dogstar), 
max(rockstar) ) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 31</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Extending paths through the trellis:
Breadth First vs. Depth First Search</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Depth First Search
time
states
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 41</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Languag e HMMs for Na tural language: with un igram 
represen tations 
 A ba g of words model: 
 Any word is pos sible after any word. 
 The prob ability of a word  is ind ependent  of th e words preced ing o r succe eding it. 
 Also called a UNIGRAM model . 
P(Star) = P(Star | Dog) = P (Star | Rock) = P(Star | When you w ish upon a )
P(When you wis h upon a  star) = 
P(When ) P(you) P(wish ) P(upon) P(a) P(star ) P(END)
 END is a special sym bol that indicates the end of the word sequence 
 P(END) is necessary  w ithout it the word seq uence would never terminate 
 The total probab ility of all possib le word sequ ences m ust sum  to 1.0 
 Only  if P(END) is explicitly  defined will th e total probability = 1.0 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 54</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Languag e HMMs for Na tural language: trig ram representatio ns
	Assum ption: The probability of a word depends on t he two precedi ng 
words 
P(waltzing  | youll come a) = P(waltzing  | wholl com e a) = 
P(waltzing  | come a) 
P(youll come a waltzing matilda with me)  = 
P(youll | START) * P(come | START youll) * 
P(a | youll com e) * P(waltzing  | come a) * 
P(matilda | a waltzing) * P(with | waltzing matilda) * 
P(me | matilda with) * P(END | with me) 
 For any word sequence w1,w2,w3,w4,w5.wN 
 P(w1,w2wN) = P(w1|START) P(w2 | START w1) P(w3 | w1 w2)  
P(END | wN-1 wN) 
	Note th at the FIRST term  P(w1 | START) is a bigram term. All th e rest 
are trigram s. 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 60</text>
        </slide>
        <slide>
          <slideno>67</slideno>
          <text>Next  Class
	How the use of sub-word units com plicates the structu re 
of language HMMs in decoders 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 68</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Decodi ng word sequences
 The two instan ces of Star can be col lapsed into one to  form  a sm aller trellis 
o	This is possible beca use we are decoding based on best pa th score, instead of full f orward 
score Rock Dog Star We can only do Viterbi 
decod ing o n this collaps ed 
graph. 
Full forward d ecodin g is no 
longer possi ble 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 36</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Part I: Designing HMM-based AS R systems
Rita Singh 
Scho ol of Computer Scien ce 
Carn egie Mell on Un iversity</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Statistical Pattern Classification
	Given dat a X, find which of a num ber of classes C1, C2,CN it 
belongs to, based on known distributions of data from C1, C2, etc. 
 Bayesian Classificat ion: 
Class = Ci : i = argmaxj P(Cj)P(X|Cj) 
a priori probability of Cj Probab ility of X as given by 
the proba bility distributi on of Cj 
 The a pri ori probability accounts for the relative proportions of t he 
classes 
	If you nev er saw  any data, you would guess th e class based on  these 
probabilities  alone 
 P(X|Cj) accounts for evidence obtained from observed data X 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 3</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Breadth First Search
time
states
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 38</text>
        </slide>
        <slide>
          <slideno>64</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
 Expla nation wi th example: t hree words voca b the, rock, star 
	The gr aph in itially  begins with  bigrams of START, sin ce nothing 
precedes  START 
 Each  word in  the second level r epresents a specif ic set of two term inal 
6.345 Automatic Speech Recognition  the 
START rock 
star the 
rock 
star 
the 
rock 
star 
the 
rock 
star words in a partial word sequen ce 
This STAR has input from 
the and rock . 
WRONG Edges comi ng out of this wrongly 
connecte d STAR could h ave word 
pair conte xts that are either 
THE S TAR or ROCK S TAR. 
This is amibigu ous. A word cannot 
have inc oming edges from t wo or 
more different words 
Designing HM M-based speech recognition sy stems 65</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Decodi ng word sequences
SET 3 and its best path
Rock Star Dog Star 
dogstar3 The best path throug h 
Dog Star lies w ithin the 
dotted portio ns of the trellis 
There are four transition 
points from Do g to Star in 
this trellis 
There are four different sets 
paths throug h the dotted 
trellis, each w ith its own 
best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 27</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Languag e HMMs for fixed-leng th word sequen ces: comm and and 
control g rammar 
delete file 
all 
files open 
edit 
close marked Each word is an HMM 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 50</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Computing P( X|word )
	The total probability of X, including the contributions of 
all state sequences, is the forward probability at the final 
non-em itting node 
P( X | word ) = (stermin al ,T | word ) 
stermina l 
s3
s2
s1
Time  =1 2 3 4 5 6 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 9</text>
        </slide>
        <slide>
          <slideno>65</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
Wij is an HMM for word 
Wi that can onl y be 
accessed from HMMs for 
word W j. E.g. W12 is the 
HMM for word W1 that can only b e used 
when the pr evious word 
was W2 bigram 
initialization trigram 
loop 
W14 W13  W22 W11 
W21 
W31 
W41 
W12 
 
 W1 
W2 
W3 
W4 termination 
&lt;s&gt; Each word is an HMM 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 66</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Bayesian Classification bet ween word sequences
P(Rock,Star)P(X|RockStar) Rock Star Dog Star 
P(Dog,Star)P(X|DogStar)
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 22</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Simplification of the language HMM through lower context 
language m odels Each word is an HMM  Recognizing one of four li nes from charge of the light brigade 
	If the probabili ty of a word only depends on t he preceding word, the graph 
can be collapsed: 
 e.g. P(them |  cannon to r ight of) = P(them |  canno n to left of) = P(cannon |  of) 
to 
of Cann on them right 
left 
front in 
behind P(cannon) P(to | cannon) P(rig ht | to) 
P(in | cannon) 
P(be hind | cannon) P(of |  right) 
P(of |  left) 
P(them | of) 
P(them|b ehind) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 47</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Computing P( X|word )
 The actu al state sequ ence that g enerated X is never known. 
 P(X|word ) must therefore consider all possi ble state  sequences. 
P( X | word ) =  P( X , s | word ) 
s{allstate sequences } 
 The actu al number of state sequen ces can be v ery large 
 Cannot explicit ly su m over all  state seq uences 
	P(X|word ) can however be efficiently calculated using the 
forward rec ursion 
(s, t | word ) =(s, t 1| word )P(s | s)P( Xt | s) 
s 
     
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 7</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Refacto red recogn ition p roblem  for language H MMs which 
incorpora te with lower c ontex t langu age m odels 
 Our new statem ent of the prob lem with a sm aller langu age m odel requ irement: 
word1,word2, = 
argmaxwd1,wd2{argmaxs1,s2,,sTP(wd1,wd2,)P(X1,X2,..XT,s1,s2,,sT|wd1,wd2,)} 
 The probabilit y term  can be factored i nto individual words as 
P(wd1)P(Xwd1,Swd1|wd1).P(wd2|wd1)P(Xwd2,Swd2|wd2). 
P(wd3|wd1,wd2)P(Xwd3,Swd3 |wd3)P(wdN|wd1...wdN-1)P(XwdN,SwdN|wdN) 
 Assume contexts b eyond a g iven leng th K do not matter 
P(wdN| wdN-1, wdN-2, .., wdN-K, .., wd1) = P(wdN-| wdN-1, wdN-2,.., wdN-K) 
 Nodes with t he sam e word history wdN-1, wdN-2,.., wdN-K can be co llapsed 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 48</text>
        </slide>
        <slide>
          <slideno>61</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
 Expla nation wi th example: t hree words voca b the, rock, star 
 The gr aph in itially  begins with  bigrams of START, sin ce nothing 
precedes  START 
 Trigram s for all START word sequences 
	A new instan ce of every word is required to ensure that  the two preceding 
symbols are START word 
the 
START rock 
star P(the | START the) 
P(rock | START t he) 
P(the | START star) 
P(rock | START star) the 
rock 
star 
the 
rock 
star 
the 
rock 
star END 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 62</text>
        </slide>
        <slide>
          <slideno>63</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
6.345 Automatic Speech Recognition the 
START rock 
star the 
rock 
star 
the 
rock 
star 
the 
rock 
star  Expla nation wi th example: t hree words voca b the, rock, star 
 The gr aph in itially  begins with  bigrams of START, sin ce nothing 
precedes  START 
 Each  word in  the second level r epresents a specif ic set of two term inal 
words in a partial word sequen ce Each word is an HMM 
Any edge com ing out of this 
instance of STAR will have the 
word pair cont ext ROCK STAR 
Designing HM M-based speech recognition sy stems 64</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Language HMMs for  fixed-length word sequences
Each word is an HMM P(Rock ) P(Dog) 
P(Star|Rock ) P(Star|Dog ) 
 The word gr aph represents all allowed word sequ ences in our  example 
 The set of all allowed wor d sequences represents the allowed language 
	At a more detailed level, the figure represents an HMM composed  of the 
HMMs for all w ords in th e word graph 
 This is the L anguage HMM   the HMM for the entire allowed language 
 The languag e HMM represents t he ver tical axis o f the trellis 
 It is the trellis, and NOT  the language HMM , that is searched for  the best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 45</text>
        </slide>
        <slide>
          <slideno>60</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate trigram representations 
the 
START END rock 
star P(END | START) P(the | START the) This is wrong! This woul d app ly the proba bility 
P(the | START the) to instanc es of the the the 
(for which the c orrect proba bility value is 
P(the | the the)  Explanation with example: three word vocabulary the, rock, star 
 The gr aph in itially  begins with  bigrams of START, sin ce nothing 
precedes  START 
 Trigrams of START the.. Each word is an HMM 
P(star | START)
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 61</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Computing P( X|word )
 The actu al state sequ ence that g enerated X is never known. 
 P(X|word ) must therefore consider all possi ble state  sequences. 
P( X | word ) =  P( X , s | word ) 
s{allstate sequences } 
  
  
 
 
  
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 6</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Computing Best Path Scores for Viterbi Decoding
 The approximate score for  a word is 
P( X | word )  maxs{allstate sequences }{P( X , s | word )} 
 Written expli city 
P( X 1..Xt | word )  maxs1,s2
 ,...,sT { (s1)P( X 1| s1)P(s2| s1)P( X 2| s2)....P(sT | sT 1)P( XT | sT )} 
 The word scor e can b e be recursively computed u sing the Viterb i algorithm 
s P (t) = maxs{Ps(t 1)P(s | s)P( Xt | s)} 
    
    
       
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 14</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Languag e HMMs for Na tural language: bu ilding graphs to 
incorporate bigram represen tations Each word is an HMM  Edges from START contain START dependent word probabilities 
 Edges from Even contain Even dependent word prob abilities 
 Edges from Odd contain Odd dependent word probabilities 
P(Odd | Odd) = g 
START END ODD 
EVEN a b c d 
f P(Even | Odd) = h P(END|Odd)=i 
e 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 58</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Language HMMs for  arbitrarily long word sequences
 Previous ex amples cho se bet ween a fin ite set of kn own wo rd sequences 
 Word sequences can be of  arbitrary length 
	E.g. set of all word sequences that consist of an ar bitrar y number o f 
repetitions of  the word bang 
  
  
  
 Forming explicit word- sequence graphs of th e type weve seen  so far is not 
possible 
	The number of possible sequences (with non- zero a-priori probability) is 
potentia lly infinit e 
 Even if the longest  sequence length i s restricted, the graph will s till be large 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 51</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Decodi ng word sequences
Score(X|Dog Star)
Rock Star Dog Star 
Score(X|RockStar)
Approxim ate total proba bility
with best path score
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 23</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Depth First Search
states
time 
? 
 No incons istencies aris e if path scores cha nge mon otonic ally with i ncreasing 
path len gth 
 This can be g uarante ed by n ormaliz ing a ll state output den sity values for a 
vector at a time twith respect to the highest value d dens ity at t. 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 42</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Language HMMs for  fixed-length word sequences
Each word is an HMM  Recognizing one of four li nes from charge of the light brigade 
 
  
 
 
P(of|cannon to right) P(them|cannon t o right  of) 
to 
of Cann on 
them right 
left 
front in 
behind P(cannon) P(to| cannon) P(rig ht|cannon to) 
P(in|cann on) 
P(be hind| cannon) P(of|cannon to left) 
P(them|cannon in front  of) 
them of 
of them them 
P(fron t|cannon in) P(of|cannon in front) P(them|cannon to left of ) P(left| cannon to) 
P(them|can non behind)
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 46</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Classifying between Odd and Even
P(Odd)P(X|Odd)
P(Odd)
P(Even )P(X|Even ) 
P(Even ) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 12</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star The best path throug h 
Dog Star lies w ithin the 
dotted portio ns of the trellis 
There are four transition 
points from Dogto Star in 
this trellis 
There are four different sets 
paths throug h the dotted 
trellis, each w ith its own 
best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 24</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Decodi ng word sequences
t1
Rock Star Dog Star For a given entry point 
the best path through STAR 
is the same for both trellises Wecan choose between 
Dog and Rockright here 
because the futures of these 
paths are identical 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 33</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Decodi ng word sequences
SET 4 and its best path
Rock Star Dog Star 
dogstar4 The best path throug h 
Dog Star lies w ithin the 
dotted portio ns of the trellis 
There are four transition 
points from Do g to Star in 
this trellis 
There are four different sets 
paths throug h the dotted 
trellis, each w ith its own 
best path 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 28</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Decodi ng word sequences
Rock Star Dog Star argmax is commutative:
max(max (dogst ar), max(rocks tar) )
= 
max ( 
max(dogst ar1, rocks tar1 ), 
max (dogst ar2, rock star2 ), 
max (dogst ar3,rocks tar3), 
max(dogstar4,rockstar 4 ) 
) 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 32</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Languag e HMMs for Na tural language: b igram representatio ns
 Simple bigram  exam ple: 
 Vocabular y: START, odd, even, END 
 P(odd | START)  = a, P(even  | START) = b, P(END |  START) = c 
 a+b+c = 1.0 
 P(odd | even) = d, P(even  | even) = e, P(END |  even) = f 
 d+e+f = 1.0 
 P(odd | odd) = g, P(even | odd) = h, P(END |  odd) = i 
 g+h+i = 1.0 
 STA RT is a special sym bol, indicati ng the beg inning of the utteran ce 
 P(word | START ) is the probability that the utterance begins with word 
	Prob(odd even even odd) = 
P(odd | START)  P(even | odd) P(even | even) P (odd | even) P(EN D | odd) 
 Can be shown that the total pro bability of all w ord seq uences of all 
lengths is 1.0 
	Again, the definition of START and END s ymbols, and  all bigrams 
involving  the two, is crucial 
6.345 Automatic Speech Recognition  Designing HMM-based speech recognition systems 57</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>13
&#160;</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture13/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>28</slideno>
          <text>Graphical models for ASR 29 6.345 Automatic Speech Recognition
Training and testing DBNs
Why do we need different structures for training testing?  
Isnt training just the same as testing but w ith more of the 
variables observed?
Not al ways!
Often, during training w e have only  partial inform ation about 
some of the v ariables, e.g. the word sequenc e but not w hich 
frame goes w ith which word</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>A Practical Introduction to Graphical Models
and their use in ASR
6.345</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Graphical models for ASR 6 6.345 Automatic Speech RecognitionIs height independent of hair length?
xxxx
x
xx
xxxxx
xxxx
xxx
xxxxx
xxx
xx
x
xxxxx
x
xxx xxxxx
x xxxx
xxxxxxxx
xx
xx
x
xxxxx
x
xxxxxxxx
xxxxx
xxxxxx
H5 6 7L
shortmidlongx = female
x = male</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Graphical models for ASR 27 6.345 Automatic Speech Recognition
Switching parents
Definition:
A variable X is a s witching parent of variable Y if the value of X 
determines the parents and/or impleme ntation of Y
Example:
B
DA
CA=0 D has parent B with Gaussian distribution
A=1 D has parent C with Gaussian distribution
A=2 D has parent C with mixture Gaussian distribution</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Graphical models for ASR 13 6.345 Automatic Speech Recognition
Bayesian networks
The preceding slides are examples of simple Bay esian netw orks
Definition:
Directed acy clic graph (DAG) with a one-to-one correspondence 
between nodes ( vertice s) and variables X1, X2, ... , XN
Each node Xiwith parents pa(Xi) is associated with the local 
probabilit y function pXi|pa(Xi)
The joint probabilit y of all of the v ariables is gi ven by the product 
of the local probabilities, i.e. p(xi, ... , xN) = p(xi|pa(xi))
B
DA
C p(a)p(b|a)
p(c|b)
p(d|b,c)p(a,b,c,d)= 
p(a)p(b|a) p(c|b) p(d|b,c)
A given BN represents a family of probability distributions</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Graphical models for ASR 22 6.345 Automatic Speech Recognition
Dynamic Bayesian networks (DBNs)
BNs consisting of a structure that repeats an indefinite (or 
dynamic) number of times
Useful for modelin g time series (e.g. speech)
DA
CB
DA
CB
DA
CBframe i frame i+1 frame i-1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Graphical models for ASR 8 6.345 Automatic Speech Recognition
Is the future independent of the past 
(in a Markov process)?
Generally , no
If present state is kno wn, then yes
Qi Qi-1 Qi+1
)| ( ), | () ( ) | (
1 1 11 1 1
i i i i ii i i
qqp qqqpqp qqp
+ ++ +
=
i i i Q Q Q |1 1 +1 1 +i i Q Q</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Graphical models for ASR 34 6.345 Automatic Speech Recognition
GMTK:  Graphical Modeling Toolkit 
(J. Bilmes and G. Zweig, ICASSP 2002)
Toolkit for specifying and computing with dynamic 
Bayesian net works
Models are specified via:
Structure file: defines variables, dependencies, and form of 
associated conditional distributions
Parameter files: specif y parameters for each distribution in structure 
file
Variable distributions can be
Mixture Gaussians + variants
Multidimensional pr obabilit y tables
Sparse probabilit y tables
Deterministic (decision trees)
Provides programs for EM training, Viterbi decoding, and 
various utilities</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Graphical models for ASR 33 6.345 Automatic Speech RecognitionA feature-based DBN for ASR
frame i+1 frame  i
phone 
state
A1 A2 AN ...
Ophone 
state
A1 A2 AN ...
Op(o|a1, ... , aN)</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Graphical models for ASR 23 6.345 Automatic Speech RecognitionDBN representation of n-gram language models
Bigram:
Wi+1 Wi Wi-1. . .                                                       . . .
Trigram:
Wi+1 Wi Wi-1 . . .                                                       . . .</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Graphical models for ASR 19 6.345 Automatic Speech Recognition
Inference
Definition:
Computation of the probabilit y of one subset of the variables 
given another subset
Inference is a subroutine of:
Viterbi decoding
q* = argmaxqp(q|obs)
Maximum-lik elihood e stimation of the par ameters of the local 
probabilities
* = argmax p(obs| )</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Graphical models for ASR 24 6.345 Automatic Speech RecognitionRepresenting an HMM as a DBN
12 3
obs obs obs
=  state
=  allow ed tran sition=  variable
=  allowed dependencyHMM DBN
.3.7 .8
.21
1     2      3qiqi-1
1    .7    .3     0
2     0    .8    .2
3    0     0     1P(qi|qi-1)
q=1
q=2
q=3P(obsi| qi)frame i
Qi
obsi. . . . . .Qi-1
obsi-1Qi+1
obsi+1frame i+1 frame i-1</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Graphical models for ASR 17 6.345 Automatic Speech Recognition
Bayes-ball, contd
Boundary conditions:</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Graphical models for ASR 7 6.345 Automatic Speech Recognition
Is height independent of hair length?
Generally , no
If gender know n, yes
This is the common cause scenario
gender
hair 
lengthheightG
H L
)|( ),|()( )|(
ghp glhphplhp
=
GL H |L H</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Graphical models for ASR 2 6.345 Automatic Speech Recognition
Graphical models for ASR
HMMs (and most other common ASR models) have some 
drawbacks
Strong independence  assumptions
Single state variable per time frame
May want to model more complex structure
Multiple processes (aud io + video, speech + n oise, mult iple 
streams of acou stic featur es, articulator y features)
Dependen cies b etween these processes or bet ween acoustic 
observ ations
Graphical models provide:
General algorithms for large class of models
No need to write new code for each new model
A language with which to talk about statistical models</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Graphical models for ASR 25 6.345 Automatic Speech Recognition
Casting HMM-based ASR as a GM problem
Qi
obsi. . . . . .Qi-1
obsi-1Qi+1
obsi+1
Viterbi decoding finding the most probable settings for all  
qigiven the acoustic observations  {obsi}
Baum-Welch training finding the most likely  settings for 
the parameters of P(qi|qi-1)and  P(obsi| qi)
Both are special cases of the standard GM algorithms for 
Viterbi and EM training</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Graphical models for ASR 36 6.345 Automatic Speech Recognition
Some issues...
For some structures, exact inference ma y be 
computationall y infeasible approximate inference 
algorithms
Structure is not al ways kno wn structure learning 
algorithms</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Graphical models for ASR 12 6.345 Automatic Speech RecognitionMore explaining away ...
C4 C2
L
leakC3 C5 C1pipes faucet caulking drain upstairs
jiLC Cj i , |   )| ( ), | ()( ) | (
lcplccpcp ccp
i j ii j i
= ji C Cj i ,</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Graphical models for ASR 30 6.345 Automatic Speech Recognition
More complex GM models for recognition
HMM + auxiliary variables (Zweig 
1998, Stephenson 2001)
Noise clust ering
Speaker clustering
Dependence on pitch, speaking rate, 
etc.Q
obsQ
obsaux
Q
obsa1a2 aN . . .Articulatory/feature-based modeling
Multi-rate modeling, audio-visual speech recognition (Nefian et al. 
2002)</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Graphical models for ASR 20 6.345 Automatic Speech Recognition
Graphical models (GMs)
In general, GMs represent famili es of probabilit y distributions 
via graphs
directed, e.g. Ba yesian net works
undirected, e.g. M arkov random fie lds
combination, e .g. chain graphs
To describe a particular distribution w ith a GM, we n eed to 
specify:
Semantics :  Ba yesian net work, Ma rkov random fie ld, ...
Structure :  the graph itself
Implementation :  the form of the local f unctions (Gaussian, table, ...)
Parameters of local functions (means, covariances, table entries...)
Not all types of GMs can repr esent all sets of independence 
properties!</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Graphical models for ASR 15 6.345 Automatic Speech RecognitionMedical example
lung 
cancersmoker genesparent 
smokerprofession
Things we may want to know:
What independence assumptions does 
this model encode?
What is p(lung cancer | profession ) ?  
p(smoker | parent smoker, genes ) ?
Given some of the v ariables, what are 
the most likely values of others?
How do we es timate the loca l 
probabilities from data?</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Graphical models for ASR 18 6.345 Automatic Speech RecognitionBayes-ball in medical example
lung 
cancersmoker genesparent 
smokerprofession
According to this model:
Are a persons genes independent of whether the y have a parent 
who smokes?  What about if we kno w the person has lung 
cancer?
Is lung c ancer indepe ndent of profe ssion giv en that the pe rson is 
a smoker?
(Do th e answ ers make sense?)</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Graphical models for ASR 28 6.345 Automatic Speech RecognitionHMM-based recognition with a DBN
word
word transition
word  position
state transition
phone state
observ ationvariable name
end of utterance
frame  0 frame  i last frame
What language model does this GM implement?</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Graphical models for ASR 21 6.345 Automatic Speech Recognition
Example of undirected graphical models:
Markov random fields
Definition:
Undirected graph
Local function (potential) defined on each maximal clique
Joint probabilit y given by normalized product of potentials
Independence properties can be deduced via simple graph 
separation
B
DA
C ),,( ),( ),,,(,, , dcb ba dcbapDCB BA</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Graphical models for ASR 37 6.345 Automatic Speech Recognition
References
J. Bilmes, Graphical Models and Automatic Speech 
Recognition, in Mathematical Foundations of Speech and 
Language Processing , Institute of Mathematical Analysis 
Volumes in Mathematics Series, Springer-Verlag, 2003. 
G. Zw eig, Speech Recognition with Dy namic Ba yesian 
Networks, Ph.D. dissertation, UC Berkeley , 1998.
J. Bilmes, What HMMs Can Do, UWEETR-2002-0003, Feb. 
2002.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Graphical models for ASR 3 6.345 Automatic Speech Recognition
Outline
First half  intro to GMs
Independence &amp; condit ional independence
Bayesian net works (BNs)
*Definition
*Main problems
Graphica l models in general
Second half  dynamic Bay esian netw orks (DBNs) for speech 
recognition
Dynamic Bayesian net works --HMMs and be yond
Implem entation of ASR de coding/training us ing DB Ns
More complex DBNs for recog nition
GMTK</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Graphical models for ASR 10 6.345 Automatic Speech Recognition
Are alien abductions independent of 
daylight  savings time?
Generally , yes
If Jim doesnt show  up for lecture, no
Again, explaining-aw ay effect
Jim 
absentA D
Jalien 
abductionDST
)|( ),|()( )|(
japjdapap dap
= A D
JA D |</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Graphical models for ASR 26 6.345 Automatic Speech Recognition
Variations
Input-output HMMs
Qi. . . . . . Qi-1 Qi+1Xi-1 Xi Xi+1
Xi-1 Xi Xi+1
Factorial HMMs
Qi. . . . . . Qi-1 Qi+1
Xi-1 Xi Xi+1RiRi-1 Ri+1</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Graphical models for ASR 31 6.345 Automatic Speech RecognitionModeling inter-observation dependencie s:
Buried Markov models (Bilmes 1999)
First note that observation variable is actuall y a vector of 
acoustic observations (e.g. MFCCs)
obsQi+1 Qi Qi-1
Consider adding dependencies between observations
Add only those that are discriminat ive with respect to classifying 
the current state/phone/word</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Graphical models for ASR 5 6.345 Automatic Speech Recognition
(Statistical) conditional independenc e
XY Z Definition:  Given the random variables      ,    ,  and     ,
)|( ),|( zxpzyxp= ZYX | 
cc
)|()|( )|,( zypzxpzyxp= )|( ),|( zypzxyp=</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Graphical models for ASR 9 6.345 Automatic Speech Recognition
Are burglaries independent of earthqu akes?
Generally , yes
If alarm state kno wn, no
Explaining-a way effect:  the earthquake explains a way the 
burglary
alarmearthquake burglary
AB E
)|( ),|()( )|(
abp aebpbpebp
= BE
ABE |</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Graphical models for ASR 14 6.345 Automatic Speech Recognition
Bayesian networks, cont d
Missing edges in the graph correspond to independence 
assumptions
Joint probability  can al ways be factored according to the 
chain rule:
p(a,b,c,d) = p(a) p(b|a) p(c|a,b) p(d|a,b,c)
But by  making some independence assumptions, we get a
sparse factorization, i.e. one w ith fe wer parameters
p(a,b,c,d) = p(a) p(b|a) p(c|b) p(d|b,c)</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Graphical models for ASR 32 6.345 Automatic Speech RecognitionFeature-based modeling
Phone-based view:Brain :
Give me a []!
Lips, tongue, velum, glottis :
Right on it, sir!Lips, tongue, velum, glottis :
Right on it, sir!Lips, tongue, velum, glottis :
Right on it, sir!Lips, tongue, velum, glottis :
Right on it, sir!
(Articulatory) feature-based 
view:
Tongue :
Ummyeah, OK.Lips:
Huh?Brain :
Give me a [ ]!
Velum, glottis :
Right on it, sir !Velum, glottis :
Right on it, sir !</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Graphical models for ASR 11 6.345 Automatic Speech Recognition
Is tongue height independent of lip rounding?
Generally , yes
If F1is kno wn, no
Yet again, explaining-a way effect...
tongue 
heightlip 
roundingR H
F1
) |( ),|()( )|(
1 1 fhp frhphprhp
= R H
1|FR H</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Graphical models for ASR 4 6.345 Automatic Speech Recognition
(Statistical) independence
X Y Definition:  Given the random variables       and     ,
)( )|( xp yxp=YX
cc
)()( ),( ypxp yxp= )( )|( yp xyp=</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Graphical models for ASR 16 6.345 Automatic Speech Recognition
Determining independencies from a graph
There are several ways...
Bayes-ball algorithm (Bayes-Ball:  The Rational Pastime, 
Schachter 1998)
Ball bouncing around graph according to a set of rules
Two nodes are independent gi ven a set of obser ved nodes if a 
ball cant get from one to the other</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Graphical models for ASR 35 6.345 Automatic Speech Recognition
Example portion of structure file
variable : phone {
type: discrete hidden cardinality NUM_PHONES;
switchingparents: nil;
conditionalparents: word(0 ), wordPosition(0) using
DeterministicCPT("wordWordPos2Phone");
}
variable : obs {
type: continuous obse rved OBSERVATION_RANGE;
switchingparents: nil;
conditionalparents: phone( 0) using mixGaussian 
collection(global) mappi ng("phone2MixtureMapping");
}</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>22
23</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture22/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>27</slideno>
          <text>Conversational Systems   28
6.345 Automatic Speech Recognition (2003)
okay the next uh uh (im going to need) a (from 
denver) (about two oclock) and (go to atlanta)okay the next uh uh (im going to need) a (from 
denver) (about two oclock) and (go to atlanta)ExampleCMUs Approach
Grammar consists of ~70 autonomous semantic concepts 
(e.g., Depart Location )
Each concept is realized as a set of possible word class 
sequences, e.g., 
Depart Location =&gt; [FROM] [LOC]
which are specified through recursive transition networks (RTNs)
Semantic frame is a flat structure of key-value pairs as 
defined by the concepts
Syntactic structure is ignored
Recognizer only produces a single theory
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Conversational Systems   45
6.345 Automatic Speech Recognition (2003)
0 1 02 03 04 05 06 07 0Entire SetIn Domain (ID)Male (ID)Female (ID)Child (ID)Non-native (ID)Out of DomainExpert
% Error RateSentence
Word
Male ERs are better than females (1.5x) and children (2x)
Strong foreign accents and out-of-domain queries are hard
Experienced users are 5x better than novices
Understanding error rate is consistently lower than SERASR Error Analysis (Weather Domain)
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Conversational Systems   29
6.345 Automatic Speech Recognition (2003)
MITs Approach
TINA was designed for speech understanding
Grammar rules intermix syntax and semantics  
Probabilities are trained from user utterances
Parse tree is converted to a sema ntic frame that encapsulates the 
meaning
TINA enhances its coverage through a robust parsing 
strategy
Sentences that fail to parse are subjected to a fragment parse 
strategy
Fragments are combined in to a full semantic frame
When all things fail, resort to word spotting
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Conversational Systems   37
6.345 Automatic Speech Recognition (2003)
Typical Discourse Phenomena 
in Conversational Systems
Deictic (verbal pointing) and anaphoric (e.g., pronominal) 
reference:
1. Show me the restaurants in Cambridge.
2. What is the phone number of the third one ?
3. How do I get there from the nearest subway stop?
Ellipsis:
1. When does flight twenty two arrive in Dallas?
2. What is the departure time ()?
Fragments:
1. What is the weather today in Denver?
2. How about Salt Lake City?
Introduction || NL (Discourse) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Conversational Systems   7
6.345 Automatic Speech Recognition (2003)
Dialogue Management Strategies
Directed dialogues can be implemented as a directed graph 
between dialogue states
Connections between states are predefined
User is guided through the graph by the machine
Directed dialogues have been successfully deployed commercially
Mixed-initiative dialogues are possible when state transitions 
determined dynamically
Transitions can be determined, e.g., by E-form variable values
User has flexibility to specify constraints in any order
System can back off to a directed dialogue if desired
Mixed-initiative dialogues mainly research prototypes
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Conversational Systems   19
6.345 Automatic Speech Recognition (2003)
Spoken Language Understanding  
Spoken input differs significantly from text
False starts
Filled pauses
Agrammatical constructs
Recognition errors
We need to design natural language components that can 
both constrain the recognizer's search space and respond 
appropriately even when the input speech is not fully 
understood
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Conversational Systems   21
6.345 Automatic Speech Recognition (2003)
The U.S. DARPA-SLS Program (1990-1995)
The Community adopted a common task (Air Travel 
Information Service, or ATIS) to spur technology 
development
Users could verbally query a static database for air travel 
information
11 cities in North America (ATIS-2)
Expanded to 46 cities in 1993 (ATIS-3)
Mostly flights and fares
All systems could handle continuous speech from unknown 
speakers (~2,000 word vocabulary)
Infrastructure for technology development and evaluation 
was developed
Five annual common evaluations took place
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Conversational Systems   44
6.345 Automatic Speech Recognition (2003)
Data vs.Performance (Weather Domain)
Longitudinal evaluations show improvements
Collecting real data improves performance:
Enables increased complexity and improved robustness for 
acoustic and language models
Better match than laboratory recording conditions
Users come in all kinds051015202530354045
Apr May Jun Jul Aug Nov Apr Nov MayError Rate (%)
110100
Training Data (x1000)Word
Data
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Conversational Systems   20
6.345 Automatic Speech Recognition (2003)
ESPRIT
SPEECHSome Speech-Related Government Programs
DARPA SC ARPA SURBBN, CMU, Lincoln 
SDC, SRI, ...  
HWIM, Harpy, Hearsay
DARPA SLSATT, BBN, CMU, CRIM, 
MIT, SRI, Unisys, ...ATIS, Banking, DART, 
OM, VOYAGER, ...
ESPRIT
SUNDIAL
CNET, CSELT, 
DaimlerBenz, LogicaAir and Train TravelLE3
ARISE
CSELT, IRIT, KPN, 
LIMSI, U. Nijmegen..Train Travel1970 1990 1980 2000DARPA WSJ/BN D.C.ATT, BBN, CMU, CU, IBM, 
MIT, MITRE, SpeechWorks,SRI, +Affiliates, ...
Complex Travel
ESPRIT
MASK
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Conversational Systems   33
6.345 Automatic Speech Recognition (2003)
Tighter SR/NL Integration
Natural language analysis can provide long distance 
constraints that n-grams cannot
Examples:
What is the flight serves dinner?
What meals does flight two serve dinner?
Question: How can we design systems that will take 
advantage of such constraints?
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Conversational Systems   50
6.345 Automatic Speech Recognition (2003)
Some Dialogue Research Issues
Modeling human-human conversations?
Are human-human dialogues a good model for systems?
If so, how do we structure our systems to enable the same kinds 
of interaction found in human-human conversations?
Implementation strategies:
Directed vs.mixed-initiative with back-off (e.g., Lamel et al.)
Machine-learning of dialogue strategies (e.g., Levin et al.)
Handling user dialogue phenomena
Interruptions (via barge-in), anaphora, ellipsis
Barge-in can increase complexity of discourse
Modeling agent dialogue phenomena
Back-channel (e.g., N. Ward)
Other issues:
Detecting and recovering from errors (e.g., Walker et al.)
Matching capabilities with expectations
Introduction || NL || De velopment || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Conversational Systems   12
6.345 Automatic Speech Recognition (2003)
Typical Steps in Transforming User Query
Parsing 
Establishes syntactic organization 
and semantic content 
Translation to a Semantic Frame
Produces meaning representation 
identifying relevant constituents and their relationships
Incorporation of discourse context
Deals with fragments, pronominal 
references, etc.
Translation to a database query
Produces SQL formatted string for 
database retrievalGenerate 
Frame
Incorporate 
Context
Produce     
DB QueryProduce 
Parse TreeRecognizer Hypotheses
Parse Tree
Semantic Frame
Frame in Context
SQL
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Conversational Systems   49
6.345 Automatic Speech Recognition (2003)
Language Understanding Research Issues
Variety of methods explored to achieve robust understanding
Full grammars with back-off to  robust parse (e.g, Seneff)
Semantic grammars, template-b ased approaches (e.g., Ward)
Stochastic speech-to-meaning mode ls (e.g., Miller, Levin et al.)
Ongoing work in automatic grammar acquisition 
(e.g., Roukos et al., Kuhn et al.)
Interface mechanisms
Two-stage N-best/word-graph vs.coupled search
How do we achieve understanding during decoding?
Ongoing challenges:
Domain-independent language understanding
Will current approaches scale to more complex or general 
understanding tasks?
Integration of multimodal input s into a common understanding 
framework (e.g., Cohen, Flanagan, Waibel)
Introduction || NL || De velopment || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Conversational Systems   18
6.345 Automatic Speech Recognition (2003)
Statistical language models (i.e., n-grams) used 
for speech recognition are inappropriate for speech understanding applications, because they 
don't provide a meaning representationStatistical language models (i.e., n-grams) used 
for speech recognition are inappropriate for speech understanding applications, because they 
don't provide a meaning representation
Text based natural language processing systems 
may not be well suited for speech understanding 
applications, because they typically assume that:
 Word boundaries are known with certainty All words are known with certainty
 Sentences are well formed
 Constraints are unnecessaryText based natural language processing systems 
may not be well suited for speech understanding 
applications, because they typically assume that:
 Word boundaries are known with certainty All words are known with certainty
 Sentences are well formed
 Constraints are unnecessaryContrasting Language Models for Speech Recognition
and Natural Language Understanding
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Conversational Systems   41
6.345 Automatic Speech Recognition (2003)
An Attractive Strategy
Conduct R&amp;D of human language technologies within the
context of real application domains
Forces us to: 
* Confront critical technical issues (e.g., rejection, new word problem) 
and 
* Set priorities (e.g., better match technical capabilities with useful 
applications)
Provides a rich and continuing source of useful data
* Real data from real users are invaluable
Demonstrates the useful ness of the technology
Facilitates technology transfer
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Conversational Systems   25
6.345 Automatic Speech Recognition (2003)
Example Sentences Some Systems Can Handle 
I WOULD LIKE TO FLY FROM SAINT PAUL TO SAN JOSE 
MONDAY MORNING FROM SAN JOSE TO HOUSTON 
TUESDAY MORNING AND FROM HOUSTON TO SAINT PAUL 
ON WEDNESDAY MORNING
[UM] I WOULD LIKE TO FIND OUT WHAT FLIGHTS THERE 
ARE ON FRIDAY JUNE ELEVENTH FROM SAINT PETERSBURG    &lt;TO&gt; M- TO M-MILWAUKEE AND THEN 
FROM MILWAUKEE TO TACOMA THANK YOU
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Conversational Systems   23
6.345 Automatic Speech Recognition (2003)
SLSDatabase
TuplesPre-recorded
DataDATABASE Reference
Answer
Compare
ScoreEvaluation of SLS Using
Common Answer Specification (CAS)
Evaluation is automatic (i.e., easy), once we have:
Principles of interpretation (e.g., red-eye)
Properly annotated data, and
Comparator
But it is costly, and does not address important research 
issues such as dialogue modeling and system usefulness
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Conversational Systems   9
6.345 Automatic Speech Recognition (2003)
TodayComponents of a Conversational System
DISCOURSE 
CONTEXTDISCOURSE 
CONTEXTDIALOGUE
MANAGEMENTDIALOGUE
MANAGEMENTDATABASEGraphs
&amp; Tables
LANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGMeaning
RepresentationMeaning
Representation
MeaningLANGUAGE
GENERATIONLANGUAGE
GENERATIONSPEECH
SYNTHESISSPEECH
SYNTHESIS
SpeechSentence
SPEECH
RECOGNITIONSPEECH
RECOGNITIONSpeech
Words
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Conversational Systems   16
6.345 Automatic Speech Recognition (2003)
Understanding Words in Context 
Subtle differences in phrasing can lead to completely 
different interpretations
Is there a six A.M. flight?
Are there six A.A. flights?
Is there a flight six?
Is there a flight at sixsix could mean:
A time
A count
A flight number 
The possibility of recognition errors makes it hard to rely 
on features like the article a or the plurality of flights.
Yet insufficient syntactic/semantic analysis can lead to 
gross misinterpretations
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Conversational Systems   2
6.345 Automatic Speech Recognition (2003)
The Premise:
Everybody
wants
InformationEverybody
wants
Information
Need new
interfaces
Speech is It!
For North America
CommerceNet
Research Center (1999)
Even when
they are
on the moveEven when
they are
on the move
The interface
must be
easy to useThe interface
must be
easy to useDevicesmust besmall
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Conversational Systems   46
6.345 Automatic Speech Recognition (2003)
Examples of Spoken Dialogue Systems
Canon TARSAN (Japanese)
Info retrieval from CD-ROM
InfoTalk (Cantonese)
Transit fare
KDD ACTIS (Japanese)
Area-codes, country-codes 
and time-difference 
NEC (Japanese)
Ticket reservation
NTT (Japanese)
Directory assistance
SpeechWorks (Chinese)
Stock quotes
Toshiba TOSBURG (Japanese)
Fast food orderingCanon TARSAN (Japanese)
Info retrieval from CD-ROM
InfoTalk (Cantonese)
Transit fare
KDD ACTIS (Japanese)
Area-codes, country-codes 
and time-difference 
NEC (Japanese)
Ticket reservation
NTT (Japanese)
Directory assistance
SpeechWorks (Chinese)
Stock quotes
Toshiba TOSBURG (Japanese)
Fast food orderingAsia U.S.
AT&amp;T How May I Help You?,...
BBN Call Routing
CMU Movieline, Travel,...
Colorado U Travel
IBM Mutual funds, Travel
Lucent Movies, Call Routing,...
MIT Jupiter, Voyager, Pegasus,..
Weather, navigation, flight info
Nuance Finance, Travel,
OGI CSLU Toolkit
SpeechWorks Finance, Travel,...
UC-Berkeley BERP
Restaurant information
U Rochester TRAINS
Scheduling trainsAT&amp;T How May I Help You?,...
BBN Call Routing
CMU Movieline, Travel,...
Colorado U Travel
IBM Mutual funds, Travel
Lucent Movies, Call Routing,...
MIT Jupiter, Voyager, Pegasus,..
Weather, navigation, flight info
Nuance Finance, Travel,
OGI CSLU Toolkit
SpeechWorks Finance, Travel,...
UC-Berkeley BERP
Restaurant information
U Rochester TRAINS
Scheduling trainsEurope
CSELT (Italian)
Train schedules
KTH WAXHOLM (Swedish)
Ferry schedule
LIMSI (French)
Flight/train schedules
Nijmegen (Dutch)
Train schedule
Philips (Dutch,Fr.,German)
Flight/Train schedules
Vocalis VOCALIST (English)
Flight schedulesCSELT (Italian)
Train schedules
KTH WAXHOLM (Swedish)
Ferry schedule
LIMSI (French)
Flight/train schedules
Nijmegen (Dutch)
Train schedule
Philips (Dutch,Fr.,German)
Flight/Train schedules
Vocalis VOCALIST (English)
Flight schedules
Large-scale deployment of some dialogue systems
e.g., CSELT, Nuance, Philips, SpeechWorks
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Conversational Systems   14
6.345 Automatic Speech Recognition (2003)
Context Free Rules for Example
sentence  (display-clause truth-clause )
display-clause  display direct-object
direct-object  [determiner] (flight-event fare-event )
flight-event  flight [from-place]  [to-place]
from-place  from  a-city
to-place  to a-city
display  show-me
show-me  [please ]  show [me]
a-city  (boston dallas denver )
determiner  (a the )
...
Context free : left hand side of rule is single symbol
brackets [ ]: optional
Parentheses ( ): alternates .    
Terminal words in italics
Introduction || NL (NLU) || Development || Progress || ChallengesShow me flights from Boston to Denver</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Conversational Systems   30
6.345 Automatic Speech Recognition (2003)
Stochastic Approaches
Semantic
ModelLexical
Model
what to say how to say itmeaning sentence
MS
Choose among all possible meanings the one that 
maximizes:
(|)( )(| )()PS MPMPM SPS=
HMM techniques have been used to determine the meaning 
of utterances (ATT, BBN, IBM)
Encouraging results have been achieved, but a large body of 
annotated data is needed for training
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Conversational Systems   31
6.345 Automatic Speech Recognition (2003)
NL Re-SortN Complete
"sentence"
hypothesesparsable
sentences
SRbest 
scoring
hypothesisspeech
show me flights from boston to denver and
show me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onshow me flights from boston to denver andshow me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onAnswerSR/NL Integration via N-Best Interface
N-Best resorting has also been used as a mechanism for 
applying computationall y expensive constraints
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Conversational Systems   1
6.345 Automatic Speech Recognition (2003)
Conversational Systems*: Advances and Challenges
Introduction
Speech Understanding
Natural Language Understanding
Discourse Resolution
Dialogue Modeling
Development Issues
Recent Progress
Future Challenges
Summary
* AKA spoken language systems or spoken dialogue systems  
See article by Zue and Glass (2000) Lecture # 22
Session 2003</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Conversational Systems   15
6.345 Automatic Speech Recognition (2003)
What Makes Parsing Hard?
Must realize high coverage of well-formed sentences within 
domain
Should disallow ill-formed sentences, e.g.,
the flight that arriving in the morning
what restaurants do you know about any banks?
Avoid parse ambiguity (redundant parses)
Maintain efficiency
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Conversational Systems   24
6.345 Automatic Speech Recognition (2003)
State of the Art (The ATIS Domain)
Word (also utterance) error rate  
(ER) for spontaneous speech approaching that for read 
speech
Understanding ER &lt;10% for text 
input; complete NL analyses not 
required
ER for speech input only ~2-3% 
higher than for text input
Many more sentences 
understood than correctly 
recognized 
In most cases, ER cut by about 
half every two years 
Real-time performance achieved 
using high-end workstations
Results for answerable 
queries only110100
'91 '92 '93 '94 95Error Rate (%)Word Error
Utterance
Error
NL Error
SLS Error
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Conversational Systems   43
6.345 Automatic Speech Recognition (2003)
Data Collection
System development is chicken &amp; egg problem
Data collection has evolved considerably
Wizard-based system-based data collection
Laboratory deployment public deployment 
100s of users thousands millions
Data from realusers solving realproblems accelerates 
technology development
Significantly different fr om laboratory environment
Highlights weaknesses, allows continuous evaluation
But, requires systems providing realinformation!
Expanding corpora will require unsupervised training or 
adaptation to unlabelled data
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Conversational Systems   8
6.345 Automatic Speech Recognition (2003)
Example of MITs Mercury Travel Planning System
New user calling into Mercury flight planning system
Illustrated technical issues:
Back-off to directed dialogue when necessary (e.g., password)
Understanding mid-stream corrections (e.g., no Wednesday)
Soliciting necessary information from user
Confirming understood concepts to user
Summarizing multiple database results
Allowing negotiation with user
Articulating pertinent information
Understanding fragments in  context (e.g., 4:45)
Understanding relative dates (e .g., the following Tuesday)
Quantifying user satisfaction (e.g., questionnaire)
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Conversational Systems   11
6.345 Automatic Speech Recognition (2003)
Input Processing: Understanding
LANGUAGE
UNDERSTANDINGSemantic
RepresentationSPEECH
RECOGNITIONSpeech
Waveform
Sentence
Hypotheses
Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: "Boston"
Predicate: TO
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: " Boston "
Predicate: TO
Topic: CITY
Name: " Denver "FLIGHT
FLIGHTSDENVER SHOW TO BOSTON FROM ME
ONAND
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Conversational Systems   27
6.345 Automatic Speech Recognition (2003)
Historical Perspective on Key Players in ATIS Effort
CMU: Strictly semantic grammar, syntactic information 
mostly ignored
MIT: Grammar rules interleave syntactic and semantic 
categories
BBN, SRI:
Initial systems used syntactic grammars based on unification 
framework, with parallel semantic rules
Both sites now have a strictly semantic grammar as well
SRI combines two outputs into one system; BBN has separate 
competing systems
ATT, BBN, IBM: Stochastic approaches using HMM
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Conversational Systems   35
6.345 Automatic Speech Recognition (2003)
Generating n-grams from Parse Trees
NLU can help generate a consistent class n-gram
to        idaho_falls &lt;cty&gt; on           may &lt;mth&gt; twenty_third &lt;dy&gt;SENTENCE
CLARIFIER
DESTINATION DATE
TO CITY_NAME MONTH ON DAY
CARDINAL_DATE
to           idaho falls               on             may       twenty thirdCITY_NAME MONTH
CARDINAL_DATE
Developer identifies parse categories for class n-gram
System tags words with associated class labels
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Conversational Systems   13
6.345 Automatic Speech Recognition (2003)
Natural Language Understanding
show me flights from boston to denverflight destination sourcetopicdisplay object
predicatefull_parse
commandsentence
predicate
city city to from flight_listdestination source flightdisplaySome syntactic nodes 
carry semantic tags for
creating semantic frameClause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: "Boston"
Predicate: TO
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: " Boston "
Predicate: TO
Topic: CITY
Name: " Denver "
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Conversational Systems   48
6.345 Automatic Speech Recognition (2003)
Some Speech Recognition Research Issues
Widespread robustness to environments &amp; speakers
Channel conditions: 
* Wide-band telephone cellular
* Wide-band microphone arrays (echo cancellation)
Conversational speech phenomena
Speaker variation (native non-native)
Knowing what you dont know
Confidence scoring (utterance &amp; word)
Out-of-vocabulary word detection &amp; addition
Beyond word n-grams?
Providing coverage, constraint, and a platform for understanding
Other challenges:
Adaptation (long-term short-term)
Domain-independent acoustic and language modelling
Introduction || NL || De velopment || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Conversational Systems   32
6.345 Automatic Speech Recognition (2003)
An A* algorithm is often used to construct the top- N
sentence hypotheses
f*(p) = g(p) + h*(p)
where: f*(p) is the estimated score of the best path containing 
partial path p
g(p) is the score from the be ginning to the end of 
the partial path p, and
h*(p) is the estimated score  of the best-scoring 
extension of p
Questions:
How can information in the N-best list be captured more 
effectively?
show
ameflights
flight boston from denver toand
on
in# #Some Issues Related to Search
What are some computationally efficient choices of h*(p), even if 
inadmissible?
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Conversational Systems   22
6.345 Automatic Speech Recognition (2003)
Data Set Class A Class D Class X
ATIS-2 43% 33% 24%
ATIS-3 49% 33% 18%Data Set Class A Class D Class X
ATIS-2 43% 33% 24%
ATIS-3 49% 33% 18%
A: Context-independent queries
D: Context-dependent queriesX: Un-answerable queriesATIS Data Collection Status
Over 25,000 utterances were collected (from AT&amp;T, BBN, 
CMU, MIT, NIST, and SRI)
About 80% of the collected data (speech and transcriptions) 
were distributed for system development and training
Over 11,000 of training utterances were annotated with 
database reference answer
About 40% of the data from ATIS-3 (more cities)
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Conversational Systems   39
6.345 Automatic Speech Recognition (2003)
U: I need a flight from Boston to San Francisco
C: Did you say Boston or Austin?U: Boston, MassachusettsC: I need a date before I can access Travelocity
U: Tomorrow
C: Hold on while I retrieve the flights for you
C: I have found 10 flights meeting your sp ecification.             
When would you like to leave?
U: In the morning.
C: Do you have a preferred airline?
U: United
C: I found two non-stop United fl ights leaving in the morning Help the user narrow
down the choicesClarification
(insufficient info)Clarification
(recognition errors)
Post-Retrieval: Multiple DB Retrievals =&gt; Unique ResponseDifferent Roles of Dialogue Management
Pre-Retrieval: Ambiguous Input =&gt; Unique Query to DB
Introduction || NL (Dialogue) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Conversational Systems   47
6.345 Automatic Speech Recognition (2003)
Example Dialogue Systems
Vocabularies typically have 1000s of words
Widely deployed systems tend to be more conservative
Directed dialogues have fewer words per utterance
Word averages lowered by more confirmations
Human-human conversations use more words051015202530
CSELT SW Philips CMU CMU C LIMSI MIT MIT C AT&amp;T HumanAve Words/Utt
Ave Utts/Call
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Conversational Systems   51
6.345 Automatic Speech Recognition (2003)
Conclusion
Spoken dialogue systems are needed, due to
Miniaturization of computers
Increased connectivity
Human desire to communicate
To be truly useful, these interfaces must be conversational 
in nature
Embody linguistic competence, both input and output
Help people solve real problems efficiently
Systems with limited capabilities are emerging
Much research remains to be done</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Conversational Systems   10
6.345 Automatic Speech Recognition (2003)
Natural Language Processing Components
Understanding:
Parse input query into a meaning re presentation, to be interpreted 
for appropriate action by application domain
Select best candidate from proposed recognizer hypotheses
Discourse Resolution
Interpret each query in c ontext of preceding dialogue
Dialogue Management
Plan course of action under both expected and unexpected 
conditions; compose response frames.
Generation
Paraphrase user queries into same or different language.
Compose well-formed sentences to speak the (sequence of) 
response frames prepared by the dialogue manager.
Introduction || NL|| Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Conversational Systems   5
6.345 Automatic Speech Recognition (2003)
..
C:Yeah, [um] I'm looking for the Buford Cinema.
A:OK, and you're wanting to know what's showing there or 
... 
C:Yes, please. 
A:Are you looking for a particular movie?
C:[um] What's showing.
A:OK, one moment.
..
A:They're showing A Troll In Central Park .
C:No.
A:Frankenstein .
C:What time is that on?
A:Seven twenty and nine fifty.
C:OK, any others?disfluency
interruption, overlap
confirmation
clarification
back channel
inference
ellipsis
co-referenceThe Nature of Mixed Initiative Interactions
(A Human-Human Example)
Media Clip
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Conversational Systems   17
6.345 Automatic Speech Recognition (2003)
Multiple Roles for Natural Language 
Parsing in Spoken Language Context 
UnderstandingConstraint
Coverage100%100%
100%
Introduction || NL (NLU) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Conversational Systems   36
6.345 Automatic Speech Recognition (2003)
Some SR/NL Coupling Experiments (ATIS Domain)
MIT (Goddeau, 1992)
Probabilistic LR parser
Integrated into recognizer A* search
Achieved comparable re cognition accuracy to N-best resorting, 
but with considerably more efficiency
CMU (Ward, 1993)
Modeled semantic concept sequences through trigram; and 
terminal word sequences through bigram
Integrated into recognizer A* search
Reduced understanding (CAS) error by 10% 
SRI (Moore, 1995)
Modelled semantically meaningful fragments through trigram; and 
word classes through 4-gram
The NL score is added to the basic recognition score
Achieved ~15% word error reduction 
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Conversational Systems   34
6.345 Automatic Speech Recognition (2003)
By introducing NL constraints earlier, one can potentially 
reduce computation while improving performance
NL Re-Sortparsable
sentences
SRbest 
scoring
hypothesisspeechbest
partial theory
next word
extensionsAlternatives to N-Best Interface
Early integration can also remove the need for a statistical 
language model, which may be hard to obtain for some applications
As the vocabulary size increases, we must begin to explore 
alternative search strategies
Parallel search
Fast search to reduce word candidate list
Introduction || NL (SR/NL Integration) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Conversational Systems   4
6.345 Automatic Speech Recognition (2003)
Human ComputerInitiative
Human takes 
complete control
Computer is totally 
passiveHuman takes 
complete control
Computer is totally 
passive
H:I want to visit my grandmother.Computer maintains 
tight control
Human is highly 
restrictedComputer maintains 
tight control
Human is highly 
restricted
C:Please say the departure city.Defining the Context
Conversational systems differ in the degree with which 
human or computer takes the initiative
Directed
DialogueFree Form
DialogueMixed Initiative
Dialogue
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Conversational Systems   3
6.345 Automatic Speech Recognition (2003)
What Are Conversational Systems?
Systems that can communicate with users through a 
conversational paradigm, i.e., they can:
Understand verbal input, using
*Speech recognition
*Language understanding (in context)
Verbalize response, using
* Language generation
* Speech synthesis
Engage in dialogue with a user during the interaction
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Conversational Systems   38
6.345 Automatic Speech Recognition (2003)
MITs Discourse Module Internals
DISCOURSE
MODULEInput Frame 
Displayed List
Resolve 
Deixis
Incorporate 
Fragments
Interpreted FrameResolve 
PronounsResolve 
Definite NP
Fill 
Obligatory 
RolesUpdate 
History 
Elements
Introduction || NL (Discourse) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Conversational Systems   42
6.345 Automatic Speech Recognition (2003)
System RefinementLimited
NL
Capabilities
Data
Collection
(Wizard)
Performance
EvaluationExpanded
NL
CapabilitiesSpeech
Recognition
Data
Collection
(Wizard-less)System Development Cycle
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Conversational Systems   6
6.345 Automatic Speech Recognition (2003)
4 8 12 16 20 20+0102030405060% of Turns 
AVE # OF WORDS/TURNAgent
Client
Over 1,000 dialogues in many domains (Flammia 98)
Some lessons learned (about clients):
More than 80% of utterances are 12 words or less 
Most short utterances are c onfirmation and back channel 
communicationsStudy of human-human inte ractions can lead to 
good insights in building human-machine systems
Introduction || NL || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Conversational Systems   26
6.345 Automatic Speech Recognition (2003)
We cannot expect any natural language system to be able 
to fully parse and understand all such sentencesWe cannot expect any natural language system to be able 
to fully parse and understand all such sentencesDifficult, But Real, Sentences
I would like to find a flight from Pittsburgh to Boston on 
Wednesday and I have to be in Boston by one so I would like a flight out of here no later than 11 a.m.
I'll repeat what I said before on scenario 3 I would like a 727 
flight from Washington DC to Atlanta Georgia I would like it 
during the hours of from 9 a.m. till 2 p.m. if I can get a flight 
within that time frame and I would like it for Friday
Some database I'm inquiring about a first class flight 
originating city Atlanta dest ination city Boston any class 
fare will be all right
Introduction || NL (ATIS) || Development || Progress || Challenges</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Conversational Systems   40
6.345 Automatic Speech Recognition (2003)
Multiple Roles of Dialogue Modeling
Our definition:  For each turn, preparing the systems side of 
the conversation, including responses and clarifications
Resolve ambiguities
Ambiguous database retrieval (e.g. London, England or London, 
Kentucky)
Pragmatic considerations (e.g., too many flights to speak)
Inform and guide user  
Suggest subsequent sub-goals (e.g., what time?)
Offer dialogue-context dependent assistance upon request
Provide plausible alternatives when requested information 
unavailable
Initiate clarification sub-dialogues for confirmation
Influence other system components
Adjust language model due to dialogue context
Adjust discourse history due to pragmatics (e.g., New York)
Introduction || NL (Dialogue) || Development || Progress || Challenges</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>18
19</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture18/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>26</slideno>
          <text>Acoustic Modelling   27 6.345 Automatic Speech Recognition
Aggregation Experiments
Combining different training runs can improve performance
Three experimental systems: phonetic classification, 
phonetic recognition (TIMIT), and word recognition (RM)
Acoustic models:
-Mixture Gaussian densities, randomly initialized K-means
-24 different training trials
Measure average performance of M unique N-fold 
aggregated models (starting from 24 separate models)
% Error Phone Classification Phone Recognition Word Rec.
M=24 N=1 22.1 29.3 4.5
M=6 N=4 20.7 28.4 4.2
M=1 N=24 20.2 28.1 4.0
% Reduction 8.3 4.0 12.0</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Acoustic Modelling   18 6.345 Automatic Speech Recognition
Composing FST Lexical Networks
Four basic FST networks are 
composed to form full search network.
G: Language model
L: Lexical model
P: Pronunciation model
C: Context-dependent acoustic 
model mapping
Mathematical composed using 
the expression:
CD Acoustic Model LabelsC: CD Model MappingPhonetic UnitsP : Pronunciation Model Phonemic UnitsL: Lexical ModelWordsG : Language ModelWords
CoPoLoG</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Acoustic Modelling   6 6.345 Automatic Speech Recognition
Weather Corpus Characteristics 
Approximately 11% of data contained significant noises
Over 6% of data contained spontaneous speech effects
At least 5% of data from speakerphonesChild
9%
Female
21%
Male
70% Native
86%Non-
Native
14%Corpus dominated by American male speakers</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Acoustic Modelling   32 6.345 Automatic Speech Recognition
Committee-based Classifiers (Halberstadt, 1998)
Uses multiple acoustic feature vectors and classifiers to 
incorporate different sources of information
Explored 3 combination methods (e.g., voting, linear, indep.)
Obtains state-of-the-art phonetic classification and 
recognition results (TIMIT)
Combining 3 boundary models in Jupiter weather domain
Word error rate 10-16% relati ve reduction over baseline
Substitution error rate 14-20% re lative reduction over baseline
Acoustic Measurements % Error % Sub
B1 (30 ms, 12 MFCC, telescoping avg)  11.3 6.4 
B2 (30 ms, 12 MFCC+ZC+E+LFE, 4 cos 50ms) 12.0 6.7 
B3 (10ms, 12 MFCC, 5 cos 75ms)  12.1 6.9 
B1 + B2 + B3 10.1 5.5</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Acoustic Modelling   13 6.345 Automatic Speech Recognition
Example Phonological Rules
Example rule for /t/ deletion (destination):
{s}t {ax ix} =&gt;[tcl t] ;
Phoneme Left 
ContextRight
ContextPhonetic
Realization
Example rule for palatalization of  /s/ (miss you):
{}s {y}=&gt;s | sh ;</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Acoustic Modelling   12 6.345 Automatic Speech Recognition
Applying Phonological Rules
Phonemic baseforms are canonical representation
Baseforms may have multiple acoustic realizations
Acoustic realizations are phones or phonetic units
Example:
batter : b ae tf er
This can be realized phonetically as:
bcl b ae tcl t er
or as:
bcl b ae dx erStandard /t/
Flapped /t/</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Acoustic Modelling   28 6.345 Automatic Speech Recognition
Model Aggregation
Aggregation combines Nclassifiers, with equal weighting, 
to form one aggregate classifier
11() ()N
An
nxxN==  r r
4 Gaussians2 Gaussians
6 Gaussian sThe expected error of an aggregate classifier is less than the 
expected error of any randomly chosen constituent
N-fold aggregate classifier has Ntimes more computation
Gaussian kernels of aggregate model can be hierarchically 
clustered and selectively pruned
 Experiment: Prune 24-fold m odel back to size of smaller N-fold models</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Acoustic Modelling   8 6.345 Automatic Speech Recognition
Vocabulary
Lexicon based on syllabified LDC PRONLEX dictionaryIncorporation of common reduced words &amp; word pairsCurrent vocabulary consists of nearly 2000 words
Based on system capabilities and user queries
Type Size Examples
Geography 933 boston, alberta, france, africa 
Weather 217 temperature, snow, sunny, smog
Basic 815 i, what, january, tomorrow
Type Examples
Reduction give_me, going_to, want_to, what_is, i_would
Compound clear_up, heat_wave, pollen_count</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Acoustic Modelling   9 6.345 Automatic Speech Recognition
Example Vocabulary File
Sorted alphabetically
&lt;&gt;*
&lt;pause1&gt;
&lt;pause2&gt;
&lt;uh&gt;&lt;um&gt;&lt;unknown&gt;*
a
a_mamdon+t
new_york_city
sixtytodaytoday+s
Numbers tend to be spelled out
Each word form has separate entryUtterance start &amp; end marker
Filled pause modelsPauses at utterance start &amp; end
*d items have no acoustic realization
Out-of-vocabulary word model
&lt;&gt;d words dont count as errors
Underbars distinguish letter 
sequences from actual words
Lower case is a common convention+ symbol conventionally used for</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Acoustic Modelling   34 6.345 Automatic Speech Recognition
References
E. Bocchieri.  Vector quantization for the efficient 
computation of continuous density likelihoods.  Proc. 
ICASSP , 1993.
T. Hazen and A. Halberstadt.  Using aggregation to improve 
the performance of mixture Gaussian acoustic models.  Proc. ICASSP , 1998.
J. Glass, T. Hazen, and L. Hetherington.  Real-time 
telephone-based speech recognition in the Jupiter domain.  
Proc. ICASSP , 1999. 
A. Halberstadt.  Heterogeneous acoustic measurements and 
multiple classifiers for speech recognition.  Ph.D. Thesis, MIT, 1998.
T. Watanabe et al.  Speech recognition using tree-structured 
probability density function.  Proc. ICSLP , 1994.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Acoustic Modelling   16 6.345 Automatic Speech Recognition
The Training Sentence File
An n-gram model is estimated from training data
Training file contains one utterance per line
Words in training file must have same case and form as 
words in vocabulary file
Training file uses the following conventions:
Each clean utterance begins with &lt;pause1&gt; and ends with
&lt;pause2&gt;
Compound word underbars are ty pically removed before training
Underbars automatically re-inserted during training based on 
compound words present in vocabulary file
Special artifact units may be used for noises and other 
significant non-speech events:
&lt;clipped1&gt; , &lt;clipped2&gt; , &lt;hangup&gt;, &lt;cough&gt;, &lt;laugh&gt;</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Acoustic Modelling   22 6.345 Automatic Speech Recognition
Recognition Experiments
Collecting real data improves performance:
Enables increased complexity and improved 
robustness for acoustic and language models
Better match than laboratory recording conditions01020304050607080
Apr May Jun Jul Aug Nov Apr Nov MayError Rate (%)
110100
Training Data (x1000 )Sentence
Word
Data</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Acoustic Modelling   26 6.345 Automatic Speech Recognition
Model Aggregation
K-means and EM algorithms converge to different 
local minima from different initialization points
Performance on development data not necessarily a 
strong indicator of performance on test data  
TIMIT phonetic recognition error for 24 training trials
27.5 28 28.528.52929.530
Dev Set Error Rate (%)Test  Set Error Rate (%)Best on Dev, Worst on Test!
Correlation 
Coeff = 0.16</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Acoustic Modelling   29 6.345 Automatic Speech Recognition
1 2 4 6 8 12 242727.52828.5Aggregation Experiments
# of Aggregated Training Trials (N)Error Rate (%) on TIMIT Dev SetAverage N-fold Trial
Best and Worst Trial
Pruned 24-fold Mode</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Acoustic Modelling   14 6.345 Automatic Speech Recognition
Language Modelling
Class bi- and trigrams used to produce 10-best outputs
Training data augmented with city and state constraints
Relative entropy measure used to help select classes
200 word classes reduced perplexities and error ratesraining, snowing humidity, temperature
cold, hot, warm advisories, warnings
extended, general conditions, forecast, report
Type Perplexity % Word Error Rate
word bigram 18.4 16.0
+ word trigram 17.8 15.5
class bigram 17.6 15.6
+ class trigram 16.1 14.9</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Acoustic Modelling   20 6.345 Automatic Speech Recognition
Acoustic Models
Models can be built for segments and boundaries
Best accuracy can be achieved when both are used
Current real-time recognition uses only boundary models
Boundary labels combined into classes
Classes determined using decision tree clustering
One Gaussian mixture model trained per class
112 dimension feature vector reduced to 50 dimensions via PCA
1 Gaussian component for every 50 training tokens (based on # 
dims)
Models trained on over 100 hours of spontaneous 
telephone speech collected from several domains</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Acoustic Modelling   4 6.345 Automatic Speech Recognition
Data Collection Issues
System development is chicken &amp; egg problem
Data collection has evolved considerably
Wizard-based system-based data collection
Laboratory deployment public deployment 
100s of users thousands millions
Data from realusers solving realproblems accelerates 
technology development
Significantly different fr om laboratory environment
Highlights weaknesses, allows continuous evaluation
But, requires systems providing realinformation!
Expanding corpora requires unsupervised training or 
adaptation to unlabelled data</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Acoustic Modelling   25 6.345 Automatic Speech Recognition
Gaussian Selection
~50% of total computation is evaluation of Gaussian densities
Can use binary VQ to select mixture components to evaluate
Component selection criteria for each VQ codeword:
outside 
distance 
thresholdwithin 
codeword Those within distance threshold
 Those within codeword (i.e., ever y component used at least once)
Can significantly reduce computation with small error loss At least one component/model per codeword (i.e., only if necessary)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Acoustic Modelling   5 6.345 Automatic Speech Recognition
Data Collection (Weather Domain)
Over 756K utterances from 112K calls since May, 1997010000200003000040000500006000070000
May Jul Sep Nov Jan Mar May Jul Sep Nov Jan Mar May Jul Sep NovCalls
UtterancesInitial collection of 3,500 read utterances and 1,000 wizard 
utterances</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Acoustic Modelling   2 6.345 Automatic Speech Recognition
Example Dialogue-based Systems
Vocabularies typically have 1000s of words
Widely deployed systems tend to be more conservative
Directed dialogues have fewer words per utterance
Word averages lowered by more confirmations
Human-human conversations use more words051015202530
CSELT SW/F Philips CMU/M CMU/F LIMSI MIT/W MIT/F AT&amp;T HumanAve Words/Utt
Ave Utts/Call</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Acoustic Modelling   1 6.345 Automatic Speech Recognition
ASR for Spoken-Dialogue Systems
Introduction
Speech recognition issues
Example using SUMMIT system  for weather information
Reducing computation
Model aggregation
Committee-based classifiersLecture # 18
Session 2003</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Acoustic Modelling   30 6.345 Automatic Speech Recognition
Phonetic Classification Confusions
Most confusions occur within manner class</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Acoustic Modelling   21 6.345 Automatic Speech Recognition
Search Details
Search uses forward and backward passes:
Forward Viterbi search using bigram
Backwards A* search using bigram to create a word graph
Rescore word graph with trigram (i.e., subtract bigram scores)
Backwards A* search using trigram to create N-best outputs
Search relies on two types of pruning:
Pruning based on relative likelihood score
Pruning based maximum number of hypotheses
Pruning provides tradeoff between speed and accuracy
Search can control tradeoff between insertions and deletions
Language model biased towards short sentences
Word transition weight (wtw) heur istic adjusted to remove bias</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Acoustic Modelling   23 6.345 Automatic Speech Recognition
Error Analysis (2506 Utterance Test Set)
0 1 02 03 04 05 06 07 0Expert (ID)Out of DomainNon-native (ID)Child (ID)Female (ID)Male (ID)In Domain (ID)Entire Set
Word Error Rate (%)2%60%26%23%13%8%11%22%
50% worse than males
3s worse than males
Experienced users adapt to system!Different test set70% of speakers are male70% of test set is in domain</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Acoustic Modelling   33 6.345 Automatic Speech Recognition
Related Work
ROVER system developed at NIST [Fiscus, 1997]
1997 LVCSR Hub-5E Benchmark test
Recognizer output voting error reduction
Combines confidence-tagged word  recognition output from 
multiple recognizers
Produced 12.5% relative reduction in WER
Notion of combining multiple information sources 
Syllable-based and word-based [Wu, Morgan et al, 1998]
Different phonetic inventories [AT&amp;T]
80, 100, or 125 frames per second [BBN]
Triphone and quinphone [HTK]
Subband-based speech recognition [Bourland, Dupont, 1997]</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Acoustic Modelling   31 6.345 Automatic Speech Recognition
Committee-based Classification
Combining information sources can reduce errorChange of temporal basis affects within-class error
Smoothly varying cosine basis better for vowels and nasals
Piecewise-constant basis better for fricatives and stops
202224262830% Error
Overall
Vowel
Nasal
Weak
Fricative
StopS1: 5 averages
S3: 5 cosines</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Acoustic Modelling   11 6.345 Automatic Speech Recognition
Editing Generated Baseforms
Automatically generated baseform file should be 
manually checked for the following problems:
Missing pronunciation variants that are needed
Unwanted pronunciation variants that are present
Vocabulary words missing in PRONLEX
going_to : g ow ix ng &amp; t uw
reading : ( r iy df ix ng , r eh df ix ng )
woburn : &lt;???&gt;
going_to : g ( ow ix ng &amp; t uw , ah n ax )
reading : r eh df ix ng
woburn : w ( ow , uw ) b er n</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Acoustic Modelling   19 6.345 Automatic Speech Recognition
ix|n
IN
-3.57n|bcl

0bcl|b

-2.35b|ao
BOSTON
0ao|s

0
b|aa

0aa|s
BOSTON
-0.35
aa|z
BOSNIA
-1.23ix|n
IN
-3.57n|bcl

0bcl|b

-2.35b|ao
BOSTON
0ao|s

0
b|aa

0aa|s
BOSTON
-0.35
aa|z
BOSNIA
-1.23Arc input label
ix|n
IN
-3.57n|bcl

0bcl|b

-2.35b|ao
BOSTON
0ao|s

0
b|aa

0aa|s
BOSTON
-0.35
aa|z
BOSNIA
-1.23Arc output label
ix|n
IN
-3.57n|bcl

0bcl|b

-2.35b|ao
BOSTON
0ao|s

0
b|aa

0aa|s
BOSTON
-0.35
aa|z
BOSNIA
-1.23Arc scoreFST Example
Words share arcs in networkAlternate pronunciationsix|n
IN
-3.57n|bcl

0bcl|b

-2.35b|ao
BOSTON
0ao|s

0
b|aa

0aa|s
BOSTON
-0.35
aa|z
BOSNIA
-1.23</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Acoustic Modelling   24 6.345 Automatic Speech Recognition
00 . 511 . 522 . 533 . 54
Latency (s)A* Search Latency
Average latency .62 seconds
85% &lt; 1 second ; 99% &lt; 2 seconds
Latency not dependent on utterance length</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Acoustic Modelling   15 6.345 Automatic Speech Recognition
Defining N-gram Word Classes
Class definitions have 
class name on left and 
word on right
Class names with 
&lt;U&gt;_  forces all words 
to be equally likely
Alternate words in class 
can be placed on same 
line with  | separatorCITY ==&gt; boston
CITY ==&gt; chicago
CITY ==&gt; seattle
&lt;U&gt;_DIGIT ==&gt; one
&lt;U&gt;_DIGIT ==&gt; two
&lt;U&gt;_DIGIT ==&gt; three
DAY ==&gt; today | tomorrow</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Acoustic Modelling   10 6.345 Automatic Speech Recognition
Example Baseform File
&lt;pause1&gt; : -+
&lt;pause2&gt; : -+
&lt;uh&gt; : ah_fp
&lt;um&gt; : ah_fp m
a_m : ey &amp; eh m
either : ( iy , ay ) th er
laptop : l ae pd t aa pd
new_york : n uw &amp; y ao r kd
northwest : n ao r th w eh s td
trenton : tr r eh n tq en
winter : w ih nt erprevious symbol 
can repeat
word break 
allowing pausespecial filledpause vowel
alternatepronunciations</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Acoustic Modelling   3 6.345 Automatic Speech Recognition
Telephone-based, Conversational, ASR
Telephone bandwidths with variable handsets
Noisy background conditions
Novice users with small number of interactions
Men, women, children
Native and non-native speakers
Genuine queries, browsers, hackers
Spontaneous speech effects 
e.g., filled pauses, partial words, non-speech artifacts
Out-of-vocabulary words and out-of-domain queries
Full vocabulary needed for complete understanding 
Word and phrase spotting are not primary strategies
Mixed-initiative dialog provides little constraint to recognizer
Real-time decoding</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Acoustic Modelling   17 6.345 Automatic Speech Recognition
Example Training Sentence File
&lt;pause1&gt; when is the next flight to chicago &lt;pause2&gt;
&lt;pause&gt; to san &lt;partial&gt; san francisco &lt;pause2&gt;
&lt;pause1&gt; &lt;um&gt; boston &lt;pause2&gt; 
&lt;clipped1&gt; it be in time &lt;pause2&gt;
&lt;pause1&gt; good bye &lt;hangup&gt;
&lt;pause1&gt; united flight two oh four &lt;pause2&gt;&lt;pause1&gt; &lt;cough&gt; excuse me &lt;laugh&gt; &lt;pause2&gt;partial word,
e.g., san die(go)
all significant sounds are transcribedclipped word,e.g., ~(w)ill it</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Acoustic Modelling   7 6.345 Automatic Speech Recognition
Vocabulary Selection
Constrained domains naturally limit vocabulary sizes
2000 word vocabulary gives good coverage for weather
~2% out-of-vocabulary rate on test sets020406080100
0 1000 2000 3000
Vocabulary Size% Coverage</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>16
17</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture16/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  14
-k p u w e r z t k - xd a a v - n ae
 
 
aa - m-eh dh
-k p u w e r z t k - xd
 
 
 
  Searching Graph-Based Observation Spaces:
The Anti-Phone Model
aa - m-eh dh
-k p u w e r z t k - xdFor a segmentation, S, assign anti-phone to extra segments
 All segments are accounted for in the phonetic graph
 Alternative paths through the gr aph can be legitimately comparedCreate a unit,   , to model segments that are not phones
Path likelihoods can be decomposed into two terms:
1 The likelihood of all segments produc ed by the anti-phone (a constant)
*
,(|)argmax ( | ) ( | ) ( )(| )SNi
iWSiPx uiWP s u P U W P WiPxi=MAP formulation for most likely word sequence, W, given by:2 The ratio of phone to anti-phone likelihoods for all path segments</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  21
Modelling Landmarks
Frame-based feature vectors
LandmarksFrame-based measurements:
 Computed every 5 milliseconds
 Feature vector of 14 Mel-Scal e Cepstral Coefficients (MFCCs)
Landmark-based measurements:
 Compute average of MFCCs over 8 regions around landmark
 8 regions X 14 MFCC averages = 112 dimension vector 112 dims. reduced to 50 us ing principal component analysis</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  24
Phonetic Recognition Experiments
TIMIT acoustic-phonetic corpus
 462 speaker training corpus , 24 speaker core test set
 Standard evaluation methodolog y, 39 common phonetic classes
Segment and landmark representations based on averages 
and derivatives of 14 MFCCs, energy and duration
PCA used for data normalization and reduction
Acoustic models based on aggregated Gaussian mixtures
Language model based on phone bigram
Probabilistic segmentation computed from diphone models
Method % Error
Triphone CDHMM 27.1
Recurrent Neural Network 26.1Bayesian Triphone HMM 25.6
Anti-phone, Heterogeneous classifiers 24.4</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  20
-k p u w e r z t k - xd a a v - n ae
aa - m-eh dh
-k p u w e r z t k - xdModelling Landmarks
We can also incorporate additional feature vectors 
computed at hypothesized landmarks or phone boundaries
Every segmentation acco unts for every landmark
 Some landmarks will be transitions between lexical-units
 Other landmarks will be considered internal to a unit
Both context-independent or dependent units are possible
Effectively model transitions between phones (i.e., diphones )
Frame-based models can be used to generate segment graph</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  2
Segment network created by interconnecting spectral landmarks
WaveformSegment-Based Speech Recognition
Frame-based measurements (every 5ms)
ao
- m- aedh -kp
uwer
z t k - ax dx
Probabilistic search finds most likely phone &amp; word stringscomputers                                that                  talk</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  13
a1 a3 a5
a2 a4Feature-based Speech Recognition
Each segment, si, is represented by a single feature vector, ai
To compare different segmentations it is necessary to 
predict the likelihood of both Xand Y:  P(A|SW) = P(XY|SW)Given a particular segmentation, S, Aconsists of X, the 
feature vectors associated with S, as well as Y, the feature 
vectors associated with segments notin S:AXY=UA = {a1a2a3a4a5}
P(a1a3a5 a2a4|SW) P(a1a2a4a5 a3|SW) X = {a1a3a5}Y ={a2a4} X = {a1a2a4a5}Y ={a3}a1 a3 a5
a2 a4a1 a3 a5
a2 a4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  1
Segment-Based Speech Recognition
Introduction
Searching graph-based observation spaces
 Anti-phone modelling
 Near-miss modelling
Modelling landmarks
Phonological modellingLecture # 16
Session 2003</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  16
SUMMIT Segment-based ASR</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  12
a2 a1 a3Frame-based Speech Recognition
The acoustic likelihood, P(A|SW), is derived from the same 
observation space for all word hypothesesa1
a2 a3 a1 a2a3
P(a1 a2 a3|SW) P( a1 a2 a3 |SW) P( a1a2a3|SW)  A = {a1a2a3}Observation space, A, corresponds to a temporal sequence 
of acoustic frames (e.g., spectral slices)
Each hypothesized segment, si, is represented by the series 
of frames computed between segment start and end times</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  15
Modelling Non-lexical Units:
The Anti-phone
Since is a constant, K, we can write P(XY |SU)
assuming independence between Xand Y(| )PX Y
(| ) (|)(|)(| )( | ) ( | )(| ) (| )PX PX UP X Y S UP X Y UP X U P Y KPX PX==  = 
We need consider only segments in Sduring search:
*
,,(| )arg max ( | ) ( | ) ( )(| )SN
iWUSiPx UiWP s u P U W P WiPxi=Given a particular segmentation, S, Aconsists of X, the 
segments associated with S, as well as Y, the segments not
associated with S: P(A|SU)=P(XY |SU)
Given segmentation S, assign feature vectors in Xto valid 
units, and all others in Yto the anti-phone</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  27
Word Recognition Experiments
Jupiter telephone-based, weather-queries corpus
 50,000 utterance training set, 1806  in-domain utterance test set
Acoustic models based on Gaussian mixtures
 Segment and landmark represen tations based on averages and 
derivatives of 14 MFCCs, energy and duration 
 PCA used for data normalization and reduction
 715 context-dependent boundary classes 935 triphone, 1160 diphone context-dependent segment classes
Pronunciation graph incorporates pronunciation probabilities
Language model based on class bigram and trigram
Best performance achieved by combining models
Method % Error
Boundary models 7.6
Segment models 9.6
Combined 6.1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  7
Statistical Approach to ASR
Linguistic
DecoderLanguage
Model
Acoustic
ModelSpeech
*a r g m a x ( |)
WWP W A=WPW()
PA W(| )Signal
ProcessorAWords W*
Given acoustic observations, A, choose word sequence, W*, 
which maximizes a posteriori probability, P(W |A)
(| ) ( )(| )()PAWPWPW APA=Bayes rule is typically used to decompose P(W |A) into 
acoustic and linguistic terms</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  22
Uses forward Viterbi search in first-pass to find best pathProbabilistic SegmentationLexical Nodesh#mzra
Timet0 t1 t2t3 t4 t5 t6 t7 t8
Relative and absolute thresholds used to speed-up search</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  11
SUMMIT Segment-Based ASR
SUMMIT speech recognition is based on phonetic segments
 Explicit phone start and end times are hypothesized during search
-
Recognition is achieved by searching a phonetic graph
 Graph can be computed via acoustic  criterion or probabilistic models
 Competing segmentations make us e of different observation spaces Differs from conventional fram e-based methods (e.g., HMMs)
 Enables segment-based acoustic-phonetic modelling
 Measurements can be extracte d over landmarks and segments
aa - m-eh dh
ph er zk p uw er z t k - x d n aa v - ae
 Probabilistic decoding must acco unt for graph-based observation space</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  26
Phonological Example
Example of what you expanded in SUMMIT recognizer
 Final /t/ in what can be realized as released, unreleased, palatalized, 
or glottal stop, or flap
what you</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  8
ASR Search Considerations
***
,,,, a r g m a x( | )
WUSWUS P W U SA Can seek best path to simplify search using dynamic 
programming (e.g., Viterbi) or graph-searches (e.g., A*)
In HMMs these correspond to acoustic , state , and
language model probabilities or likelihoodsThe modified Bayes decomposition has four terms:
(| ) (| )(| )()
)( (| )PS PU W U PAS U WPWW PAWUSPA=*argmax ( | ) argmax ( | )
WWSUWP W A P W U S A== A full search considers all possible segmentations, S, and 
units, U, for each hypothesized word sequence, W</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  17
Anti-Phone Framework Properties
Models entire observation space, using both positive and 
negative examples
Log likelihood scores are normalized by the anti-phone
 Good scores are positive, bad scores are negative
 Poor segments all have negative scores
 Useful for pruning and/or rejection Anti-phone is not used for lexical access
No prior or posterior probabilities used during search
 Allows computation on demand and/or fastmatch
 Subsets of data can be used for training
Context-independent or -dependent models can be used
Useful for general pattern matching problems with graph-
based observation spaces</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  5
Committee-based Phonetic Classification
202224262830% Error
Overall
Vowel
Nasal
Weak
Fricative
StopS1: 5 averages
S3: 5 cosines
Combining information sources can reduce errorChange of temporal basis affects within-class error
 Smoothly varying cosine basi s better for vowels and nasals
 Piecewise-constant basis be tter for fricatives and stops</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  29
References
J. Glass, A Probabilistic Framework for Segment-Based 
Speech Recognition, to appear in Computer, Speech &amp; 
Language , 2003.
D. Halberstadt, Heterogeneous Acoustic Measurements 
and Multiple Classifiers for Speech Recognition, Ph.D. 
Thesis, MIT, 1998.
M. Ostendorf, et al., From HMMs to Segment Models: A 
Unified View of Stochastic Modeling for Speech Recognition, Trans. Speech &amp; Audio Proc., 4(5), 1996.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  3
Segment-based Speech Recognition
Acoustic modelling is performed over an entire segment
Segments typically correspond to phonetic-like units
Potential advantages:
 Improved joint modelling of time/spectral structure
 Segment- or landmark-based acoustic measurements
Potential disadvantages:
 Significant increase in model and search computation
 Difficulty in robustly training model parameters</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  9
Examples of Segment-based Approaches
HMMs
 Variable frame-rate (Ponting et al., 1991, Alwan et al., 2000)
 Segment-based HMM (Marcus, 1993)
 Segmental HMM (Russell et al., 1993)
Trajectory Modelling
 Stochastic segment models (Ostendorf et al., 1989)
 Parametric trajectory models (Ng, 1993)
 Statistical trajectory models (Goldenthal, 1994)
Feature-based
F E A T U R E  (Cole et al., 1983)
S U M M I T  (Zue et al., 1989)
 LAFF (Stevens et al., 1992)</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  25
Phonological Modelling
Words described by phonemic baseforms
Phonological rules expand baseforms into graph, e.g.,
 Deletion of stop bursts in syllable coda 
(e.g., laptop )
 Deletion of /t/ in various environments
(e.g., intersection , destination, craft s)
 Gemination of fricatives and nasals 
(e.g., this s ide, in n ome)
 Place assimilation 
(e.g., did y ou(/d ih jh uw/))
Arc probabilities, P(U|W),can be trained
Most HMMs do not have a phonological component</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  6
Phonetic Classification Experiments 
(A. Halberstadt, 1998)
TIMIT acoustic-phonetic corpus
 Context-independent classification only
 462 speaker training corpus , 24 speaker core test set
 Standard evaluation methodolog y, 39 common phonetic classes
Several different acoustic representations incorporated
 Various time-frequency resoluti ons (Hamming window 10-30 ms)
 Different spectral representations (MFCCs, PLPCCs, etc) Cosine transform vs. piecewis e constant basis functions
Evaluated MAP hierarchy and committee-based methods
Method % Error
Baseline 21.6
MAP Hierarchy 21.0Committee of 8 Classifiers 18.5*
Committee with Hierarchy 18.3
* Development set performance</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  19
a3 a1 a5
a2 a4a3
a2 a4a4a5
a4a5 a1
a2Creating Near-miss Subsets
A1= {a1a2}    A2= {a1a2a3}    A3= {a3}    A4= {a3a4a5}    A5= {a4a5}Near-miss subsets, Ai, associated with any segmentation, S, 
must be mutually exclusive, and exhaustive: A=UAi  AiS
Temporal criterion guarantees proper near-miss subsets
 Abutting segments in Saccount for all times exactly once
 Finding all segments spanning a time creates near-miss subsets
A= U AiS   S = {{a1a3a5}, {a1a4}, {a2a5}}a5A4,A5 a4A4,A5 a3A2,A3,A4 a2A1,A2a1
a2
a1A1,A2</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  23
Lexical Nodesh#mzra
Timet0 t1 t2t3 t4 t5 t6 t7 t8Second pass uses backwards A*search to find N-best paths
Viterbi backtrace is used as future estimate for path scoresProbabilistic Segmentation (cont)
Block processing enables pipelined computation</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  28
Summary
Some segment-based speech recognition techniques 
transform the observation space from frames to graphs
Graph-based observation spaces allow for a wide-variety of alternative modelling methods to frame-based approaches
Anti-phone and near-miss mode lling frameworks provide a 
mechanism for searching graph-based observation spaces
Good results have been achieved for phonetic recognition
Much work remains to be done!</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  10
Segment-based Modelling at MIT
Baseline segment-based modelling incorporates:
 Averages and derivatives of spec tral coefficients (e.g., MFCCs)
 Dimensionality normalization via principal component analysis
 PDF estimation via Gaussian mixtures
Example acoustic-phonetic modelling investigations, e.g.,
 Alternative probabilistic clas sifiers (e.g., Leung, Meng)
 Automatically learned feature meas urements (e.g., Phillips, Muzumdar)
 Statistical trajectory models (Goldenthal)
 Hierarchical probabilistic featur es (e.g., Chun, Halberstadt)
 Near-miss modelling (Chang) Probabilistic segmentation (Chang, Lee) Committee-based classifiers (Halberstadt)</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  4
Hierarchical Acoustic-Phonetic Modelling
Homogeneous measurements can compromise performance
 Nasal consonants are classified  better with a longer analysis window
 Stop consonants are classified be tter with a shorter analysis window
1618202224
10 12.5 15 17.5 20 22.5 25 27.5 30
Window Duration (ms)% Classification Error Nasal
Stop
Class-specific information extraction can reduce error</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.345 Automatic Speech Recognition Segment-based ASR  18
A
BA
AB
A-uw d pe r z
m-k xdh
aa -eh
tk -Beyond Anti-Phones: Near-Miss Modelling
Anti-phone modelling partitions the observation space into 
two parts (i.e., on or not on a hypothesized segmentation)
Near-miss modelling partitions the observation space into a set of mutually exclusive, collectively exhaustive subsets
 One near-miss subset pre-computed for each segment in a graph
 Temporal criterion can guarantee proper near-miss subset generation 
(e.g., segment A is a near-miss of B iff As mid-point is spanned by B)
During recognition, observations in a near-miss subset are 
mapped to the near-miss model of the hypothesized phone
Near-miss models can be just an anti-phone, but can potentially be more sophisticated (e.g., phone dependent)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>5
6</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture5/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Speech Signal Representation 
 Fourier Analysis 
 Discr ete-time Fourier transform 
 Short-time Fourier transform 
 Discr ete Fourier transform 
 Cepstral Analysis 
 The complex cepstrum and the cepstrum 
 Computational considerations 
 Cepstral analysis of speech 
 Applications to speech recognition 
 Mel-Fr equency cepstral representation 
 Performance Comparison of Various Representations 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 1</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>An Example 
p[n]= [n]+  [n  N ] 0 &lt;&lt; 1 
P(z)= 1+ zN 
P(z)= log 
=log P(z) =log 1+ zN 
1  ()(zN )1 
= 
P(z)= 
p[n]=  
n=1 
 
n=1 
 
r=1 (1)n+1 n 
n 
(1)n+1 n 
n 
(1)r+1 r 
r z nN 
(zN )n 
[n  rN ] 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 15</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Things to Ponder ... 
	Are there other spectral representations that we should consider (e.g., models of 
the human auditory system)? 
	What about representing the speech signal in terms of phonetically motivated 
attributes (e.g., formants, durations, fundamental frequency contours)? 
	How do we make use of these (sometimes heter ogeneous) featur es for 
recognition (i.e., what are the appropriate methods for modelling them)? 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 27</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Comparison of Windows (contd) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 7</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Cepstral Analysis 
 Cepstral analysis for convolution is based on the observation that: 
x[n]= x1[n]  x2[n]  X (z)= X1(z)X2(z) 
By taking the complex logarithm of X (z),then 
 log{X (z)} =log{X1(z)} +log{X2(z)} = X(z) 
 If the complex logarithm is unique, and if X (z) is a valid z-transform, then 
 x1(n)+  x(n)=  x2(n) 
The two convolved signals will be additive in this new, cepstral domain. 
 If we restrict ourselves to the unit circle, z = ej, then: 
X (ej)=l og |X(ej)| + j arg{X(ej)} 
It can be shown that one approach to dealing with the problem of uniqueness is 
to require that arg{X(ej)} be a continuous, odd, periodic function of . 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 13</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Cepstral Analysis (contd) 
 To the extent that X (z)=l og{X(z)} is valid, 
  +     x[n]=21 
   
 



  +
1=  2   



  +


 c[n]=21 
  X (ej) ejnd 
complexlog{X (ej)} ejnd cepstrum 
log |X (ej)| ejnd cepstrum 
 It can easily be shown that c[n] is the even part of x[n]. 
 If  x[n] be recovered from c[n]. This is known as the x[n] is real and causal, then  
Minimum Phase condition. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 14</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>References 
1.	Tohkura, Y., A Weighted Cepstral Distance Measur e for Speech Recognition," IEEE 
Trans. ASSP , Vol. ASSP-35, No. 10, 1414-1422, 1987. 
2.	Mermelstein, P. and Davis, S., Comparison of Parametric Representations for 
Monosyllabic Word Recognition in Continuously Spoken Sentences," IEEE Trans. 
ASSP , Vol. ASSP-28, No. 4, 357-366, 1980. 
3.	Meng, H., The Use of Distinctive Features for Automatic Speech Recognition ,SM 
Thesis, MIT EECS, 1991. 
4.	Leung, H., Chigier , B., and Glass, J., A Comparative Study of Signal Represention 
and Classication Techniques for Speech Recognition," Proc. ICASSP ,Vol.II, 
680-683, 1993. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 28</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Short-T ime Fourier Analysis 
(Time-Dependent Fourier Transform) 
w [ 50 - m ] w [ 100 - m ] w [ 200 - m ] 
x [ m ] 
m 
0 n = 50 n = 100 n = 200 
+ 
Xn(ej)= w[n  m]x[m]ejm 
m= 
 If n is xed, then it can be shown that: 
  
Xn(ej)= 21 
W (ej)ejnX (ej(+))d 
 
	The above equation is meaningful only if we assume that X (ej) represents the 
Fourier transform of a signal whose properties continue outside the window , or 
simply that the signal is zero outside the window . 
	In order for Xn(ej) to correspond to X (ej), W (ej) must resemble an impulse 
with respect to X (ej). 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 3</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Example of Cepstral Analysis of Fricative 
(Tapering Window) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 22</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>A Wideband Spectrogram 
Two plus seven is less than ten 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 8</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Signal Representation Comparisons 
	Many researchers have compared cepstral representations with Fourier -, LPC-, 
and auditory-based representations. 
	Cepstral representation typically out-performs Fourier -and LPC-based 
representations. 
Example: Classication of 16 vowels using ANN (Meng, 1991) 
80 
66.1 
54.0 
61.7 
44.5 
61.6 
45.0 
61.2 
36.6 Clean Data 
Noisy Data 
70 Testing Accuracy (%) 60 
50 
40 
30 
Auditory Model MFSC MFCC DFT 
Acoustic Representation 
	Performance of various signal representations cannot be compar ed without 
considering how the featur es will be used, i.e., the pattern classication 
techniques used. (Leung, et al., 1993). 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 26</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Examples of Various Spectral Representations 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 11</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Comparison of Windows 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 6</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Cepstral Analysis of Speech 
 For voiced speech: 
 
s[n]= p[n]  g[n]  v[n]  r[n]= p[n]  hv [n]= hv [n  rNp ]. 
r= 
 For unvoiced speech: s[n]= w[n]  v[n]  r[n]= w[n]  hu[n]. 
	Contributions to the cepstrum due to periodic excitation will occur at integer 
multiples of the fundamental period. 
	Contributions due to the glottal waveform (for voiced speech), vocal tract, and 
radiation will be concentrated in the low quefrency region, and will decay rapidly 
with n. 
	Deconvolution can be achieved by multiplying the cepstrum with an appropriate 
window , l[n]. 
s [n ] x 
x [n ] D [ * y [ x [D [  * y [-1 
x ] 
n] n ] ]n] 
w [ n] l [n ] 
wher e D is the characteristic system that converts convolution into addition. 
 Thus cepstral analysis can be used for pitch extraction and formant tracking. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 18</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>A Narrowband Spectrogram 
Two plus seven is less than ten 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 9</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>The Use of Cepstrum for Speech Recognition
Many current speech recognition systems represent the speech signal as a set of 
cepstral coecients, computed at a xed frame rate. In addition, the time 
derivatives of the cepstral coecients have also been used. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 23</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Cepstral Analysis of Speech 
Voiced 
u [ n ] H ( z ) s [ n ] 
Unvoiced 
	The speech signal is often assumed to be the output of an LTI system; i.e., it is 
the convolution of the input and the impulse response. 
	If we are interested in characterizing the signal in terms of the parameters of 
such a model, we must go through the process of de-convolution. 
 Cepstral, analysis is a common procedur e used for such de-convolution. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 12</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Statistical Properties of Cepstral Coecients 
(Tohkura, 1987) 
From a digit database (100 speakers) over dial-up telephone lines. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 24</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Discrete-T ime Fourier Transform 
 +    X (ej)= x[n]e jn   n= 
   


 x[n]=21 
X (ej)ejnd
 
+  
 Sucient condition for conver gence: x[n] &lt; + 
n= 
 Although x[n] is discrete, X (ej) is continuous and periodic with period 2. 
 Convolution/multiplication duality: 
 
   
   
 y[n]= x[n]  h[n]
 Y (ej)= X (ej)H(ej) 
y[n]= x[n]w[n] 
  
Y (ej)=21 
W (ej )X (ej())d 
 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 2</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Computational Considerations 
	We now replace the Fourier transform expressions by the discrete Fourier 
transform expressions : 
 
      
      N1 
Xp [k]= x[n]e j 2 
N kn 0  k  N  1 
n=0 
Xp [k]= log{Xp [k]} 0  k  N  1 
N 1 
  xp [n]=1 Xp [k] ej 2 
N kn 0  n  N  1N 
k=0 
 Xp [k] is a sampled version of   X (ej). Therefore, 
 
  xp [n]= x[n + rN ] 
r= 
 	Likewise:  
cp [n]= c[n + rN ] 
r= 
wher e, N 1 
cp [n]=1 log |Xp [k]| ej 2 
N kn 0  n  N  1 N k=0 
 To minimize aliasing, N must be large. 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 17</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Hamming Window 
 2n
w[n]= 0.54  0.46cos N  1 , 0  n  N  1
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Rectangular Window 
w[n]=1, 0  n  N  1
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 4</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Example of Cepstral Analysis of Vowel 
(Rectangular Window) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 19</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Example of Cepstral Analysis of Vowel 
(Tapering Window) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 20</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Discrete Fourier Transform 
x[n]  X [k]= X (z) |
z=ej 2k nM 
Npoints Mpoints 
 N1   M
  X [k]= x[n]e j 2k n


 n=0 

 M1


M  x[n]= 1  
X [k]ej 2k n  M k=0 
In general, the number of input points, N, and the number of frequency samples, 
M,need notbethe same. 
 If M&gt;N , we must zero-pad the signal 
 If M&lt;N , we must time-alias the signal 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 10</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>An Example (contd) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 16</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Example of Cepstral Analysis of Fricative 
(Rectangular Window) 
6.345 Automat ic Speech Recogn ition (2003) Speech Signal Representaion 21</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>11
12</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture1112/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>27</slideno>
          <text>Human Language Entropy (Shannon, 1951) 
 An attempt to estimate language entropy of humans 
	Involved guessing next words in order to measur e subjects 
probability distribution 
 Letters were used to simplify experiments 
TH E R E I S N O R E V E R S E 
1 1 1 511 2 1 1 2 1 1 15 117 1 1 1 2 
ON A M OTO R C Y C L E A ... 
1 3 2 1 2 2 7 1 1 1 1 4111 1 1 3 ... 
    24  6  2 H =  P(i)log2 P(i) P(1) = 37 P(2) = 37 P(3) = 37 
  Shannon estimated H  1 bit/letter 
6.345 Automatic Speech Recognition Language Modelling 28</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Integration into Viterbi Search 
Preceding Following 
Words Words 
Bigrams can be eciently incorporated into Viterbi search using an 
intermediate node between words 
 Interpolated: Q (wi )=( 1  i ) 
 Back-o: Q (wi )= q(wi ) 
6.345 Automatic Speech Recognition Language Modelling 19 P(wj) P(wj|wi) 
Q(wi)</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Adaptive Language Models 
	Cache-based language models incorporate statistics of recently 
used words with a static language model 
P(wi |hi )= Pc (wi |hi )+ (1  )Ps (wi |hi ) 
	Trigger -based language models increase word probabilities when 
key words observed in history hi 
 Self triggers provide signicant information 
 Information metrics used to nd triggers 
 Incorporated into maximum entropy formulation 
6.345 Automatic Speech Recognition Language Modelling 41</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>V Interpolation Example 
1
P(wi |wi1)= 2f (wi |wi1)+ 1f (wi )+ 0
 
 
  
  x 
x x 
+ 2 
1 
0 1 
V f (w i ) f (w i |w i1) 
6.345 Automatic Speech Recognition Language Modelling 14</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Problems with n-grams 
 Unable to incorporate long-distance constraints 
 Not well suited for exible word order languages 
 Cannot easily accommodate 
 New vocabulary items 
 Alternative domains 
 Dynamic changes (e.g., discourse) 
 Not as good as humans at tasks of 
 Identifying and correcting recognizer errors 
 Predicting following words (or letters) 
 Do not captur e meaning for speech understanding 
6.345 Automatic Speech Recognition Language Modelling 30</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>1
2
345
6
789IBM Trigram Example (cont) 


61 62 
63 
64 
65 
66 role
thing
that
to
contact
parts
point
for
issues
and the next be
from two
in
to
are
with
were
requiring
still


being
during
I
involved
would
within
metting of
months &lt;&gt;
years
meetings
to
weeks
days
6.345 Automatic Speech Recognition Language Modelling 11</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Exponential Language Models 
 P(wi |hi ) modelled as product of weighted featur es fj (wihi ) 
j
jfj (wihi )
1 P(wi |hi )= Z(hi ) e 
wher e s are parameters, and Z(hi ) is a normalization factor 
 Binary-valued featur es can express arbitrary relationships 
e.g., fj (wihi )=
1 wi = A &amp; wi1= B 
0 else 
	When E(f (wh )) corresponds to empirical expected value, 
ML estimates for s correspond to maximum entropy distribution 
 ML solutions are iterative, and can be extremely slow 
 Demonstrated perplexity and WER gains on large vocabulary tasks 
6.345 Automatic Speech Recognition Language Modelling 40</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Evaluating Language Models
 Recognition accuracy 
 Qualitative assessment 
 Random sentence generation 
 Sentence reordering 
 Information-theor etic measur es 
6.345 Automatic Speech Recognition Language Modelling 20</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Context-Free Grammars (CFGs) 
VP 
NP 
V D N
display the ights 
 Language space dened by context-fr ee rewrite rules 
e.g., A = BC | a 
 More powerful representation than FSNs 
	Stochastic CFG rules have associated probabilities which can be 
learned automatically from a corpus 
 Finite coverage can present diculties for ASR
6.345 Automatic Speech Recognition Language Modelling 5</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Example of Word Clustering 
A A_M 
AFTERNOON 
AMERICAN AN ANY 
AUGUST 
AVAILABLE BE 
BOOK BOSTON 
CITY 
CLASS COACH 
CONTINENTAL COST DAY 
DELTA DOLLARS DOWNTOWN 
EASTERN ECONOMY EIGHT 
EIGHTY 
EVENING 
FARE 
FARES FIFTEEN FIFTY 
FIRST_CLASS FIVE FLY 
FORTY FOUR 
FRIDAY GET 
GIVE GO 
GROUND HUNDRED 
INFORMATION 
IT JULY KNOW 
LEAST 
MAKE MAY MEAL 
MEALS 
MONDAY MORNING NINE 
NINETY 
NONSTOP 
NOVEMBER O+CLOCK 
OH ONE 
ONE_WAY P_M 
PLANE RETURN 
ROUND_TRIP 
SATURDAY SCENARIO 
SERVED SERVICE SEVEN 
SEVENTY SIX 
SIXTY 
STOPOVER 
SUNDAY TAKE 
TELL THERE THIRTY 
THIS THREE 
THURSDAY TICKET 
TIME TIMES 
TRANSPORTATION TRAVEL 
TUESDAY TWENTY TWO 
U_S_AIR UNITED 
USED 
WILL ZERO 
NIL 4 6 0 2 
DALLAS_FORT_WORTH 
WEDNESDAY WASHINGTON 
FIND AIRCRAFT 
AIRPLANE LOWEST 
CHEAPEST 
THESE 
THOSE 
WOULD 
NEED OAKLAND 
BALTIMORE 
KIND 
TYPE EARLIEST 
LATEST 
WANT MOST SAN_FRANCISCO 
PITTSBURGH 
DALLAS PHILADELPHIA 
ATLANTA 
DENVER 
6.345 Automatic Speech Recognition Language Modelling 33</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>n n-gram Issues: Sparse Data (Jelinek, 1985) 
 Text corpus of IBM patent descriptions 
 1.5 million words for training 
 300,000 words used to test models 
 Vocabulary restricted to 1,000 most frequent words 
	23% of trigrams occurring in test corpus were absent from 
training corpus! 
	In general, a vocabulary of size V will have Vn-grams (e.g., 
20,000 words will have 400 million bigrams, and 8 trillion 
trigrams!) 
6.345 Automatic Speech Recognition Language Modelling 12</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>PCNG Example 
NT 2 NT 4 
NT 1 NT 3 NT 0 NT 0 
Please show me the cheapest ight from Boston to Denver 
NT 2 the NT 3 from NT 0 NT 4 
6.345 Automatic Speech Recognition Language Modelling 37</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>IBM Sentence Reordering 
would I report directly to you 
I would report directly to you 
now let me mention some of the disadvantages 
let me mention some of the disadvantages now 
he did this several hours later 
this he did several hours later 
this is of course of interest to IBM 
of course this is of interest to IBM 
approximately seven years I have known John 
I have known John approximately seven years 
these people have a fairly large rate of turnover 
of these people have a fairly large turnover rate 
in our organization research has two missions 
in our missions research organization has two 
exactly how this might be done is not clear 
clear is not exactly how this might be done 
6.345 Automatic Speech Recognition Language Modelling 24</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Word-Pair Grammars 
show  me me  all the  ights 
 the  restaurants 
 Language space dened by lists of legal word-pairs 
 Can be implemented eciently within Viterbi search 
 Finite coverage can present diculties for ASR 
	Bigrams dene probabilities for all word-pairs and can produce a 
nonzer o P(W ) for all possible sentences 
6.345 Automatic Speech Recognition Language Modelling 6</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Perplexity Examples 
Domain Size Type Perplexity 
Digits 11 All word 11 
Resour ce 1, 000 Word-pair 60 
Management Bigram 20 
Air Travel 2, 500 Bigram 29 
Understanding 4-gram 22 
WSJ Dictation 5, 000 Bigram 80 
Trigram 45 
20, 000 Bigram 190 
Trigram 120 
Switchboar d 23, 000 Bigram 109 
Human-Human Trigram 93 
NYT Characters 63 Unigram 20 
Bigram 11 
Shannon Letters 27 Human  2 
6.345 Automatic Speech Recognition Language Modelling 26</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>P r r 
N r n 
n r Good-T uring Example (Church and Gale, 1991) 
 GT estimate for an item occurring r times out of N is 

 r+1=(r +1 ) =
wher e nr is the number of items occurring r times 
	Consider bigram counts from a 22 million word corpus of AP news 
articles (273,000 word vocabulary) 
r n r r  
0 
1 
2 
3 
4 
5 74, 671, 100, 000 
2, 018, 046 
449, 721 
188, 933 
105, 668 
68, 379 0.0000270 
0.446 
1.26 
2.24 
3.24 
4.22 
6.345 Automatic Speech Recognition Language Modelling 18</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Sentence Reordering (Jelinek, 1991) 
 Scramble words of a sentence 
 Find most probable order with language model 
 Results with trigram LM 
 Short sentences from spontaneous dictation 
 63% of reordered sentences identical 
 86% have same meaning 
6.345 Automatic Speech Recognition Language Modelling 23</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>1
2
345
6
789IBM Trigram Example (Jelinek, 1997) 
The are to know the issues 
This will have this problems 
One the understand these the 
Two would do
A also get
Three do the
Please need use
In provide
We insert
 
 
96 write 
97 me 
98 resolve 


1639
1640
1641
problems
any
a
problem
them
all
necessary
data
information
above
other
time
people
operators
tools


jobs MVS 
old 


reception 
shop important 
6.345 Automatic Speech Recognition Language Modelling 10</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>PCNG Experiments 
 Air-Travel Information Service (ATIS) domain 
 Spontaneous, spoken language understanding 
 21,000 train, 2,500 development, 2,500 test sentences 
 1,956 word vocabulary 
Language Model #Rules # Params Perplexity 
Word Bigram 0 18430 21.87 
+ Compound Words 654 20539 20.23 
+ Word Classes 1440 16430 19.93 
+ Phrases 2165 16739 15.87 
PCNG Trigram 2165 38232 14.53 
PCNG 4-gram 2165 51012 14.40 
6.345 Automatic Speech Recognition Language Modelling 38</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Language Modelling for Speech Recognition 
 Introduction 
 n-gram language models 
 Probability estimation 
 Evaluation 
 Beyond n-grams 
6.345 Automatic Speech Recognition Language Modelling 1</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Language Entropy 
 The average logpr ob LP is related to the overall uncertainty of the 
language, quantied by its entropy 
1  
H =  lim P(W )log2 P(W ) 
n n W 
 If W is obtained from a well-behaved source (ergodic), P(W ) will 
conver ge to the expected value and H is 
1 1 H =  lim log2 P(W )  log2 P(W ) n&gt;&gt; 1 
n n n 
 The entropy H is a theor etical lower bound on LP 
1  1    lim P(W )log2 P(W )  lim P(W )log2 P(W ) 
n n n nW W 
6.345 Automatic Speech Recognition Language Modelling 27</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Perplexity vs. Error Rate (Rosenfeld et al., 1995) 
 Switchboar d human-human telephone conversations 
 2.1 million words for training, 10,000 words for testing 
 23,000 word vocabulary , bigram perplexity of 109 
 Bigram-generated word-lattice search (10% word error) 
Trigram Condition Perplexity % Word Error 
Trained on Train Set 92.8 49.5 
Trained on Train &amp; Test Set 30.4 38.7 
Trained on Test Set 17.9 32.9 
No Parameter Smoothing 3.2 31.0 
Perfect Lattice 3.2 6.3 
Other Lattice 3.2 44.5 
6.345 Automatic Speech Recognition Language Modelling 45</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Random Sentence Generation: 
Air Travel Domain Bigram 
Show me the ight earliest ight from Denver
How many ights that ight leaves around is the Eastern Denver
I want a rst class
Show me a reservation the last ight from Baltimor e for the rst
I would like to y from Dallas
I get from Pittsbur gh
Which just small
In Denver on October
I would like to San Francisco
Is ight ying
What ights from Boston to San Francisco
How long can you book a hundr ed dollars
I would like to Denver to Boston and Boston
Make ground transportation is the cheapest
Are the next week on AA eleven ten
First class
How many airlines from Boston on May thirtieth
What is the city of three PM
What about twelve and Baltimor e
6.345 Automatic Speech Recognition Language Modelling 21</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>wBack-O n-grams (Katz, 1987) 
 ML estimates are used when counts are large 
	Low count estimates are reduced (discounted) to provide 
probability mass for unseen sequences 
 Zero count estimates based on weighted (n  1)-gram 
 Discounting typically based on Good-T uring estimate 
 
 f (w2|w1) c(w1w2)    
P(w2|w1)= fd (w2|w1) &gt;c(w1w2) &gt; 0   q(w1)P(w2) c(w1w2)=0 
	Factor q(w1) chosen so that P(w2|w1)=1 
2 
 High order n-grams computed recursively 
6.345 Automatic Speech Recognition Language Modelling 16</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Clustering words 
 Many words have similar statistical behavior 
 e.g., days of the week, months, cities, etc. 
 n-gram performance can be improved by clustering words 
 Hard clustering puts a word into a single cluster 
 Soft clustering allows a word to belong to multiple clusters 
 Clusters can be created manually , or automatically 
 Manually created clusters have worked well for small domains 
 Automatic clusters have been created bottom-up or top-down 
6.345 Automatic Speech Recognition Language Modelling 31</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Example of LM Impact (Lee, 1988) 
 Resour ce Management domain
 Speaker -independent, continuous-speech corpus
 Sentences generated from a nite state network
 997 word vocabulary
 Word-pair perplexity  60, Bigram  20
 Error includes substitutions, deletions, and insertions
No LM Word-Pair Bigram 
%WordError Rate 29.4 6.3 4.2 
6.345 Automatic Speech Recognition Language Modelling 7</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>References 
	X. Huang, A. Acero, and H. -W. Hon, Spoken Language Processing , 
Prentice-Hall, 2001. 
	K. Church &amp; W. Gale, A Comparison of the Enhanced Good-T uring 
and Deleted Estimation Methods for Estimating Probabilities of 
English Bigrams, Computer Speech &amp; Language , 1991. 
	F. Jelinek, Statistical Methods for Speech Recognition, MIT Press, 
1997. 
	S. Katz, Estimation of Probabilities from Sparse Data for the 
Language Model Component of a Speech Recognizer . IEEE Trans. 
ASSP-35, 1987. 
 K. F. Lee, The CMU SPHINX System, Ph.D. Thesis, CMU, 1988. 
	R. Rosenfeld, Two Decades of Statistical Language Modeling: 
Wher e Do We Go from Here?, IEEE Proceedings, 88(8), 2000. 
 C. Shannon, Prediction and Entropy of Printed English, BSTJ, 1951. 
6.345 Automatic Speech Recognition Language Modelling 46</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>More References 
	L. Bahl et al., A Tree-Based Statistical Language Model for Natural 
Language Speech Recognition, IEEE Trans. ASSP-37 , 1989. 
	P. Brown et al., Class-based n-gram models of natural language, 
Computational Linguistics , 1992. 
	R. Lau, Adaptive Statistical Language Modelling, S.M. Thesis, MIT, 
1994. 
	M. McCandless, Automatic Acquisition of Language Models for 
Speech Recognition, S.M. Thesis, MIT, 1994. 
	R. Rosenfeld et al., Language Modelling for Spontaneous Speech, 
Johns Hopkins Workshop, 1995. 
	A. Stolcke, Entropy-based Pruning of Backo Language Models, 
http://www .nist.gov/speech/publications/darpa98/html/lm20/lm20.htm , 
1998. 
6.345 Automatic Speech Recognition Language Modelling 47</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Trigger Examples (Lau, 1994) 
	Triggers determined automatically from WSJ corpus 
(37 million words) using average mutual information 
 Top seven triggers per word used in language model 
Word Triggers 
stocks stocks index investors market 
dow average industrial 
political political party presidential politics 
election president campaign 
foreign currency dollar japanese domestic 
exchange japan trade 
bonds bonds bond yield treasury 
municipal treasury s yields 
6.345 Automatic Speech Recognition Language Modelling 42</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>N  
N 
r  
n 
N n 
N 
r r 
N n 
n r Good-T uring Estimate 
	Probability a word will occur r times out of N ,given  
p (r|)= r (1  )Nr 
 Probability a word will occur r +1 times out of N +1 
N +1 pN+1(r +1|)= r +1 pN (r|) 
 Assume nr words occuring r times have same value of  
r r+1 pN (r|)  pN+1(r +1|)  
 	Assuming large N , we can solve for  or discounted r 
 
 r+1  = P = r =(r +1 ) 
6.345 Automatic Speech Recognition Language Modelling 17</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Language Model Requirements 
 Coverage  Constraint 
 Understanding NLP  
 
 
6.345 Automatic Speech Recognition Language Modelling 3</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>)=  LM Formulation for ASR 
	Language model probabilities P(W ) are usually incorporated into 
the ASR search as early as possible 
	Since most searches are performed unidir ectionally , P(W ) is 
usually formulated as a chain rule 
P(W )=
n
i=1
P(wi | &lt;&gt;,...,w i1
n
i=1
P(wi |hi )
wher e hi = {&lt;&gt;,...,w i1} is the word history for wi 
 hi is often reduced to equivalence classes (hi ) 
P(wi |hi )  P(wi |(hi )) 
Good equivalence classes maximize the information about the 
next word wi given its history (hi ) 
	Language models which require the full word sequence W are 
usually used as post-pr ocessing lters 
6.345 Automatic Speech Recognition Language Modelling 8</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Language Modelling for Speech Recognition 
 	Speech recognizers seek the word sequence W which is most 
likely to be produced from acoustic evidence A 
P(W |A)= max P(W |A)  max P(A|W )P(W ) 
W W 
	Speech recognition involves acoustic processing, acoustic 
modelling, language modelling, and search 
	Language models (LMs) assign a probability estimate P(W ) to 
word sequences W = {w1,...,w n} subject to 
P(W )= 1 
W 
	Language models help guide and constrain the search among 
alternative word hypotheses during recognition 
6.345 Automatic Speech Recognition Language Modelling 2</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Word Class n-gram models 
 Word class n-grams cluster words into equivalence classes 
W = {w1,...,wn}{c1,...,c n} 
 If clusters are non-overlapping, P(W ) is approximated by 
n 
P(W )  P(wi |ci )P(ci | &lt;&gt;,...,c i1) 
i=1 
 Fewer parameters than word n-grams 
 Relatively easy to add new words to existing clusters 
 Can be linearly combined with word n-grams if desired 
6.345 Automatic Speech Recognition Language Modelling 34</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Random Sentence Generation: 
Air Travel Domain Trigram 
What type of aircraft
What is the fare on ight two seventy two
Show me the ights Ive Boston to San Francisco on Monday
What is the cheapest one way
Okay on ight number seven thirty six
What airline leaves earliest
Which airlines from Philadelphia to Dallas
Id like to leave at nine eight
What airline
How much does it cost
How many stops does Delta ight ve eleven oclock PM that go from
What AM
Is Eastern from Denver before noon
Earliest ight from Dallas
I need to Philadelphia
Describe to Baltimor e on Wednesday from Boston
Id like to depart before ve oclock PM
Which ights do these ights leave after four PM and lunch and &lt;unknown &gt;
6.345 Automatic Speech Recognition Language Modelling 22</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Finite-State Networks (FSN) 
show me all the ights 
give restaurants 
display 
 Language space dened by a word network or graph 
 Describable by a regular phrase structur e grammar 
A = aB | a 
 Finite coverage can present diculties for ASR
 Graph arcs or rules can be augmented with probabilities
6.345 Automatic Speech Recognition Language Modelling 4</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>j  j  
j  j 
V n-gram Interpolation 
 Probabilities are a linear combination of frequencies 
P(wi |hi )=
 f (wi |j (hi ))
 =1
1
e.g., P(w2|w1)= 2f (w2|w1)+ 1f (w2)+ 0
 s computed with EM algorithm on held-out data 
 Dier ent s can be used for dier ent histories hi 
c(w1)
c(w1)+ k
 Simplistic formulation of s can be used  =
 Estimates can be solved recursively: 
P(w3|w1w2)= 3f (w3|w1w2)+( 1  3)P(w3|w2) 
P(w3|w2)= 2f (w3|w2)+( 1  2)P(w3) 
6.345 Automatic Speech Recognition Language Modelling 13</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Entropy-based Pruning (Stolcke, 1998) 
 Uses KL distance to prune n-grams with low impact on entropy 
D(P  P)=
P(wi |hj )P(wi |hj )log P(wi |hj )i,j PP PP
PP
= eD(PP)  1
1. Select pruning threshold  
2. Compute perplexity increase from pruning each n-gram 
3. Remove n-grams below , and recompute backo weights 
 Example: resorting Broadcast News N -best lists with 4-grams 
 Bigrams Trigrams 4-grams Perplexity %WER 
0 11.1M 14.9M 0 172.5 32.9 
0 11.1M 14.9M 3.3M 163.0 32.6 
109 7.8M 9.6M 1.9M 163.9 32.6 
108 3.2M 3.7M 0.7M 172.3 32.6 
107 0.8M 0.5M 0.1M 202.3 33.9 
6.345 Automatic Speech Recognition Language Modelling 44</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>j  
j n i i Deleted Interpolation 
1. Initialize s (e.g., uniform distribution) 
2.	Compute probability P(j|wi ) that the jth frequency estimate was 
used when word wi was generated 
f (wi |j (hi )) P(wi |hi )= jf (wi |j (hi )) P(j|wi )= P(wi |hi ) j 
3. Recompute s for ni words in held-out data 
1  
 = P(j|wi ) 
4. Iterate until conver gence 
6.345 Automatic Speech Recognition Language Modelling 15</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>1 2 3 
1 2 
 n-gram Language Models 
	n-gram models use the previous n  1 words to represent the 
history (hi )= {wi1 ,...,w i(n1)} 
 Probabilities are based on frequencies and counts 
c(www ) e.g., f (w 3|w 1 w 2)= c(ww ) 
	Due to sparse data problems, n-grams are typically smoothed 
with lower order frequencies subject to 
P(w|(hi )) = 1 
w 
 Bigrams are easily incorporated in Viterbi search 
	Trigrams used for large vocabulary recognition in mid-1970 s and 
remain the dominant language model 
6.345 Automatic Speech Recognition Language Modelling 9</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Phrase Class n-grams (PCNG) (McCandless, 1994) 
	Probabilistic context-fr ee rules parse phrases 
W = {w1,...,w n}{u1,...,u m } 
 n-gram produces probability of resulting units 
	P(W ) is product of parsing and n-gram probabilities 
P(W )= Pr (W )Pn(U ) 
	Intermediate representation between word-based n-grams and 
stochastic context-fr ee grammars 
 Context-fr ee rules can be learned automatically 
6.345 Automatic Speech Recognition Language Modelling 36</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Language Model Pruning 
 n-gram language models can get very large (e.g., 6B/n-gram ) 
 Simple techniques can reduce parameter size 
 Prune n-grams with too few occurr ences 
 Prune n-grams that have small impact on model entropy 
 Trigram count-based pruning example: 
 Broadcast news transcription (e.g., TV, radio broadcasts) 
 25K vocabulary; 166M training words ( 1GB), 25K test words 
Count Bigrams Trigrams States Arcs Size Perplexity 
0 6.4M 35.1M 6.4M 48M 360MB 157.4 
1 3.2M 11.4M 2.2M 17M 125MB 169.4 
2 2.2M 6.3M 1.2M 10M 72MB 178.1 
3 1.7M 4.4M 0.9M 7M 52MB 185.1 
4 1.4M 3.4M 0.7M 5M 41MB 191.9 
6.345 Automatic Speech Recognition Language Modelling 43</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Bottom-Up Word Clustering (Brown et al., 1992) 
	Word clusters can be created automatically by forming clusters in 
a stepwise-optimal or greedy fashion 
	Bottom-up clusters created by considering impact on metric of 
merging words wa and wb to form new cluster wab 
 Example metrics for a bigram language model: 
 Minimum decrease in average mutual information 
 
I = P(wiwj )log2 P(wj |wi ) 
i,j P(wj ) 
 Minimum increase in training set conditional entropy 
 
H =  P(wiwj )log2 P(wj |wi ) 
i,j 
6.345 Automatic Speech Recognition Language Modelling 32</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Quantifying LM Complexity 
 One LM is better than another if it can predict an n word test 
corpus W with a higher probability P(W ) 
	For LMs representable by the chain rule, comparisons are usually 
based on the average per word logpr ob, LP 
1 1   LP =  log2 P(W )=  log2 P(wi |(hi )) n n i 
 A more intuitive representation of LP is the perplexity 
PP =2LP 
(a uniform LM will have PP equal to vocabulary size) 
 PP is often interpr eted as an average branching factor 
6.345 Automatic Speech Recognition Language Modelling 25</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Predictive Clustering (Goodman, 2000) 
 For word class n-grams : P(wi |hi )  P(wi |ci )P(ci |ci1 ...) 
 Predictive clustering is exact: P(wi |hi )= P(wi |hici )P(ci |hi ) 
 History , hi , can be cluster ed dier ently for the two terms 
	This model can be larger than the n-gram , but has been shown to 
produce good results when combined with pruning 
6.345 Automatic Speech Recognition Language Modelling 35</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Decision Tree Language Models (Bahl et al., 1989) 
 Equivalence classes represented in a decision tree 
 Branch nodes contain questions for history hi 
 Leaf nodes contain equivalence classes 
 Word n-gram formulation ts decision tree model 
 Minimum entropy criterion used for construction 
 Signicant computation requir ed to produce trees 
6.345 Automatic Speech Recognition Language Modelling 39</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Why do n-grams work so well? 
 Probabilities are based on data (the more the better) 
 Parameters determined automatically from corpora 
 Incorporate local syntax, semantics, and pragmatics 
	Many languages have a strong tendency towar d standar d word 
order and are thus substantially local 
	Relatively easy to integrate into forwar d search methods such as 
Viterbi (bigram) or A 
6.345 Automatic Speech Recognition Language Modelling 29</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>3
4</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture34new/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>11</slideno>
          <text>Fricative Durations
DurationProbability Density unadjusted for frequency
0.0 0.05 0.10 0.15 0.20 0.25 0.3002468 1 0 1 2 1 4UNVOICED
VOICED
Voiced fricatives tend to be shorter than unvoiced fricatives.
6.345 Automatic Speech Recognition Speech Sounds 12</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>What is this word?
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3
6.345 Automatic Speech Recognition Speech Sounds 33</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Some Examples
Affix
3Affix
2Affix
1Nucleus Inner
CodaOuter
CodaOuter
OnsetInnerOnset
crown kr a w n
edged flE J d
links lI 4 k s
dwarves dw a r v z
stick st I k
sixths sI k s T s
6.345 Automatic Speech Recognition Speech Sounds 43</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Word-Initial Consonants from MWP Dictionary
-of hy human sf sphere tr true
bbe Jjust sk school ts tsunami
bl black k can skl sclerosis tw twenty
br bring kl class skr screen ty tuesday
by beauty kr cross skw square Tthief
Cchild kw quite sky skewer Tr through
ddo ky curious sl slow Tw thwart
dr drive llike sm small Dthe
dw dwell m more sn snake vvery
ffor mw moire sp special vw voyager
fl oor my music spl split vy view
fr from n not spr spring w was
fy few p people spy spurious yyou
ggood pl place st state zzero
gl glass pr price str street zlzloty
gr great pw pueblo sw sweet zw zweiback
gw guava py pure S she Zgenre
hhe rright Sr shrewd
hw which sso t to
6.345 Automatic Speech Recognition Speech Sounds 39</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>/s/-Stop Durations
ptk01020304050607080VOT Duration (ms)
Unvoiced stops are unaspirated in / s/ stop sequences.
6.345 Automatic Speech Recognition Speech Sounds 22</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Assignment 2
6.345 Automatic Speech Recognition Speech Sounds 48</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Acoustic Realizations of /r/
rock curt car
/rak// k5t// kar/
6.345 Automatic Speech Recognition Speech Sounds 45</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Voicing Cues for Stops
There are many voicing cues for a stop.
6.345 Automatic Speech Recognition Speech Sounds 21</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>What is this word?
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
6.345 Automatic Speech Recognition Speech Sounds 15</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Spectrograms of Semivowels
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
we ye reed lee
/wi// yi// rid// li/
6.345 Automatic Speech Recognition Speech Sounds 31</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Words Containing / r/a n d/ l/
Affix
3Affix
2Affix
1Nucleus Inner
CodaOuter
CodaOuter
OnsetInnerOnset
rock ra k
crock kr a k
curt k5 t
cart ka r t
car ka r
lick lI k
bottle ba ,l t
kill kI l
6.345 Automatic Speech Recognition Speech Sounds 44</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Friendly Little Consonant Chart
"Somewhat more accurate, yet somewhat less useful."
Labial Alveolar Palatal VelarPlace of Articulation
Voicing:  Unvoiced  VoicedNasalFricative StopManner of Articulation/CRp b
f v
mDental
/CR
T D
n4s z S Zt d k gThe Semi-vowels:
is like an extremeyi
is like an extremewu
is like an extremelo
is like an extremer5
The Affricates:
is likeCt +S
is likeJd +ZThe Odds and Ends:
 h(unvoiced h)
 H(voiced h)
 F(flap)     F (nasalized flap)
 ?(glottal stop)Weak (Non-strident) Strong (Strident)Rob's
6.345 Automatic Speech Recognition Speech Sounds 14</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Spectrograms of Aricates and Aspirant
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
each huge
/iC// hyuJ/
6.345 Automatic Speech Recognition Speech Sounds 36</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>What is this word?
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
6.345 Automatic Speech Recognition Speech Sounds 24</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Spectrograms of Nasals
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
simmer sinner singer
/sIm5// sIn5// sI45/
6.345 Automatic Speech Recognition Speech Sounds 27</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Spectrograms of Unvoiced Fricatives
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
fee thief see she
/fi// Tif// si// Si/
6.345 Automatic Speech Recognition Speech Sounds 10</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>What is this word?
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
6.345 Automatic Speech Recognition Speech Sounds 37</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Acoustic Realizations of /l/
lick bottle kill
/lIk// batl// kIl/
6.345 Automatic Speech Recognition Speech Sounds 46</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Examples of Front and Back Velars
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
keep cot
/kip// kOt/
6.345 Automatic Speech Recognition Speech Sounds 23</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Examples of Stop Voicing Contrast
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
pop bob
/pap// bab/
6.345 Automatic Speech Recognition Speech Sounds 19</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Examples of Fricative Voicing Contrast
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
sue zoo face phase
/su// zu// fes// fez/
6.345 Automatic Speech Recognition Speech Sounds 13</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>What is this word?
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
6.345 Automatic Speech Recognition Speech Sounds 28</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Allophonic Variations at Syllable Boundaries
nitrate night rate
/na tret // nat ret /
6.345 Automatic Speech Recognition Speech Sounds 47</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Spectrograms of Unvoiced Stops
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4kHz kHz
008816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4kHz kHz
008816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
poop toot kook
/pup// tut// kuk/
6.345 Automatic Speech Recognition Speech Sounds 18</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Spectrograms of the Cardinal Vowels
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
kHz kHzWide Band Spectrogram
kHz kHz
012345678
012345678Time (seconds)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7kHz kHz
0 08 816 16Zero Crossing Rate
dB dBTotal Energy
dB dBEnergy -- 125 Hz to 750 Hz
Waveform
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
beet bat bott boot
/bit// b@t// bat// but/
6.345 Automatic Speech Recognition Speech Sounds 4</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Singleton Stop Durations
bdgptk01020304050607080VOT Duration (ms)
Voice onset times (VOTs) are longer for unvoiced stops.
6.345 Automatic Speech Recognition Speech Sounds 20</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Happy Little Vowel Chart
"So inaccurate, yet so useful."
SCHWAS:
Plain [ {] About [ {bat]
Front [ |] Roses [ roz|z]
Retroflex [ }] Forever [ f}Ev}]F2 Increases
F1 IncreasesMID LOW HIGHFRONT
BACKu/CR
O U
ua o^,{E@eIiTENSE = Towards Edgestends to be longer
LAX = Towards Centertends to be shorterRob's
ThinkF3ismightylow?Yourpal5
isthe
waytogo!
6.345 Automatic Speech Recognition Speech Sounds 7</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Fricative Energy
Average Total EnergyProbability Density unadjusted for frequency
-100 -90 -80 -70 -60 -50 -400.0 0.02 0.04 0.06NON-STRIDENT
STRIDENT
Strident fricatives tend to be stronger than non-strident fricatives.
6.345 Automatic Speech Recognition Speech Sounds 11</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>1
2</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture1/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>27</slideno>
          <text>Introduction   28 6.345 Automatic Speech Recognition
References (on reserve at Barker)
Huang, Acero, &amp; Hon, Spoken Language Processing , 
Prentice-Hall, 2001.
Jelinek, Statistical Methods for Speech Recognition , MIT 
Press, 1997.
Rabiner &amp; Juang, Fundamentals of Speech Recognition , 
Prentice-Hall, 1983.
Duda, Hart, &amp; Stork, Pattern Classification , Wiley &amp; Sons, 
2001.
Stevens, Acoustic Phonetics , MIT Press, 1998.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Introduction   26 6.345 Automatic Speech Recognition
Assignments
 There will be 9 weekly assignments
Problems that expand on the lecture material
Lab assignments to reinforce the lecture material
Assignments are due the following week on Wednesday
 Lab work will be done in the computer lab 
Lab sign-up (on the course web page) is necessary
Solutions will be provided</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Introduction   27 6.345 Automatic Speech Recognition
Term Project
Investigate a contrasting condition in an ASR experiment
We will provide different recognizers and domains for you to 
select from, and will work with you to select a topic
You choose:
Evaluation condition: e.g., phonetic  classification, word recognition)
Database (e.g., TIMIT, RM, Jupiter, Aurora, )
Recognizer (e.g., Sphinx, Summit, GMTK, )
Contrasting condition (e.g., signal representation, acoustic model, 
language model)
Requirements:
Proposal
Experiments (the bulk of the work)
Write-up
 Presentation on extended last day of class</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Introduction   22 6.345 Automatic Speech Recognition
(Real) Data Improves Performance (Weather Domain)
Longitudinal evaluations show improvements
Collecting real data improves performance:
Enables increased complexity and improved robustness for 
acoustic and language models
Better match than laboratory recording conditions
Users come in all kinds051015202530354045
Apr May Jun Jul Aug Nov Apr Nov MayError Rate (%)
110100
Training Data (x1000)Word
Data
97 99 98</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Introduction   23 6.345 Automatic Speech Recognition
But We Are Far from Done!
Corpus                               Speech            Lexicon  Word Error   Human Error       
Type                Si ze Rate (%)         Rate (%)
Digit Strings (phone) spontaneous 10 0.3 0.009Resource Management read 1000 3.6 0.1ATIS spontaneous 2000 2 --Wall Street Journal read 64000 6.6 1Radio News mixed 64000 13.5 --Switchboard (phone) conversation 10000 19.3 4Call Home (phone) conversation 10000 30 --Corpus                               Speech            Lexicon  Word Error   Human Error        
Type                Si ze Rate (%)         Rate (%)
Digit Strings (phone) spontaneous 10 0.3 0.009Resource Management read 1000 3.6 0.1ATIS spontaneous 2000 2 --Wall Street Journal read 64000 6.6 1Radio News mixed 64000 13.5 --Switchboard (phone) conversation 10000 19.3 4Call Home (phone) conversation 10000 30 --</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Introduction   19 6.345 Automatic Speech Recognition
Conversational Interfaces: The Next Generation
Enables us to converse with machines (in much the same 
way we communicate with one another) in order to create, 
access, and manage information and to solve problems
Augments speech recognition technology with natural 
language technology in order to understand the verbal input
Can engage in a dialogue with a user during the interaction
Uses natural language to speak the desired response
Is what Hollywood and every futurist says we should 
have!</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Introduction   5 6.345 Automatic Speech Recognition
Automatic Speech Recognition
An ASR system converts the speech signal into words
The recognized words can be
The final output, or
The input to natural language processingASR
SystemASR
System
Speech
SignalRecognized
Words</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Introduction   9 6.345 Automatic Speech Recognition
Examples Contrasting 
Read and Spontaneous Speech (Navigation Domain)
Filled and unfilled pauses: read, spontaneous
Lengthened words: read, spontaneous
False starts: read, spontaneous</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Introduction   6 6.345 Automatic Speech Recognition
Application Areas for Speech Based Interfaces 
Mostly input (recognition only)
Simple command and control
Simple data entry (over the phone)
Dictation
Interactive conversation (understanding needed)
Information kiosks 
Transactional processing
Intelligent agents</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Introduction   3 6.345 Automatic Speech Recognition
Speech interfaces are ideal for information access and 
management when:
 The information space is broad and complex,
 The users are technically naive, or Only telephones are available.Speech interfaces are ideal for information access and 
management when:
 The information space is broad and complex,
 The users are technically naive, or Only telephones are available.Virtues of Spoken Language
Natural: Requires no special training 
Flexible: Leaves hands and eyes free
Efficient: Has high data rate
Economical: Can be transmitted/received inexpensively</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Introduction   7 6.345 Automatic Speech Recognition
Basic Speech Recognition Challenges
Co-articulation
Speaker independence
Dialect variations
Non-native speakers
Spontaneous speech
Disfluencies
Out-of-vocabulary words
Language modelling
Noise robustness</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Introduction   4 6.345 Automatic Speech Recognition
Phonological: gas shortage
fish sandwich
Phonotactic: blit vnuk
Contextual: It is easy to recognize speech
It is easy to wreck a nice beachSyntactic: I am flying to Chicago tomorrow  
tomorrow I flying Chicago am toDiverse Sources of Constraint for 
Spoken Language Communication
Phonetic: let us pray
lettuce sprayAcoustic: human vocal tract
Semantic: Is the baby crying
Is the bay bee crying</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Introduction   12 6.345 Automatic Speech Recognition
* There are, of course, many exceptions.ASR Trends*: Then and Now
before mid 70's mid 70s - mid 80s after mid 80s
Recognition whole-word and sub-word units sub-word units
Units: sub-word units
Modeling heuristic and template matching mathematical 
Approaches: ad hoc and formal
rule-based and deterministic and probabilistic 
declarative data-driven and data-driven
Knowledge heterogeneous homogeneous homogeneous 
Representation: and complex and simple and simple
Knowledge intense knowledge embedded in automatic 
Acquisition: engineering simple structure learningbefore mid 70's mid 70s - mid 80s after mid 80s
Recognition whole-word and sub-word units sub-word units
Units: sub-word units
Modeling heuristic and template matching mathematical 
Approaches: ad hoc and formal
rule-based and deterministic and probabilistic 
declarative data-driven and data-driven
Knowledge heterogeneous homogeneous homogeneous 
Representation: and complex and simple and simple
Knowledge intense knowledge embedded in automatic 
Acquisition: engineering simple structure learning</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Introduction   15 6.345 Automatic Speech Recognition
Important Lessons Learned
Statistical modeling and data-driven approaches have 
proved to be powerful
Research infrastructure is crucial:
Large amounts of linguistic data
Evaluation methodologies
Availability and affordability of computing power lead to 
shorter technology development cycles and real-time systems 
Performance-driven paradigm accelerates technology 
development
Interdisciplinary collaboration produces enhanced 
capabilities (e.g., spoken language understanding)</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Introduction   18 6.345 Automatic Speech Recognition
Demo: Simple Telephone Transactions
Developed by SpeechWorks International (there are others)
Shipping cost information for Fedex (1-800-GO-FEDEX)
Provides information on:
* Package types
* Source and destination zip codes* Weight, size, value* Service type
Handles all US rate information calls
Automated Brokerage System for E*Trade
Supports quotes and trades 
* Using symbols or names 
* For stocks, options, and mutual funds
Users can barge in at any time
Nationwide deployment for over 450,000 customers</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Introduction   8 6.345 Automatic Speech Recognition
The acoustic realization of a phoneme depends strongly on 
the context in which it occurs
TEA
 BEATEN
 TREE
 STEEP
 CITY
Frequency
TimePhonological Variation Example</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Introduction   25 6.345 Automatic Speech Recognition
Course Logistics
 Lectures:Two sessions/week,  1.5 hours/session
 Labs: All week during school hours
Grading
9 Assignments 45%
2 Quizzes 30%
Term Project (about 4 weeks) 25%</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Introduction   14 6.345 Automatic Speech Recognition
Examples of ASR Performance
Speaker-independent, continuous-
speech ASR now possible
Digit recognition over the telephone 
with word error rate of 0.3%
Error rate cut in half every two years 
for moderate vocabulary tasks
Error for spontaneous speech more 
than twice that of read speech
Conversational speech, involving 
multiple speakers and poor acoustic environment, remains a challenge
Tens of hours of training data to 
port to a different domain
Statistical modeling using automatic 
training achieves significant advances
0.1110100
1987
1989199119931995199719992001
YearWord Error Rate (%)Digits 1K, Read
2K, Sponaneous 20K, Read
64K, Broadcast 10K, Conversational</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Introduction   20 6.345 Automatic Speech Recognition
A Conversational System Architecture
DISCOURSE 
CONTEXTDISCOURSE 
CONTEXTDIALOGUE
MANAGEMENTDIALOGUE
MANAGEMENTDATABASEGraphs
&amp; Tables
SPEECH
RECOGNITIONSPEECH
RECOGNITIONSpeech
WordsLANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGMeaning
RepresentationMeaning
Representation
MeaningLANGUAGE
GENERATIONLANGUAGE
GENERATIONSPEECH
SYNTHESISSPEECH
SYNTHESIS
SpeechSentence</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Introduction   13 6.345 Automatic Speech Recognition
Speech Recognition: Where Are We Now?
High performance, speaker-independent speech recognition 
is now possible
Large vocabulary (for cooperative speakers in benign 
environments)
Moderate vocabulary (for spontaneous speech over the phone)
Commercial recognition systems are now available
Dictation (e.g., Dragon,  IBM, L&amp;H, Philips)
Telephone transactions (e.g., AT&amp;T, Nuance, Philips, 
SpeechWorks, TellMe, etc.)Scansoft
When well-matched to applications, technology is able to 
help perform real work</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Introduction   16 6.345 Automatic Speech Recognition
Applying    Constraints
Recognized
Words
SearchSearchMajor Components in a Speech Recognition System
Speech recognition is the problem of deciding on
How to represent the signal
How to model the constraints
How to search for the most optimal answerRepresentationRepresentationSpeech
SignalLexical
ModelsLexical
ModelsAcoustic
ModelsAcoustic
ModelsLanguage
ModelsLanguage
ModelsTraining DataTraining Data</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Introduction   24 6.345 Automatic Speech Recognition
Course Outline
RepresentationRepresentationSpeech
SignalRecognized
Words
SearchSearchLexical
ModelsLexical
ModelsAcoustic
ModelsAcoustic
ModelsLanguage
ModelsLanguage
ModelsAcoustic Theory of
Speech ProductionAcoustic Theory ofSpeech Production
Signal
RepresentationSignalRepresentationProperties of
Speech SoundsProperties ofSpeech Sounds
Vector Quantization
&amp; ClusteringVector Quantization&amp; ClusteringPattern
RecognitionPatternRecognitionLanguage
ModelingLanguageModeling
Search
AlgorithmsSearchAlgorithms
Hidden Markov
ModelingHidden MarkovModelingSegmental
ModelsSegmentalModelsAcoustic-
Phonetic
ModelingAcoustic-
Phonetic
Modeling
Robust 
ASRRobust ASR
Adaptation
AdaptationFinite-State
TransducersFinite-StateTransducersParalinguistic Information
Speech Understanding
Multi-Modal InterfacesParalinguistic Information
Speech Understanding
Multi-Modal Interfaces
Graphical 
ModelsGraphical Models</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Introduction   21 6.345 Automatic Speech Recognition
Demo: Conversational Interface
Jupiter weather information system
Access through telephone
500 cities worldwide
Harvest weather information from the Web several times daily
J u p i t e r
A conversational interface for on-line
weather information over the phone.
1-888-573-8255
(outside the USA: 1-617-258-0300 )
http://www.sls.lcs.mit.edu/jupiter
Spoken Language Systems Group,
MIT Laboratory for Computer Science</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Introduction   10 6.345 Automatic Speech Recognition
Sometimes Real Data will Dictate
Technology Requirements (City Name Domain)
Technology Required Example
Simple word spotting Um, Braintree
Complex word spotting Eh yes, Avis rent-a-car in 
BostonHello, please Brighton, uh, can I have the number 
of Earthscape, in, uh, on
Nonantum Street
Speech understanding Woburn, uh, Somerville. 
I'm sorry</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Introduction   2 6.345 Automatic Speech Recognition
SpeechSpeech
TextTextRecognitionSpeechSpeech
TextTextSynthesis
Understanding GenerationCommunication via Spoken Language
MeaningMeaningHuman
ComputerInput Output</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Introduction   1 6.345 Automatic Speech Recognition
Introduction to Automatic Speech Recognition
Lectures: Jim Glass &amp; guest lecturers
Introduction to ASR
Problem definition
State of the art examples
Course overview
Lecture outline
Assignments
Term Project
GradingLecture # 1
Session 2003</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Introduction   17 6.345 Automatic Speech Recognition
Demo: Continuous Dictation
IBM ViaVoice running on a ThinkPad 
Trained for a quiet office (classroom performance not optimal)</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Introduction   11 6.345 Automatic Speech Recognition
Parameters that Characterize the 
Capabilities of ASR Systems
Parameters Range
Speaking Mode: Isolated word to continuous speechSpeaking Style: Read speech to spontaneous speech
Enrollment: Speaker-dependent to speaker-independent
Vocabulary: Small (&lt;20 words) to large (&gt;50,000 words)Language Model: Finite-state to context-sensitivePerplexity: Small (&lt;10) to large (&gt;200)SNR: High (&gt;30dB) to low (&lt;10dB)Transducer: Noise-cancelling microphone to cell phoneParameters Range
Speaking Mode: Isolated word to continuous speech
Speaking Style: Read speech to spontaneous speech
Enrollment: Speaker-dependent to speaker-independent
Vocabulary: Small (&lt;20 words) to large (&gt;50,000 words)
Language Model: Finite-state to context-sensitive
Perplexity: Small (&lt;10) to large (&gt;200)
SNR: High (&gt;30dB) to low (&lt;10dB)
Transducer: Noise-cancelling microphone to cell phone</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>7
8</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture7/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>Gaussian Distributions 
	Gaussian PDFs are reasonable when a featur e vector can be 
viewed as perturbation around a reference Probability Density 
0.1 0.2 0.3 0.4 0.0
4 2 0 2 4
x

 Simple estimation procedur es for model parameters 
 Classication often reduced to simple distance metrics 
 Gaussian distributions also called Normal
6.345 Automatic Speech Recognition Parame tric Classie rs 2</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Density Estimation 
 Used to estimate the underlying PDF p(x|i ) 
 Parametric methods: 
 Assume a specic functional form for the PDF 
 Optimize PDF parameters to t data 
 Non-parametric methods: 
 Determine the form of the PDF from the data 
 Grow parameter set size with the amount of data 
 Semi-parametric methods: 
 Use a general class of functional forms for the PDF 
 Can vary parameter set independently from data 
 Use unsupervised methods to estimate parameters 
6.345 Automatic Speech Recognition Pattern Classication 9</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Gaussian Distributions: 
Multi-Dimensional Properties 
	If the ith and jth dimensions are statistically or linearly 
independent then E(xixj )= E(xi )E(xj ) and ij =0 
	If all dimensions are statistically or linearly independent, then 
ij =0 i = j and  has non-zer o elements only on the diagonal 
	If the underlying density is Gaussian and  is a diagonal matrix, 
then the dimensions are statistically independent and 
d 
p(x)= p(xi ) p(xi )  N (i,ii ) ii = i 2 
i=1 
6.345 Automatic Speech Recognition Parame tric Classie rs 10</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>ML Estimation: Alternative Distributions
[s] Log Duration: Normal DistributionProbability Density 
0.4 0.6 0.8 1.0 1.2 1.4 0.2
 0.0
-3.5 -3.0 -2.5 -2.0 -1.5 
log Dur ation (sec) 
6.345 Automatic Speech Recognition Parame tric Classie rs 8</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Maximum Likelihood Parameter Estimation 
	Maximum likelihood parameter estimation determines an 
estimate  for parameter  by maximizing the likelihood L() of 
observing data X = {x1,...,x n} 
 =arg max L() 

 Assuming independent, identically distributed data 
L()= p(X|)= p(x1,...,x n|)=
n 
p(xi |) 
i=1 
 ML solutions can often be obtained via the derivative 
 L()=0  
 For Gaussian distributions log L() is easier to solve 
6.345 Automatic Speech Recognition Parame tric Classie rs 4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Bayes Risk 
 Dene cost function ij and conditional risk R(i |x): 
 ij is cost of classifying x as i whenitisreally j 
 R(i |x) is the risk for classifying x as class i 
R(i |x)=
M 
ijP(j |x) 
j=1 
	Bayes risk is the minimum risk which can be achieved: 
Choose i if R(i |x) &lt;R(j |x) j = i 
 Bayes risk corresponds to minimum P(error|x) when 
 All errors have equal cost (ij =1,i = j) 
 There is no cost for being correct (ii =0) 
j=iP(j |x)=1  P(i |x)
 R(i |x)=
6.345 Automatic Speech Recognition Pattern Classication 7</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Kullback-Liebler Distance 
	Can be used to compute a distance between two probability mass 
distributions, P(zi ),and Q (zi ) 
P(zi )D(P  Q )= P(zi )log Q (zi )  0 
i 
 Makes use of inequality log x  x  1 
iP(zi ) iP(zi )  1) = Q (zi )  P(zi )=0 P(zi )log Q (zi )  P(zi )( Q (zi ) 
i 
 Known as relative entropy in information theory 
	The divergence of P(zi ) and Q (zi ) is the symmetric sum 
D(P  Q )+ D(Q  P) 
6.345 Automatic Speech Recognition Pattern Classication 4</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Gaussian ML Estimation: One Dimension
 The maximum likelihood estimate for  is given by: 
 (xi  )2 
n n 
L()= p(xi |)=  1 e 22 
i=1 i=1 2 
1   
log L()=  22(xi  )2  n log 2 
i 
 log L() 1  
(xi  )=0 =  2 
i 
1  
 = xi n i 
 The maximum likelihood estimate for  is given by: 
2=1  
(xi    )2 
n i 
6.345 Automatic Speech Recognition Parame tric Classie rs 5</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Gaussian Classier: i=2I
Fordistributions with acommon covariance structur ethedecision
regions arehyper -planes.
-1 0 1 2 3 4 5-1012345
024024
024024
6.345Automatic SpeechRecognition Parame tricClassie rs17</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>3 Class Classication (Atal &amp; Rabiner , 1976) 
 Distinguish between silence, unvoiced, and voiced sounds 
 Use 5 featur es: 
 Zero crossing count 
 Log energy 
 Normalized rst autocorr elation coecient 
 First predictor coecient, and 
 Normalized prediction error 
 Multivariate Gaussian classier , ML estimation 
 Decision by squar ed Mahalanobis distance 
	Trained on four speakers (2 sentences/speaker), 
tested on 2 speakers (1 sentence/speaker) 
6.345 Automatic Speech Recognition Parame tric Classie rs 21</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>References 
	Huang, Acero, and Hon, Spoken Language Processing , 
Prentice-Hall, 2001. 
	Duda, Hart and Stork, Pattern Classication , John Wiley &amp; Sons, 
2001. 
	Atal and Rabiner , A Pattern Recognition Approach to 
Voiced-Unvoiced-Silence Classication with Applications to 
Speech Recognition, IEEE Trans ASSP , 24(3), 1976. 
6.345 Automatic Speech Recognition Parame tric Classie rs 25</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Gaussian Classier: i = 
 Each class has the same covariance structur e  
 The equivalent discriminant functions are: 
1 gi (x)=  2(x  i )t 1(x  i )+ log P(i ) 
	If each class is equally likely , the minimum error decision rule is 
the squared Mahalanobis distance 
 The discriminant functions remain linear expressions: 
gi (x)= wit x + i0 
wher e 1i wi = 
1 ti0=  2 i 1i +log P(i ) 
6.345 Automatic Speech Recognition Parame tric Classie rs 18</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Probability Basics 
	Discr ete probability mass function (PMF): P(i ) 
P(i )=1 
i 
	Continuous probability density function (PDF): p(x) 
p(x)dx =1 
	Expected value: E(x) 
E(x)= xp(x)dx 
6.345 Automatic Speech Recognition Pattern Classication 3</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Multivariate Gaussian Classier 
p(x)  N (, ) 
	Requir es a mean vector i , and a covariance matrix i for each of 
M classes {1,  ,M } 
	The minimum error discriminant functions are of form: 
gi (x)= log P(i |x)= log p(x|i )+log P(i ) 
1 d 1 gi (x)=  2(x  i )t  
i 1(x  i )  2 log 2  2 log |i | +log P(i ) 
	Classication can be reduced to simple distance metrics for many 
situations 
6.345 Automatic Speech Recognition Parame tric Classie rs 15</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Bayes Theorem 
PDF
 
 p(x|1) p(x|2) 
x 
Dene: {i } aset of M mutually exclusive classes 
P(i ) a priori probability for class i 
p(x|i ) PDF for featur e vector x in class i 
P(i |x) a posteriori probability of i given x 
From Bayes Rule: P(i |x)= p(x|i )P(i ) 
p(x) 
M 
wher e p(x)= p(x|i )P(i ) 
i=1 
6.345 Automatic Speech Recognition Pattern Classication 5</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Parametric Classiers 
 Gaussian distributions 
 Maximum likelihood (ML) parameter estimation 
 Multivariate Gaussians 
 Gaussian classiers 
6.345 Automatic Speech Recognition Parame tric Classie rs 1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Bayes Decision Theory 
	The probability of making an error given x is: 
P(error|x)=1  P(i |x) if decide class i 
	To minimize P(error|x) (and P(error)): 
Choose i if mathP(i |x) &gt;P(j |x) j = i 
 For a two class problem this decision rule means: 
Choose 1 if p(x|1)P(1) &gt;p(x|2)P(2) ;else 2 p(x) p(x) 
 This rule can be expressed as a likelihood ratio: 
Choose 1 if p(x|1) P(2) 
p(x|2) &gt;P(1) ; else choose 2 
6.345 Automatic Speech Recognition Pattern Classication 6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Pattern Classication 
 Introduction 
 Parametric classiers 
 Semi-parametric classiers 
 Dimensionality reduction 
 Signicance testing 
6.345 Automatic Speech Recognition Pattern Classication 1</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Diagonal Covariance Matrix: =2I
=20
02
3-Dimensional PDF PDFContour
-4
-2
0
2
4-4-2024
-4
-2
0
2
4 -4 -2 0 2 4-4-2024
6.345Automatic SpeechRecognition Parame tricClassie rs11</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Pattern Classication 
Goal: To classify objects (or patterns) into categories (or classes) 
 Featur e 
Extraction  Classier  
Observation Feature Vector Class 
s x i 
Types of Problems: 
1.	Supervised: Classes are known beforehand, and data samples of 
each class are available 
2.	Unsupervised: Classes (and/or number of classes) are not known 
befor ehand, and must be inferr ed from data 
6.345 Automatic Speech Recognition Pattern Classication 2</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Gaussian ML Estimation: One Dimension
[s] Duration (1000 utterances, 100 speakers)Probability Density 
4 6 8 10 2
 0
0.05 0.10 0.15 0.20 0.25 0.30 
Duration (sec) 
(   40 ms)   120 ms,  
6.345 Automatic Speech Recognition Parame tric Classie rs 6</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Multivariate ML Estimation 
	The ML estimates for parameters  = {1,..., l } are determined by 
maximizing the joint likelihood L() of a set of i.i.d. data 
X = {x1,..., xn} 
n 
L()= p(X|)= p(x1,  , xn|)= p(xi |) 
i=1 
 To nd   we solve L()= 0,or  log L()= 0 
   = { 1 ,  , l } 
 The ML estimates of  and  are: 
1    
 = xi =1 (xi   )t  )(xi   n ni i 
6.345 Automatic Speech Recognition Parame tric Classie rs 14</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Gaussian Distributions: One Dimension
 One-dimensional Gaussian PDFs can be expressed as: 
 (x  )2 
p(x)=  1 e 22  N (,2)
2 
	The PDF is center ed around the mean 
 = E(x)= xp(x)dx 
	The spread of the PDF is determined by the variance 
2= E((x  )2)= (x  )2p(x)dx 
6.345 Automatic Speech Recognition Parame tric Classie rs 3</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Discriminant Functions 
 Alternative formulation of Bayes decision rule 
	Dene a discriminant function, gi (x), for each class i 
Choose i if gi (x) &gt;gj (x) j = i 
 Functions yielding identical classication results: 
gi (x)= P(i |x) 
= p(x|i )P(i ) 
= log p(x|i )+log P(i ) 
 Choice of function impacts computation costs 
	Discriminant functions partition featur e space into decision 
regions , separated by decision boundaries 
6.345 Automatic Speech Recognition Pattern Classication 8</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Gaussian Classier: i = 2I 
	Each class has the same covariance structur e: statistically 
independent dimensions with variance 2 
 The equivalent discriminant functions are: 
gi (x)= x  i 2 
+log P(i )22 
	If each class is equally likely , this is a minimum distance classier , 
a form of template matching 
 The discriminant functions can be replaced by the following linear 
expression: 
gi (x)= wit x + i0 
1wher e wi =  1
2 i and i0=  22 iti +log P(i ) 
6.345 Automatic Speech Recognition Parame tric Classie rs 16</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Gaussian MAP Estimation: One Dimension
 For a Gaussian distribution with unknown mean : 
p(x|)  N(,2) p()  N(0,2 
0) 
 MAP estimates of  and x are given by: 
p(|X )=
n 
p(xi |)p()  N(n,2 
n) 
i=1 
p(x|)p(|X )d  N(n,2 +2 
n) p(x|X )=
n2 
0 
n2 
0 +2  + 
n22 
0 2 
n 
0 +2 =
2 
02 
n2 
0 +2 wher e n =
 As n increases, p(|X ) conver ges to ,and p(x|X ) conver ges to 
the ML estimate  N(,2) 
6.345 Automatic Speech Recognition Parame tric Classie rs 24</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>General Covariance Matrix: ij=0
=21
11
3-Dimensional PDF PDFContour
-4
-2
0
2
4-4-2024
-4
-2
0
2
4 -4 -2 0 2 4-4-2024
6.345Automatic SpeechRecognition Parame tricClassie rs13</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Gaussian Classier: i Arbitrary 
 Each class has a dier ent covariance structur e i 
 The equivalent discriminant functions are: 
1 1 gi (x)=  2(x  i )t  
i 1(x  i )  2 log |i | +log P(i ) 
 The discriminant functions are inher ently quadratic : 
gi (x)= xt Wi x + wit x + i0 
1wher e Wi =  2 
i 1 
wi =i1 i 
1 t 1 i0=  2 i  
i 1 i  2 log |i | +log P(i ) 
6.345 Automatic Speech Recognition Parame tric Classie rs 19</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Diagonal Covariance Matrix: ij=0 i=j
=20
01
3-Dimensional PDF PDFContour
-4
-2
0
2
4-4-2024
-4
-2
0
2
4 -4 -2 0 2 4-4-2024
6.345Automatic SpeechRecognition Parame tricClassie rs12</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>ML Estimation: Alternative Distributions
[s] Duration: Gamma DistributionProbability Density 
4 6 8 10 2
 0
0.05 0.10 0.15 0.20 0.25 0.30 
Duration (sec) 
6.345 Automatic Speech Recognition Parame tric Classie rs 7</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Gaussian Classier: i Arbitrary 
For distributions with arbitrary covariance structur es the decision 
regions are dened by hyper -spher es. 
3 
2 
1 
0 
-1 
-2 
-3 
-1 0 1 2 3 4 5 
6.345 Automatic Speech Recognition Parame tric Classie rs 20</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Gaussian Distributions: Multiple Dimensions
 A multi-dimensional Gaussian PDF can be expressed as: 
1 
p(x)= (2)d/ 1
2||1/2 e  2(x  )t 1(x  ) 
 N (, ) 
 d is the number of dimensions 
 x = {x1,...,x d } is the input vector 
	 = E(x)= {1,..., d } is the mean vector 
	= E((x  )(x  )t ) is the covariance matrix with elements ij , 
inverse 1 , and determinant || 
 ij = ji = E((xi  i )(xj  j )) = E(xixj )  ij 
6.345 Automatic Speech Recognition Parame tric Classie rs 9</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Maximum A Posteriori Parameter Estimation
	Bayesian estimation approaches assume the form of the PDF 
p(x|) is known, but the value of  is not 
 Knowledge of  is contained in: 
 An initial a priori PDF p() 
 A set of i.i.d. data X = {x1,...,x n} 
 The desired PDF for x is of the form 
p(x|X )= p(x, |X )d = p(x|)p(|X )d 
 The value  that maximizes p(|X ) is called the maximum a 
posteriori (MAP) estimate of  
n 
p(|X )=p(X|)p() =  p(xi |)p()p(X ) i=1 
6.345 Automatic Speech Recognition Parame tric Classie rs 23</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>9
10</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-345-automatic-speech-recognition-spring-2003/resources/lecture9/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>19</slideno>
          <text>Tree Representation (with cumulative scores) 
S 1 
 
A 2 
   
B 3 
    
C 7 
  
E F E D G F G 5 8 6 4 5 10 8 
       
7 H 9 H 8 F 7 H 6 H 11 H 9 
8 H 
H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 20</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Search Space Tree 
S 
 
A 
   
B 
    
C 
  
E F E D G F G 
      
H H F H H H H 
H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 12</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>5 
 
6     Best First Search Example 
7 
5 F 8 E 6 D 4 G G 5
7 8 7 6
S 1 
 
A 2  
B 3  
C 
     S 1 
 
B 3 
 
E 
H H F H H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 21</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Tree Representation (with future estimates) 
S 5 
 
A 5 
   
B 6 
    
C 9 
  
E F E D G F G 7 9 8 6 6 10 9 
       
7 H 9 H 8 F 8 H 6 H 11 H 9 
8 H 
H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 25</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Time-Synchronous DP Example 
S  A 
 B  
 E 
 
 D 
 C  
 F  
 
 G  
 H S  B 
 G  H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 30</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>7 8 6 6 
 
8  
6  
7  
8 
     
 N -Best Search Example 
S 5 
 
A 5  
B 6  
C 
     S 5 
 
B 6 
  
A 5 
   
E E 
H H 9 
7 F 9 E E 8 D D 6 G G 6
7 H H 8 F F 8 H H 6
8
H8
H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 28</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Dynamic Time Warping (DTW) 
	Objective: an optimal alignment between variable length 
sequences T = {t1,..., tN } and R = {r1,..., rM } 
	The overall distortion D(T , R) is based on a sum of local distances 
between elements d(ti, rj ) 
	A particular alignment warp, , aligns T and R via a 
point-to-point mapping,  =(t,r ), of length K 
tt (k)  rr (k)1  k  K 
 The optimal alignment minimizes overall distortion 
D(T , R) = min D(T , R) 
 
K1  
D(T , R)= d(tt (k), rr (k))mkM k=1 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 6</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Breadth First Search Example 
S 
 
A  
B  
C 
 
E  
F  
E  
D  
G  
F  
G S 
 
A 
 
E  
F  
B 
 
E  
G  
C 
 
F  
G  
D 
H H       
H H H H F F H H H H H H 
H H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 17</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Graph Representations of Search Space
 Search spaces can be represented as directed graphs 
S  A 
 B 
 
C  D  
 E 
 
  F 
  G  
 
 H 
 Paths through a graph can be represented with a tree 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 11</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Computing DTW Alignment 
5 3 4 4 1
1 3 4 4 5 4 3 3 2 2 1 1 3 3 3 2 
1 3 2 4 
C B D A G F E H 
m 
2 
1 
1 [m,n]
m-1
m-2
n-2 n-1 n
S
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 10</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>References 
	Cormen, Leiserson, Rivest, Introduction to Algorithms , 2nd 
Edition, MIT Press, 2001. 
	Huang, Acero, and Hon, Spoken Language Processing , 
Prentice-Hall, 2001. 
	Jelinek, Statistical Methods for Speech Recognition. MIT Press, 
1997. 
 Winston, Articial Intelligence, 3rd Edition, Addison-W esley , 1993. 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 33</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Inadmissible Search Variations 
 Can use a beam width to prune current hypotheses 
 Beam width can be static or dynamic based on relative score 
	Can use an approximation to a lower bound on Alookahead for 
N -best computation 
 Search is inadmissible, but may be useful in practice 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 31</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Estimating Future Scores 
	Partial path scores, (1,i), can be augmented with future 
estimates, (i), of the remaining cost 
 = (1,i)+ (i) 
	If (i) is an under estimate of the remaining cost, additional paths 
can be pruned while maintaining admissibility of search 
 Asearch uses 
 Best-rst search strategy 
 Pruning 
 Futur e estimates 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 24</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Dynamic Programming (DP) 
 DP algorithms do not employ a greedy strategy 
	DP algorithms typically take advantage of optimal substructur e 
and overlapping subpr oblems by arranging search to solve each 
subpr oblem only once 
 Can be implemented eciently: 
 Node j retains only best path cost of all (i,j) 
 Previous best node id needed to recover best path 
 Can be time-synchr onous or asynchr onous 
	DTW and Viterbi are time-synchr onous searches and look like 
breadth-rst with pruning 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 29</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Graph Search Algorithms 
 Iterative methods using a queue to store partial paths 
	On each iteration the top partial path is removed from the 
queue and is extended one level 
 New extensions are put back into the queue 
 Search is complete when goal is reached 
 Depth of queue is potentially unbounded 
 Weighted graphs can be searched to nd the best path 
 Admissible algorithms guarantee nding the best path 
	Many speech-based search problems can be congur ed to 
proceed time-synchr onously 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 13</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Breadth First Search 
 Searches space by pursuing all paths in parallel 
 Path extensions are put on bottom of queue 
 Queue is not reordered or pruned 
 Queue can grow rapidly in spaces with many paths 
 Not generally used to nd the best path 
 Can be made much more eective with pruning 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 16</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Pruning Partial Paths 
	Both greedy and dynamic programming algorithms can take 
advantage of optimal substructur e: 
 Let (i,j) be the best path between nodes i and j 
	If k is a node in (i,j): 
(i,j)= {(i,k),(k,j)} 
	Let (i,j) be the cost of (i,j) 
(i,j) = min ((i,k)+ (k,j)) 
k
 Solutions to subpr oblems need only be computed once 
	Sub-optimal partial paths can be discar ded while maintaining 
admissibility of search 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 22</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Best First Search 
 Used to search a weighted graph 
	Uses greedy or step-wise optimal criterion, wher eby each iteration 
expands the current best path 
	On each iteration, the queue is resorted accor ding to the 
cumulative score of each partial path 
	If path scores exhibit monotonic behavior , (e.g., d(ti, rj )  0), 
search can terminate when a complete path has a better score 
than all active partial paths 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 18</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Tree Representation (with node scores) 
S 1 
 
A 1 
   
B 2 
    
C 6 
  
E F E D G F G 3 6 3 1 2 3 1 
       
2 H 1 H 2 F 3 H 1 H 1 H 1
1
H 
H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 19</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Depth First Search Example 
S 
 
A  
B  
C 
 
E  
F S 
 
A 
 
E  
F  
E  
D  
G  
B 
 
E  
D  
G  
F  
G  
F  
C 
 
F  
G  
G 
H H       
H H H H F F H H H H H H 
H H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 15</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>DTW Issues 
	Endpoint constraints: 
t (1) = r (1) = 1 t (K)= N r (K)= M 
	Monotonicity: 
t (k +1 )  t (k) r (k +1 )  r (k) 
 Path weights, mk, can inuence shape of optimal path 
	Path normalization factor , M, allows comparison between 
dier ent warps (e.g., with dier ent lengths) 
K 
M = mk 
k=1 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 7</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Depth First Search 
 Searches space by pursuing one path at a time 
 Path extensions are put on top of queue 
 Queue is not reordered or pruned 
 Not well suited for spaces with long dead-end paths 
 Not generally used to nd the best path 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 14</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Word-Based Template Matching 
 Decision 
Rule 
Spoken  
Word 
Refer ence 
Templates Pattern 
Similarity 
Output 
Word Word 
 Whole word representation: 
 No explicit concept of sub-wor d units (e.g., phones) 
 No across-wor d sharing 
 Used for both isolated- and connected-wor d recognition
 Popular in late 1970s to mid 1980s
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 2
 Featur e 
Measur ement</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Template Matching Mechanism 
	Test pattern, T , and reference patterns, {R1,..., RV },are 
represented by sequences of featur e measur ements 
	Pattern similarity is determined by aligning test pattern, T ,with 
reference pattern, Rv , with distortion D(T , Rv ) 
	Decision rule chooses reference pattern, R , with smallest 
alignment distortion D(T , R) 
R = arg min D(T , Rv ) 
v 
	Dynamic time warping (DTW) is used to compute the best possible 
alignment warp, v , between T and Rv , and the associated 
distortion D(T , Rv ) 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Dynamic Time Warping &amp; Search 
 Dynamic time warping 
 Search 
 Graph search algorithms 
 Dynamic programming algorithms 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 1</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>DTW Issues: Global Constraints
slope = 2 
(1,M) 
                  1slope =                   2 Legal range 
(1,1) (N,1) (N,M) 
                  1slope =                   2 
slope = 2 
Local constraints exclude portions of search space 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 9</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>9 8 6 
 
6   ASearch Example 
9 
7 F F 9 E E 8 D 6 G G 6
8 6
S 5 
 
A 5  
B 6  
C 
       S 5 
 
B 6 
 
E 
F H H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 26</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Beam Search Example 
S  A 
 B  
 E 
 
 D 
 C  C  F  
 G  
 H S  B 
 G  H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 32</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>8 6 5 
 
6    Best First Search with Pruning 
7 
5 F F 8 E E 6 D 4 G G 5
7 7 6
S 1 
 
A 2  
B 3  
C 
       S 1 
 
B 3 
 
E 
H F H H 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 23</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>N -Best Search 
 Used to compute top N paths 
 Can be re-scor ed by more sophisticated techniques 
 Typically used at the sentence level 
 Can use modied A search to rank paths 
 No pruning of partial paths 
 Completed paths are removed from queue 
	Can use a threshold to prune paths, and still identify 
admissibility violations 
 Can also be used to produce a graph 
	Alternative methods can be used to compute N -best outputs (e.g., 
asynchr onous DP) 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 27</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>DTW Issues: Local Continuity 
m [n,m] m [n,m] 
TYPE I m - 1 TYPE III m - 1 
m - 2 m - 2 
n - 2n - 1 n n - 2n - 1 n 
m [n,m] m [n,m] 
TYPE II m - 1 TYPE IV m - 1 
m - 2 m - 2 
n - 2n - 1 n n - 2n - 1 n 
Local constraints determine alignment exibility 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 8</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Alignment Example 
m m
M
Reference M
1 1
0Warp 
n
1 N
Test 
n
01 N
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Digit Alignment Examples 
Match Mismatch 
6.345 Automatic Speech Recognition Dynamic Time Warping &amp; Search 5</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
