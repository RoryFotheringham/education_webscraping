<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/</course_url>
    <course_title>Signals, Systems and Inference</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Electrical Engineering </list>
      <list>Signal Processing </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 18 Power Spectral Density
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec18/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>y[.] obtained by passing x[.] through 
resonant 2nd-order filter H(z), 
poles at 0.95e^{j /3} 
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>iid signal x[n], uniform in [-0.5,+0.5] 
2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>0 0.2 0.4 0.6 0.8 100 0.2 0.4 0.6 0.8 10
v/(2p) v/(2p)   
 
1234M 16, T 50
1234M 16, T 200v/(2p) v/(2p)Periodogram  averaging (illustrating 
the Einstein-Wiener-Khinchin theorem) 
M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
00 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
v/(2p) v/(2p) v/(2p) v/(2p) 
M = 4, T = 50 4 
3 
2 
1 
0 M = 4, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
7</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>v    
   Extracting the portion of x(t) in a 
specified frequency band 
x(t) H(jv) y(t) 
1 H(jv)  
-v0 v0
4</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu  
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
9</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Power Spectral Density (PSD) 
6.011, Spring 2018 
Lec 18 
1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Periodogram  averaging (illustrating 
the Einstein-Wiener-Khinchin theorem) 
M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
00 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
v/(2p) v/(2p) v/(2p) v/(2p) 
M = 4, T = 50 4 
3 
2 
1 
0 M = 4, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
v/(2p) 
M = 16, T = 50 4 
3 
2 
1 
0 
v/(2p) v/(2p) 
M = 16, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
v/(2p) 
8</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 14 LMMSE Estimation, Orthogonality
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec14/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>Applying orthogonality  gives the 
normal equations 
Eh
Ye-L
j=1 aj Xej
Xei i
=0
2 CX1X1 CX1X2  CX1XL3 2 a13 2 CX1Y3 
6664C
X2X1 CX2X2  CX2XL a2 CX2Y7 6 7 6 77 6 7 6 77 65 4 75=6475... . . . ............
C
XLX1CXLX2  CXLXL aL CXLY
(CXX)a = cXY
MMSE: C2 
Y -cY X(CXX)-1 cXY = C2 
Y -cY X.a 
8</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
9</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LMMSE estimation, orthogonality 
6.011, Spring 2018 
Lec 14 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 4 State-Space Models
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec4/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Exponential unit sample response 
h[n]=  n-1u[n-1] + d [n]h[n] 2.0 
1.5 
1.0 
0.5 
0.0-5 0 5 10 15 20 
n b = 2.0
n = 0.8d = 1.0
2</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Delay-adder-gain system 
x[n] 
+p[n] 
+y[n] 1 1 
-0.51 1.5 q2[n]
q1[n]D 
D 
4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Defining properties of DT 
state-space models 
 
q[n+ 1] = f q[n],x[n],n
 
y[n]= g q[n],x[n ],n
State evolution property
Instantaneous output property
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>General transfer function of a causal 
DT LTI system with distinct poles 
x[n] y[n] d 
b1
z -n1
bL
z -nL+
3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>State-Space Models 
6.011, Spring 2018 
Lec 4 
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
6</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 3 Energy Spectral Density
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec3/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>Zero lag (n=0): 
Parseval , Rayleigh, Plancherel 
1 Z
p[0] = X
x[k]v[k]= P (ej)d 2 -k 
so
1 Z 
j)V (e -j)dX 
x[k]v[k]= X(e2 -k 
5</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Similarly  
... if y[n]= h  x[n]t h e n
j) S yy(ej)= Y (ej)Y (e 
j) = H(ej)X(ej)X(e j)H(e 
2 j)  j) = H(e  S
xx(e 
9</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Energy of x[.] in a specified band 
x[n] H(ej) 
H(ej) y[n] 
1 
-0 0 
10</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Magnitude spectrum of noisy signal 
15</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.011: Signals, Systems &amp; 
Inference 
Lec 3 
Energy spectral density 
1</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>ESD of noisy signal 
16</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>+ unit-intensity white Gaussian noise
14</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>ESD of noise-free signal 
13</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Heart rate variability 
ECG signal 
(a) 
0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 1.2 ECG amplitude (mV) 
54 56 58 60 62 64 66 Time (sec) 
Instantaneous HR signal 
45 50 55 60 65 70 75 80 Time (sec) (b) 
1.1 1.15 1.2 1.25 1.3 x(t) (beats/sec) 
Power spectrum 
(c) 
0 10 20 30 40 50 60 70 80 90 D
xx (e ) j 
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 Frequency (Hz) 1 
17</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Magnitude spectrum of 
noise-free signal 
12</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
18</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Noise-free signal 
11</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 13 Vector Picture for First- and Second-Order Statistics, MMSE and LMMSE Estimation
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec13/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
7</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>A geometric picture 
Think of X and Y as vectors, with inner product E[XY ] 
X = E[X.1] : inner product of X and random variable 1
E[X2] : squared length of X 
X = X -X : vector dierence between X and random variable Xe
OX : length of Xe
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Orthogonality 
E[XY ]=0
Correlation is 0, but not uncorrelated!
Uncorrelated = zero covariance,i . e . , E[XY ]= E[X]E[Y ]
6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Vector picture for first- and 
second-order statistics; MMSE and LMMSE estimation 
6.011, Spring 2018 
Lec 13 
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Covariance and correlation 
Covariance: eX,Y = E[(X -X)(Y -Y )] 
= E[XY ] -XY 
Correlation| {z }
rX,Y 
Shorthand notation: eXY ,rXY 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Geometric interpretation of 
correlation coefficient 
Y -mY
sY
u = cos -1r X -mX
sX
5</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 23 Neyman-Pearson Testing, Signal Detection
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec23/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Testing for diabetes 
 World Health Organization. All rights reserved. This content is excluded from our Creative Commons license. For more 
information, see https://ocw.mit.edu/help/faq-fair-use/ 
Screening for Type 2 Diabetes, WHO 2003 
4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Likelihood ratio test (LRT) 
implemention of MAP rule 
H1  
&gt; p1.fR|H (r|H1) p0.fR|H (r|H0)&lt; 
H0  
H1  
(r)= fR|H (r|H1) 
fR|H (r|H0) &gt; 
&lt; 
H0  p0 
p1 = 
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Terminology 
 prevalence (p1) 
 (conditional ) probability of detection, 
sensitivity , true positive rate, recall 
 specificity, true negative rate 

 (conditional) probability of false alarm, false 
positive rate (= 1 specificity) 
 (conditional) pr
obability of a miss, false 
negative rate (= 1  sensitivity) 
 positive predictive value, 
precision 
 negative predictive value 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Testing for prostate cancer 
Courtesy of Elsevier, Inc., https://www.sciencedirect.com . Used with permission. 
For clinically significant cancer, MP-MRI was more sensitive (93%) 
than TRUS-biopsy (48%) and less specific (41%) for MP-MRI vs 96%  
for TRUS-biopsy. 5.9% of 740 patients reported serious adverse events, 
including 8 cases of sepsis. Ahmed et al., Lancet Feb 2017 
5</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Neyman-Pearson testing. 
Signal detection 
6.011, Spring 2018 
Lec 23 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 7 Full Modal Solution, Asymptotic Stability, Reachability and Observability
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec7/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Asymptotic stability of DT system 
In order to have q[n] ! 0 for all q[0] , we require 
{|i| &lt; 1}L
1 
i.e., all eigenvalues (natural frequencies)
inside unit circle
7</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Modal solution of CT system ZIR 
q(t)= LX
ivie&gt;it 
X 1 
with the weights {i}L determined by the initial condition: 1 
L
q(0) = ivi 
1 
2</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Underlying structure of LTI DT state-
space system with L distinct modes 
x[n] y[n] d 
b1
z -n1
bL 
z -nL+1
L
12</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Full modal solution, asymptotic 
stability, reachability and observability 
6.011, Spring 2018 
Lec 7 
1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Modal solution of DT system ZIR 
Could parallel CT development, but lets proceed dierently: 
2 A10 0  0 3 
0 A20  0 6 76 7
A[ v1 v2  vL ]=[ v1 v2  vL ] 6 76. . . . . 7
. . . . .6
. .. . . 74 5 
0 0
0  AL
or AV = V
or A = VV -1
or An =(VV -1) (VV -1)= VnV -1
5</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
14</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 8 Matrix Exponential, ZIR+ZSR, Transfer Function, Hidden Modes, Reaching Target States
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec8/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Matrix exponential, 
ZIR+ZSR, transfer function, hidden modes, reaching target states 
6.011, Spring 2018 
Lec 8 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Underlying structure of LTI DT state-
space system with L distinct modes 
x[n] y[n] d 
b1
z -n1
bL 
z -nL+1
L
3</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
14</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Decoupled structure of CT LTI system in 
modal coordinates 
x(t) y(t) d 
b1l1
s -n1
bLlL
s -nL+
9</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Key properties of matrix exponential 
A.0 e = I 
d At At AtA e = Ae = e dt 
At1 At2 A(t1+t2)e e = e 
A1 A2 A1+A 2but e 6 e = e 
unless t
he two matrices commute 
11</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 15 Normal Equations, Random Processes
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec15/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
11</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Applying orthogonality  gives the 
normal equations 
Eh
Ye-L
j=1 aj Xej
Xei i
=0
2 CX1X1 CX1X2  CX1XL3 2 a13 2 CX1Y3 
6664C
X2X1 CX2X2  CX2XL a2 CX2Y7 6 7 6 77 6 7 6 77 65 4 75=6475... . . . ............
C
XLX1CXLX2  CXLXL aL CXLY
(CXX)a = cXY
MMSE: C2 
Y -cY X(CXX)-1 cXY = C2 
Y -cY X.a 
6</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Random process 

X(t; c) Amplitude 
c 
t1 t 
9</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Normal equations 
Random processes 
6.011, Spring 2018 
Lec 15 
1</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Signal ensemble for outcomes a,b,c,d; 
&amp; determination of RXX(t1,t2) 
t 
t 
t 
t X(t) = Xa(t) 
X(t) = Xb(t) 
X(t) = Xc(t) 
X(t) = Xd(t) 
t1 t2
10</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Estimating mean vector and 
covariance matrix from data 
Given N independent measurements: Xi ,i =1, ,N
N
Estimate of mean: X = 1 X
Xi bN 1 
N1 X
Estimate of covari an ce: CbXX = ( Xi  bX)(Xi  bX)T
N  1 1
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Geometric picture 
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Zestimates 
 Zillow. All rights reserved. This content is excluded from our Creative Commons license.  
For more information, see https://ocw.mit.edu/help/faq-fair-use/
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Zillow (founded 2006) 
 Zillow. All rights reserved. This content is excluded from our Creative Commons license.  
For more information, see https://ocw.mit.edu/help/faq-fair-use/
2</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Random variable 
Real line 
X(c) 
c 
8</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 12 Probabilistic Models, Random Variables
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec12/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Joint pdf 
 The MathWorks, Inc. All rights reserved. This content is excluded from our Creative Commons license. For more information, 
see https://ocw.mit.edu/help/faq-fair-use/ MultivariateNormalGaussian-Matlab 
4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Random variable 
Real line  
X(c) 
c 
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Sample space and events 
Sample space  
Collection of 
outcomes (event) A specic 
outcome c 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Probabilistic models,  
random variables 
6.011, Spring 2018 
Lec 12 
1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Marginal pdfs 
This image is in the public domain. Source: Wikimedia Commons 
5</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
6</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 2 Transforms
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec2/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>Convolution in time to 
multiplication in frequency 
Putting together frequency response, 
spectral content, and superposition, we find 
y[n]=h  x[n] 
in the time domain translates to 
Y (ej)=H(ej)X(ej) 
in the frequency domain. 
8</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Classes of DT signals that have DTFTs 
Absolutely summable (finite action, have 
continuous spectra) 
 or not, but  
Square summable (finite energy, spectra 
have discontinuities) 
 or not, but  
Bounded (finite amplitude, spectra involve 
generalized functions like impulses) 
7</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
10</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Using frequency response to specify 
response to sinusoidal inputs 
x[n]= A cos(0n + ) 
y[n]= |H(ej0 )| A cos 
0n +  + \H(ej0 ) 
4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.011: Signals, Systems &amp; 
Inference 
Lec 2 
Transforms 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 22 Hypothesis Testing
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec22/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Binary hypothesis testing (example) 
6</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Binary hypothesis testing (example) 
7</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Hypothesis testing 
6.011, Spring 2018 
Lec 22 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Again choosing between 
H=H0 and H=H1 but now given R=r, 
for min P(error|R=r) 
H1  
&gt; P (H1|R = r) P (H0|R = r)&lt; 
H0  
Pick whichever hypothesis has 
maximum a posteriori probability 
3</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Binary hypothesis testing (example) 
PM PFA p1PM p0PFA + = P(error) 
8</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu  
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
10</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Implementing the 
maximum a posteriori (MAP) rule 
H1  
&gt; P (H1|R = r) P (H0|R = r)&lt; 
H0  
H1  
&gt; p1.fR|H (r|H1) p0.fR|H (r|H0)&lt; 
H0  
4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Choosing between H=H0 and H=H1
with minimum P(error) 
P (H0 is true) = P (H = H0)= P (H0)= p0 
P (H1 is true) = P (H = H1)= P (H1)= p1 
) choose more probable hypothesis for min P (error)
H1  
&gt; P (H1) P (H0)&lt; 
H0  
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Likelihood ratio test (LRT) 
implemention of MAP rule 
H1  
&gt; p1.fR|H (r|H1) p0.fR|H (r|H0)&lt; 
H0  
H1  
(r)= fR|H (r|H1)
fR|H (r|H0)&gt; 
&lt; 
H0  p0
p1= 
5</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Terminology 
prevalence (p1)
(conditional ) probability of detection,
sensitivity , true positive rate, recall
specificity, true negative rate
(conditional) probability of false alarm, false
positive rate (= 1 specificity)
(conditional) probability of a miss, false
negative rate (= 1  sensitivity)
positive predictive value, precision
negative predictive value
9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 19 Einstein-Wiener-Khinchin Theorem, PSD Applications, Modeling Filters
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec19/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Einstein-Wiener-Khinchin theorem, 
PSD applications, modeling filters 
6.011, Spring 2018 
Lec 19 
1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Periodogram  averaging (illustrating 
the Einstein-Wiener-Khinchin theorem) 
M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
00 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
v/(2p) v/(2p) v/(2p) v/(2p) 
M = 4, T = 50 4 
3 
2 
1 
0 M = 4, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
v/(2p) 
M = 16, T = 50 4 
3 
2 
1 
0 
v/(2p) v/(2p) 
M = 16, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
v/(2p) 
5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>0 0.2 0.4 0.6 0.8 100 0.2 0.4 0.6 0.8 10
v/(2p) v/(2p)   
 
1234M 16, T 50
1234M 16, T 200v/(2p) v/(2p)Periodogram  averaging (illustrating 
the Einstein-Wiener-Khinchin theorem) 
M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
00 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
v/(2p) v/(2p) v/(2p) v/(2p) 
M = 4, T = 50 4 
3 
2 
1 
0 M = 4, T = 200 4 
3 
2 
1 
00 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 
4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Heart rate variability 
ECG signal 
(a) 
0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 1.2 ECG amplitude (mV) 
54 56 58 60 62 64 66 Time (sec) 
Instantaneous HR signal 
45 50 55 60 65 70 75 80 Time (sec) (b) 
1.1 1.15 1.2 1.25 1.3 x(t) (beats/sec) 
Power spectrum 
(c) 
0 10 20 30 40 50 60 70 80 90 D
xx (e ) j 
1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 Frequency (Hz) 1 
7</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu  
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
9</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Respiratory model 
cf. Khoos 
textbook for N=1 
6</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 21 Wiener Filtering Illustrations
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec21/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
7</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Wiener deconvolution of a noisy 
blurred image 
Mathworks blog posts by: 
Prof. Stan Reeves, ECE Dept., Auburn University 
Reeves, Stan. " Digital image processing using MATLAB: reading 
image files". MathWorks.  Sept. 27, 2011. 
Reeves, Stan. " Image deblurring  Wiener filter ." MathWorks. 
Nov. 2, 2007. 
6</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Unconstrained Wiener filter structure 
-mx my 
+ + x[n] h[] y[n] 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>E.g.: Wiener deconvolution 
of a noisy blurred image** 
Two-dimensional convolution + noise: 
x[k, l]=P P
j g[i, j]y[k -i, l -j]+v[k, l]i 
**From 2007 Mathworks blog post by 
 Prof. Stan Reeves, ECE Dept., Auburn University 
5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>E.g.: Wiener  deconvolution  
of a noisy blurred signal 
v[n] 
y[n] 
r[n] x[n] H[z] G[z] y[n] + 
Known, stable system Wiener flter 
4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Wiener filtering illustrations 
6.011, Spring 2018 
Lec 21 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Unconstrained Wiener filter solution 
-mx my 
y[n] H(ej) = Dyx(ej) 
Dxx(ej) + + x[n] 
3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 11 State Feedback, Observer-Based Feedback
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec11/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Observer-based controller 
w[n] 
q[n] 
A, b, cT+y[n] p[n] x[n] 
[n] Plant +
y[n] 
gTq[n] 
A, b, cT[n]q y[n] -
Observer +
B 
6</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>y[n]
1[n]w[n]
[n
 +A good model 
x[n] bq[n]
A, b, cT, d by[n]
3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Observer configuration 
w[n] 
x[n] q[n] 
A, b, cTy[n] 
+
Z[n] Plant 
y[n] 
q[n] 
A, b, cT[n]q y[n] -+
Observer 
B 
4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>State feedback 
x[n] 
p[n] 
gT+ A, b, cT
q[n] 
5</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Control of inverted Pendulum 
Observer-based controller: 
State feedback control: 0.1
0.05
0
0 2 8 10 4 6
Time (sec)
p(t) = 0, v(t) = 0, Z(t) = 0
x(t) generated bydirect state feedbackPendulum
angle (q1)
-0.050.15
1
0.5
0
0 2 8 10 4 6
Time (sec)Controller
input (x)
-0.51.5
1
0.5
0
0 2 8 10 4 6
Time (sec)Pendulum
angle (q1)
-0.51.5
1
0.5
0
0 2 4 6 8 10
Time (sec)Controller
input (x)
-0.51.5p(t) = 0, v(t) = 0, Z(t) = 0x(t) generated byobserver-based feedback /
1 = -7, / 2 = -18
/1 = 14, / 2 = 5Actual
Estimate
7</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>State feedback, observer-based 
feedback 
6.011, Spring 2018 
Lec 11 
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>System (plant) 
w[n] 
x[n] y[n] 
1[n] q[n] 
A, b, cT, d +
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 17 LTI Filtering of WSS Processes
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec17/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>iid signal x[n], uniform in [-0.5,+0.5] 
120 -0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5 
0 20 40 60 80 100 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LTI filtering of WSS processes 
6.011, Spring 2018 
Lec 17 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>1200   DTFT magnitude |X(ej)|, 0 to 2 
1000 0 1 2 3 4 5 6 7 
0 200 400 600 800 
3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>0 0.2 0.4 0.6 0.8 101
0 0.2 0.4 0.6 0.8 101
v/(2p) v/(2p)   
     
0 0.2 0.4 0.6 0.8 101234M  4, T  50
0 0.2 0.4 0.6 0.8 101234M  4, T  200
234M 16, T 50
234M 16, T 200v/(2p) v/(2p)Transform magnitudes for 4 
realizations of a 1 Bernoulli process 
M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
0 M = 1, T = 50 4 
3 
2 
1 
00 0.5 1 0 0.5 1 0 0.5 1 0 0.5 1 
v/(2p) v/(2p) v/(2p) v/(2p) 
= = = = 
4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
5</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 20 Wiener Filtering
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec20/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Wiener filtering 
6.011, Spring 2018 
Lec 20 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Unconstrained Wiener filter solution 
-mx my
y[n] H(ej) = Dyx(ej) 
Dxx(ej) + + x[n] 
3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Compared with static LMMSE estimator 
-mx my
x[n] y[n] H(ej) = Dyx(ej) 
Dxx(ej) + + 
-mXmY
X cXY (CXX)-1 T Y + +
4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Unconstrained Wiener filter structure 
-mx my
+ + x[n] h[] y[n]
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 24 Matched Filtering
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec24/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>On-off signaling in noise 
6
d Gen.
p(t)
h(t)n(t)
Dec.1010011011 0 0 1
0 1 1 1 0110 1 0 00 10 200 400 600 800 1000 1200-20-10
20 200 400 600 800 1000 120002
0 200 400 600 800 1000 1200-1-2
2
1
0
0 200 400 600 800 1000 1200010</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Matched filtering 
6.011, Spring 2018 
Lec 24 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Matched filter performance 
f(g|H0) f(g|H1)
g = a r[n]s[n]g sU
EE
n PM PFA
3</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Antipodal signaling 
7
d Gen.
Dec.
0 200 400 600 800 1000 1200-20-10
20 200 400 600 800 1000 120002
0 200 400 600 800 1000 1200-2-2
2
0
0 200 400 600 800 1000 12000101010011011001
10 1 0 0 1 1 0 1 1 0 0 1p(t)
n(t)
h(t)</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
12</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Matched filter properties 
Matched filter output in noise-free case (and before
sampling) is the deterministic autocorrelation of the signal:
g[n]=R ss[n]
Matched filter frequency response magnitude accentuates
frequencies where signal has strength relative to (spectrally
flat) noise
Matched filter frequency response phase cancels signal
phase characteristic to allow all components to contribute atsampling time
Matched filter maximizes SNR of sample fed to thresholdtest
5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Matched filtering for detecting 
known signal in white Gaussian noise 
g[n] g[0] r[n] 
LTI, h[] Threshold g 
n = 0 H1  
H0  7 
6 
2</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Pulse compression for radar 
Read the simulation example from
https://en.wikipedia.org/wiki/Pulse_compression 
8</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 16 Wide-sense Stationary Processes, LTI Filtering of WSS Processes
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec16/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>y=h*x, with h[n] = (0.5)n u[n] 
8</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Random process 

X(t; c) Amplitude 
c 
t1 t 
2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>y=h*x, with h[n] = [n] - [n-1] 
7</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Xa(t)
 Xb(t)
 Xc(t)
 Xd(t)Signal ensemble for outcomes a,b,c,d; 
&amp; determination of RXX(t1,t2) 
t 
t 
t 
t X(t) = xa(t) 
X(t) = xb(t) 
X(t) = xc(t) 
X(t) = xd(t) 
t1 t2
3</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
10</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>|H| when h[n]=(0.5)n u[n] 
9</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>iid signal x[n], uniform in [-0.5,+0.5] 
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Wide-sense stationary processes; 
LTI filtering of WSS processes 
6.011, Spring 2018 
Lec 16 
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>y=h*x, with h[n] = [n] + [n-1] 
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Courtesy of Alex Albright. Used with permission. 
Weather plot was generated with code adapted from Bradley Boehmke. 
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 1 Introduction
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec1/</lecture_pdf_url>
      <lectureno>0</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>Blood pressure regulation  
(Guyton 1972) 
Courtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission. 
5</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
15</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Blood pressure regulation +++ 
Courtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission. 
8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.011: Signals, Systems &amp; 
Inference 
MIT, Spring 2018 
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Weather prediction 
Courtesy of NOAA. This image is in the public domain. 
2</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The interventions 
medicationsnutrition fluids (infusion pumps) 
ventilator 
controls 
head-of-bed 
angle 
 source unknown. All rights reserved. This content is excluded from our Creative Commons license. 
For more information, see https://ocw.mit.edu/help/faq-fair-use/ 
4</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>What well (un)cover 
 Signal estimation. (Chapter 12) 
 Hypothesis testin g. (Chapter 9) 
 Some intimations of mach ine learning: training and applying quadratic 
discriminators in feature space. (Based on Chapter 9) 
 Signal detection. (Chap ter 13) 
 Hidden Markov model s (briey, as a counterpoint to LTI state-space mod-
els). 
14</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Time-based capnography! 
Exhaled CO2 partial
pressure (PeCO2) 
vs. time 
Non-invasive 
Effort-independent 
Portable (point-of-
care) 
Oridion. Microstream  Bedside Capnography 
Monitoring - CS08653 Data Sheet, 2012. 
 Medtronic . All rights reserved. This content is excluded from our Creative Commons license. 
For more information, see https://ocw.mit.edu/help/faq-fair-use/ 
9</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Blood pressure regulation + 
Courtesy of Elsevier, Inc., https://www.sciencedirect.com. Used with permission. 
7</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Mechanistic model for capnography 
10</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Baroreflex 
6</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>12</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>What well (un)cover 
 Brief review of linear, time-invariant (LTI) system models in continuous 
and discrete time (CT and DT), and in the frequency domain. Determin -
istic autocorrelation. (Sec tions 1.11.3) 
 Statespace models  (mainly LTI). (Chapters 4, 5 and 6) 
 Brief review of ran dom variables. (Chapter 7) 
 Estimation. (Chapt er 8) 
 Stationary random  processes in time and frequency domains. (Chapters 
10 and 11) 
13</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>The measurements 
respiratory rate, tidal volume bedside end-tidal carbon dioxidemonitor 
electrocardiogram 
intracranial 
pressure 
central venous, right
ventricular, pulmonary 
artery pressures 
fluid drainage 
cerebrospinal 
fluid drainage systemic arterial 
pressure 
blood 
oxygenation 
temperature 
urine output 
bed with weight 
Imaging studies, lab results, clinical assessment measuring capability 
 source unknown. All rights reserved. This content is excluded from our Creative Commons license. 
For more information, see https://ocw.mit.edu/help/faq-fair-use/ 
3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 6 Modal Solution of Undriven CT LTI State-Space Models
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec6/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>Phase plane trajectories 
5 
4 
3 2 1 0 
-1 
-2 
-3 
-4 
-5 -10 State trajectories for different initial conditions 
[-6, 3.0 5] 
[-3.2, 1. 5] 
[2, -0.9] 
[4, -2.1] 
[8, -4] 
-8 -6 -4 -2 0 2 4 6 8 10 
q1(t) q 2(t) 
5</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Acoustics and Vibration Animations 
Have fun exploring the animations created by 
Prof. Dan Russell, Penn State 
7</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
8</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Glucose-insulin system 
From Messori et al.,  
IEEE Control Systems 
Magazine 
 IEEE . All rights reserved. This content is excluded from our Creative Commons license. Feb 2018 
For more information, see https://ocw.mit.edu/help/faq-fair-use/ 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Modal solution of undriven CT 
LTI state-space models 
6.011, Spring 2018 
Lec 6 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>UVA/Padova  model (FDA approved!) 
From Messori et al.,  
IEEE Control Systems 
Magazine
 IEEE. All rights reserved. This content is excluded from our Creative Commons license. Feb 2018For more information, see https://ocw.mit.edu/help/faq-fair-use/ 
3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 9 Observers for State Estimation
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec9/</lecture_pdf_url>
      <lectureno>23</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Performance of real-time simulation 
Actual 
Estimate 
Error Damping parameter b = 0.2, 
input torque x(t) = unit-amplitude pulse of duration 5 seconds 
0.15 
0.1 
0.05 Pendulum angle (q 1 ) 
-0.05
-0.1
-0.2
-0.3-0.25-0.150 q1
q1
q1
0 2 4 6 8 10 12 14 16 18 
Time (sec) 20 
7</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Observer performance (with no 
measurement noise) 
Undamped suspended pendulum, 
input torque x(t) = unit-amplitude pulse
Actual of duration 5 seconds Estimate 
Error Observer gains /1 = -7 and /2 = -2 
0 1 2 3 4 5 6 7 8 9 
Time (sec) 1.5 
0.5Pendulum angle (q 1 ) 
-0.5
-1.5-10 1 
q1
q1q1
10 
9</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Observers for state estimation 
6.011, Spring 2018 
Lec 9 
1</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Observer performance (with 
measurement noise) 
-6-5-4-3-2-10 1 2 3 
Actual 
Estimate 
q1Pendulum angle (q 1 ) q1
0.0 0.5 1.0 1.5 2.0 
Time (s) 
10</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>y[n]
1[n]w[n]
[n
 +A good model 
x[n] [nbq ]
A, b, cT, d by[n]
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Observers 
4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Hidden modes of composite systems: 
series (cascade) connections 
x(t) = x1(t) 
H1(s) y1(t) = x2(t) 
H2(s) y2(t) = y(t) 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>System (plant) 
w[n] 
x[n] y[n] 
1[n] q[n] 
A, b, cT, d +
5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Observer configuration 
w[n] 
x[n] q[n] 
A, b, cTy[n] 
+
Z[n] Plant 
y[n] 
q[n] 
A, b, cT[n]q y[n] -+
Observer 
B 
8</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
12</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Hidden modes of composite systems: 
feedback and parallel connections 
x1(t) y1(t) = y(t) 
x(t) + 
-H1(s) 
H2(s) 
y2(t) x2(t) 
x1(t) 
x2(t) y2(t) y1(t) 
y(t) x(t) + H1(s) 
H2(s) 
3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 5 State-Space Models, Equilibrium, Linearization
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec5/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Numerical solution of CT state-space 
model 
. qi(t0) 
qi(t0) 
qi(t) 
t 
t0 +  
t0 
4</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
11</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Integrator-adder-gain system 
-1 x(t) y(t) q . 
2(t) q . 
1(t) q2(t) q1(t)

-2
8 +
5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Equilibrium 
For a time-invariant nonlinear system 
with a constant input, an initial state that 
the system remains at: 
DT : q= f(q, x)
CT : 0 = f(q, x)
8</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Mechanistic model for capnography 
6</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Defining properties of CT 
state-space models 
  
q(t)= f q(t),x(t),t
  
y(t)= g q(t),x(t),t
 State evolution property 
 Instantaneous output property 
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>State variables are (relevant) 
memory variables 
In physical systems, the natural state variables 
are typically related to energy storage 
mechanisms: 
capacitor voltages or charges, inductor currents or fluxes, 
positions and velocities of masses, 
 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>State-Space Models, 
Equilibrium, Linearization 
6.011, Spring 2018 
Lec 5 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.011 Signals, Systems and Inference, Lecture 10 Observers, State Feedback
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-011-signals-systems-and-inference-spring-2018/resources/mit6_011s18lec10/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>Observer configuration 
w[n] 
x[n] q[n] 
A, b, cTy[n] 
+
Z[n] Plant 
y[n] 
q[n] 
A, b, cT[n]q y[n] -+
Observer 
B 
5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>y[n]
1[n]w[n]
[n
 +A good model 
x[n] bq[n]
A, b, cT, d by[n]
4</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
https://ocw.mit.edu 
6.011 Signals, Systems and Inference
Spring 2018 
For information about citing these materials or our Terms of Use, visit: https: //ocw.mit.edu/terms. 
9</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>System (plant) 
w[n] 
x[n] y[n] 
1[n] q[n] 
A, b, cT, d + 
3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Observer performance (with 
measurement noise) 
-6-5-4-3-2-10 1 2 3 
Actual 
Estimate 
q1Pendulum angle (q 1 ) q1
0.0 0.5 1.0 1.5 2.0 
Time (s) 
6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Observers, state feedback 
6.011, Spring 2018 
Lec 10 
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Observers 
2</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>State feedback 
x[n] 
p[n] 
gT+ A, b, cT
q[n] 
8</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
