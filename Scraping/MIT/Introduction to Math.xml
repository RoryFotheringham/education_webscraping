<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/</course_url>
    <course_title>Introduction to Mathematical Programming</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Mathematics </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Simplex method II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>n! vertices:
 
Foreachvertex  2n1nn2 dierentbases(n =8)foreach vertex  33,554,432
 
bases.
 
5.2 Perturbations Slide 18 
	 (P)min cx (P)min cx    
 2  
s.t. Ax = b s.t. Ax = b +    .  .  .  
m 
x  0	 x  0. 
5.2.1 Theorem Slide 19 
 1 &gt; 0: for all 0 &lt;  &lt;1 
   
 . Ax = b +  ..  
m 
x  0 
is non-degenerate. 
5.2.2 Proof Slide 20 
Let B1,..., Br be all the bases. 
    r	   b1 + Br   + Br m 
11+ 1m
B1  .   . 
. r  b +  ..  =  .  
m b r + Br 
m1+   + Br m 
m	 mm 
where: 	   r  Br Br	   b11 1m 1 
B1  . .  ,B1  .  
r =  . . . .  r b =  . .  
Br Br	 r   m1 mm bm Slide 21 
r 	 + Br +   + Br is a polynomial in  bi i1 im m 
	 Roots i,r 
1,i,r 
2,...,r 
i,m 
r 	 If  = i,r 
1,...,r + Bir 
1+   + Br m = 0.  i,m  bi im  
	 Let 1 the smallest positive root  0 &lt;&lt;1 all RHS are 0 
=
non-degeneracy.
 
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 6: The Simplex Method II</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
 Revised Simplex method 
 The full tableau implementation 
 Anticycling 
2 Revised Simplex 
Slide 2 
Initial data: A, b, c 
1. Start with basis B = [AB(1),. . . , AB(m)] 
and B1 . 
2. Compute p  = c  
BB1 
cj = cj p  Aj 
 If cj  0; x optimal; stop. 
 Else select j: cj &lt; 0. 
Slide 3 
3. Compute u = B1Aj. 
 If u  0  cost unbounded; stop 
 Else 
4.   = min xB(i) = uB(l) 
1im,ui &gt;0 ui ul 
5. Form a new basis B by replacing AB(l) with Aj. 
6. yj =  , yB(i) = xB(i)  ui 
Slide 4 
7. Form [B1|u] 
8. Add to each one of its rows a multiple of the lth row in order to make the
 
last column equal to the unit vector el.
 
1 The rst m columnsis B . 
2.1 Example 
Slide 5 
min x1+5x2 2x3 
s.t.x1+ x2+ x3  4 
x1  2 
x3  3 
3x2+ x3  6 
x1,x2,x3  0 
Slide 6 
1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>5.6.1 Example 
Slide 27 
	 j=3 
1	05 3   
	 2 4 6 1   
3	07 9   
	 xB(1)/u1 =1/3 and xB(3)/u3 =3/9 = 1/3. 
	 We divide the rst and third rows of the tableau by u1 = 3 and u3 =9,
 
respectively, to obtain:
 
1/3	 05/3 1   
	     
1/3	 07/9 1   
	 Since 7/9 &lt; 5/3, the third row is chosen to be the pivot row, and the
 
variable xB(3) exitsthebasis.
 
5.6.2 Uniqueness 
Slide 28 
	 Why lexicographic pivoting rule always leads to a unique choice for the
 
exiting variable?
 
	 Otherwise, two rows in tableau proportional  rank(B1A) &lt;m 
 
rank(A)&lt; m
 
5.7	 Theorem Slide 29 
If simplex starts with all the rows in the simplex tableau, other than the zeroth 
row, lexicographically positive and the lexicographic pivoting rule is followed, 
then 
(a)	 Every row of the simplex tableau, other than the zeroth row, remains
 
lexicographicallypositivethroughoutthe algorithm.
 
(b)	 The zeroth row strictly increases lexicographically at each iteration. 
(c)	 The simplex method terminates after a nite number of iterations. 
5.8	 Smallest subscript 
pivoting rule 
Slide 30 
1.	 Find the smallest jfor which the reduced cost cj is negative and have the
 
column Aj enterthebasis.
 
2.	 Out of all variables xi that are tied in the test for choosing an exiting
 
variable, select the one with the smallest value of i.
 
7</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>B= {A1,A3,A6,A7}, BFS: x =(2,0,2,0,0,1,4)  
 c =(0,7,0,2,3,0,0)     1100 0 100 
     100 0  B1  1 10 0  B = , =  011 0   1 11 0  
010 1 1 10 1 
(u1,u3,u6,u7)  = B1A5 =(1,1,1,1)  
 =min 2
1 , 1
1 , 4
1 =1, l =6 
l =6(A6 exitsthebasis). Slide 7   0 100 1 
[B1|u]=  1 10 0 1  
 1 110 1  
1 101 1   10 1 0
 
1  0 0 1 0
     B =  11 10 
 
00 1 1
 
2.2 Practical issues Slide 8 
 Numerical Stability 
B1 needs to be computed from scratch once in a while, as errors accu
mulate 
 Sparsity 
B1 is represented in terms of sparse triangular matrices 
3 Full tableau implementation 
Slide 9 
c  B1bc  c  B1AB B 
B1b B1A 
or, in more detail, 
 cBxB 
xB(1) 
. . . 
xB(m) c1 ... cn 
| | 
B1A1 .. . B1An 
| | 
2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5.3 Lexicography 
Slide 22 
L  u is lexicographically larger than v, u &gt; v, if u  v and the rst =
 
nonzero component of u v ispositive.
 
 Example: 
L (0, 2, 3, 0) &gt; (0, 2, 1, 4), 
L (0, 4, 5, 0) &lt; (1, 2, 1, 2). 
5.4 Lexicography-Pertubation 
5.4.1 Theorem Slide 23 
Let B be a basis of Ax = b, x  0. Then B is feasible for Ax = b + 
 (,...,m) , x  0 for suciently small  if and only if 
L ui =(bi,Bi1,...,Bim) &gt; 0, i 
B1 =(Bij) 
(B1b)i =(bi) 
5.4.2 Proof Slide 24 
B is feasible for peturbed problem  B1 (b +(,...,m)  ) 0  
bi + Bi1+   + Bimm  0  i 
 First non-zero component of ui = (bi,Bi1,. . . ,Bim)is positive  i. 
5.5 Summary 
1. We start with: (P): Ax = b, x  0 Slide 25 
2. We introduce (P): Ax = b + (,. . . ,m)  , x  0 
3. A basis is feasible + non-degenerate in (P) ui L &gt; 0 in (P). 
4. If we maintain ui L &gt; 0 in (P)  (P) is non-degenerate  
nite in (P)for suciently small . Simplex is 
5.6 Lexicographic pivoting rule 
1. Choose an entering column Aj arbitrarily, as long as cj &lt; 0; u = B1Aj. Slide 26 
2. For each i with ui &gt; 0, divide the ith row of the tableau (including the 
entryin the zeroth column)by ui and choose the lexicographically smallest 
row. If row l is lexicographicallysmallest, then the lth basic variable xB(l) 
exits the basis. 
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>3.1 Example 
min 10x1  12x2  12x3 
s.t. x1 + 2x2 + 2x3  20 
2x1 + x2 + 2x3  20 
2x1 + 2x2 + x3  20 
x1,x2,x3  0 
min 10x1  12x2  12x3 
s.t. x1 + 2x2 + 2x3 + x4 = 20 
2x1 + x2 + 2x3 + x5 = 20 
2x1 + 2x2 + x3 + x6 = 20 
x1,. . . ,x6  0 
BFS: x = (0,0,0,20,20,20)  
B=[A4,A5,A6] 
x1 x2 x3 x4 x5 x6 
0 10 12 12 0 0 0 
x4 = 20 1 2 2100 
x5 = 20 2* 1 201 0 
x6 = 20 2 2 1001 Slide 10 
Slide 11 
c  = c  cB  B1A = c  =(10,12,12,0,0,0) Slide 12 
x1 x2 x3 x4 x5 x6 
0 7 2 0 50 
0 1.5 1* 1 0.5 0 
10.5 1 0 0.5 0 
01 1 0 11 100 
10 
10 x4 = 
x1 = 
x6 = 0 
Slide 13 
x1 x2 x3 x4 x5 x6 
120 0 4 0 2 40 
x3 = 10 01.5 1 1 0.5 0
 
x1 = 0 1 1 0 1 1 0
 
x6 = 10 0 2.5* 0 1 1.5 1
 
Slide 14 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>x1 x2 x3 x4 x5 x6 
136 00 0 3.6 1.6 1.6 
x3 = 4 00 1 0.4 0.4 0.6 
x1 = 4 10 0 0.6 0.4 0.4 
x2 = 4 01 0 0.4 0.6 0.4 
Slide 15 
A = (0,0,0) B = (0,0,10) 
E = (4,4,4) . 
.
. . . x3 
C = (0,10,0) 
D = (10,0,0) 
x1 x2 
4 Comparison of implementations 
Slide 16 
Full tableau 
Memory O(mn) 
Worst-case time O(mn) 
Best-case time O(mn) Revised simplex
 
O(m2)
 
O(mn)
 
O(m2)
 
5 Anticycling 
5.1 Degeneracy in Practice 
Slide 17 
Does degeneracy really happen in practice? 
n 
xij =1 
j=1 
n 
xij =1 
i=1 
xij  0 
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Simplex method I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 5: The Simplex Method I</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 Improving the Cost 
Slide 5 
	Suppose cj = cj cB  B1Aj &lt; 0
Can we improve the cost?
	Let dB = B1Aj
dj =1,di =0,i B(1),...,B(m),j.
 = 
	Let y = x +   d, &gt; 0 scalar 
Slide 6 
  cy cx =   c  d 
 =   (cB dB + cj dj ) 
=   (cj c  B1Aj )B 
=   cj 
Thus,if cj &lt; 0 cost will decrease. 
4 Unboundness 
Slide 7 
	Is y = x +  d feasible?
Since Ad =0  Ay = Ax = b
	y  0 ?
If d  0  x +  d 0    0
 objective unbounded.
5 Improvement 
Slide 8 
If di &lt; 0,then xi xi + di 0    di 
   = min  xi 
{i|di&lt;0} di 
   = min  xB(i) 
{i=1,...,m|dB(i) &lt;0} dB(i) 
5.1 Example 
Slide 9 
min x1+5x2 2x3 
s.t.x1+ x2+ x3  4 
x1  2 
x3  3 
3x2+ x3  6 
x1,x2,x3  0 
2</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	ReducedCosts 
	Optimality conditions 
	Improving the cost 
	Unboundness 
	The Simplex algorithm 
	TheSimplex algorithm ondegenerateproblems 
2 Matrix View 
Slide 2 
 min cx 
s.t. Ax = b 
x  0 
x =(xB , xN ) xB basic variables 
xN non-basic variables 
A =[B, N]
Ax = b  B  xB + N  xN = b
 xB + B1NxN = B1b 
 xB = B1b B1NxN 
2.1 Reduced Costs Slide 3 
  z	= cB xB + cN xN 
= c  
B (B1b B1NxN )+c  
N xN 
= c  B1b +(c  c  B1N )xN B N B 
cj = cj c  
B B1Aj reduced cost 
2.2 Optimality Conditions 
Slide 4 
RecallTheorem: 
	x BFS associated with basis B 
	c reduced costs
Then
	If c  0  x optimal 
	x optimal and non-degenerate  c  0 
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Slide 18 
3.	Compute u = d = B1Aj . 
	If u 0  cost unbounded; stop 
	Else 
4.	  = min xB(i) = uB(l) 
1im,ui &gt;0 ui ul 
5.	Form a new basis by replacing AB(l) with Aj . 
6. yj =  
yB(i) = xB(i)   ui
7.1 Finite Convergence 
Slide 19 
Theorem: 
	P = {x |Ax = b, x  0}= 
	Every BFS non-degenerate
Then
	Simplex method terminates after a nite number of iterations 
	At termination, we have optimal basis B or we have a direction d : Ad =
0, d 0, c  d &lt; 0 and optimal cost is .
7.2 Degenerate problems 
Slide 20 
	  can equal zero(why?)  y = x, although B = B. 
	Evenif   &gt; 0, there might be a tie
xB(i)
min  
1im,ui &gt;0 ui 
nextBFSdegenerate. 
	Finite termination not guaranteed; cycling is possible. Slide 21 
-x 
y f g 
h g 
.x2=0 x3=0x4=0 x5=0 
x6=0 
x1=0 c 
5</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>7.3 Pivot Selection Slide 22 
	Choices for the entering column: 
(a) Choose a column Aj , with cj &lt; 0, whose reduced cost is the most
negative.
(b) Choose a column with cj &lt; 0 for which the corresponding cost de
crease  |cj |islargest.
	Choices for the exiting column:
smallest subscript rule: out of all variableseligibletoexitthebasis,choose
one with the smallest subscript.
7.4 Avoiding Cycling 
Slide 23 
	Cycling can be avoided by carefully selecting which variables enter and
exitthebasis.
	Example: among all variables cj &lt; 0, pick the smallest subscript;
among all variableseligibletoexitthebasis,pick theonewith thesmallest
subscript.
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>x3 
x1 (0,0,3) (1,0,3) 
(2,0,2) 
(2,2,0) (0,2,0) (0,1,3) 
Slide 10 
Ax
1 2 A2 A3 A4 A5 A6 A7 x1 Slide 11   x2     
  1111000	 4 	x3   1000100   2  	  x4  =     0010010   3  	x5    0310001	 6 	x6  
x7 
B =[A1, A3, A6, A7] 
BFS: x =(2, 0, 2, 0, 0, 1, 4) 	Slide 12     1100	 0 100 
 B =  1000  
, B1 =  1 100  
c =(0, 7, 0, 2, 3, 0, 0) 	0110   1 110 
0101 1 101
    
d1	 1 
    
d5 =1,d2 = d4 =0,  d3  = B1A5 =  1 	 Slide 13 	d6   1  
d7 1 
  y	= x + d  =(2,0, 2+,0,, 1,4) 
What happens as  increases? 
  =min{i=1,...,m|dB(i)&lt;0} xB
di (i) = 
min (2
1) , (1
1) , (4
1) =1. 
l =6(A6 exitsthebasis). 
New solution 
y =(1, 0, 3, 0, 1, 0, 3)  Slide 14 
Newbasis B =(A1, A3, A5, A7) Slide 15 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>x3 
x1 (0,0,3) (1,0,3) 
(2,0,2) 
(2,2,0) (0,2,0) (0,1,3) 
 1 1 0 0   1 0 1 0  
 x21010  1  00 10  	  B =  ,B =	  	0100   11 10 
0101 00 11
  1 c	= c cB A =(0, 4, 0, 1, 0, 3, 0) B Need to continue, column A4 entersthebasis. 
6 Correctness 
Slide 16 
 xB(l) = min  xB(i) =   
dB(l) i=1,...,m,dB(i)&lt;0 dB(i) 
Theorem 
	B = {AB(i) ,i=l, Aj }basis 
	y = x +  d is a BFS associated with basis B. 
7 The Simplex Algorithm 
Slide 17 
1. Start with basis B =[AB(1),..., AB(m)]
and a BFS x.
2.	Compute cj = cj cB  B1Aj 
	If cj  0; x optimal; stop. 
	Else select j : cj &lt; 0. 
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Geometry II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>/6 	 Represen tation  of  P olyhedra  /6/./1  Theorem  Slide  /3/0  A  n onempt y  a nd  b ounded  p olyhedron  is the  con v ex  h ull  of its  extreme  p oin ts/.  
y. 
a'i*x = bi* P 
Q . 
. z 
u /8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>/1/5/./0/8/1J///6/./2/5/1J  In tro duction  t o  M athematical  Programming  Lecture  /3 /: G eometry  of  Linear  Optimization  I I</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Large scale optimization I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>3.3 Formulation Slide 10 
Decisionvariables: xj =numberof rollscutbypattern jcharacterizedby vector 
Aj : 
n 
min xj 
j=1   b1 n  . Aj  xj =  ..  
j=1 bm 
xj  0 (integer) 
Slide 11 
	 Huge number of variables. 
	 Can we apply columngeneration,thatisgeneratethepatterns Aj onthe
 
y?
 
3.4 Algorithm 
Slide 12 
Idea: Generate feasible patterns as needed. 
     W  0 0 0 w1  0  W   0  0    w2    1) Startwithinitialpatterns:  0  ,  0  ,  W   ,  0  w3 
0 0 0 W  w4 
Slide 13 
2) Solve: 
min x1 +   + xm
 
x1A1 +   + xmAm = b
 
xi  0
 
Slide 14 
3) Compute reduced costs 
cj =1p  Aj for all patterns j 
If cj  0 current set of patterns optimal 
If cs &lt; 0  xs needs to enter basis 
How are we going to compute reduced costs cj =1  p  Aj for all j? (huge 
number) 
3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 14: Large Scale Optimization, I</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>4.2 Two-stage problems 
Slide 30 
	 Random scenarios indexed by w =1,...,k. Scenario w hasprobability
 
w.
 
	 First stage decisions: x: Ax = b,x  0. 
	 Second stage decisions: yw: w =1,...,k. 
	 Constraints:
 
Bwx + Dwyw = dw, yw  0.
 
4.2.1 Formulation Slide 31 
 	 min	 cx + 1f1y1 +   + kfkyk 
Ax = b 
B1x + D1y1 = d1 
B2x + D2y2 = d2 Slide 32 .	 . .	.. . . . .
 
Bkx + Dkyk = dk
 
x, y1, y2,..., yk  0.
 
Structure: xy1 y2 y3 y4 
Objective 
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>3.4.1 Key Idea 
Slide 15 
4) Solve 
m 
 z =max piai 
i=1 m 
s.t. wiai  W 
i=1 
ai  0, integer 
Thisistheintegerknapsackproblem 
Slide 16 
	 If z   1  1p  Aj &gt; 0 j  current solution optimal 
	 If z  &gt; 1  s:1 p  As &lt; 0  Variable xs becomes basic, i.e., a new
 
pattern As will enter the basis.
 
	 Perform min-ratio test and update the basis. 
3.5 Dynamic Programming 
Slide 17 
F(u)= max p1a1 +   + pmam 
s.t.w1a1 +   + wmam  u 
ai  0, integer 
	 For u  wmin, F(u)=0. 
	 For u  wmin
 
F(u)= max {pi + F(u wi)}
 
i=1,...,m 
Why ? 
3.6 Example 
Slide 18 
max 11x1 +7x2 +5x3 + x4 
s.t. 6x1 +4x2 +3x3 + x4  25 
xi  0,xi integer
 
F(0) =0
 
F(1) =1
 
F(2) =1+F(1)=2
 Slide 19 F(3) = max(5+ F(0) , 1+F(2))= 5
 
F(4) = max(7+ F(0) , 5+F(1), 1+F(3))= 7
 
F(5) = max(7+ F(1) , 5+F(2), 1+F(4))= 8
 
F (6) = max(11 + F (0) , 7+ F (2), 5+ F (3), 1+ F (5)) = 11
 
F (7) = max(11 + F (1) , 7+ F (2), 5+ F (3), 1+ F (4)) = 12
 
F (8) = max(11 + F (2), 7+ F (4) , 5+ F (5), 1+ F (7)) = 14
 
F (9) =11+ F (3) = 16
 
F (10) =11+ F (4) = 18
 
F (u) =11+ F (u  6) = 16 u  11
 
4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Pattern:(a1,...,am)integers:
 
m
 
aiwi  W 
i=1 
3.1 Problem 
	 Given wi,bi,i =1,..., m (bi: number of rolls of width wi demanded,
 
and W (widthof large rolls):
 
	 Find how to cut the large rolls in order to minimize the number of rolls
 
used.
 
3.2 Concrete Example 
	 What is the solution for W =70,w1 =21,w2 =9,b1 =20,b2 =21? 
	 feasiblepatterns:(2, 3),(3, 0),(0, 7), (2, 0) 
	 Solution1:(2, 3) : 7 rolls;(3, 0) : 2 rolls: 9 rolls total 
	 Solution2:(0, 7) : 3,(3, 0) : 6,(2, 0) : 1 : 10 rolls total 
	 W =70,w1 =20,w2 =11,b1 =12,b2 =17 
	 Feasiblepatterns: 1
0  
, 2
0  
, 3
0  
, 0
1  
, 1
1  
, 2
1  
, 0
2  
, 1
2  
, 2
2  
, 0
3  
, 1
3  
, 
    
0100
4	 , 4 , 5 , 6
	 x1,...,x15 = # of feasible patterns of the type  
01 
,...,  
60 
respectively 

 
min x1 +   + x15
  	       1 2 0 12 s.t.x1 + x2 +   + x15 = 0 0 6 17 
x1,...,x15  0 
 	       0 0 3 12 	 Example: 2 +1 +4 = 7 rolls used 6 5 0 17 
 	       0 0 3 12 4 + +4 =	 9 rolls used 4 1 0 17 
	 Any ideas? 
2 Slide 6 
Slide 7 
Slide 8 
Slide 9</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>State 
1 
2 
3 
4 
Cap. 
8,000 
10,000 
8,000 
10,000 W. contr. 
160 
160 
90 
90 
DecisionVariables: S: steel capacity,
 
Pi,Wi : i =1,..., 4 production plan under state i. 
max 58S +0.25Z1 +0.25Z2 +0.25Z3 +0.25Z4 
s.t. 
Ass. 1 0.3W1 + 0.5P1  8 
Mol. 1 W1 + P1  21 
Ste. 1 S + 1.5W1 + P1  0 
W.d. 1 W1  15 
P.d. 1 P1  16 
Obj. 1 Z1 + 160W1 + 100P1 = 0 
Ass. 2 0.3W2 + 0.5P2  8 
Mol. 2 W2 + P2  21 
Ste. 2 S + 1.5W2 + P2  0 
W.d. 2 W2  15 
P.d. 2 P2  16 
Obj. 2 Z2 + 160W2 + 100P2 = 0 
Ass. 3 0.3W3 + 0.5P3  8 
Mol. 3 W3 + P3  21 
Ste. 3 S + 1.5W3 + P3  0 
W.d. 3 W3  15 
P.d. 3 P3  16 
Obj. 3 Z3 + 160W3 + 100P3 = 0 
Ass. 4 0.3W4 + 0.5P4  8 
Mol. 4 W4 + P4  21 
Ste. 4 S + 1.5W4 + P4  0 
W.d. 4 W4  15 
P.d. 4 P4  16 
Obj. 4 Z4 + 160W4 + 100P4 = 0 
4.1.4 Solution 
Solution: S = 27, 250lb. 
1 
2 
3 
4 
S, Wi, Pi  0 
Wi 
15,000 
15,000 
12,500 
5,000 
6 
Pi 
4,750 
4,750 
8,500 
16,000 Prob. 
0.25 
0.25 
0.25 
0.25 
Slide 25
 
Slide 26
Slide 27
Slide 28
Slide 29</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>F (25) = 11+ F (19) = 11+11+ F (13) = 11+11+11+ F (7) = 33+12 = 45 
x  = (4, 0, 0, 1) 
4 Stochastic Programming 
4.1 Example 
Steel (lbs)
 
Molding machine (hrs)
 
Assembly machine (hrs)
 
Demand limit (tools/day)
 
Contribution to earnings
 
($/1000 units) 
4.1.1 Random data Wrenches
 
1.5
 
1.0
 
0.3
 
15,000
 
$130*
 Pliers 
1.0 
1.0 
0.5 
16,000 
$100 Slide 20 
Cap. 
27,000 
21,000 
9,000* Slide 21 
max 130W +100P 
s.t.W  15
 
P  16
 
1.5W + P  27
 
W + P  21
 
0.3W +0.5P  9
 
W,P  0
 
Slide 22  1   8000 with probability 2  Assembly capacity is random:  1  10, 000 with probability 2 
 1  160 with probability  2  Contribution from wrenches:  1  90 with probability 2 
4.1.2 Decisions Slide 23 
 Need to decide steel capacity in the current quarter. Cost 58$/1000lbs. 
 Soon after, uncertainty will be resolved. 
 Next quarter, company will decide production quantities. 
4.1.3 Formulation Slide 24 
5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1.	 The idea of column generation 
2.	 The cutting stock problem 
3.	 Stochastic programming 
2 Column Generation 
Slide 2 
	 For x n and n large consider the LOP: 
 min cx 
s.t. Ax = b 
x  0 
	 Restrictedproblem 
min cixi 
iI 
s.t. Aixi = b 
iI 
x  0 
2.1 Two Key Ideas 
Slide 3 
	 Generate columns Aj only as needed. 
	 Calculate mini ci eciently without enumerating all columns. 
3 The Cutting Stock Problem 
Slide 4 
	 Company has a supply of large rolls of paper of width W. 
	 bi rolls of width wi,i =1,..., m needtobeproduced. 
	 Example: w =70 inches, can be cut in 3 rolls of width w1 =17 and 1 roll
 
of width w2 =15, waste:
 
70(3 17+1 15) =4 
Slide 5 
	 Givenw1,...,wm and W therearemany cuttingpatterns:(3, 1)and(2, 2)
 
for example
 
3 17+1 15  70 
2 17+2 15  70 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Discrete optimization II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec25/</lecture_pdf_url>
      <lectureno>25</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	Cutting plane methods 
	Branch and bound methods 
2 Cutting plane methods 
Slide 2 
 min	cx 
s.t. Ax = b 
x  0 
x integer, 
LP relaxation 
 min	cx 
s.t. Ax = b 
x  0. 
2.1	Algorithm 
Slide 3 
	Solve the LP relaxation. Let x  be an optimal solution. 
	If x  is integer stop; x  is an optimal solution to IP. 
	If not, add a linear inequality constraint to LP relaxation that all integer
solutions satisfy, but x  does not; go to Step 1.
2.2	Example 
Slide 4 
	Let x  be an optimal BFS to LP ralxation with at least one fractional
basic variable.
	N: set of indices of the nonbasic variables. 
	Is this a valid cut? 
xj  1. 
jN 
2.3	The Gomory cutting 
plane algorithm 
Slide 5 
	Let x  be an optimal BFS and B an optimal basis. 
 
xB + B1AN xN = B1b. 
	aij = B1Aj , ai0 = B1b i . i
1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Objective value =22.2 
x1 =1, x 2 =0, x 3 =0.6, x 4 =1
 Objective value =22 
x1 =1, x 2 =0.5, x 3 =0, x 4 =1 Objective value =22 
x1 =1, x 2 =0, x 3 =1 , x 4 =0.5 x3 =0 x3 =1
 Objective value =22.2 
x1 =1, x 2 =0, x 3 =0.6, x 4 =1
 Objective value =22 
x1 =1, x 2 =0.5, x 3 =0, x 4 =1 Objective value =22 
x1 =1, x 2 =0, x 3 =1, x 4 =0.5
 Objective value =21.66 
x1 =1, x 2 =0.33, x 3 =1, x 4=0 Objective value =22 
x1 =0.75, x 2 =0, x 3 =1, x 4 =1 x3 =0 x3 =1 
x4 =1 x4 =0 
LP relaxation	 Slide 12
max 12x1 +8x2 +7x3 +6x4 
s.t. 8x1 +6x2 +5x3 +4x4  15
x1  1,x2  1,x3  1,x4  1
x1,x2,x3,x4  0
LP solution: x1 =1,x2 =0,x3 =0.6,x4 =1Prot=22.2 
3.2.1 Branch and bound tree Slide 13
3.3	Pigeonhole Problem Slide 14
Slide 15
	Thereare n +1pigeonswith n holes. We wanttoplacethepigeonsinthe Slide 16
holes in such a way that no two pigeons go into the same hole.
	Let xij =1ifpigeon i goesintohole j, 0 otherwise. 
Slide 17
	Formulation1: 
xij =1,i =1,...,n +1 j 
xij + xkj  1, j,i k= 
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Objective value =21 
x1 =0 , x 2 =1 , x 3 =1 , x 4 =1 Infeasible x1 =1 x1 =0 Objective value =22.2 
x1 =1, x 2 =0, x 3 =0.6, x 4 =1
 Objective value =22 
x1 =1, x 2 =0.5, x 3 =0, x 4 =1 Objective value =22 
x1 =1, x 2 =0, x 3 =1, x 4 =0.5
 Objective value =21.66 
x1 =1, x 2 =0.33, x 3 =1, x 4=0 Objective value =22 
x1 =0.75, x 2 =0, x 3 =1, x 4 =1 x3 =0 x3 =1 
x4 =1 x4 =0 
	Formulation2: 
xij =1,i =1,...,n +1 j  n+1 xij  1, ji=1 
Whichformulationisbetterfortheproblem?	 Slide 18 
	Thepigeonholeproblemisinfeasible. 
	ForFormulation1,feasiblesolution xij = n 1 forall i,j. O(n3)constraints.
Nearly complete enumeration is needed for LP-based BB, since the prob
lem remains feasible after xing many variables.
	Formulation2Infeasible. O(n)constraints. 
	Mesage: Formulation of the problem is important! 
3.4 Preprocessing 
Slide 19 
	An eective way of improving integer programming formulations prior to and
duringbranch-and-bound.
	LogicalTests 
	Removal of empty(all zeros) rows and columns; 
	Removal of rows dominated by multiples of other rows; 
	strengthening the bounds within rows by comparing individual variables 
and coecients to the right-hand-side. 
	Additionalstrengthening maybepossibleforintegral variablesusing round
ing. 
	Probing : Setting temporarily a 0-1 variable to 0 or 1 and redo the logical
tests. Force logical connection between variables. For example, if 5x +4y +z 
8,x,y,z {0, 1}, then by setting x = 1, we obtain y = 0. This leads to an
inequality x + y  1.
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>xi + aij xj = ai0. 
jN 
 Since xj  0 for all j, 
xi + aij xj  xi + aij xj = ai0. 
jN jN 
 Since xj integer, 
xi + aij xj ai0. 
jN 
 Valid cut 
2.4 Example 
Slide 6 
min x1  2x2 
s.t. 4x1 +6x2  9 
x1 + x2  4 
x1,x2  0 
x1,x2 integer. 
We transform the problem in standard form 
min x1  2x2 
s.t. 4x1 +6x2 + x3 =9 
x1 + x2 + x4 =4 
x1,...,x4  0 
x1,...,x4 integer. 
LP relaxation: x1 =(15/10, 25/10).	 Slide 7 
 1 1 25 x2 + x3 + x4 = . 10 10 10 
	Gomory cut
x2  2.
 Add constraints x2 + x5 =2, x5  0 
 New optimal x2 =(3/4, 2). 
 One of the equations in the optimal tableau is 
1 6 3 x1  x3 + x5 = . 4 4 4 
	New Gomory cut
x1 x3 + x5  0,
 New optimal solution is x3 =(1, 2). 
Slide 8 
2</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>... 
x2 &lt; 2 -3x 1 + 5x2 &lt; 7 
1 2 3 
4 3 2 
1 
0 x2 
x1 x1 
x2 
x3 
3 Branch and bound 
Slide 9 
1.	Branching: Select an active subproblem Fi 
2.	Pruning: If the subproblem is infeasible, delete it. 
3.	Bounding: Otherwise, compute a lower bound b(Fi)for the subproblem. 
4.	Pruning: If b(Fi) U, the current best upperbound, delete the subproblem. 
5. Partitioning:If b(Fi)&lt;U,eitherobtainanoptimal solutiontothesubproblem
(stop), or breakthe corresponding problem into further subproblems, which are
added to the list of active subproblem.
3.1 LP Based Slide 10 
	Computethelowerboundb(F )bysolvingtheLP relaxationofthediscrete
optimizationproblem.
	From the LP solution x , if there is a component xi  whichisfractional,
we create two subproblems by adding either one of the constraints
  xi xi , or xi xi . 
Note that both constraints are violated by x  . 
	If there are more than 2 fractional components, we use selection rules like
maximum infeasibility etc. to determine the inequalities to be added to
theproblem
	Selecttheactivesubproblemusingeitherdepth-rst orbreadth-rst search
strategies.
3.2 Example 
Slide 11 
max 12x1 +8x2 +7x3 +6x4 
s.t. 8x1 +6x2 +5x3 +4x4  15 
x1,x2,x3,x4 arebinary. 
3</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5 
6 7 3 1 2 
4 
4 Application 
4.1 Directed TSP 
4.1.1 Assignment Lower Bound 
Slide 20 
Given a directed graph G =(N,A)with n nodes, and a cost cij for every arc, 
nd atour(adirected cyclethat visits all nodes) of minimum cost. 
n nmin i=1 j=1 cij xij 
n s.t.: i=1 xij =1,j =1,...,n, 
n xij =1,i =1,...,n, j=1 
xij {0, 1}. 
Slide 21 
4.2 Improving BB 
Slide 22 
 Better LP solver 
 Use problem structure to derive better branching strategy 
 Better choice of lower bound b(F )-better relaxation 
 Better choice of upper bound U -heuristic to get good solution 
 KEY: Start pruning the search tree as early as possible 
6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 25: Exact Methods 
forDiscreteOptimization</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Simplex method III</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>The feasible set has 2n vertices 
	The vertices can be ordered so that each one is adjacent to and has lower
cost than the previous one.
	There exists a pivoting rule under which the simplex method requires
2n 1 changes of basis before it terminates.
7 The Diameter of polyhedra 
Slide 21 
	Given a polyhedron P, and x, y vertices of P,thedistance d(x, y)isthe
minimum number ofjumpsfromone vertexto an adjacent oneto reach y
starting from x.
	Thediameter D(P)is the maximum of d(x, y)x, y. 
Slide 22 
	(n,m)as the maximum of D(P)over all bounded polyhedrain n that
are represented in terms of m inequality constraints.
	u(n,m)islike(n,m)but for possibly unbounded polyhedra. 
7.1 The Hirsch Conjecture 
Slide 23 
m (2,m)= , u(2,m)= m 2 2 
. 
. . . 
. . . . 
. 
. . . 
. . 
. 
(a)	 (b) 
	Hirsch Conjecture: (n,m) m n. 
Slide 24 
	Weknowthat  n u(n,m) m n + 5 
2 (n,m) u(n,m)&lt;m1+log2 n =(2n)logm 
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>4 
x5 = 2 
x2 = 0 
x7 = 2 
x3 =1/3 
0 
x1 = 1 
x2 =1/2 
x7 = 0 
x3 =1/3 x1 x2 x3 x4 x5 x6 x7 x8 
4 0 0 2 0 4 0 1 
2* 0 0 1 1 1 0 1 
1 0 1 0 1/2 0 1 
2 0 0 1 0 2 1 1 
0 0 1 1/3 0 0 0 1/3 
Slide 9 1/2 
x1 x2 x3 x4 x5 x6 x7 x8 
0 
1 
0 
0 
0 
x1 = 
x2 = 
x3 = 00 0 2 20 1 
00 1/21/2 1/20 1/2 
10 3/41/41/40 3/4 
00 0 1 11 0 
01 1/30 001/3 
 
1 
1/2 
1/3 Slide 10 
x1 x2 x3 x4 
    
1 0 0 1/2 
0 1 0 3/4 
0 0 1 1/3 
3 A complete Algorithm for LO 
Slide 11 
Phase I: 
1. By multiplying someof theconstraintsby 1,changetheproblemsothat
b  0.
2. Introducey1,...,ym,if necessary, and applythesimplexmethodtomin m
i=1 yi. 
3. If cost&gt; 0, originalproblemisinfeasible; STOP. 
4. If cost= 0, afeasible solutiontothe originalproblemhasbeenfound. 
5. Drivearticialvariablesout ofthebasis,potentially eliminating redundant
rows.
Slide 12 
Phase II: 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>1. Let the nal basis and tableau obtained from Phase I be the initial basis 
andtableauforPhaseII. 
2. Compute the reduced costs of all variables for this initial basis, using the 
cost coecients of the originalproblem. 
3. Apply the simplex method to the original problem. 
3.1 Possible outcomes Slide 13 
1. Infeasible: Detected at Phase I. 
2. A has linearly dependent rows: Detected at Phase I, eliminate redundant 
rows. 
3. Unbounded(cost= ): detectedat Phase II. 
4. Optimal solution: Terminate at Phase II in optimality check. 
4 The big-M method 
Slide 14 n m 
min cj xj + M yi 
j=1 i=1 
s.t. Ax + y = b 
x, y  0 
5 The Column Geometry 
Slide 15 
min c  x 
s.t. Ax = b 
e  x = 1 
x  0 
        
x1 A1 + x2 A2 +    + xn An = b 
c1 c2 cn z 
Slide 16 
Slide 17 
6 Computational eciency 
Slide 18 
Exceptionalpracticalbehavior:linearin n 
Worst case 
max xn 
s.t.  x1  1 
xi1  xi  1xi1,i =2,...,n 
Slide 19 
Theorem Slide 20 
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>z 
b . B 
D 
E F C 
G H I 
. 
. . 
initial basis 
z 
. 
b . . . . . . 
. 
. next basis 
optimal basis 1 
2 3 
4 
5 6 
7 
8 
(a) (b) x1 x2 x3 
x1 x2 
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 7: The Simplex Method III</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	Finding an initial BFS 
	The complete algorithm 
	The column geometry 
	Computational eciency 
	The diameter of polyhedra and the Hirch conjecture 
2 Finding an initial BFS 
Slide 2 
	Goal: Obtain a BFS of Ax = b, x  0
ordecidethatLOPisinfeasible.
	Special case: b  0
Ax  b, x  0
 Ax + s = b, x, s  0 
s = b, x = 0 
2.1 Articial variables Slide 3 
Ax = b, x  0 
1.	Multiply rows with 1toget b  0. 
2. Introduce articial variables y, start with initial BFS y = b, x =0, and
apply simplexto auxiliaryproblem
min y1 + y2 + ... + ym 
s.t. Ax + y = b 
x, y  0 
Slide 4 
3.	If cost &gt; 0  LOP infeasible; stop. 
4.	If cost =0and noarticial variableisinthebasis,thenaBFS wasfound. 
5. Else, allyi  =0,butsomearestillinthebasis.Say wehave AB(1),..., AB(k)
inbasis k&lt;m. There are mk additional columns of A to form a basis.
Slide 5 
6. Drive articial variables out of the basis: If lth basic variable is arti
cial examine lth row of B1A. If all elements = 0  row redundant.
Otherwise pivot with 0 element.
 = 
1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>2.2 Example 
Slide 6 
min x1 + x2 + x3 
s.t. x1 +2x2 +3x3 =3 
x1 +2x2 +6x3 =2 
4x2 +9x3 =5 
3x3 + x4 =1 
x1,...,x4  0. 
min x5 + x6 + x7 + x8 
s.t. x1 +2x2 +3x3 + x5 =3 
x1 +2x2 +6x3 + x6 =2 
4x2 +9x3 + x7 =5 
3x3 + x4 + x8 =1 
x1,...,x8  0. 
= 3 
= 2 
= 5 
= 1 
= 
= 
= 
= 
= 2 
= 0 
= 2 
=1/3 Slide 7 
x1 x2 x3 x4 x5 x6 x7 x8 
0 8 21 1 0000 
12 30 1000 
12 60 0100 
04 90 0010 
0 0 3 1* 0001 11 
x5 
x6 
x7 
x8 
x1 x2 x3 x4 x5 x6 x7 x8 
0 8 18 00001 
1 2 3 01000 
1 2 6 00100 
0 4 9 00010 
0 0 3* 10001 
Slide 8 
x1 x2 x3 x4 x5 x6 x7 x8 
0 8 0 6000 7 
12 0 1100 1 
1 2* 0 2010 2 
04 0 3001 3 
00 11/3 0 0 01/3 
2 10 
3 
2 
5 
1 
4 x5 
x6 
x7 
x4 
x5 
x6 
x7 
x3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Geometry I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>/1/5/./0/8/1J///6/./2/5/1J  In tro duction  t o  M athematical  Programming  Lecture  /2/:  Geometry  of  Linear  Optimization  I</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Equalit y  h  o  ld  s if ai
0 x  /=  bi
/;; i /2  I /;; s in  ce ai 
spans  /&lt;
n/;; ai
0 x  /=  bi 
has  a  u nique  solution  x  /=  x  
 /. /8</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Large scale optimization II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1. Dantzig-Wolfe decomposition 
2. Key Idea 
3. Bounds 
2 Decomposition 
Slide 2 
  min c1x1 + c2x2 
s.t. D1x1 + D2x2 = b0 
F 1x1 = b1 
F 2x2 = b2 
x1, x2  0 
 Relation with stochasticprogramming? 
 Firmsproblem 
2.1 Reformulation Slide 3 
 Pi = xi  0 |F ixi = bi ,i =1, 2 
 xij , j  Ji extreme points of Pi 
 wik , k  Ki, extreme rays of Pi. 
 For all xi  Pi 
xi = ij xij + ik wik , 
jJi kKi 
ij  0 and ik  0 
ij =1,i =1, 2 
jJi Slide 4 
min j 
1c  
1xj 
1 + 1 k c  
1w k 
1 + j 
2c  
2xj 
2 + 2 k c  
2w k 
2 
jJ1 kK1 jJ2 kK2 
s.t. j 
1D1xj 
1 + 1 kD1w k 
1 + j 
2D2xj 
2 
jJ1 kK1 jJ2 
k k + 2 D2w2 = b0 
kK2 
1 j =1 
jJ1 
j 
2 =1 
jJ2 
ij  0,ik  0,  i,j,k. 
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>By weak duality, 
  z  qb0 + z1 + z2 
 = qb0 + r1 + r2 +(z1 r1)+(z2 r2) 
= z +(z1 r1)+(z2 r2), 
7.2 Example 
Slide 16 
 (1,2)=(0.8, 0.2) 
 cB =(22, 17), z =(22, 17)  (0.8, 0.2) = 21 
 r = 4; z1 =(1, 1, 2)  (2, 1, 2) = 5. 
21  z  21+(5)(4) = 22 
  z = 21.5 
6</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Huge # variables, m0 + 2 constraints	 Slide 5 
	 A bfs is available with a basis matrix B 
  	 p = cB B1; p =(q,r1,r2) 
	 Is B optimal? 
	 Check reduced costs
 
(c1  q  D1)x1 j r1
 
(c1  q  D1)w1 k 
	 Huge number of them 
3 Key idea 
Slide 6 
Consider subproblem: 
  min (c1 qD1)x1 
s.t. x1  P1, 
	 If optimal cost of subproblem is , an extreme ray w k 
1 isgenerated:
 
(c1  q  D1)w1 k &lt; 0, i.e., reduced cost of 1 k is negative; Generate column
 
[D1w1 k , 0, 0] 
 
	 If optimal cost is nite and smaller than r1, then, an extreme point x1 j
 
is generated: (c1   q  D1)x1 j &lt;r1, i.e., reduced cost of 1 j is negative;
 
Generate column[D1x1 j ,1 , 0] 
 
	 Otherwise, reduced costs are nonnegative 
	 Repear for subproblem: 
  min (c2 qD2)x2 
s.t. x2  P2, 
4 Remarks 
Slide 7 
	 Economicinterpretation 
	 Applicability of the method 
   min c1x1 + c2x2 +   + ctxt 
s.t. D1x1 + D2x2 +   + Dtxt = b0 
F ixi = bi, i =1, 2,...,t 
x1, x2,..., xt  0. 
2</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>min cx 
s.t. Dx = b0 
Fx	 = b 
x  0, 
4.1 Termination Slide 8 
	 Finitetermination 
	 Algorithmmakessubstantialprogressinthebeginning,butvery slowlater
 
on
 
	 nofasterthantherevised simplexmethod applied totheoriginalproblem 
	 Storage with t subproblems 
	 Original: O  
(m0 + tm1)2 
	 Decompositionalgorithm O (m0 +t)2 forthetableauof themasterprob
 
lem, and t times O(m12)for subproblems.
 
	 If t =10 and if m0 = m1 is much larger than t, memory requirements for
 
decompositionalgorithmareabout100timessmallerthanrevisedsimplex
 
method.
 
5 Example 
Slide 9 
 
min 4x1  x2  6x3 
s.t. 3x1 +2x2 +4x3 = 17 
1  x1  2 
1  x2  2 
1  x3  2. 
	 P = {x 3 |1  xi  2,i =1, 2, 3}; eight extreme points; 
	 Masterproblem: 
8 
j Dxj =17, 
j=1 
8 
j =1, 
j=1 
Slide 10 
	 x 1 =(2, 2, 2) and x 2 =(1, 1, 2); Dx1 =18, Dx2 =13 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>18 13 B1 0.2 2.6  B = ; = 11 0.23.6 
  2 
cB(1) = c  x 1 =  
4 1 6  	 2  = 22, 
2 
 1 
cB(2) = c  x 2 =  
4 1 6  	 1  = 17. 
2 
 p  =  
q  r  
= cB  B1 =  
22	 17  
B1 =  
1 4  
. 
 
  c qD = 4 1 6](1) 32 4 =[11 2], 
optimal solution is x 3 =(2, 1, 2) with optimal cost 5  r = 4 
 Generate the column corresponding to 3 . 
Slide 11 
x1 = (2,2,2) A 
B. . x2 = (1,1,2) 
x3 = (2,1,2) 
(1,1,1) (1,2,1) 
(2,2,1) (2,1,1) (1,2,2) x3 
x1 x2 
6 Starting the algorithm 
Slide 12 
m0 
min yt
 
t=1
 	  
s.t.  ij Dixij + ikDiwik  + y = b0 
i=1,2 jJi kKi 
j 
1 =1 
jJ1 
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>2 j =1 
jJ2 
ij  0,ik  0,yt  0,  i,j,k,t. 
7 Bounds 
Slide 13 
	 Optimal cost z  
	 z cost of feasible solution obtained at some intermediate stage ofe decom
 
position algorithm.
 
	 ri bethevalueofthedualvariableassociated withtheconvexity constraint
 
forthe ith subproblem
 
	 zi optimal cost in the ith subproblem 
	 Then, 
 z +(zi ri) z  z. 
i 
7.1 Proof Slide 14 
Dual of master problem 
max q  b0 + r1 + r2 
s.t. q  D1xj 
1 + r1  c  
1xj 
1,  j  J1, 
q  D1w k 
1  c  
1w k 
1 ,  k  K1, 
q  D2xj 
2 + r2  c  
2xj 
2,  j  J2, 
q  D2w k 
2  c2  w2 k ,  k  K2. 
Slide 15 
	 (q,r1,r2)dual variables 
 qb0 + r1 + r2 = z 
	 z1 is the optimal cost in the rst subproblem: 
min (c1 x1 j q  D1x1j )= z1, 
jJ1 
min (c  
1w k 
1 q  D1w1 k)  0. 
kK1 
	 (q,z1,z2)is a feasible solution to the dual of master problem 
5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 15: Large Scale Optimization, II</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Semidefinite optimization</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec23/</lecture_pdf_url>
      <lectureno>23</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>6.6 MAXCUT Slide 26 
	Given G =(N,E)undirected graph, weights wij  0 on edge(i,j) E 
	Find a subset S  N: 
iS,jSwij is maximized 
	xj =1for j  S and xj = 1for j  S
nn1 MAXCUT : max 4 wij (1xixj ) 
i=1 j=1 
s.t.xj {1,1},j =1,...,n 
6.6.1 Reformulation Slide 27 
	Let Y = xx  ,i.e., Yij = xixj 
	Let W =[wij ] 
	EquivalentFormulation 
nn 
4 i=1 j=1 1 MAXCUT : max wij W  Y 
s.t.xj {1,1},j =1,...,n 
Yjj =1,j =1,...,n 
 Y = xx 
6.6.2 Relaxation Slide 28 
 	Y = xx  0 
	Relaxation 
nn 
4 i=1 j=1 1 RELAX : max wij W  Y 
s.t.Yjj =1,j =1,...,n 
Y	 0 
Slide 29 
 
MAXCUT  RELAX 
	It turns out that: 
0.87856 RELAX  MAXCUT  RELAX 
	The value of the SDO relaxation is guaranteed to be no more than 12% 
higher than the value of the very dicult to solve problem MAXCUT 
8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 23: Semidenite Optimization</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>7 Barrier Algorithm for SDO 
Slide 30 
	X  0  1(X) 0,...,n(X) 0 
	Naturalbarriertorepel X fromtheboundary1(X)&gt; 0,...,n(X)&gt; 0: 
n
 log(i(X))= 
j=1 
n
j=1 
Slide 31 
	Logarithmic barrier problem 
min B(X)= CX log(det(X))  
s.t. Ai  X = bi ,i =1, ...,m, 	
X 0
	Derivative: B(X)= C X1 
KKT  
Ai  X = bi ,i =1,. ..,m, 
X 0, log( i(X))= log(det(X)) 
C X1 = m 
i=1 yiAi. 
	Since X is symmetric, X = LL  . 
S = X1 = L  1L1 
1 L  SL = I  
 
Ai  X = bi ,i =1,. ..,m, 
X 0, X = LL  
m 
i=1 
I  1 L  SL = 0 
 Nonlinear equations: Take a Newton step analogously to IPM for LO. yiAi + S = C 
0Barrier algorithm needs On log iterations to reduce duality gap from 0   to  
9</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>8 Conclusions 
Slide 32 
	SDOis a verypowerful modeling tool 
	SDO represents the present and future in continuous optimization 
	Barrier Algorithm is very powerful 
	Research software available
10</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1. Preliminaries 
2. SDO 
3. Duality 
4. SDO Modeling Power 
5. Barrier Algorithm for SDO 
2 Preliminaries 
Slide 2 
 A symmetric matrix A ispositive semidenite(A  0)if and only if 
 u Au  0 u  Rn 
 A  0 if and only if all eigenvalues of A are nonnegative 
nn 
j=1  A  B = Aij Bij 
i=1 
2.1 The trace Slide 3 
 The trace of a matrix A isdened 
n
trace(A)= Ajj 
j=1 
 trace(AB)= trace(BA) 
 A  B =trace(A  B) 
3 SDO 
Slide 4 
 C symmetric n  n matrix 
 Ai,i =1,...,m symmetric n  n matrices 
 bi,i =1,...,m scalars 
	Semidenite optimizationproblem(SDO)
(P): min C  X
s.t. Ai  X = bi i =1,...,m 
X  0 
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Thereisno nitealgorithmforsolvingSDO.Thereisasimplexalgorithm,
butitisnotanitealgorithm.Thereisnodirect analog of a basicfeasible
solution for SDO.
	Given rational data, the feasible region may have no rational solutions.
Theoptimal solutionmay nothaverational componentsorrational eigen
values.
6 SDO Modeling Power 
6.1	Quadratically 
Constrained Problems Slide 15 
min (A0x + b0)  (A0x + b0)c0 x d0 
s.t. (Aix + bi)  (Aix + bi)ci x di  0 , 
i =1,...,m 
(Ax + b)  (Ax + b)c  x d  0  
	
I Ax + b  
 0 (Ax + b)  c  x + d 
Slide 16 
min	 t 
s.t. (A0x + b0)  (A0x + b0)c0 x d0 t  0 
(Aix + bi)  (Aix + bi)c  
ix di  0,  i 
Slide 17 
 
min t 
	
I A0x + b0  
s.t.  0 
(A0x + b0)  c0 x + d0 + t 
	
I Aix + bi  
 0  i 
(Aix + bi)  ci x + di 
6.2	Eigenvalue Problems 
Slide 18 
	X: symmetric n  n matrix 
	max(X)= largest eigenvalue of X 
	1(X) 2(X)   m(X)eigenvalues of X 
5</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>4 Duality 
Slide 9 m
(D) yibi : max 
i=1 
m
yiAi + S = C s.t. 
i=1 
S  0 
Equivalently, 
m
(D) yibi 
m : max 
i=1 
C  yiAi  0 s.t. 
i=1 
4.1 Example 
Slide 10 
(D) max 11y1 +19y2 
1 
03 
175 804 307 
S  0 
(D) max 11y1 +19y2 
    01 028 123 
7 260 290 + S s.t. + y2 = y1 
s.t. 
 11y1 0y2 20y1 2y2 31y1 8y2 
20y1 2y2 93y1 6y2 07y1 0y2 
 0 
m 31y1 8y2 07y1 0y2 75y1 4y2 
4.2 Weak Duality 
Slide 11 
Theorem Given a feasible solution X of(P) and afeasible solution(y,S) of 
(D), 
X  X  0 C  yibi = S  
i=1 
m 
i=1 
and(D)and SX = 0 
3 If C  X  yibi = 0, then X and(y,S) are each optimal solutions to(P)</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Theorem max(X) t  t  I X  0 
 
k
i(X) t  t k  s trace(Z) 0 
i=1 
Z  0 
Z X + s I  0 
n
 Recalltrace(Z)= Zii 
i=1 
6.3	Optimizing 
Structural Dynamics 
Slide 19 
 Select xi, cross-sectional area of structure i, i =1,...,n 
 M (x)= M0 + 
i xi Mi, mass matrix 
 K(x)= K0 + 
i xi Ki, stiness matrix 
 Structure weight w = w0 + 
i xiwi 
	Dynamics
M (x)d + K(x)d = 0
Slide 20 
 d(t)vector of displacements 
n
 di(t)= ij cos(j t j ) 
j=1 
 det(K(x)M(x)2)=0; 1  2    n 
1/2  Fundamentalfrequency: 1 = min(M(x),K(x)) 
	We want to bound the fundamental frequency
1    M (x)2 K(x) 0
 Minimize weight Slide 21 
Problem: Minimize weight subject to 
Fundamentalfrequency 1   
Limits on cross-sectional areas 
Formulation 
min w0 + 
i xi wi 
s.t. M(x)2 K(x) 0 
li  xi  ui 
6</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.4	Measurements with Noise Slide 22 
	x: ability of a random student on k tests
E[x]= x, E[(x x)(x x)  = 
 y: score of a random student on k tests 
	v: testing error of k tests, independent of x
E[v]= 0, E[vv  ]= D,diagonal(unknown)
	y = x + v; E[y]= x
E[(y x)(y x)  ]= = + D
 Objective: Estimate reliably xand  
Slide 23 
 Take samples of y from which we can estimate x,
  ex: total ability on tests 
  ey: total test score 
 Reliability of test:= 
 Var[ex] e  e e  De = =1 Var[e  y] e  e e  e 
Slide 24 We can nd a lower bound on the reliability of the test 
min	e  e 
s.t. + D = 
,D  0 
D diagonal 
Equivalently, 
max e  De 
s.t. 0  D  
D diagonal 
6.5	Further Tricks Slide 25 
  
B	C  
 A =  0  D CB1C  0 CD 
  c	b  
 x Ax +2b  x + c  0, x   0 bA 
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>4.3 Proof Slide 12 
	We must show that if S  0 and X  0,then S  X  0 
	Let S = PDP  and X = QEQ  where P, Q are orthonormal matrices
and D, E are nonnegative diagonal matrices

S  X =trace(S  X)= trace(SX)
=trace(PDP  QEQ  ) 
n 
j=1  QEQ   QEQ  trace(DP P ) Djj (P P )jj  0, = = 
since Djj  0 and the diagonal of P  QEQ  P must be nonnegative. 
	Supposethattrace(SX)=0. Then 
n
Djj (P  QEQ  P )jj =0 
j=1 
	Then, for each j =1,...,n, Djj =0 or(P  QEQ  P )jj =0. 
	The latter case implies that the jth row of P  QEQ  P is all zeros. There
fore, DP  QEQ  P =0, and so SX = PDP  QEQ  = 0.
4.4 Strong Duality 
Slide 13 
	(P)or(D)might not attain their respective optima 
	There might be a duality gap, unless certain regularity conditions hold 
Theorem 
	If there exist feasible solutions Xfor (P) and (y,S)for(D) such that
X 0, S 0
  	then,both(P)and(D)attain their optimal values zP and zD 
  	z = zP D 
5 SDO vs LO 
Slide 14 
	Theremaybea niteorinnitedualitygap. Theprimal and/ordual may
or may not attain their optima. Both problems will attain their common
optimal value if both programs have feasible solutions in the interior of
the semidenite cone.
4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3.1 Example 
Slide 5 
n =3 and m =2 
A1 = 
 101 
037 
, A2 =  028 
260 
, C =  123 
290  
175 804 307 
b1 =11,b2 =19 
X =  x11 x12 x13 
x21 x22 x23 
x31 x32 x33  
Slide 6 
(P): min x11 + 4x12 + 6x13 + 9x22 + 7x33 
s.t. x11 + 2x13 + 3x22 + 14x23 + 5x33 = 11 
4x12 + 16x13 + 6x22 + 4x33 = 19 
X = 
 x11 x12 x13 
x21 x22 x23 
x31 x32 x33 
 0 
3.2 LO as SDO 
LO : min c  x Slide 7 
s.t. Ax = b 
x  0 
= 
 ai1 0 ... 0 
0 ai2 ... 0 
... . .... .. . . 
, C = 
 c1 0 ... 0 
0 c2 ... 0 
... . ... . .. . . 
 Ai 
0 0 . . . ain 0 0 . . . cn 
Slide 8 
(P): min C  X 
s.t. Ai  X = bi, i = 1,. . . ,m 
Xij = 0, i = 1,. . . ,n, j = i+ 1,. . . ,n 
X  0 
X = 
 x1 0 . . . 0 
0 x2 . . . 0 
. . . . . . . . . . . . 
0 0 . . . xn 
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Interior point methods III</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec22/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>4 The Primal-Dual Barrier Algorithm 
Slide 8 
1. (Initialization)Start with x0 &gt; 0, s0 &gt; 0, p0, and set k =0 
2. (Optimalitytest) If(sk)  xk &lt; stop; else go to Step 3. 
3. (Computation of Newton directions) 
(sk)  xk 
k  = n
Xk = diag(x k 
1 ,...,x k
n)
Sk = diag(s1 k ,...,s nk)
Solve linear system 
  k    A 00 dx Axk  b 
    k   0 A  I  dpk  =   A  p+ sk  c  
Sk 0 Xk dk XkSke   k e s 
Slide 9 
4. (Findstep lengths) 
k 
iPk = min 1, min  (dx
k{i|(dk )i &lt;0} )i x x
k 
k = min 1, min  si 
D (dk{i|(dks )i&lt;0} s )i 
5. (Solution update) 
x k+1 = x k + Pk dk
x 
p k+1 = p k + k dk 
Dp 
k+1 k + k dk s = s Ds 
6. Let k := k +1 andgotoStep 2 
5 Insight on behavior 
Slide 10 
 AneScaling 
dane = X2 I  A  (AX2A  )1AX2 c 
 Primalbarrier 
dprimalbarrier =  
I  X2A  (AX2A  )1A  
Xe  1 X2 c  
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1. The Barrier Problem 
2. Solving Equations 
3. The Primal-Dual Barrier Algorithm 
4. Insight on Behavior 
5. Computational Aspects 
6. Conclusions 
2 The Barrier Problem 
Slide 2 
Barrierproblem: 
n 
min B(x)= cx   logxj 
j=1 
s.t. Ax = b 
KKT:    1 1 c   ,..., + A  p()= 0 x1() xn() 
Ax()= b, x() 0 
2.1 Optimality Conditions 
Slide 3 Set sj()= xj()
Ax()= b
x()  0
A  p()+s()= c
s()  0
sj()xj()=  or
X()S()e = e
X()=diag x1(),...,xn(), S()=diag s1(),...,sn() 
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 Solving Equations 
  Slide 4 
Ax  b 
 F (z)=  A  p + s  c  
XSe  e 
z =(x,p,s),r =2n + m 
Solve 
F (z  )= 0 
3.1 Newtons method Slide 5 
F (z k + d) F (z k)+J(z k)d 
Here J(zk)isthe r  r Jacobian matrix whose(i,j)thelement is given by 
Fi(z) 
zj z=zk 
F (z k)+J(z k)d = 0 
Set zk+1 = zk + d (d isthe Newton direction) Slide 6 
(x k ,p k ,s k)current primal and dual feasible solution 
Newtondirection d =(dk ,dpk ,dk)x s
     
A 00 dk
x Axk  b 
    k   0 A  I  dpk  =   A  p + s k  c  
Sk 0 Xk dk
s XkSke  ke 
3.2 Step lengths 
Slide 7 
k+1 k + k dk x = x Px 
p k+1 = p k + k dk 
Dp 
s k+1 = s k + k dk 
Ds 
To preserve nonnegativity, take 
k 
ik = min 1, min  x,P {i|(dk )i&lt;0} (dk)i x x  k  
k si 
D = min 1, min  , 
{i|(dks )i&lt;0} (dks )i 
0 &lt; &lt; 1 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>For  =  
dcentering = I  X2A  (AX2A  )1A Xe 
	Notethat 1 dprimalbarrier = dcentering + dane  
	When  is large, then the centering direction dominates, i.e., in the beginning,
the barrier algorithm takes steps towards the analytic center
	When  is small, then the ane scaling direction dominates, i.e., towards the
end, the barrier algorithm behaves like the ane scaling algorithm
6 Computational aspects of IPMs 
Slide 11 
Simplex vs. Interiorpoint methods(IPMs) 
	Simplex method tends to perform poorly on large, massively degenerate
problems, whereas IP methods are much less aected.
	Key step in IPMs 
2AXk
kA  is usually written as A  d = f 
2InimplementationsofIPMs AX  
kAX2 
where L is a square lower triangular matrix called the Cholesky factor 
	Solve system A  = LL  , 
2AXk
by solving the triangular systems
Ly = f, L  d = y
	The construction of L requires O(n3)operations; but the actual compu
tational eort is highly dependent on the sparsity (number of nonzero
entries) of L
	Largescaleimplementations employheuristics(reorderrowsand columns
of A)to improve sparsity of L. If L is sparse, IPMs are stronger.
7 Conclusions 
Slide 12 
	IPMs represent the present and future of Optimization. 
	Very successfulin solving very largeproblems. 
	Extend to general convex problems 
4 A  d = f</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 22: Primal-dual Barrier
Interior Point Algorithm</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Network flows II
Courtesy of Prof. Andreas Schulz. Used with permission.</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in 
arc (3,2)? 
2 3 1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in arc (2,6)? 
2 3 6 
1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in arc (7,1)? 
2 3 6 4 1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in arc (1,2)? 
2 3 6 4 3 
1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 Note: there are 
two different ways 
of calculating the flow on (1,2), and 
both ways give a 
flow of 4. Is this a 
coincidence? 
2 3 6 4 34 
3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 
What is the potential for node 5? 0 
-5 -6 
-2 -1 
-4 These are the node potentials 
associated with this tree. They 
do not depend on arc flows, nor on costs of non-tree arcs. 1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 0 
-5 -6 
-2 -1 
-4 -1 
1 
3 6 
4 5 2 7 0 
-5 -6 
-2 -1 
-4 -1 7 
2 -3 Node potentials Original costs 1 
3 6 
4 5 2 7 
2 
5 -3 Flow on arcs Reduced costs 
2 3 6 4 34 
1 
3 6 
4 5 2 7 
0 Flow on arcs 
2 3 6 4 34 1 
3 6 
4 5 2 7 
32 0 3 1 34 
6</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 
3 6 
4 5 2 7 
1 
3 6 
4 5 2 7 
1 
3 6 
4 5 2 7 
0 
0 
0 
1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in 
arc (4,3)? 1 
3 6 
4 5 2 7 1 
3 -6 
-4 1 
2 3 0 
0 
0 What is the flow in arc (5,3)? 
2 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>110 5 2 
4 3 2 4 
5 -15 
1 2 
4 7 10 
3 6 
6 
3 5 6 
-5 
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>1 
3 6 
4 5 2 7 
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 Here is a spanning 
tree with arc costs. How can one choose 
node potentials so 
that reduced costs of tree arcs are 0? 1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 There is a redundant constraint in the minimum cost flow problem. 
One can set p
1 arbitrarily. We 
will let p1 = 0. 
What is the node potential for 2? 0 
1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 
What is thenode potential for 7? 0 
-5 1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 
What is the potential for node 3? 0 
-5 -6 
1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 
What is the potential for node 6? 0 
-5 -6 
-2 1 
3 6 
4 5 2 7 5 -6 
3 -4 
-2 1 
What is the potential for node 4? 0 
-5 -6 
-2 -1 
5</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 6.251J / 15.081J Introduction to Mathematical Programming
Fall 2009</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>9</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>3 
6 
5 2 1 
7 3 
4 
5 
4 
4 2 4 
2 
3 3 1 
1 
3 
6 
5 2 1 
7 3 
5 
4 
4 2 4 
3 
6 
5 2 1 
7 2 
6 
5 
3 1 3 
3 
6 
5 2 1 
7 1 
7 
4 
2 0 2 
3 
6 
5 2 1 
7 1 
7 
4 
2 2 
4</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>1 
3 6 
4 5 2 7 1 
3 6 
4 5 2 7 
1 
3 6 
4 5 2 7 
8</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Network flows I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>5 Min-Cost Flow 
5.1 Description 
Slide 5 
	Determine a least cost shipment of a commodity through a network in
order to satisfy demands at certain nodes from available supplies at other
nodes. Arcs have capacities and cost associated with them
	Distribution of products 
	Flow of items in a production line 
	Routing of cars through street networks 
	Routing of telephone calls 
5.2 In LOP Form Slide 6 
	Network G =(N,A). 
	Arc costs c : A R. 
	Arc capacities u : A N. 
	Nodebalances b : N R. 
min  
cij xij
(i,j)A
s.t.  
xij   
xji = bi for all i  N 
j:(i,j)A j:(j,i)A 
xij  uij for all(i,j) A 
xij  0 for all(i,j) A 
6 Outline 
Slide 7 
	Shortest path applications 
	Maximum Flow applications 
	Minimum cost ow applications 
7 Shortest Path 
7.1 Interword Spacing in LATEX 
Slide 8 
The spacing between words and characters is normally set
automatically by LATEX. Interword spacing within one line
is uniform. LATEX also attempts to keep the word spacing
for dierent lines as nearly the same as possible.
2</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>7.5.1 Transformation costs Slide 18 
	 = cost of inserting a letter in B 
	 = cost of deleting a letter from B 
	g(bi,dj )= cost of mutating a letter bi into dj 
7.5.2 Transformation steps 
Slide 19 
1.	Add or delete letters from B so as to make |B  |= |D|. 
2.	Align B  and D 
3.	Mutate letters of B  sothat B  = D. 
7.5.3 Algorithm 
Slide 20 
	f(b1  bp,d1  dq ): the min cost of transformingB into D by thethree
steps above. We obtain this cost by a recursive way.

f(  ,d1  dj )= j, j=1,...,q
f(b1  bi,  )= i, i =1,...,p.
Slide 21 
Substitution 
B  = b1  bi 
D = d1  dj 
f(b1  bi,d1  dj ) 
= f(b1  bi1,d1  dj1)+g(bi,dj ) Slide 22 
	Addition of dj
B  = b1  bi  
D = d1   dj 
f(b1  bi,d1  dj )= f(b1  bi,d1  dj1)+. 
	Deletion of bi:
f(b1  bi,d1  dj )= f(b1  bi1,d1  dj )+
Slide 23 
Recursion 
f(b1  bi,d1  dj ) 
= min{f(b1  bi1,d1  dj1)+g(bi,dj ), 
f(b1  bi,d1  dj1)+, 
f(b1  bi1,d1  dj )+} 
Slide 24 
The shortest path from 00 to 32 
5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>8.2 Preemptive Scheduling 
	m identical machines to process n jobs 
	Job j mustbeprocessedfor pj periods, j=1,...,n 
	It can not start before period rj and must be competed before period dj 
	We allowpreemption,i.e., we candisrupt theprocessing of onejob with 
another 
	Problem Find a schedule(whichjobisprocessedby which machineat 
whichperiod) such that alljobsareprocessed aftertheirreleasetimesand 
completedbeforetheirdeadlines 
	Cj : completiontimeofjob j: We need to have 
rj + pj Cj dj for all j 
8.3 Formulation 
	Rank all release times and deadlines in ascending order. The ordered 
list of numbers divides the time horizon into a number of nonoverlapping 
intervals. 
	Tkl betheinterval that startsintheperiod k and endsinperiod l.During 
Tkl, we can process any job j that has been released (rj  k) and its 
deadlinehas notyetbeen reached(l  dj ). 
8.3.1 Example 
	4jobs with release times 3, 1, 3, 5, and deadlines 5, 4, 7, 9. 
	The ascending list of release times and deadlines is 1,3,4,5,7,9. 
	Fiveintervals: T13, T34, T45, T57, T79. 
8.3.2 Network 
	Nodes: source s, sink t, a node corresponding to eachjob j, and a node 
corresponding to each interval Tkl. 
	Arcs:(s,j), with capacity pj . Flow represents the number of periods of 
processing thatjob j receives. 
	Arcs:(Tkl,t), with capacity m(l k). Flow represents the total number 
of machine-periods of processing during Tkl. 
	Arcs:(j,Tkl)if rj  k  l  dj with capacity lk. Flow represents the 
number ofperiodsthatjob j isprocessedduring Tkl. 
7 Slide 27 
Slide 28 
Slide 29 
Slide 30 
Slide 31 
Slide 32</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>8 Maximum Flow 
8.1 The tournament problem 
Slide 25 
	Each of n teams plays against every other team a total of k games. 
	Eachgame endsin a win or aloss(nodraws) 
	xi: the number of wins of team i. 
	X set of allpossible outcome vectors(x1,...,xn) 
	Given x =(x1,...,xn)decide whether x  X 
8.1.1 Formulation Slide 26 
	Supply nodes T1,...,Tn represent teams with supply x1,...,xn 
	Sincetotal number of winstotal number ofgames, we musthave 
xi = n(n 1)k/2 
	Demand nodes 
G12,...,G1n,G23,...,G2n,...,Gij ,...,Gn1,n 
denotegamesbetween Ti and Tj withdemand k. 
	Arcs: (Ti,Gij ),(Tj ,Gij ). The ow from Ti to Gij represents the total
number of games between i and j wonby i
	Transportation model feasible if and only if x  X 
6</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>s t 
Tkl jpj 
l -k 
m( l -k) 
9 Min-Cost Flow 
9.1 Passenger Routing 
Slide 33 
	United Airlines has seven daily ights from BOS to SFO, every two hours,
starting at 7am.
	Capacities are 100, 100, 100, 150, 150, 150, and . 
	Passengers suering from overbooking are diverted to later ights. 
	Delayed passengers get $200 plus $20 for every hour of delay. 
	Suppose that today the rst six ighs have 110, 160, 103, 149, 175, and 140
conrmed reservations.
Determine the most economical passenger routing strategy! 
8</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The spacing between words and characters is normally set auto
maticallybyLATEX. Interwordspacing withinonelineisuniform.
LATEX also attempts to keep the word spacing for dierent lines
as nearly the same as possible.
7.2 Interword Spacing in LATEX (2) 
Slide 9 
	The paragraph consists of n words,indexedby1, 2,...,n. 
	cij is the attractiveness of a line if it begins with i and ends with j 1. 
	(LATEX uses a formula to compute the value of each cij .) 
Forinstance, 
c12 = 10, 000 c13 = 1, 000 
c14 =100 c1,37 = 100, 000 
... 
7.3 Interword Spacing in LATEX (3) 
Slide 10 
	Theproblemofdecomposingaparagraphintoseverallinesoftexttomax
imizetotal attractiveness canbeformulated as a shortestpathproblem.
	Nodes? Arcs? Costs? 
7.4 Project Management 
Slide 11 
	Aproject consists of a set ofjobs and a set ofprecedence relations 
	Given a set A ofjobpairs(i,j)indicating thatjob i cannot start before
jobj is completed.
	ci duration ofjob i 
	Find the least possible duration of the project 
7.4.1 Formulation Slide 12 
	Introduce two articial jobs s and t, of zero duration, that signify the
beginning and the completion of the project
	Add(s,i)and(i,t)to A 
	pi timethatjob i begins 
	(i,j) A: pj pi + ci 
	Projectduration: pt ps 
3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 16: Network Flows, I</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Networks 
	Electrical&amp;PowerNetworks 
	RoadNetworks 
	AirlineRoutes 
	InternetBackbone 
	PrintedCircuitBoard 
	SocialNetworks 
2 Common Thrust Slide 1 
Slide 2 
Move some entity(electricity, a consumerproduct, aperson, a vehicle, a mes
sage, ...) fromonepointtoanotherintheunderlying network,aseciently as 
possible. 
1.	Learn how to model application settings as network ow problems. 
2.	Study ways to solve the resulting models. 
3 Shortest Path 
3.1 Description 
Slide 3 
	Identify a shortestpathfrom agiven source nodeto agiven sink node. 
	Finding a path of minimum length. 
	Finding a path taking minimum time. 
	Finding a path of maximum reliability. 
4 Maximum Flow 
4.1 Description 
Slide 4 
	Determine the maximum ow that can be sent from a given source node
to a sink node in a capacitated network.
	Determining maximum steady-state ow of 
	petroleum products in a pipeline network 
	cars in a road network 
	messages in a telecommunication network 
	electricity in an electrical network 
1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Slide 13 
 
min	pt ps 
s.t pj pi ci,  (i,j) A. 
	Dual 
max	 
cifij
(i,j)A
s.t.  
fji   
fij = bi 
{j|(j,i)A} {j|(i,j)A} 
fij 0 
Slide 14 
	bs = 1, bt =1, and bi =0for i = s,t. 
	Shortestpathproblem, where eachprecedence relation(i,j)  A corre
sponds to an arc with cost of ci. 
Slide 15 
Activity
s
A
B
C
D
E
t
0 
0 S ImmediatePredecessor
S
S
A,B
A
C,D
E
14 
14 
3 5 A D 
B C Time(ci)
0
14
3
5
7
10
0
Slide 16 
7 
10 T 
E 
Slide 17 7.5	DNA Sequencing 
	Given two sequences of letters, say 
B = b1  bp and D = d1  dq 
	How similar are the two sequences? 
	What is the min cost of transforming B to D? 
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Duality theory III</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 11: Duality Theory IV</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Slide 9 
	x  S.  satisfying 0 &lt;  1, y + (x y)  S (S convex) 
	||y x ||2 ||y + (x y) x ||2 
= ||y x  ||2 +2(y x  )  (x y)+ 2||x y||2 
	2(y x )  (x y)+ 2||x y||2  0. 
	Divide by ,(y x )  (x y)  0, i.e., 
  (y x )  x  (y x )  y 
    =(y x )  x +(y x )  (y x ) 
  &gt;	(y x )  x . 
 	c = y x proves theorem 
6 Farkas lemma 
Slide 10 
Theorem:If Ax = b, x  0 is infeasible, then there exists a vector p such that 
p  A  0  and p  b &lt; 0. 
	S = y there exists x such that y = Ax, x  0 b /S. 
	S is convex; nonempty; closed;
S is the projection of {(x,y)|y = Ax, x  0}onto the y coordinates,
is itself a polyhedron and is therefore closed.
	b /S: p suchthat p  b &lt; p  y for every y  S. 
	Since 0  S, we must have p  b &lt; 0. 
	Ai and &gt; 0, Ai  S and p  b &lt;p  Ai 
	Divideby  and then take limit as  tendstoinnity: p  Ai 0  p  A  
0  
7 Duality theorem 
Slide 11 
min c  x	 max p  b 
s.t.	Ax b s.t. p  A = c  
p  0 
and we assume that the primal has an optimal solution x  . We will show that 
thedualproblemalsohasafeasiblesolutionwith thesamecost. Strongduality 
follows then from weak duality. Slide 12 
	I = {i |a  
ix  = bi} 
	We next show: if ai d  0 for every i  I,then c  d 0 
3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>a  
i(x  + d)aix  = bi for all i  I. 
 If i/ I, a  
ix  &gt;bi hence a  
i(x  + d)&gt;bi. 
 x  + d isfeasible 
Slide 13 
 By optimality x  , c  d  0 
 By Farkas lemma 
c = piai. 
iI 
 For i/ I, we dene pi =0, so p  A = c  . 
 
   pb = pibi = piaix = cx , 
iI iI 
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	Overview and objectives 
	WeistrassTheorem 
	Separating hyperplanestheorem 
	Farkas lemma revisited 
	Duality theorem revisited 
2 Overview and objectives 
Slide 2 
	So far: Simplex  Duality  Farkaslemma 
	Disadvantages: specialized to LP, relied on a particular algorithm 
	Plantoday: Separation(AGeometricproperty)  Farkaslemma 
Duality
	Purely geometric, generalizes to general nonlinear problems, more funda
mental
3 Closed sets 
Slide 3 
	A set S n is closed if x1 ,x2 ,... is a sequence of elements of S that
converges to some x n,then x  S.
	Everypolyhedronis closed. 
4 Weierstrass theorem 
Slide 4 
If f : n  is a continuous function, and if S is a nonempty, closed, and 
bounded subset of n, then there exists some x   S such that f(x ) f(x) 
for all x  S. Similarly, there exists some y   S such that f(y ) f(x)for 
all x  S. 
Note: Weierstrass theorem is not valid if the set S is not closed. Consider, 
S = {x | x&gt; 0}, f(x)= x 
5 Separation 
Slide 5 
Theorem: Let S be a nonempty closed convex subset of n andlet x  n: 
x  /S. Then, there exists some vector c n such that c  x  &lt; c  x for all 
x  S. 
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>.x * 
c S 
w 
S .x * 
.. 
D B 
y 
S .x * 
..y x c 
q . 
(a) (b) 
5.1 Proof Slide 6 
 Fix w  S 
 B = x ||x x ||||w x || , 
 D = S B 
 D = , closed and bounded. Why? 
 Consider min ||x x || 
Slide 7 
Slide 8  By Weierstrass theorem there exists some y  D such that 
||y x ||||x x ||,  x  D. 
x  S and x /D, ||x x ||&gt; ||w x ||||y x ||. 
 y minimizes ||x x ||x  S. 
 Let c = y x  
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Geometry III</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1. Projections of Polyhedra 
2. Fourier-Motzkin Elimination Algorithm 
3. Optimality Conditions 
2 Projections of polyhedra 
Slide 2 
 k : n k projects x onto its rst k coordinates: 
k (x)= k(x1,...,xn)=(x1,...,xk ). 
 
k(S)= k(x)|x  S ;
 
Equivalently
 
k(S)=(x1,...,xk) there exist xk+1,...,xn 
s.t. (x1,...,xn) S. 
x1 x2 x3 
P 1( S ) P 2( S ) 
2.1 The Elimination Algorithm 
2.1.1 By example 
Slide 3 
	 Considerthepolyhedron
 
x1 + x2  1
 
1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>2.3 Implications 
	 Let P n+k be a polyhedron. Then, the set 
x n  there exists y k such that(x,y) P 
is also a polyhedron. 
	 Let P n be a polyhedron and let A be an m  n matrix. Then, the
 
set Q= {Ax |x  P}is also a polyhedron.
 
	 The convex hull of a nite number of vectors is a polyhedron. 
2.4 Algorithm for LO 
 	 Consider mincx subjectto x  P. 
 	 Dene a new variable x0 and introduce the constraint x0 = cx. 
	 Applytheeliminationalgorithmn timestoeliminatethevariablesx1,...,xn 
	 We are left with the set 
Q= x0 |there exists x  P such that x0 = cx ,
 
and the optimal cost is equal to the smallest element of Q.
 
3 Optimality Conditions 
3.1 Feasible directions 
	 Weareatx  P and wecontemplatemoving awayfrom x,inthedirection
 
of a vector d n.
 
	 We need to consider those choices of d that do not immediately take us
 
outside the feasible set.
 
	 A vector d n is said tobe a feasible direction at x, if there exists a
 
positive scalar  for which x + d  P.
 
3 Slide 5 
Slide 6 
Slide 7 
Slide 8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 4: Geometry of Linear Optimization III</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>x1 + x2 +2x3  2 
2x1 +3x3  3 
x1 4x3  4 
2x1 + x2 x3  5. 
 We rewrite these constraints 
0  1x1 x2 
x3  1(x1/2)(x2/2) 
x3  1(2x1/3) 
1+(x1/4)  x3 
52x1 + x2  x3. 
 Eliminate variable x3, obtaingpolyhedron Q 
0  1x1 x2 
1+x1/4  1(x1/2)(x2/2) 
1+x1/4  1(2x1/3) 
52x1 + x2  1(x1/2)(x2/2) 
52x1 + x2  1(2x1/3). 
2.2 The Elimination Algorithm 
Slide 4 
1. Rewrite jn 
=1 aij xj  bi intheform 
n1 
ainxn  aij xj + bi,i =1,...,m; 
j=1 
if ain =0, divide both sides by ain. By letting x =(x1,...,xn1)that P 
is represented by: 
 xn  di + fix, if ain &gt; 0, 
 dj + fj x  xn, if ajn &lt; 0, 
 0  dk + fkx, if akn =0. 
2. Let Qbethepolyhedronin n1 denedby: 
  dj + fj x  di + fix, if ain &gt; 0 and ajn &lt; 0, 
 0  dk + fkx, if akn =0. 
Theorem:
 
Thepolyhedron Q constructed by the elimination algorithm is equal to the
 
projectionn1(P)of P.
 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Slide 9 
	 x be a BFS to the standard form problem corresponding to a basis B. 
	 xi =0,i  N, xB = B1B. 
	 We consider moving away from x, to a new vector x + d, by selecting a
 
nonbasic variable xj and increasing it to a positive value , while keeping
 
the remaining nonbasic variables at zero.
 
	 Algebraically, dj =1, and di = 0 for every nonbasic index i otherthan j. 
	 The vector xB of basic variables changes to xB + dB . 
	 Feasibility: A(x + d)= B  Ad = 0. 
	 0 = Ad = n
i=1 Aidi = m
i=1 AB(i)dB(i) + Aj = BdB + Aj  dB =
 
B1Aj .
 
	 Nonnegativity constraints? 
	 If x nondegenerate, xB &gt; 0;thus xB + dB  0 for  is suciently 
small. 
	 If xdegenerate,then d is not always a feasible direction. Why? 
	 Eects in cost?
 
 Cost change: c  d = cj cB B1Aj Thisquantity is called reduced cost
 
cj of the variable xj .
 
3.2 Theorem Slide 10 
	 x BFS associated with basis B 
	 c reduced costs
 
Then
 
	 If c  0  x optimal 
	 x optimal and non-degenerate  c  0 
3.3 Proof 
	 y arbitrary feasible solution 
	 d = y x  Ax = Ay = b  Ad = 0 
 BdB + 
iN 
 dB =  
iN 
 c  d = c  
B Slide 11 
Aidi = 0 
B1Aidi 
dB + cidi 
iN Slide 12  = (ci cB B1Ai)di = cidi
 
iN iN
 
4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Since yi  0 and xi =0,i  N,then di = yi xi  0,i  N 
 c  d = c  (y x) 0  c  y  c  x 
 x optimal
 
(b)Yourturn
 
5</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Problems with exponentially many constraints</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>1 
2 
3 
4 1,2 
1,3 
1,4 
2,3 
2,4 
3,4 2 
4 4 
4 
4 
2 9 
6 
7 1 s t 
5 Conclusions 
Slide 19 
	Ellipsoid algorithm can characterize the complexity of solving LOPs with
an exponential number of constraints
	For practical purposes use dual simplex 
	Ellipsoid method is an important theoretical development, not a practical
one
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 The separation problem 
Slide 5 
Given a polyhedron P n and a vector x n, the separation problem is 
to: 
	Either decide that x  P, or 
	Find a vector d such that d  x &lt; d  y for all y  P 
What is the separation problem for 
aixi |S|, for all subsets S of {1,...,n}? 
iS 
4 Polynomial solvability 
4.1	Theorem Slide 6 
If we can solve the separation problem (for a family of polyhedra) in time 
polynomial in n and log U, then we can also solve linear optimization problems 
in time polynomial in n and log U. If log U  Cn log U0, then it is also 
polynomial in log U0 
	Proof ? 
	Converse is also true 
	Separation and optimization are polynomially equivalent 
4.2	Minimum Spanning 
Tree (MST) 
Slide 7 
	How do telephone companies bill you? 
	It used to be that rate/minute: Boston  LA proportional to distance in
MST
	Other applications: Telecommunications, Transportation (good lower bound
for TSP)
Slide 8 
	Given a graph G =(V,E) undirected and Costs ce,e  E. 
	Find a tree of minimum cost spanning all the nodes. 
1, if edge e is included in the tree 	Decision variables xe = 0, otherwise 
Slide 9 
	The tree should be connected. How can you model this requirement? 
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 19: Problems with exponentially 
many constraints</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>4.4 Probability Theory 
Slide 13 
	Events A1,A2 
	P (A1)=0.5,P (A2)=0.7, P (A1 A2)  0.1 
	Are these beliefs consistent? 
	General problem: Given n events Ai i  N = {1,...,n}, beliefs 
P(Ai)  pi, i  N, 
P(Ai Aj )  pij , i,j  N, i&lt;j. 
	Given the numbers pi and pij , which are between 0 and 1, are these beliefs
consistent?
4.4.1 Formulation Slide 14 
x(S)=P iS Ai i/ , S Ai 
x(S)  pi, i  N, 
{S|iS} 
x(S)  pij , i,j  N, i&lt;j, 
{S|i,jS} 
x(S)=1, 
S 
x(S)  0,  S. Slide 15 
The previous LP is feasible if and only if there does not exist a vector (u, y,z) such 
that 
yij + ui + z  0,  S, 
i,jS,i&lt;j iS 
pij yij + piui + z 1, 
i,jN,i&lt;j iN 
yij  0,ui  0,	 i,j  N, i&lt;j. Slide 16 
Separation problem: 
	   z	+ min f(S)= yij + ui  0? 
S i,jS,i&lt;j iS 
     Example: y12 = 2, y13 = 4, y14 = 4, y23 = 4, y24 = 1, y34 = 7, 
u1  = 9, u2  = 6, u3  = 4, u4  = 2, and z  =2	 Slide 17 
Slide 18 
	The minimum cut corresponds to S0 = {3,4}with value c(S0) = 21. 
	f(S0)= y  u  = 7+4+2= 1ij + i
i,jS0 ,i&lt;j iS0
	f(S)+ z   f(S0)+ z  = 1+2=1 &gt; 0,  S 
	Given solution (y  ,u  ,z ) is feasible 
4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Let S be a set of vertices. Then S and V \S should be connected 
i  S  Let (S)= {e =(i,j)  E : j  V \S 
 Then, 
xe  1 
e(S) 
 What is the number of edges in a tree? 
 Then, xe = n 1 
eE 
4.2.1 Formulation Slide 10 
IZMST = min cexe
eE
 xe 1   S  V,S = ,V    e(S) 
H xe = n 1   eE  xe {0,1}. 
How can you solve the LP relaxation? 
4.3	The Traveling Salesman 
Problem Slide 11 
Given G =(V,E) an undirected graph. V = {1,...,n}, costs ce  e  E. Find 
a tour that minimizes total length. 
4.3.1 Formulation 	 Slide 12 
1, if edge e is included in the tour. xe = 0, otherwise. 
min cexe
eE
s.t. xe  2,S E 
e(S) 
xe =2,i  V 
e(i) 
xe {0,1} 
How can you solve the LP relaxation? 
3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	Problems with exponentially many constraints 
	The separation problem 
	Polynomial solvability 
	Examples: MST, TSP, Probability 
	Conclusions 
2 Problems 
2.1 Example 
Slide 2 
min cixi 
i 
aixi |S|, for all subsets S of {1,...,n} 
iS 
	There are 2n constraints, but are described concisely in terms of the n
scalar parameters a1,...,an
	Question: Suppose we apply the ellipsoid algorithm. Is it polynomial? 
	In what? 
2.2 The input 
Slide 3 
	Consider min c  x s.t. x  P 
	P belongs to a family of polyhedra of special structure 
	A typical polyhedron is described by specifying the dimension n and an
integer vector h of primary data, of dimension O(nk), where k  1 is some
constant.
	In example, h =(a1,...,an) and k =1 
	U0 be the largest entry of h 
Slide 4 
	Given n and h, P is described as Ax  b 
	A has an arbitrary number of rows 
	U largest entry in A and b. We assume 
log U Cn log U0 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Discrete optimization I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec24/</lecture_pdf_url>
      <lectureno>24</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>PTSP = {x  R|E|;  xe  2,  xe =2cut 
e(S) e(i) 
0  xe 1}
PTSP R|E|
sub = {x  ; xe =2
e(i)
xe |S|1 
e(S) 
0  xe 1} 
Slide 31 
	Theorem: PTSP = PTSP  CH(H)cut sub 
	Nobody knows CH(H)fortheTSP 
7 Minimum Matching 
Slide 32 
	Given G =(V,E);ce costs on e  E. Find a matching of minimum cost. 
	Formulation: 
min cexe 
s.t. xe =1,i  V 
e(i) 
xe {0,1} 
	Is the LP ralaxation CH(H)? 
Slide 33 
Let 
PMAT = {x  R|E| :  xe =1 
e(i) 
xe  1 |S|=2k+1,S = 
e(S) 
xe 0} 
Theorem: PMAT = CH(H) 
8 Observations 
Slide 34 
	For MST, Matching there are ecient algorithms. CH(H)isknown. 
	ForTSP  ecient algorithm. TSP is an NP hard problem. CH(H)
is not known.
	Conjuecture: The convexhull ofproblemsthat arepolynomially solvable
are explicitly known.
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>n 
P2 = {(x,y): yij =1, 
j=1 
0  xj  1 m 
yij m  xj, 
i=1 
0  yij  1 
Slide 15 
	Let 
Z1 = mincx + hy, Z2 = mincx + hy 
(x,y) P1 (x,y) P2 
	Z2  Z1  IZ1 = IZ2 
4.3 Implications 
Slide 16 
	Finding IZ1(=IZ2)isdicult. 
	Solving to nd Z1,Z2 is an LP. Since Z1 is closer to IZ1 several methods
(branchandbound) would workbetter(actually muchbetter).
	Suppose that if we solve mincx + hy,(x,y)  P1 we nd an integral
solution. Have we solved the facility location problem?
Slide 17 
	Formulation1isbetter thanFormulation2. (Despitethefactthat1has
a larger number of constraints than 2.)
	What is then the criterion? 
4.4 Ideal Formulations Slide 18 
	Let P be an LP relaxation for a problem 
	Let
H = {(x,y): x {0,1}n}P
	ConsiderConvexHull(H) 
= {x : x = ix i ,i =1,i  0,x i  H} 
i i 
Slide 19 
	The extreme points of CH(H)have {0,1}coordinates. 
	So, if we know CH(H) explicitly, then by solving min cx + hy,(x,y) 
CH(H)we solve the problem.
	Message:Quality offormulationisjudgedby closenessto CH(H).
CH(H) P1  P2
4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
 Modeling with integer variables 
 What is a good formulation? 
 Theme: The Power of Formulations 
2 Integer Programming 
2.1 Mixed IP Slide 2 
(MIP) max c  x + h  y 
s.t. Ax + By b 
nR+n Zx+(x  0,x integer) 
y  (y  0) 
nZ+ 2.2 Pure IP Slide 3 
 (IP) max cx 
s.t. Ax  b 
x  
Important special case: Binary IP 
 (BIP) max cx 
s.t. Ax  b 
x  {0,1}n 
2.3 LP Slide 4 
 (LP) max c x 
s.t. By  b 
y  Rn 
+ 
3 Modeling with Binary Variables 
3.1 Binary Choice 
 Slide 5 
1, if event occurs x  0, otherwise 
Example1: IPformulation of theknapsackproblem 
n : projects, total budget b 
aj : cost of project j 
cj : value of project j Slide 6 
1, ifproject j is selected. xj = 0, otherwise. 
1</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>n 
max cjxj 
j=1 
s.t.	 ajxj  b 
xj {0,1} 
3.2	Modeling relations 
Slide 7 
 At most one event occurs 
xj 1 
j 
 Neither or both events occur 
x2 x1 =0 
 If one event occurs then, another occurs 
0  x2  x1 
 If x =0,then y =0; if x =1,then y is uncontrained 
0  y  Ux, x {0,1} 
3.3	The assignment problem 
Slide 8 
n people
m jobs
cij : cost of assigning person j to job i.
1 person jis assigned to job i xij	= 0 
min cij xij
n
s.t. xij = 1 each job is assigned 
j=1
m
xij  1 each person can do at most one job.
i=1
xij {0, 1}
3.4	Multiple optimal solutions 
Slide 9 
 Generate all optimal solutions to a BOP. 
 max	cx 
s.t.	Ax  b 
x {0,1}n 
 Generatethirdbest? 
 ExtensionstoMIO? 
2</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>9 Summary 
Slide 35 
1. AnIPformulationisbetterthan another oneif thepolyhedraof theirLP
relaxations are closer to the convex hull of the IP.
2. A good formulation can have an exponential number of constraints. 
3. Conjecture: Formulations characterize the complexity of problems. If a
problem is solvable in polynomial time, then the convex hull of solutions
isknown.
8</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>5	Minimum Spanning 
Tree (MST) 
Slide 20 
	How do telephone companies bill you? 
	It used to be that rate/minute: Boston  LAproportional todistancein
MST
	Otherapplications:Telecommunications,Transportation(goodlowerbound
forTSP)
Slide 21 
	Given a graph G =(V,E)undirected and Costs ce,e  E. 
	Find a tree of minimum cost spanning all the nodes. 
1, if edge e isincludedinthetree 	Decision variables xe = 0, otherwise 
Slide 22 
	The tree should be connected. How can you model this requirement? 
	Let S be a set of vertices. Then S and V \S should be connected 
i  S 	Let (S)= {e =(i,j) E : j  V \S 
	Then, 
xe  1 
e(S) 
	What is the number of edges in a tree? 
	Then, xe = n 1 
eE 
5.1 Formulation Slide 23 
IZMST = min cexe 
 
   
H    
Is this a good formulation? eE  xe 1  S  V,S = ,V 
e(S)  xe = n 1 
eE 
xe  {0,1}. 
Slide 24 
Pcut = {x  R|E| :0  x  e, 
xe = n 1 
eE 
xe 1  S V,S = ,V } 
e(S) 
Is Pcut the CH(H)? 
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 24: Discrete Optimization</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5.2 What is CH(H)? 
Slide 25 
Let 
Psub = {x  R|E| : xe = n 1 
eE 
xe |S|1  S  V, S ,V } = 
eE(S) 
i  S E(S)= e =(i,j): j  S 
Why is this a valid IP formulation?	 Slide 26 
	Theorem: Psub = CH(H). 
	 Psub isthebestpossibleformulation. 
	MESSAGE: Good formulations can have an exponential number of con
straints.
6	The Traveling Salesman 
Problem 
Slide 27 
Given G =(V,E)an undirected graph. V = {1,...,n}, costs ce  e  E. Find 
a tour that minimizes total length. 
6.1 Formulation I 	 Slide 28 
1, if edge e isincludedinthetour. xe = 0, otherwise. 
min cexe
eE
s.t. xe  2,S E 
e(S) 
xe =2,i  V 
e(i) 
xe {0,1} 
6.2 Formulation II Slide 29 
min cexe 
s.t. xe |S|1,S  E 
eE(S) 
xe =2,i  V 
e(i) 
xe {0,1} 
Slide 30 
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>3.5 Nonconvex functions 
 How to model min c(x), wherec(x)is piecewise linear but not convex? 
4	What is a good formulation? 
4.1 Facility Location 
	Data
N = {1...n} potentialfacility locations
I = {1...m} set of clients
cj : cost of facility placed at j
hij : cost of satisfying client i fromfacility j.
 Decision variables 
1, a facility is placed at location j xij =0, otherwise 
yij = fraction of demand of client i 
satisedby facility j. 
n mn 
IZ1 =min cjxj + hijyij 
j=1 i=1 j=1 
n 
s.t.	 yij =1 
j=1 
yij  xj 
xj {0,1},0 yij  1. 
Consider an alternative formulation. 
n mn 
IZ2 =min cjxj + hijyij 
j=1 i=1 j=1 
n 
s.t. yij =1 
j=1 
m 
yij m  xj 
i=1 
xj {0,1},0 yij  1. 
Are both valid?
Which one is preferable?
4.2 Observations 
 IZ1 = IZ2,sincetheintegerpointsbothformulationsdenearethesame. 
 n	  	0  xj  1 P1 = {(x,y): yij =1,yij  xj, 0  yij 1 
j=1 
3 Slide 10 
Slide 11 
Slide 12 
Slide 13 
Slide 14</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Robust optimization</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>/summationdisplay 
/summationdisplay 
/summationdisplay 
/summationdisplay 7.1.2 Results 
8 Robust 0-1 Optimization 
	Nominal combinatorial optimization: 
 minimize cx 
subjectto x X {0,1} n . 
	RobustCounterpart: 
 Z  = minimize cx + max djxj 
{S| SJ,|S|=} 
jS 
subject to x X, 
	WLOG d1 d2 ... dn. 
8.1 Remarks 
	Examples: the shortest path, the minimum spanning tree, the minimum 
assignment,thetraveling salesman,thevehiclerouting and matroidinter
sectionproblems. 
	Other approaches to robustness are hard. Scenario based uncertainty: 
 minimize max(c1x,c2x) 
subjectto x  X. 
is NP-hard for the shortest path problem. 
8.2 Approach 
Primal :Z = min cx +max djxjuj xX j 
s.t. 0  uj  1,  j 
uj   
j 
Dual :Z  = min cx +min + yj xX j 
s.t.yj +   djxj,  j 
yj,  0 
6 Slide 21
Slide 22
Slide 23
Slide 24</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>/summationdisplay 
/summationdisplay 
/summationdisplay /summationdisplay 
/summationdisplay /summationdisplay 
/summationdisplay /summationdisplay 
/summationdisplay 
/summationdisplay 8.3 Algorithm A 
Slide 25 
	Solution: yj =max(djxj ,0) 
 
Z  = min + (cjxj +max(djxj ,0)) xX,0 j 
	Since X {0,1}n , 
max(djxj ,0) = max(dj ,0) xj 
 
Z  = min + (cj +max(dj ,0))xj xX,0 j 
Slide 26 
	d1 d2 ... dn dn+1 =0. 
	For dl  dl+1, 
n l 
min + cjxj + (dj )xj = xX,dldl+1 j=1 j=1 
n l 
dl+ min cjxj + (dj dl)xj = Zl xX 
j=1 j=1 
	
n l 
 Z = min dl+ min cjxj + (dj dl)xj. 
l=1,...,n+1 xX 
j=1 j=1 
8.4 Theorem 3 Slide 27 
	Algorithm A correctly solves the robust 0-1 optimization problem. 
	It requires at most |J|+1 solutions of nominal problems. Thus, If the
nominalproblemispolynomially time solvable,thenthe robust0-1 coun
terpartis alsopolynomially solvable.
	Robust minimum spanning tree, minimum assignment, minimum match
ing, shortest path and matroid intersection, are polynomially solvable.
9 Experimental Results 
9.1 Robust Sorting 
Slide 28 
minimize cixi 
iN 
subjectto xi = k 
iN 
. x {0,1}n 
7</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/summationdisplay 
/summationdisplay	 /summationdisplay /braceleftBigg /bracerightBigg 
/summationtext 
/summationdisplay /summationdisplay 
/braceleftBigg /bracerightBigg /summationdisplay 
/summationdisplay 
/summationdisplay 
/summationdisplay 	We willguaranteethatif naturebehaveslikethisthentherobustsolution
will be feasible deterministically. Even if more than i change, then the
robust solution will be feasible with very high probability.
6.1 Problem /braceleftBigg/bracerightBigg	 Slide 11 
 minimize cx + max djxj
{S0| S0J0,|S0|0} ||
jS0 
subject to	 max xj aijxj + 
{Si| SiJi ,|Si|i} aij||bi, i 
j jSi 
u l x 
xi Z, i =1,...k.
6.2 Theorem 1 Slide 12 
The robust problem can be reformulated has an equivalent MIP: 
 minimize cx + z00 + jJ0 p0j 
subjectto aijxj +zii + pij bi i 
j jJi 
z0 + p0j djyj jJ0 
zi +pij aijyj i =0,j Ji 
pij,yj,zi 0 i,j Ji 
yj xj yj j 
lj xj  j uj 
xi Z i =1,...,k. 
6.3 Proof Slide 13 
Given a vector x , we dene: 
	  i(x )= max aijxj. 
{Si| SiJi,|Si|=i} ||
jSi 
This equals to: 
	  i(x )= max aijxjzij ||
jJi 
s.t. zij i 
jJi 
0 zij 1 i,j Ji.	Slide 14 
Dual: 
i(x  )= min pij +izi 
jJi 
 s.t.zi + pij aij|xj|jJi 
pij 0 jJi 
zi 0 i. 
3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>010
10
10
10
10
0 1 2 3 4 5 6 7 8 9 10 4 3 2 1 Approx bound 
Bound 2 

 Violation Probability 
0 0.5 
2.8 4.49 101 
36.8 5.71 103 
82.0 5.04 109 
200 0 i 
OptimalValue 
5592 
5585 
5506 
5408 
5283 Reduction 
0% 
0.13% 
1.54% 
3.29% 
5.50% 
	wi are independently distributed and follow symmetric distributions in
[wi i,wi + i];
	c is not subject to data uncertainty. 
7.1.1 Data Slide 20 
	|N|=200, b =4000, 
	wi randomly chosen from {20,21,..., 29}. 
	ci randomly chosen from {16,17,..., 77}. 
	i =0.1wi. 
5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Papers 
Slide 1 
	B. and Sim, The Price of Robustness, Operations Research, 2003. 
	B. and Sim, Robust Discrete optimization, Mathematical Programming,
2003.
2 Structure 
Slide 2 
Motivation  
	DataUncertainty 
	RobustMixedIntegerOptimization 
	Robust0-1Optimization 
	RobustApproximationAlgorithms 
RobustNetworkFlows  
	ExperimentalResults 
	Summary and Conclusions 
3 Motivation 
Slide 3 
	Theclassicalparadigminoptimizationistodevelopamodel that assumes
thattheinputdataisprecisely known and equal to some nominal values.
This approach, however, does not take into account the inuence of data
uncertainties on the quality and feasibility of the model.
	Can we design solution approaches that are immune to data uncertainty,
that is they are robust?
Slide 4 
	Ben-Tal andNemirovski(2000): 
In real-world applications of Linear Optimization (Net Lib li
brary), one cannot ignore the possibility that a small uncer
tainty in the data can make the usual optimal solution com
pletely meaningless from a practical viewpoint. 
1</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>/summationdisplay 
/summationdisplay 
	/summationdisplay 
/summationdisplay /summationdisplay 
/summationdisplay   Z() 
0 8822 
10 8827 
20 8923 
30 9059 
40 9627 
50 10049 
60 10146 
70 10355 
80 10619 
100 10619 
Z  ()= 
9.1.1 Data 
|N|=200; 
 k =100;  % change in Z()
0%
0.056%
1.145%
2.686%
9.125%
13.91%
15.00%
17.38%
20.37%
20.37%
 minimize cx + () 
501.0 
493.1 
471.9 
454.3 
396.3 
371.6 
365.7 
352.9 
342.5 
340.1 % change in () 
0.0% 
-1.6% 
-5.8% 
-9.3% 
-20.9% 
-25.8% 
-27.0% 
-29.6% 
-31.6% 
-32.1% 
max djxj 
{S| SJ,|S|=} 
jS 
subjectto xi = k 
iN 
. x {0,1}n 
Slide 29 
	cj  U[50,200]; dj  U[20,200]; 
	Fortesting robustness,generateinstances such that each cost component
independently deviates with probability  =0.2 from the nominal value
cj to cj + dj.
9.1.2 Results Slide 30 
10 Robust Network Flows 
Slide 31 
Nominal 
min cijxij
(i,j)A
s.t. xij  xji = bi i N 
{j:(i,j)A} {j:(j,i)A}
0 xij uij (i,j)A.
X set of feasible solutions ows.  
Robust 	
 Z  =min cx + max dijxij 
{S| SA,|S|} 
(i,j)S 
subjectto x X. 
8</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3.1 Literature Slide 5 
	Ellipsoidaluncertainty;Robust convexoptimizationBen-Tal andNemirovski
(1997),El-Ghaouiet. al(1996)
	Flexible adjustment of conservativism 
	Nonlinear convex models 
	Not extendable to discrete optimization 
4 Goal 
Slide 6 
Develop an approach to address data uncertainty for optimization problems 
that: 
	It allows to control the degree of conservatism of the solution; 
	It is computationally tractable both practically and theoretically. 
5 Data Uncertainty 
Slide 7 
minimize cx 
subjectto Ax  b 
l  x  u 
xi Z,i =1,...,k, 
WLOG data uncertainty aects only A and c, but not the vector b. Slide 8 
	(Uncertainty for matrix A): aij,j  Ji is independent, symmetric
andbounded randomvariable(but with unknowndistribution) aij,j  Ji
thattakes valuesin[aij aij,aij +aij].
	(Uncertainty for cost vector c): cj,j  J0 takesvaluesin[cj,cj+dj]. 
6 Robust MIP 
Slide 9 
	Consider an integer i  [0,|Ji|],i =0,1,...,m. 
	i adjusts the robustness of the proposed method against the level of
conservativeness of the solution.
	Speaking intuitively, it is unlikely that all of the aij, j  Ji will change.
We want to be protected against all cases that up to i of the aijs are
allowed to change.
Slide 10 
	Nature will be restricted in its behavior, in that only a subset of the
coecients will change in order to adversely aect the solution.
2</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 13: Robust Optimization</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>3000 
2500 
2000 
1500 
1000 
500 
0
Distributions of path cost
3 4 5 6 7 8 9  = 0 
 = 3 
 = 6 
 = 10 
11 Experimental Results 
Slide 35 
12 Conclusions 
Slide 36 
Robust counterpart of a MIP remains a MIP, of comparable size.  
Approach permits exibility of adjusting the level of conservatism in terms of  
probabilistic bound of constraint violation
For polynomial solvable 0-1 optimization problems with cost uncertainty, the
  
robust counterpart is polynomial solvable. Slide 37 
Robust network ows are solvable as a series of nominal network owproblems.  
Robust optimization is tractable for stochastic optimization problems without  
the curse of dimensionality 
10</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>/summationtext
/parenleftBigg /parenrightBigg /parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg 
/summationdisplay /summationdisplay /parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg 
/summationdisplay 
/summationdisplay |Ji| 
5 
10 
100 
200 i
5
8.3565
24.263
33.899
Table 1: Choice of i as a function of |Ji|so that the probability of constraint 
violationislessthan1%. 
6.4 Size Slide 15 
	OriginalProblemhas n variables and m constraints 
m 	Robust counterpart has 2n + m + l variables, where l = i=0 |Ji| is the
number of uncertain coecients, and 2n + m + l constraints.
6.5 Probabilistic Guarantee 
6.5.1 Theorem 2 Slide 16 
Let x be an optimal solution of robust MIP. 
(a) If A is subject to the model of data uncertainty U: 
	  
 n n 
Pr aijxj &gt;bi 2n  (1) l + l  ,
/summationdisplay 
 1 /summationdisplay n /summationdisplay n 
j	 l= l=+1 
n = |Ji|,  = i+n and  =  ; bound is tight. 2 
(b) As n  
	  
 n n  
21 
n (1) n
l +  n
l 1i 
n 1 . 	  
l= l=+1 
Slide 17 
Slide 18 
7 Experimental Results 
7.1 Knapsack Problems 
Slide 19  
maximize cixi 
iN 
subjectto wixi  b 
iN 
x {0,1}n . 
4</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>/summationdisplay 
/parenleftbigg /parenrightbigg 
/summationdisplay j i 
j 
j i i (cij, uij) 
(cij, uij) (0, / dij) (dij,) (0,) (cost, capacity) 
10.1 Reformulation Slide 32 
	
 Z =min Z(), 
0 
 Z()=+min cx + pij 
(i,j)A 
subjectto	pij dijxij  (i,j)A 
pij 0 (i,j)A 
x X. 
Equivalently  
/summationdisplay	 Z()=+min cx + dij max xij  ,0 dij 
(i,j)A 
subjectto	x X. 
10.2 Network Reformulation Slide 33 
Theorem: For xed wecansolvetherobustproblemasanetworkowproblem 
10.3 Complexity 
Slide 34 
 Z()is a convex function and for all 1,2 0, we have 
|Z(1)Z(2)||A||1 2|. 
For any xed  and every &gt; 0, we can nd a solution X with robust
  
objective value |A| x 
Z= c  x+ max dijxij 
{S| SA,|S|} 
(i,j)S 
suchthat
Z  Z(1+)Z 
by solving 2log2(|A|/)+3 network ow problems, where  = max{uijdij : 
(i,j)A}. 
9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Duality theory II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 10: Duality Theory III</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Theorem: The absence of arbitrage condition holds if and only if there
exists a nonnegative vector q =(q1,...,qm), such that the price ofeach
asset i isgivenby
m 
pi = qsrsi. 
s=1 
	Applications to options pricing 
4 Cones and extreme rays 
4.1 Denitions Slide 9 
	A set C n is a cone if x  C for all   0 and all x  C 
	A polyhedron of the form P = {x n |Ax  0}is called a polyhedral
cone
4.2 Applications 
Slide 10 
	P = x n |Ax  b , y  P 
	The recession cone at y 
RC = d n |y + d  P,   0 
	It turns out that 
RC = d n |Ad  0 
	RC independent of y 
Slide 11 
4.3 Extreme rays 
Slide 12 
A x =0 of a polyhedral cone C n is called an extreme ray if there are 
n 1 linearly independent constraints that are active at x 
4.4 Unbounded LPs Slide 13 

Theorem: Consider the problem of minimizing cx over a polyhedral cone C = 
{x n | A  
ix  0,i =1,...,m} that has zero as an extreme point. The 
optimal cost is equal to  if and only if some extreme ray d of C satises 
	 
cd &lt; 0. Theorem:Considertheproblemofminimizing cx subjectto Ax  b, Slide 14 
and assume that the feasible set has at least one extreme point. The optimal 
costisequal to  if and onlyif someextremeray d of thefeasiblesetsatises 

cd &lt; 0.
What happens when the simplex method detects an unbounded problem?
3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>x1 
2
3 w 1 
y..
. 
. recession 
cone x1 
x
2 xx w 2 
5 Resolution Theorem 
Slide 15 
P = x n |Ax  b 
be a nonempty polyhedron with at least one extreme point. Let x1 ,..., xk be 
theextremepoints, andlet w1 ,..., wr be a complete set of extreme rays of P. 
k r  k 
Q= ix i + j wj  i  0,j  0, i =1 . 
i=1 j=1 i=1 
Then, Q= P. 
5.1 Example 
Slide 16 
x1 x2 2 
x1 + x2  1 
x1,x2  0 
Slide 17 
 Extremepoints: x1 =(0, 2), x2 =(0, 1), and x3 =(1, 0). 
 Extreme rays w1 =(1, 1) and w2 =(1, 0). 
   2 0 1 1 2 1 2 y = = + + = x + w + w . 2 1 1 0 
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 Asset Pricing 
Slide 4 
	n dierent assets 
	m possible states of nature 
	one dollar invested in some asset i, and state of nature is s, we receive a
payo of rsi
	m  n payo matrix: 
	  r11 ... r1n 
R =  .... ..  
. .. 
rm1 ... rmn 
Slide 5 
	xi: amount held of asset i. A portfolio of assets is x = x1,...,xn . 
	Anegativevalueof xi indicatesa shortpositioninasset i: this amounts
toselling |xi|unitsof asset i atthebeginning oftheperiod,with apromise
to buy them back at the end. Hence, one must pay out rsi|xi| if state s
occurs, which is the same as receiving a payo of rsixi
Slide 6 
	Wealth in state s from a portfolio x 
n 
ws = rsixi. 
i=1 
	w = w1,...,wm , w = Rx 
	pi: price of asset i, p = p1,...,pn 
 	Cost of acquiring x is px. 
3.1 Arbitrage 
Slide 7 
	Central problem: Determine pi 
	Absence of arbitrage: no investor can get a guaranteed nonnegative
payooutof anegativeinvestment. Inotherwords,anyportfoliothatpays
o nonnegative amounts in every state of nature, must have nonnegative
cost.
if Rx  0, then px  0. 
Slide 8 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>'x = 0 x2 x3 
-a1 
x1 x1 x2 .y a1
a2'x= 0 . 
. z. 
-a2 
(a) (b) 
4</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>A2 A3 
. A1 
p 
b 
1 Outline 
Slide 1 
 Farkaslemma 
 Assetpricing 
 Cones and extreme rays 
 Representation of Polyhedra 
2 Farkas lemma 
Slide 2 
Theorem:
Exactly one of the following two alternatives hold:
1. x  0 s.t. Ax = b. 
2. p s.t. p  A  0  and p  b &lt; 0. 
2.1 Proof Slide 3 

  If x  0 s.t. Ax = b, and if p  A  0  ,then p  b = p  Ax  0 

  Assume there is no x  0 s.t. Ax = b 
(P)max 0  x (D) min p  b 
s.t. Ax = b s.t. p  A  0  
x  0 
(P)infeasible  (D)either unbounded or infeasible 
Since p = 0 isfeasible  (D)unbounded 
p : p  A  0  and p  b &lt; 0 
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5.2 Proof Slide 18 
	Q P. Let x  Q: 
k r 
x = ix i + j wj
i=1 j=1
k
i,j  0 i=1 i =1. 
	y = k
i=1 ixi  P and satises Ay  b. 
	Awj  0 for every j: z = r
j=1 j wj satises Az  0. 
	x = y + z satises Ax  b andbelongsto P. 
Slide 19 
For the reverse, assume there is a z  P, such that z/ Q. 
k r 
max 0i + 0j 
i=1 j=1 
k r 
s.t. ix i + j wj = z 
i=1 j=1 
k 
i =1 
i=1 
i  0, i =1,...,k, 
j  0, j=1,...,r, 
Is this feasible?	 Slide 20 
	Dual 

min pz + q 
s.t. p  xi + q  0, i =1,...,k, 

pwj  0, j =1,...,r. 
	This is unbounded. Why? 

	There exists afeasible solution(p,q)whose cost pz + q&lt; 0 
	p  z &lt; p  xi for all i and p  wj  0 for all j. 
Slide 21 
 

min px 
s.t. Ax  b. 
	If the optimal cost is nite, there exists an extreme point xi which is
optimal. Since z is a feasible solution, we obtain p  x i  p  z, which is a
contradiction.
	If the optimal cost is , there exists an extreme ray wj such that
p  wj &lt; 0, which is again a contradiction
6</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>The Ellipsoid method</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	Ecient algorithms and computational complexity 
	The key geometric result behind the ellipsoid method 
	The ellipsoid method for the feasibility problem 
	The ellipsoid method for optimization 
2 Ecient algorithms 
Slide 2 
	TheLOproblem 
min cx 
s.t. Ax = b 
x  0 
	ALOinstance
min 2x +3y
s.t.x + y  1 
x,y  0 
	A problem is a collection of instances 
2.1 Size Slide 3 
	The size of an instance is the number of bits used to describe the instance,
according to a prespecied format
	A number r U 
r = ak2k +ak12k1 ++ a121 + a0 
is representedby(a0,a1,...,ak)with k log2 U
	Size of r is log2 U+2 
	Instance ofLO:(c,A,b) 
Sizeis  
(mn + m +n) log2 U+2 
2.2 Running Time 
Slide 4 
Let A be an algorithm which solves the optimization problem .
If there exists a constant &gt; 0 such that A terminates its computation after at most
f(I)elementary steps for each instance I,then A runsinO(f)time.
Elementary operations are 
	variable assignments  comparison of numbers 
	random access to variables  arithmetic operations 
	conditional jumps  ... Slide 5 
1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>3.1 The algorithm intuitively 
	Problem: Decide whether agivenpolyhedron 
P = x n |Ax  b 
is nonempty 
  ax  axt 0011 0011 Et+1 
Et 
xt xt+1 P 
a x  b 
	Key property: We can nd a new ellipsoid Et+1 that covers the half
ellipsoid and whosevolumeisonly afractionof thevolumeof theprevious 
ellipsoid Et 
3.2 Key Theorem 
	E = E(z,D)be an ellipsoid in  n; a nonzero n-vector. 
	H = {x  n |a  x a  z} 
1 Da z = z + , n +1 
a  Da 
n 2 2 Daa  D D =	 . n2 1 D n +1 a  Da 
 	The matrix D is symmetric and positive denite and thus E = E(z,D) is an 
ellipsoid 
  E H E
Vol(E  )&lt;e 1/(2(n+1)) Vol(E)
  
3 Slide 11 
Slide 12 
Slide 13</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>4.1 Sliding objective 
Slide 30 
	We rst run the ellipsoid method to nd a feasible solution x0  P = 
x n |Ax  b . 
	We apply the ellipsoid method to decide whether the set 
P  x n |c  x &lt; c  x0 
is empty. 
	Ifitisempty,then x0 is optimal. If it is nonempty, we nd a new solution 
 x1 in P with objective function value strictly smaller than cx0. 
Slide 31 
	More generally, every time a better feasible solution xt is found, we take
P {x n |c  x &lt; c  xt}as the new set of inequalities and reapply the
ellipsoid method.
. . xt+1 
xt 
P c' x &lt;c ' xt+1 
c' x &lt;c ' xt -c 
4.2 Performance in practice 
Slide 32 
	Very slow convergence, close to the worst case 
	Contrast with simplex method 
	The ellipsoid method is a tool for classifying the complexity of linear
programmingproblems
8</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>x2 
x1 E E' 
3.3 Illustration Slide 14 
3.4 Assumptions 
Slide 15 
	Apolyhedron P is full-dimensional if it has positive volume 
	Thepolyhedron P is bounded: there exists a ball E0 = E(x0,r2I), with
volume V, that contains P
	Either P is empty, or P has positive volume, i.e., Vol(P) &gt;v for some
v&gt; 0
	E0, v,V , are a priori known 
	We can make our calculations in innite precision; square roots can be
computed exactly in unit time
3.5 Input-Output 
Slide 16 
Input: 
	A matrix A and a vector b that dene the polyhedron P = {x n | 
 aix  bi,i =1,...,m} 
	A number v, such that either P is empty or Vol(P)&gt;v 
4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>For any x  P, we have 
  ai(k)x  bi(k) &gt; ai(k)xk 
	Hence, P  Hk = x n |a  
i(k)x  a  
i(k)xk 
	Therefore, P  Ek Hk Slide 21 
Bykeygeometricproperty, Ek Hk Ek+1;hence P Ek+1 and the induction is 
complete 
Vol(Et+1) 1/(2(n+1)) &lt;e Vol(Et) 
Vol(Et ) t /(2(n+1)) &lt;e Vol(E0) 
Vol(Et )&lt;V e 2(n+1) log V /(2(n+1)) Ve log V = vv	 v 
 Iftheellipsoidmethodhasnotterminated after t iterations,thenVol(P)Vol(Et ) 
v. This implies that P is empty 
3.9 Binary Search 
Slide 22 
	P = x | x  0,x  1,x  2,x  3 
	E0 =[0,5], centered at x0 =2.5 
	Since x0 /P, the algorithm chooses the violated inequality x  2 and
constructs E1 that contains the interval E0 {x |x  2.5}=[0,2.5]
	The ellipsoid E1 istheinterval[0,2.5] itself 
	Its center x1 =1.25belongsto P 
	This is binary search 
3.10 Boundedness of P Slide 23 
Let A bean mn integermatrixandlet b avectorin n .Let U bethelargest 
absolute value of the entries in A and b. 
Every extreme point of the polyhedron P = {x n |Ax  b}satises 
(nU)n  xj  (nU)n ,j=1,...,n 
Slide 24 
	All extreme points of P are contained in 
PB = x  P |xj|(nU)n,j =1,...,n 
	Since PB  E 0,n(nU)2nI , we can start the ellipsoid method with E0 = 
E	0,n(nU)2nI 
  n 2 Vol(E0) V =2n(nU)n =(2n)n(nU)n 
6</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>3.11 Full-dimensionality 
Slide 25 
Let P = {x n | Ax  b}. We assume that A and b have integer entries, 
which are bounded in absolute value by U. Let 
1  (n+1)  = (n +1)U . 2(n +1) 
Let 
P = x n |Ax  be , 
where e =(1,1,..., 1). 
(a) If P is empty, then P is empty. 
(b) If P is nonempty, then P isfull-dimensional.	 Slide 26 
Let P = x n |Ax  b be a full-dimensional bounded polyhedron, where 
theentriesof A and b areintegerandhaveabsolutevalueboundedby U.Then, 
2Vol(P)&gt;v = n n(nU)n (n+1) 
3.12 Complexity 
Slide 27 
	P = {x n | Ax  b}, where A, b have integer entries with magni
tude bounded by some U and has full rank. If P is bounded and either
empty or full-dimensional, the ellipsoid method decides if P is empty in
O	nlog(V/v) iterations 
	v = nn(nU)n 2 (n+1),V =(2n)n(nU)n 2 
	Number of iterations On4 log(nU) 
Slide 28 
	If P is arbitrary, we rst form PB,thenperturb PB toform PB, and apply the
ellipsoid method to PB,
Number of iterations is On 6 log(nU) .  
	It has been shown that only O(n 3 logU) binary digits of precision are needed,
and the numbers computed during the algorithm have polynomially bounded
size
	The linear programming feasibility problem with integer data can be solved in
polynomial time
4 The ellipsoid method for optimization 
Slide 29 
min c  x	 max b   
s.t. Ax b, s.t. A   = c 
	 0. 
By strong duality, both problems have optimal solutions if and only if the following 
system of linear inequalities is feasible: 
 	  bp = cx, Ax b, Ap = c, p 0. 
LO with integer data can be solved in polynomial time. 
7</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Aball E0 = E(x0,r2I)with volume at most V, such that P  E0 
Output: A feasible point x   P if P is nonempty, or a statement that P is 
empty 
3.6 The algorithm 
Slide 17 
1. (Initialization) 
Let t  = 2(n +1)log(V/v); E0 = E(x0,r 2I);D0 = r 2I; t =0. 
2. (Mainiteration) 
 	If t = t stop; P is empty. 
	If xt P stop; P is nonempty. 
 	If xt /P nd a violated constraint, that is, nd an i suchthat aixt &lt;bi. 
	Let Ht = {x  n |ai x ai xt}. Find an ellipsoid Et+1 containing Et Ht:
Et+1 = E(xt+1,Dt+1)with
1 Dtai xt+1 = xt +  , n +1 a  Dtai i
n 2 2 Dtaiai Dt Dt+1 = n2 1 Dt n +1 a  
iDtai . 
t := t +1.  
3.7 Correctness Slide 18 
Theorem: LetP beaboundedpolyhedronthatiseitheremptyorfull-dimensional 
and for which the prior information x0, r, v, V is available. Then, the ellipsoid 
methoddecidescorrectly whether P isnonempty ornot,i.e.,if xt1 /P,then 
P is empty 
3.8 Proof Slide 19 
	If xt  P for t&lt;t , then the algorithm correctly decides that P is
nonempty
	Suppose x0,..., xt1 /P. We will show that P is empty. 
	We prove by induction on k that P  Ek for k =0,1,...,t  . Note
that P  E0, by the assumptions of the algorithm, and this starts the
induction.
Slide 20 
	Suppose P  Ek for some k&lt;t . Since xk /P, there exists a violated 
	  inequality: ai(k)x  bi(k) be a violated inequality, i.e., ai(k)xk &lt; bi(k),
where xk is the center of the ellipsoid Ek
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 18: The Ellipsoid method</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>A brute force algorithm for solving the min-cost ow problem: 
Consider all spanning trees and pick the best tree solution among the feasible ones. 
Suppose we had a computer to check 1015 trees in a second. It would need more than 
109 years to nd the best tree for a 25-node min-cost ow problem. 
It would need 1059 years for a 50-node instance. 
Thats not ecient! 
Ideally, we would like to call an algorithm ecient when it is suciently fast to be 
usable in practice, but this is a rather vague and slippery notion. Slide 6 
The following notion has gained wide acceptance: 
An algorithm is considered ecient if the number of steps it performs for 
any input is bounded by a polynomial function of the input size. 
Polynomials are, e.g., n, n 3, or 106 n 8 . 
2.3 The Tyranny of 
Exponential Growth 
Slide 7 
100n logn 10n 2 n 3.5 2n n! n n2 
109/sec 1.19109 600,000 3,868 41 15 13  
1010 /sec 1.081010 1,897, 370 7,468 45 16 13  
Maximum input sizes solvable within one hour. 
2.4 Punch line Slide 8 
The equation
ecient = polynomial
has been accepted as the best available way of tying the empirical
notion of a practical algorithm to a precisely formalized mathe
matical concept.
2.5 Denition Slide 9 
An algorithm runs in polynomial time if its running time is O(|I|k), where |I| 
is the input size, and all numbers in intermediate computations can be stored 
withO(|I|k)bits. 
3 The Ellipsoid method 
Slide 10 
 D is an n  n positive denite symmetric matrix 
 A set E of vectors in n of theform 
E = E(z,D)=  
x n |(x z)  D1(x z) 1  
is called an ellipsoid with center z n 
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Interior point methods I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>.... 
....x2 
1 2 
. 
2 x1 
7</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 20: The Ane Scaling Algorithm</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
	History 
	Geometricintuition 
	Algebraicdevelopment 
	AneScaling 
	Convergence 
	Initialization 
	Practicalperformance 
2 History 
Slide 2 
	In1984,KarmakaratAT&amp;T invented interiorpointmethod 
	In 1985, Ane scaling invented at IBM + AT&amp;T seeking intuitive ver
sion of Karmarkars algorithm
	In early computational tests, A.S. far outperformed simplex and Kar
markars algorithm
	In 1989, it was realised Dikin invented A.S. in 1967 
3 Geometric intuition 
3.1 Notation Slide 3 
 min cx 
s.t. Ax = b 
x  0 
anditsdual 
max p  b 
s.t. p  A  c  
	P = {x |Ax = b, x  0} 
	{x  P |x &gt; 0}the interior of P and its elements interior points 
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6 Convergence 
6.1 Assumptions 
Slide 16 
AssumptionsA: 
(a)	The rows of the matrix A arelinearly independent. 
(b)	The vector c is not a linear combination of the rows of A. 
(c)	There exists an optimal solution. 
(d) There exists a positive feasible solution. 
AssumptionsB: 
(a)	Every BFS to the primal problem is nondegenerate. 
(b) At every BFS to the primal problem, the reduced cost of every nonbasic 
variable is nonzero. 
6.2 Theorem Slide 17 
If we apply the long-step ane scaling algorithm with  =0, the following hold: 
(a)	For the Long-step variant and under Assumptions A and B, and if 0 &lt;&lt; 1,
x k and p k converge to the optimal primal and dual solutions
(b) For the second Long-step variant, and under Assumption A and if 0 &lt; &lt; 
2/3, the sequences x k and p k converge to some primal and dual optimal solutions, 
respectively 
7 Initialization 
Slide 18 
min cx + Mxn+1 
s.t. Ax +(b Ae)xn+1 = b 
x,xn+1  0 
8 Example 
Slide 19 
max x1 +2x2 
s.t.x1 + x2  2 
x1 + x2  1 
x1,x2  0 
9 Practical Performance 
Slide 20 
 Excellent practical performance, simple 
 Major step: invert AX2 
kA  
 Imitates the simplex method near the boundary 
6</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>5 Ane Scaling 
5.1 Inputs 
Slide 13 
 (A, b, c); 
 an initial primal feasible solution x0 &gt; 0 
 the optimality tolerance &gt; 0 
 theparameter   (0,1) 
5.2 The Algorithm 
Slide 14 
1.	(Initialization)Start with some feasible x0 &gt; 0;let k =0. 
2.	(Computation of dual estimates and reduced costs) Given some feasible
x k &gt; 0,let
Xk = diag(x1 k ,...,x nk ), 
p k =(AX2 
kA  )1AXk2 c, 
r k = c A  p k . 
3.	(Optimalitycheck) Let e =(1,1,..., 1). If rk  0 and e  Xkrk &lt;,then
stop; thecurrentsolution xk isprimal -optimal and pk isdual -optimal.
4.	(Unboundednesscheck)IfX2 
krk  0 thenstop; theoptimal costis . 
5.	(Update of primalsolution) Let 
X2 
kr k 
x k+1 = x k  . ||Xkrk|| 
5.3 Variants Slide 15 
||u|| =maxi |ui|,(u)= max{ui |ui &gt; 0} 
 (u)||u|| ||u|| 
 Short-step method. 
 Long-step variants 
X2 k 
x k+1 = x k  kr
||Xkrk|| 
X2 
kr k 
x k+1 = x k  (Xkrk) 
5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>d = x y 
min c  d 
s.t. Ad = 0 
||Y 1d|| 
4.2 Solution Slide 7 
If rows of A are linearly independent and c is not a linear combination of the 
rows of A,then 
	optimal solution d  : 
Y 2(c A  p)d  =   , p =(AY 2A  )1AY 2 c. |Y (c A  p)| 
	x = y + d   P 
 	  	cx = cy |Y (c A  p)|&lt; cy 
4.2.1 Proof Slide 8 
 	AY 2A  is invertible;if not, there exists some z 0 suchthat z AY 2A  z =0 = 
	w = YA  z; w  w =0  w = 0 
	Hence A  z = 0 contradiction 
	Since c is not a linear combination of the rows of A, c A  p =0 and d  is well
dened
	d  feasible 
Y 1d  = 	 Y (c A  p)  ||Y 1d  ||=  |Y (c A  p)| 
Ad  = 0, since AY 2(c A  p)= 0 
 
 	  cd =(c pA)d 
=	(c  p  A)YY 1d 
 |Y (c A  p)|||Y 1d|| 
 |Y (c Ap)|. 
Slide 9 
 
	    cd =(c pA)d 
  Y 2(c A  p) = (c pA)	  |Y (c A  p)| 
    
Y (c A  p) Y (c A  p) 
=	   |Y (c A  p)| 
 =	|Y (c Ap)|. 
	c  x = c  y + c  d  = c  y |Y (c A  p)| 
3</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>c 
. . 
. 
x0 x1 x2 
3.2 The idea Slide 4 
4 Algebraic development 
4.1 Theorem Slide 5 
  (0,1), y n: y &gt; 0, and 
 n 
S = x n  (xi 
2 yi)2 
 2 . yii=1 
Then, x &gt; 0 for every x  S 
Proof 
 x  S 
 (xi yi)2  2 yi 2 &lt;yi 2 
|xi yi|&lt;yi; xi + yi &lt;yi, and hence xi &gt; 0 Slide 6 
x  S is equivalent to |Y 1(x y)| 
Replace original LP: 
min cx 
s.t. Ax = b 
|Y 1(x y)|. 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>4.3 Interpretation 
Slide 10 
	y be a nondegenerate BFS with basis B 
	A =[BN] 
	Y = diag(y1,...,ym,0,..., 0) and Y 0 = diag(y1,...,ym), then AY = 
[BY 0 0] 
p	=(AY 2A  )1AY 2 c 
= 0 BY 2
0cB (B  )1Y 2B1
=(B  )1 cB 
	Vectors p dual estimates 
	r = c A  p becomes reduced costs: 
r = c A  (B  )1 cB 
	Underdegeneracy? 
4.4 Termination Slide 11 
y and p be primal and dual feasible solutions with 
 cy b  p &lt; 
y  and p  be optimal primal and dual solutions. Then, 
c  y   c  y &lt; c  y  + , 
b  p  &lt; b  p  b  p  
4.4.1 Proof Slide 12 
	c  y   c  y 
 	By weak duality, b  p  cy 
 	Since cy b  p &lt;, 
c  y &lt; b  p +   c  y  +  
b  p  = c  y   c  y &lt; b  p +  
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Formulations</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>11</slideno>
          <text>0x1 the 
C t,j l.cjl 
n.t. Az 2 b 
1x1 = max{s, s] 
mi11 C qiqi 
Ax 2 b 
r.i 5 z.? 
"j 5 zj 
&gt;Iinimizing Picirc~sisc lincnr convex 
functii:,n ,ran lbc moi:lcllcil I:,y 24 
 power of LO 
min 
Idea: 
5.t. 
Message: LO</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Variables 
.I, invcst,cd Y; 
C'~I,../I~: ami:,unt invcstcd pcrii:,d = 1. 
1.0GCu.~h3 1.00B 1.i5D + 1.40E 
s.t. .l+ 
 C:+D+ C'udhl 5 1 
CIA.S/L~ B 5 0.3.1+ l.lC+ 
 l.OGC'u.~hl 
Cil.sI~3 l.OE &lt; l.Ol+ 0.3A 
 1.06C'ash2 
11 
l:,roilucts, m raw nlatcrials 
hi: 
a,~:ailal:,lc i. 
aij: # nlatcrial i proi:luct j needs ori:lcr ti:, lbc proi:lucc,S. 
Formulat,ion 
rj = i:,f pri:,,Suct j 
pri:,iSucc,S. 
n C qis,i 
j=l 
12.2.1 Decision 
. . . .E: amount in millions 
in cash in 
 t, t 
 2, 3 
max + + 
+ + 
 + 
13 Manufacturing 
13.1 Data 
units of material 
units of in 
13.2 
13.2.1 Decision variables 
amount 
max</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>You cxpcct the l:,ricc i:,f i i:,nc scar fii:,m now will bc ri 
You 
 ]:,a- cal:,ital-gains t,ax thc rat? ,311 any t,hc 
t,imc i:,f the sale. 
You want ti:, raise C ami:,unt crash aft,cr taxes. 
You 1%) 
Example: You sell shares per sharc; you ha,~:c them 
$30 per Vet 
- - 
Five invcstmcnt .-i. B. C, D. 
.-i. C, and D a,~:ailal:,lc 
availablc 
carns 6% per year. 
C;asli Flowper 
 Ilivest,ed 
of 
pay 
 in transaction costs 
1.000 at $50 
 bought 
at sharc: cash is: 
SO x 1.000 
 0.30 x (SO 30) x 1,000 that 
 stock 
a at 
 of 30% capital gains at 
choices E 
arc 
 in 1993. 
B is in 1994. 
Cash 
S1.OOO.OOO in 1993. 
12.1 
 Dollar</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Tlie pre-algorit,hmic 
Fonrier, Mct,hoil 
for synbcm linear 
inc,:lualit,ics. 
de Val1i.e Poussirl simplcx-like mcthoi:l fin 
ol,jcct,ivc 
 functii:,n wit,h al:,ii:,- 
lute 
vo11 Nenlnann, 1028 game t,hcory, iluality. 
Farkas, Minkowski: 
 Caratl~i.odory, 
 Fi:,un,Sa,tionn 
Tlie 
 lnoderll 
Dantzig. 51ml,lcx mcthoil 
Applicat,ions. 
Large Siralc 
Opt,imizat,ion. 
thci:,ry. 
The cllipsi:,ii:l 
 algi:,rit,hm. 
pi,int 
algi:,rit,hmn. 
Scnlidcfinit,c all&lt;\ 
 ~~ti~niration. 
Ri:,bust Ol:,timizat,ion. 
8 LOPS 
Applicabilit,y 
Transportat,ion 
traffic ci:,nt,ri:,l, Crew schci:luling, 
&gt;Iovcmcnt i:,f Truck Loails 7 History of LO 
7.1 period 
1826 solving of 
la 
values. 
1870 -1930 
7.2 period 
George 1047 
1950s 
1960s 
1970s Complexity 
1979 
1980s Interior 
1990s 
 conic 
2000s 
Where do Arise? 
8.1 Wide 
. 
Air</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Managerrlerlt 
11 11 
ilcstinatii:,ns 
irlasscs (for 
Rc~cnucs T!, 1.j.;. ZJ 
Ca1:)acitics: i = I. .TI; C 0.i. .I = I, .IL 
Expected iicnlancls: D:) 
Forillulatioil 
Variables 
Q,,: Q-class cu~tomers me 
1;,: Y-class cu~tomers i j zr.,::c2%, +v:,Y;, 
Managerrlerlt 
F\'c cstimat,~ t,hat RVI hns gcncrat,cil in,rrcmcntal rcvcnuc fi,r 
Anlcri,ran the three ycnrs ali:,nc. 
 i:,nc-time benefit. 
F\'c cxprt RM gcncrat,c lcnst 
 millii:,n nnnually for the fi:,rcsccnl:,lc 
.-is me invest the cnhanccmcnt 
 DIV.-i?rlO me cxl:,cct t,o 
cnpt,urc 
 even lnrgcr rcvcnuc 18 Revenue 
18.1 Data 
origins. 
.I hub 
2 simplicity), (2-class. Ti-class . 
, . . . 
 . . 
. 
18.2 LO 
18.2.1 Decision . #of 
 accept from i to j . # of accept from to 
maximize 
19 Revenue 
$1.4 billion in 
Airlines in last 
 This is not a 
to at $500 
future. 
 continue to in of 
nn premium.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>t,o 
1. Define your ilccision variables clearly. 
ITrritc ci:,nstraints ani:l ol:,jcctivc funct,ion. 
for~ril~latiorl? 
fi,rmulation with a 
 numl:,cr 
 variaI:,lcs anil const,raint,s, anil t,hc mat,rix 
sparse. 
proble~ll 
furlctiorls 
:.S+R 
sl. s2 E ,Y 
f(As1 (lh)s?) 5 Xf(sl)+ (lPA)f(s2) 
,f 
 ci:,ncavc - (z) 
ci:,nvcx. 
0x1 the LO 
(z) dn z ci, 
.5.t. 3 b 20 Messages 
20.1 How formulate? 
2. 
What is a good LO 
A 
 of 
A is 
21.1 The general 
22 Convex 
f 
. 
(z) if f 
power of 23 
.For all 
+ 
min f = maxi, + 
Az</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Opti~rlization 
Fermat, 
mi11 f(r) x: scalar 
Euler, 
nlin ~(sI, r,") 
s.t,. (Jk(1.1,. .: r,") = 0 k = 1,. 77L 
Lagrange 
 Prol:,lcms in in fin it,^ 
dimcnsii:,ns, iralirulus variat,ions. 
Nonlinear Optirrlizatiovl 
The general 
Linear 
Forlnt~lat,ioli 
mininlizc 31.1 + 1.2 
sul,jcct ti:, 1.1 2 
21.1 1.2 2 
1.1 2 &gt; 
~ninimizc c'z 
sul,jcct ti:, Az 2 
z20 4 
6 History of 
1638:Newton, 1670 
1755 
Lagrange, 1707 
. . . . 
. ., . 
Euler, of 
5 
5.1 problem 
What is Optimization? 
6.1 
+ 21.2 
2 
+ 
3 
0.1.2 0 
h</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Exparlsiovi 
C;olistraiilt,s 
Dt: fc,rccast,ccl clcmani:l fi:n electricit,:: ::car 
Et: cxisti~lg cal:,acity (in nvailablc 
c,: ci:,st ti:, l:,roclucc 11\I\\'  using capncity 
lit: ti:, proi:lucc 1MTV using nuclcnr cnpacit,~ 
morc 
 nu,rlca,r 
ycnri 
Nuclear Inst :7cars 
r,: ami:,unt ci:,al cal:,acity lint ycar 
yt: nmount i:,f capncit,:: I:,rought i:,n line ycnr 
u:,: tot,nl ci:,al cal:,acity ycar 
zt: t,otal cnpacit,:: in :;car 
~iant,s ti:, &gt;icckl:: night,shift fi:,r llurscs 
D, iicmancl fi:n j = 1 T 
Evcry llursc works ri:,m 
14 Capacity 
14.1 Data and 
at t 
oil) nt t 
coal 
cost 
No than 20% 
Coal plants last 20 
plants 15 
14.2 Decision Variables 
of brought on in t. 
in t. 
in t. 
t. 
15 Scheduling 
15.1 Decision variables 
Hospital 
 mnkc 
 its 
nurses, . . . 
5 clays in a</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>hire 
 mininl~lm nnrnbcr nurses 
rj: startiqg t,hcir week i:,n cia- 
Managerrlerlt 
indust,ry 
1978 
- Clarricrs only allowc~i ti:, ircrt,ain ri:,utcs. Hcnirc airlincs 
iYi:,rt,h~scst. Eastern, Southwest, ctc. 
- Fares 
 ilct,cr~nincil I:,:: Clivil Acrona,utics Boaril lbascil mileage 
and othcr ci:,st,s (C.4B li:,ngcr SLII)E 26 
Post Dercgl~lati~n 
anyone ,ran anywhcrc 
farcs ~ictcrminc~i I:,y (and thc markct) 
17 Managerrlerlt 
Huge anil fixcil cost,s 
Vcry li:,m variablc ci:,st,s pcr passenger ($lO/passcngcr i:,r lcss) 
cci:,nomically ci:,mpctitivc cnvironmcnt 
Ncar-pcrfcct infi:,rmat,ion and ncgligil:,lc ci:,st infi,rmatii:,n 
pcrishal:,lc invcnt,ory 
l\lult~il~lc farcs Goal: of 
Decision Variables 
# nurscs 
 j 
16 Revenue 
16.1 	 The 
Deregulation in 
fly such as 
(CAB) on 
no 
 exists) 
fly, 
carrier 
Revenue 
sunk . 
Strong 
of 
Highly 
Result:</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>9 
Trar~sportatioll 
Dat,a 
171 
 11 ~snrchouscs 
.si supply i 1 m 
ti,? 
i:lcrnnn,S jth &gt;iarchousc, j = 11 
9.2.1 Fur~~l~~latiuri 
rij = numl:,cr i:,f t,o send i i j 
Sorting 
11 Invest ~r~ev~t urlder 
You have purchnsci:l .5i sharcs i:,f i pricc yi. i 11 
Cl~rrrcnt price i pi 10 Problem 
9.1 . plants. . of ith plant, = . . . . of I . . . 
9.2 Decision Variables 
units 
through LO 
taxation . stock at = 1,. . . . . of stock is</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 6.251J / 15.081J Introduction to Mathematical Programming
Fall 2009</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Formula,tionn: Lcc. 
Ctcomct,ry: Lcc. 2-4 
Simplex l\lcthoi:l: Lcc. 5-8 
Thcory: Lc,:. 
.-inalysin: Lei:. 
Robust Lcc. 
Large ncalc ol:,timizat,ion: Lcc. 
Flo~is: Lcc. 16-17 
The Ellipsi:,ii:l methi:,,$: Lcc. 18-19 
1:)oint mcthoi:ls: Lcc. 20-21 
Scmii:lcfinitc opt,imizatii:,n: Lcc. 
~i~, 
bl~ctc Opt,imizatii:,n: Lcc. 24-2; 
Requirements 
30%) 
Mii:ltcrm 30%) 
Final 
Iml:,ortant brakcr: cont,ribut,ions t,o ,class 
Lnc CPLEX fi:n si:,lving 
 ol:,timiza,tion problems 
of 
Optimizatio~l 
LOPS Arisc? 
Examplcs Fi:,rmulatii:,ns 1 Structure of Class 
I 
Duality 
Sensitivity 
9-11 
12 
Optimization: 13 
14-15 
Nctmork 
Interior 
22 . .. 
2 
Homcmorkn: 
Exam: 
Exam: 40% 
tic 
of 
3 Lecture Outline 
History 
Whcrc 
of</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Sensitivity analysis</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec12/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Motivation 
1.1	Questions 
Slide 1 
 z =min cx 
s.t.	Ax = b
x  0
 Howdoes z depend globally on c? on b? 
 Howdoes z change locally if either b, c, A change? 
 Howdoes z change if we add new constraints, introduce new variables? 
 Importance: Insight about LO and practical relevance 
2 Outline 
Slide 2 
1. Global sensitivity analysis 
2. Local sensitivity analysis 
(a) Changes in b 
(b) Changes in c 
(c) A new variable is added 
(d) A new constraint is added 
(e) Changes in A 
3. Detailed example 
3 Global sensitivity analysis 
3.1	Dependence on c 
Slide 3 
 G(c)= min cx 
s.t.	Ax = b 
x  0 
 iG(c)= mini=1,...,N cx is a concave function of c 
3.2	Dependence on b 
Slide 4 
Primal	 Dual 
 F(b)= min cx F(b)= max p  b s.t.	Ax = b s.t.	p  A  c  
x  0 
F(b)= maxi=1,...,N (pi) b is a convex function of b 
1</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>/bracketleftBigg /bracketrightBigg  Suppose you can hire 1h of nishing overtime at $7. Would you do it? 
 Another check 
  12 8 
 cBB1 =(0, 20, 60) 	02 4  = 
0 0.51.5 
(0, 10, 10) 
6.6 Reduced costs Slide 26 
 What does it mean that the reduced cost for x2 is5? 
 Suppose you are forced to produce x2 =1(1 table) 
 How much will the prot decrease? 
8x1 + x3 + s1 +6  1 =48 s1 =26 
4x1 +1.5x3 +2  1 =20  x1 =0.75 
2x1 +0.5x3 +1.5 1 =8 x3 =10 
 z  z =(60 0.75+20 10) (60 2+20 8+30 1) = 35+30 = 5 Slide 27 
Another way to calculate the same thing: If x2 =1 
Direct prot from table +30 
Decrease wood by -6 6 0 =0 
Decrease nishing hours by -2 2 10 = 20 
Decrease carpentry hours by -1.5 1.5 10 = 15 
TotalEect 5 
Suppose prot from tables increases from $30 to $34. Should it be produced? 
At$35?At$36? 
6.7 Cost ranges 
Slide 28 
Supposeprotfromdesksbecomes60+. For what values ofdoes current
basis remain optimal?
Optimality conditions:
cj c  
B B1Aj  0  
12 8 

  p = cB B1 =[0, 20, (60+)]	0 2 4
0 0.51.5
= [0, 100.5, 10+1.5] 
Slide 29 
s1,x3,x1 arebasic 
Reduced costs of non-basic variables 
8</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5.5 Changes in A 
Slide 17 
 Suppose aij  aij + 
 Assume Aj does not belong in the basis 
 Feasibility conditions: B1b  0, unaected 
 Optimality conditions: cl  cB  B1Al  0, l /negationslashj, unaected = 
 Optimality condition: cj  p  (Aj +ei) 0  cj  pi  0 
 Whatif Aj is basic? BT, Exer. 5.3 
6 Example 
6.1 A Furniture company 
Slide 18 
 A furniture company makes desks, tables, chairs 
 The production requires wood, nishing labor, carpentry labor 
Desk Table(ft) Chair Avail. 
Prot 60 
Wood (ft) 8 
Finish hrs. 4 
Carpentry hrs. 2 
6.2 Formulation 
Decision variables: 30 
6 
2 
1.5 20 
1 
1.5 
0.5 
x1 =#desks, x2 =#tables, x3 = # chairs 
max 60x1 +30x2 +20x3 
s.t. 8x1 +6x2 + x3 
4x1 +2x2 +1.5x3 
2x1 +1.5x2 +0.5x3 
x1,x2,x3 
6.3 Simplex tableaus 
Initialtableau: 
s1 = 
s2 = 
s2 = 0 
48 
20 
8 s1 s2 s3 
0 0 0 
1 
1 
1 
6 -
48 
20 
8 
Slide 19 
 48 
 20 
 8 
 0 
x1 
-60 
8 
4 
2 x2 
-30 
6 
2 
1.5 x3 
-20 
1 
1.5 
0.5 Slide 20</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6 
c2 = c2  p  A2 = 30+[0, 10 0.5, 10+1.5]  2  =5+1.25 
1.5 
cs2 =10 0.5 
cs3 =10+1.5 
Current basis optimal: 
 
5+1.25  0  
10 0.5  0 4    20 
10+1.5  0  
 56  c1  80 solution remains optimal. 
If c1 &lt; 56, or c1 &gt; 80 current basis is not optimal. 
Suppose c1 =100( =40) Whatwouldyoudo? 
6.8 Rhs ranges 
Slide 30 
Suppose nishing hourschangeby becoming(20+) Whathappens?   48 12 8 48 
B1  20+  =  02 4  20+  
8 0 0.51.5 8  24+2
=  8+2   0
2 0.5
4    4 current basis optimal Slide 31 
Note that even if current basis is optimal, optimal solution variables change: 
s1 =24+2 
x3 =8+2 
x1 =2 0.5 
z =60(2 0.5)+20(8+2) =280+10 
Slide 32 Suppose =10 then 	  
s1 44 
 x3  =  25   inf. (Use dualsimplex) 
x1 3 
6.9 New activity 
Slide 33 
Suppose the company has the opportunity to produce stools 
Prot $15; requires 1 ft of wood, 1 nishing hour, 1 carpentry hour 
Should the company produce stools? 
max	60x1 +30x2 +20x3 +15x4
8x1 +6x2 +x3 +x4 +s1 =48
4x1 +2x2 +1.5x3 +x4 +s2 =20
2x1 +1.5x2 +0.5x3 +x4 +s3 =8
xi  0 
9</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 12: Sensitivity Analysis</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>/parenleftBigg /parenrightBigg 1 
 c4 cB B1A4 = 15 (0, 10, 10) 1 =5  0 
1 
Current basis still optimal. Do not produce stools 
10</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>5.2.2 xj basic 
Slide 13 
cB  cB = cB +ej 
Then, 
 [c  cBB1A]i  0  ci  [cB +ej]  B1Ai  0 
[B1A]ji = aji 
ci ci ci  aji  0  max    min 
aji &lt;0 aji aji &gt;0 aji 
What if  is outside this range? use primal simplex 
5.3	A new variable is added Slide 14 
	 min	cx min cx + cn+1xn+1 
s.t.	Ax = b  s.t. Ax + An+1xn+1 = b 
x  0 x  0 
In the new problem is xn+1 =0 or xn+1 &gt; 0? (i.e., is the new activity prof
itable?) Slide 15 
Currentbasis B. Is solution x = B1b,xn+1 =0 optimal? 
 Feasibility conditions are satised 
 Optimality conditions: 
 cn+1  cB B1A n+1  0  cn+1  p  An+1  0? 
 If yes, solution x = B1b,xn+1 =0 optimal 
 Otherwise, use primal simplex 
5.4	A new constraint is added Slide 16 
min cxmin	cx s.t. Ax = b s.t.	Ax = b   a m+1x = bm+1 x  0 x  0 
If current solution feasible, it is optimal; otherwise, apply dual simplex 
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>' ( x3) 
( c +q d) ' 2
( c + q d) 
x1) 
. . . ( x) 
4
( c +q d) ' ( x) ( c +q d) ' ( 
x1 optimal x2 optimal x3 optimal x4 optimal q 
( p3) ( b * + q d) 
( p2) ( b * + q d) ( p1) ( b * + q d) 
q f ( q ) 
' 
' 
' 
q1 q2 
2</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Final tableau: 
s1 = 
x3 = 
x1 = s1 s2 s3 x1 x2 x3 
280 0 10 10 0 5 0 
24 1 2 -8 0 -2 0 
8 0 2 -4 0 -2 1 
2 0 -0.5 1.5 1 1.25 0 
6.4 Information in tableaus Slide 21 
 Whatis B?  
118 
B =  01.54  
00.52 
 Whatis B1?   12 8 
B1 =  02 4  
0 0.51.5 
Slide 22 
 What is the optimal solution? 
 What is the optimal solution value? 
 Is it a bit surprising? 
 What is the optimal dual solution? 
 Whatisthe shadowprice ofthe wood constraint? 
 Whatistheshadowpriceofthe nishing hoursconstraint? 
 What is the reduced cost for x2? 
6.5 Shadow prices 
Slide 23 
Why the dual price of the nishing hours constraint is 10? 
 Supposethat nishing hoursbecome21(from20). 
 Currently only desks(x1)and chairs(x3)areproduced 
 Finishing and carpentry hours constraints are tight 
 Does this change leaves current basis optimal? Slide 24 
New Previous 
8x1 + x3 + s1 =48 s1 =26 24 New solution: 4x1 +1.5x3 =21  x1 =1.5 2 
2x1 +0.5x3 =8 x3 =10 8 
Solution change: 
 z  z =(60 1.5+20 10) (60 2+20 8) =10 
Slide 25 
7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>/parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg bj bj max    min  
ji &gt;0 ji ji &lt;0 ji Slide 10 
     
Within this range 
 Currentbasis B is optimal 
   z = cBB1(b +ei)= cBB1b +pi 
 Whatif =? 
 Whatif &gt; ? 
Current solution is infeasible, but satises optimality conditions  use
dual simplex method
5.2 Changes in c 
Slide 11 
cj  cj +
Is current basis B optimal?
Need to check:
1. Feasibility: B1b  0, unaected 
 2. Optimality: c  cB B1A  0  , aected 
There are two cases: 
 xj basic 
 xj nonbasic 
5.2.1 xj nonbasic 
Slide 12 
cB unaected 
(cj +)  cB  B1Aj  0  cj +  0 
Solution optimal if  cj 
Whatif = cj? 
Whatif &lt; cj? 
4</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>4 Local sensitivity analysis 
 z =min cx 
s.t.	Ax = b 
x  0 
What does it mean that a basis B is optimal? 
1.	Feasibility conditions: B1b  0 
 2. Optimality conditions: c  cB B1A  0  Slide 5 
Slide 6 
	Suppose that there is a change in either b or c for example 
	How do we nd whether B is still optimal? 
	Need to check whether the feasibility and optimality conditions are satis
ed
5 Local sensitivity analysis 
5.1	Changes in b 
Slide 7 
bi becomes bi +,i.e. 
(P) min c  x (P  ) min c  x 
s.t.	Ax = b  s.t. Ax = b +ei
x  0 x  0
	B optimalbasisfor(P) 
	Is B optimalfor(P  )? 
Need to check: 
1.	Feasibility: B1(b +ei) 0 
 2.	Optimality: c  cB B1A  0  
Observations: 
1.	Changes in b aectfeasibility 
2.	Optimality conditions are not aected 
B1(b +ei) 0 
ij =[B1]ij 
bj =[B1b]j 
Thus, 
(B1b)j +(B1 ei)j  0  bj +ji  0 
3 Slide 8 
Slide 9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Interior point methods II</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
1.	Barrier Methods 
2.	The Central Path 
3.	Approximating the Central Path 
4.	The Primal Barrier Algorithm 
5.	Correctness and Complexity 
2 Barrier methods 
Slide 2 
min f(x) 
s.t.gj(x)0,j=1,...,p 
hi(x)=0,i =1,...,m 
S = gj(x)&lt; 0,j =1,...,p, {x|
hi(x)=0,i =1,...,m} 
2.1 Strategy 
Slide 3 
	Abarrierfunction G(x)is a continous function with the property that is
approaches as one of gj(x)approaches 0 from negative values.
	Examples: 
p	 p   1 G(x)=  log(gj(x)),G(x)=  gj(x)j=1	 j=1 
Slide 4 
Consider a sequence of k:0 &lt;k+1 &lt;k and k 0. 	 
	Considertheproblem 
x k =argminxS  
f(x)+ kG(x)  
	Theorem Every limitpoint xk generated by a barrier method is a global
minimum of the original constrained problem.
1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.1 Proof   Slide 23 
 Claim(byinduction): |1 
k XkSke e|  
For k =0 we have assumed it  
 Assumeitholdsfor k; 
 1  1  
|XkSke e |= |k XkSke e |k+1  
= | 1 
1 
k XkSke e +1
 e | 
  1 
|1 
k XkSke e |+1
 ||e|| 
  +1
n 
=  
k+1 k  We next show that ||X1d||&lt; 1, where d = x x .k 
d solves  
k+1X2 k+1X1  k d A p =  k e c, 
Ad = 0 
 By left-multiplying the rst equation by d 
k+1dX2 d k+1X1  k d =  k e c 
||X1d|| 2 =  dX2d k k 
 
X1 1 = k e cd k+1 
 1 k k= X
k 1 e (s + A p ) d k+1 
 
X1 1 k = k e sd k+1 
 
=  1 XkSke eX1d k+1 k 
 1   e X1d  |k+1 XkSke |||k || 
 ||X
k 1d|| 
hence, ||X1d||&lt; 1.k 
7</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>k	

 k = k  0 =1
 
+ 
n  0 ek
+  n  0
After  
0	 00 +n n(1+)  + n (s ) x (1+)  log    log (1)= K 
iterations, the primal barrier algorithm nds primal and dual solutions x K ,
(p K , s K),thathavedualitygap(s K) x K less than or equal to 
7 Complexity 
Slide 24 
	Work per iteration involves solving a linear system with m + n equations
in m + n unknowns. Given that m n,the workperiterationis O(n3).
	0 =(s0)  x0: initial duality gap. Algorithm needs 
O nlog 0  
 
iterations to reduce the duality gap from 0 to , with O(n3) arithmetic
operationsperiteration.
9</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>x* . . 
. . 
x(0.01) 
x(0.1) 
x(1) 
x(10) . central path 
. 
analytic center c 
2.2	Primal path-following 
IPMs for LO Slide 5 
 (P) min cx (D) max b  p 
s.t. Ax = b s.t. A  p + s = c 
x 0	 s 0 
Barrierproblem: 
n 
 min B(x)= cx  logxj 
j=1 
s.t. Ax = b 
Minimizer: x() 
3 Central Path 
Slide 6 
 As  varies, minimizers x()form the central path 
 lim0 x()exists and is an optimal solution x  totheinitialLP 
 For = , x()is called the analytic center 
n 
min logxj  
j=1 
s.t. Ax = b 
Slide 7 
3.1	Example 
Slide 8 
min x2 
s.t.x1 + x2 + x3 =1 
x1,x2,x3 0 
2</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>x* . . 
. . 
x(0.01) 
x(0.1) 
x(1) 
x(10) . central path 
. 
analytic center c 
Solution:  
  1 d()= I X2A  (AX2A  )1A xe  X2 c 
p() =(AX2A  )1A(X2 c xe) 
4.1 The Newton connection Slide 16 
	d()isthe Newton direction; process of calculating this direction is called
a Newton step
	Starting with x, the new primal solution is x + d() 
	The corresponding dual solutionbecomes(p,s)= p(),c A  p() 
	Wethendecrease  to = ,0 &lt; &lt; 1 
4.2 Geometric Interpretation 
Slide 17 
	Take one Newton step so that x would be close to x()
Measure of closeness  
 	 1   
 XSe e , 
0 &lt;&lt; 1, X =diag(x1,..., xn)S =diag(s1,..., sn)
As 0, the complementarity slackness condition will be satised
  
Slide 18 
5</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5 The Primal Barrier Algorithm 
Slide 19 
Input 
(a) (A, b, c);A has full row rank; 
(b)	x0 &gt; 0, s0 &gt; 0, p0; 
(c) optimalitytolerance &gt; 0; 
(d)	0, and , where 0 &lt; &lt; 1. Slide 20 
1.	(Initialization) Start with some primal and dual feasible x0 &gt; 0, s0 &gt;
0, p0, and set k =0.
2.	(Optimalitytest) If(sk)  xk &lt; stop; else go to Step 3. 
3.	Let 
Xk = diag(x k 
1 ,...,x k
n), 
 k+1 = k 
Slide 21 
4.	(Computation of directions)Solve the linear system 
k+1X2 k+1X1 k d A  p =  e ck 
Ad = 0 
5.	(Update ofsolutions) Let 
x	k+1 = x k + d, 
k+1 p = p, 
s k+1 = c A  p. 
6.	Let k := k+1 and go to Step 2. 
6 Correctness 
  Slide 22 
Theorem Given  =1 + n, &lt; 1,(x0 ,s0 ,p0),(x0 &gt; 0, s0 &gt; 0): 
 1  	X0S0e e .  0  
Then, after  + n (s 0)  x 0(1+)K = log   (1) 
iterations,(xK ,sK ,pK)isfound: 
(s K)  x K . 
6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>.. 
x1 x2 x3 
Q 
the central path the analytic 
center of Q the analytic 
center of  P (1/3, 1/3, 1/3) 
(1/2, 0, 1/2) P 
	Q= x |x =(x1,0,x3),x1 +x3 =1, x 0}, set of optimal solutions to
originalLP
	The analytic center of Q is(1/2,0,1/2) 
min x2 logx1 logx2 logx3 
s.t.x1 + x2 + x3 =1 
min x2 logx1 logx2 log(1x1 x2). 
x1() =1x2() 
2 
1+3 1+92 +2 x2()= 2 
x3() =1x2() 
2 
The analytic center: (1/3,1/3,1/3) Slide 9 
3.2 Solution of Central Path Slide 10 
	Barrierproblemfordual: 
n 
max pb +  logsj 
j=1 
s.t. pA + s = c 
	Solution(KKT):
Ax()= b
x()  0
A p()+s()= c
s()  0 
X()S()e = e 
3</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>We next show that x k+1 and(p k+1 , s k+1) are primal and dual feasible Since  
Ad =0, we have 
k+1 Ax= b
x k+1 = x k + d = Xk(e + X
k 1d)&gt; 0,
because X1d&lt; 1
k|| ||
A p k+1 + s k+1 = c, 
by construction and 
s k+1 = c A p k+1 =  k+1Xk 1 
k d)&gt; 0, (e X1 
because X1d&lt; 1k|| ||
 
k+1 k dj xj = xj 1+ k , xj 
k+1  k+1 dj sj = k 1 k . x xj j 
Therefore, 
1 k+1 k+1 1 k dj  k+1 dj 
k+1 xj sj 1= k+1 xj 1+ k k 1 k 1   xj xj xj 2 dj = . k xj 
 D =diag(d1,...,dn),||u||1 = i |ui|. Note that ||u||||u||1 
 1 Xk+1Sk+1e e = X2D2 ek |k+1 | | | 
k 1 |X2D2 e|
= e X
k 2D2 e 
= e DX
k 2De 
= k d dX2 
= X
k 1d2 |
 | 
 ( )2 
= , 
and hence the induction is complete. 
 Since at every iteration 
 1  
|k XkSke e | 
1 kk  k xjsj 1  
n k(1)(s k) x k n k(1+) 
8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 21: Primal Barrier
InteriorPointAlgorithm</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Slide 11 
	Theorem: If x  , p , and s  satisfy optimality conditions, then they are
optimal solutionstoproblemsprimal anddualbarrierproblems.
	Goal: Solvebarrierproblem 
n 
min B(x)= cx  logxj 
j=1 
s.t. Ax = b 
4 Approximating the central path 
Slide 12 
B(x)  
xi = ci xi 
2B(x)  = x x2 
i i 2 
2B(x) =0,i = jxixj 
Given a vector x &gt; 0:	 Slide 13 
n  B(x)B(x + d) B(x)+ di + xii=1 
1 n 2B(x)didj2 xixji,j=1 
1  = B(x)+(c eX1)d + d  X2d 2
X =diag(x1,...,xn) Slide 14 
Approximatingproblem: 
1  min (c eX1)d + d  X2d 2
s.t. Ad = 0 
Solution(fromLagrange): 
c X1 e + X2d A  p = 0 
Ad = 0 
Slide 15 
	System of m+n linearequations,with m+n unknowns(dj,j =1,...,n,
and pi,i =1,...,m).
4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Duality theory I</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>4 Dual Simplex Algorithm 
4.1 Motivation Slide 5 
	In simplex method B1b  0 
	Primal optimality condition
c  c  B1A  0 
B 
same as dualfeasibility 
	Simplexisaprimal algorithm: maintains primalfeasibilityand works
towards dualfeasibility
	Dual algorithm: maintains dualfeasibility and workstowards primal 
feasibility 
 cB xB 
xB(1) 
. . . 
xB(m) 
	Do not require B1b  0 Slide 6 
c1 
| 
B1A1 
| . . . 
. . . cn 
| 
B1An 
| 
	Require c 0 (dualfeasibility) 
	Dual cost is
p  b = c  B1b = cB  xB
 B 
	If B1b  0 then both dual feasibility and primal feasibility, and also
same cost  optimality
	Otherwise, change basis 
4.2 An iteration Slide 7 
1.	Start with basis matrix B and all reduced costs  0. 
2.	If B1b  0 optimal solution found; else, choose l s.t. xB(l) &lt; 0. 
3. Consider the lth row(pivot row) xB(l),v1,...,vn. If ivi  0 then dual
optimal cost = + and algorithm terminates.
Slide 8 
4.	Else, let j s.t. 
cj ci = min |vj | {i|vi &lt;0} |vi| 
5.	Pivot element vj : Aj enters the basis and AB(l) exits. 
3</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5.2 Degeneracy and uniqueness 
Slide 16 
	If dual has a nondegenerate optimal solution, the primal problem has a
unique optimal solution.
	It is possible, however, that dual has a degenerate solution and the dual
has a unique optimal solution.
6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 9: Duality Theory II</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>a1 
a3 c 
B a1 
a2 A 
a1 a2 a3 c 
a1 a1 a4 a4 a5 
a5 c 
c c 
C 
D 
a1 
a2 a3 c 
x * a1 a2 
a3 
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>1 c 
A B 
C 
D E x2 
x1 1 2 b 
A B C D 
E p2 
p1 1 
1 1/2 . . .. . . 
. . 
. . 
(a)	 (b) 
4.3 An example 
min	x1 + x2 
s.t.x1 +2x2  2 
x1  1 
x1,x2  0 
min x1 + x2	 max 
s.t.	x1 +2x2 x3 =2 s.t. 
x1 x4 =1 
x1,x2,x3,x4  0 Slide 9 
2p1 + p2 
p1 + p2  1 
2p1  1 
p1,p2  0 
Slide 10 
x1 x2 x3 x4 
0 1 1 0 0 
x3 = 2 1 2* 1 0 
x4 = 1 1 0 0 1 
Slide 11 
x1 x2 x3 x4 
1 1/2 0 1/2 0 
x2 = 1 1/2 1 1/2 0 
x4 = 1 1* 0 0 1 
x1 x2 x3 x4 
3/2 0 0 1/2 1/2 
x2 = 1/2 01 1/21/2 
x1 = 110 0 1 
4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>c 
A B
C 
D.
..	
. B D . . .. C.
. A'
A''x2	 p2 
A 
p1 
b x1 
(a)	 (b) 
5 Duality and Degeneracy 
Slide 12 
	Any basis matrix B leads to dual basic solution p  = cB  B1 . 
	The dual constraint p  Aj = cj is active if and only if the reduced cost cj
is zero.
	Since p is m-dimensional, dual degeneracy implies more than m reduced
costs that are zero.
	Dual degeneracy is obtained whenever there exists a nonbasic variable
whose reduced cost is zero.
5.1 Example 
Slide 13 
min 3x1 + x2 max 2p1 
s.t.x1 + x2 x3 =2 s.t.p1 +2p2  3 
2x1 x2 x4 =0 p1 p2  1 
x1,x2,x3,x4  0, p1,p2  0. 
Equivalentprimalproblem 
min 3x1 + x2 
s.t.x1 + x2  2 
2x1  x2  0 
x1,x2  0. 
Slide 14 
Slide 15 
	Four basic solutions in primal: A, B, C, D. 
	Six distinct basic solutions in dual: A, A  , A  , B, C, D. 
	Dierent bases may lead to the same basic solution for the primal, but
to dierent basic solutions for the dual. Some are feasible and some are
infeasible.
5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
 Strict complementary slackness 
 Geometry of duality 
 The dual simplex algorithm 
 Duality and degeneracy 
2 Strict Complementary Slackness 
Slide 2 
Assume that both problems have an optimal solution: 
min c  x max p  b 
 s.t. Ax  b s.t. p  A  c 
x  0, p  0. 
There exist optimal solutions to the primal and to the dual that satisfy 
 For every j, either xj &gt; 0 or p  Aj &lt;cj . 
  For every i, we have either aix &gt;bi or pi &gt; 0. 
2.1 Example 
Slide 3 
min 5x1 +5x2 
s.t.x1 + x2  2 
2x1 x2  0 
x1,x2  0. 
 Is(2/3, 4/3) strictly complementary? 
 Which are all the strictly complementary solutions? 
3 The Geometry of Duality 
Slide 4 
min cx 
 s.t. aix  bi,i =1,...,m 
max p  b 
m 
s.t. 
piai = c 
i=1 
p  0 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Simplex method IV</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-251j-introduction-to-mathematical-programming-fall-2009/resources/mit6_251jf09_lec08/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>8.3 Example 
Slide 15
 
min 13x1 + 10x2 +6x3 max 8p1 +3p2 
s.t. 5x1 + x2 +3x3 =8 s.t. 5p1 +3p2  13
 
3x1 + x2 =3 p1 + p2  10
 
x1 ,x2 ,x3  0 3p1  6
 
Is x  =(1, 0, 1)  optimal? Slide 16
 
5p1 +3p2 =13, 3p1 =6 
 p1 =2,p2 =1 
Objective=19 
5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>15.081J/6.251J Introduction to Mathematical 
Programming 
Lecture 8: Duality Theory I</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6 Relations between primal and dual 
Slide 11 
Finite opt. Unbounded Infeasible 
Finite opt. * 
Unbounded * 
Infeasible * * 
7 Economic Interpretation 
Slide 12 
	 	x optimal nondegenerate solution: B1b &gt; 0 
	 	Suppose b changesto b + d for some small d 
	 	How is the optimal cost aected? 
	 	For small d feasibilty unaected 
	 	Optimality conditions unaected 
	 	New cost c  B1(b + d)= p  (b + d)B 
	 	If resource i changesby di, cost changes by pidi: Marginal Price 
8 Complementary slackness 
8.1 Theorem Slide 13 
Let x primal feasible and p dual feasible. Then x, p optimal if and only if 
 pi(aix bi)=0, i 
 xj (cj pAj )=0, j 
8.2 Proof Slide 14 
	 	ui = pi(ai  x bi)and vj =(cj p  Aj )xj 
	 	If x primal feasible and p dual feasible, we have ui  0 and vj  0 for all
 
i and j.
 
	 	Also 
 cx p  b = ui + vj . 
i j 
 	 	By the strong duality theorem, if x and p are optimal, then cx = p  b 
 
ui = vj = 0 for all i, j.
 
 Conversely,if ui = vj = 0 for all i, j,then c  x = p  b,
 
 x and p are optimal.
 
4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 Outline 
Slide 1 
 Motivation of duality 
 General form of the dual 
 Weak and strong duality 
 Relations between primal and dual 
 EconomicInterpretation 
 Complementary Slackness 
2 Motivation 
2.1 An idea from Lagrange 
Slide 2 
Consider the LOP, called the primal with optimal solution x  
 min cx 
s.t. Ax = b 
x  0 
Relax the constraint 
g(p)= min c  x + p  (b  Ax) 
s.t. x  0 
g(p) c  x  + p  (b  Ax)= c  x  
Getthetightestlowerbound,i.e., 
maxg(p) 
g(p) = min c  x + p  (b Ax) 
x0 
   = pb + min (c pA)x 
x0 
Notethat  
  0, if c  p  A  0  ,min (c pA)x = 
x0 , otherwise.
 
Dual max g(p)  max p  b
 
s.t. p  A  c  
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.251J / 15.081J Introduction to Mathematical Programming 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Corollary:
 
If x isprimalfeasible, p is dual feasible, and p  b = c  x, then x is optimal in
 
the primal and p is optimal in the dual.
 
5 Strong Duality 
Slide 8 
Theorem: If the LOP has optimal solution, then so does the dual, and optimal
 
costs are equal.
 
Proof:
 
min cx 
s.t. Ax = b 
x  0 
Apply Simplex; optimal solution x,basis B. 
Optimality conditions: 
c  cB  B1A  0  
Slide 9 
Dene p  = c  B1  p  A  c  
B 
 p dualfeasiblefor 
max p  b 
s.t. p  A  c  
 p  b = cB  B1b = cB  xB = cx 
 x, p are primal and dual optimal 
5.1 Intuition Slide 10 
x * a1 a2 a3 
p1a1 p2a2 c 
. 
3</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 General form of the dual
 
Primal 

min cx 
s.t. a
i
 x  bi i  M1 
a
i
 x  bi i  M2 
a
i
 x = bi i  M3 
xj  0 j  N1 
xj  0 j  N2 
xj &gt;
&lt;0 j  N3 
3.1 Example 
min x1 +2x2 +3x3 
s.t. x1 +3x2 =5 
2x1  x2 +3x3  6 
x3  4 
x1  0 
x2  0 
x3 free, 
Primal 
constraints	 
variables min 
 bi 
 bi 
= bi 
 0 
 0 
&gt; 
&lt;0 Dual 
max 
s.t. 
max 
s.t. 
max 
 0 
 0 
&gt; &lt;0 
 cj 
 cj 
= cj 
min cx	 max p  b 
s.t. Ax = b s.t. p  A  c  
x  0 
  b min cx	 max p 
s.t. Ax  b s.t. p  A = c  
p  0 
4 Weak Duality 
Slide 7 
Theorem: 
If x is primal feasible and p isdualfeasiblethen p  b  c  x 
Proof 
p  b = p  Ax  c  x 
2 Theorem: The dual of the dual is the primal. 
3.2 A matrix view 
 Slide 3 
p  b 
pi  0 
pi  0 
p &gt; 
i &lt;0 
p  Aj  cj 
p  Aj  cj 
p  Aj = cj i  M1 
i  M1 
i  M3 
j  N1 
j  N2 
j  N3 
5p1 + 6p2 + 4p3 
p1 free 
p2  0 
p3  0 
p1 + 2p2  1 
3p1  p2  2 
3p2 + p3 = 3. Slide 4 
Slide 5 
dual 
variables 
constraints 
Slide 6</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
