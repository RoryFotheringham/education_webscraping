<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/</course_url>
    <course_title>Computational Models of Discourse</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Humanities </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Temporal Ordering in Discourse (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>Assigning Time Values(2)
Rule-based approach for propagation of TIMEX: 
Positional offsets from reference time: next  
month, this coming Thursday 
	Implicit offset based on verb tense: The rst 
shipment was loaded Thursday 
 Use of lexical markers: after, until 
Accuracy: above 80% 
Temporal Relations in Discourse	 13/24</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Ordering Events:Method 
n-class classication 
 Anchor Relations: at, before, after, undened 
 Reference Time moves: keep, revert, shift 
Temporal Relations in Discourse 14/24</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Results 
83,810 (80% training, 10% development/testing)
	64% accuracy (note that some connectives are
interchangeable)
	Best features: verb, verb classes, syntactic structure, 
position 
	No impact: nouns, adjectives, temporal 
Temporal Relations in Discourse	 24/24</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Reichenbachs Topology of Tense 
Reichenbach (1947) 
 Point of speech (S) 
 Point of event (E) 
 Point of reference (R) 
John has climbed Everest. (ER=S)
John is in the class. (E=R=S)
John will climb Everest (S&lt;E=R)
John had climbed Everest (E&lt;R&lt;S)
John will have climbed Everest (S&lt;E&lt;R)
Temporal Relations in Discourse 5/24</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Results 
Tested on 2069 clauses 
ANCHORS REF MOVES 
MAJORITY 76.9%(AT) 65.75% (KEEP) 
ML 80.2% 71.8% 
Temporal Relations in Discourse 16/24</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Temporal Processing: Knowledge Sources 
(Lascarides&amp;Asher, 1993) 
	Tense and aspect 
Mike entered the room. He had drunk/was drinking the 
wine. 
	Temporal Adverbials/Connectives 
A drunken man died in the central Philippines when he put a recracker under his armpit. 
	World Knowledge
Mike kissed the girl he met at a party .
Temporal Relations in Discourse	 7/24</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Model Features:Others 
	Syntactic Feature (complexity) 
	Argument Feature (verb tendency to take objects, 
PPs, ...) 
Relative Position of S and M  
Temporal Relations in Discourse	 22/24</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Resolving Temporal Expressions 
(Mani&amp;Wilson, 2001) 
	Primary Goal: Disambiguation of speaker and
reference time dependent time markers
now, today, next Tuesday
	Secondary Goal: Construction of event chronologies 
Temporal Relations in Discourse	 9/24</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Parameter Estimation 
Decision tree on one-feature classiers  
Standard feature selection  
Temporal Relations in Discourse 23/24</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Tense as Discourse Anaphora 
(Webber, 1988) 
John went into the orist shop.
He had promised Mary some owers.
She said she wouldnt forget him if he forgot.
So he picked up three rose, two white ones, and one
pink.
Temporal Relations in Discourse 6/24</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Model Features:Lexico-Semantic 
 Verb Identity 
 Verb Class (15 classes of WordNet, 200 Levin) 
 Noun Identity 
 Noun Class (25 WordNEt classes) 
 Adjective 
Temporal Relations in Discourse 21/24</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Today 
 Time marker disambiguation 
 Event ordering 
 Inter-sentential 
 Intra-sentential 
Temporal Relations in Discourse 4/24</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Model 
t=argmax tjP(S M,tj,S S)=
=argmax tjP(S M)P(tj|SM)P(S SSM,tj)= |
=argmax tjP(tj|SM)P(S SSM,tj)= |
=argmax tjP(tj)P(S M|tj)P(S SSM,tj) |
Temporal Relations in Discourse 18/24</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Analysis
	In some texts, timeline is a backbone of discourse 
organization 
	(Newspaper) texts are rife with time switches
	We can typically recreate a timeline of events
	Temporal information is reected via tense, 
connectives, temporal markers 
Temporal Relations in Discourse	 2/24</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Processing Temporal Relations: Why?
	Question-Answering: 
Did Stewart meet a probation ofcer before her sentence? 
	Dialogue Systems: 
What ights to NY do you have in a week from today? 
Information Extraction  
John was killed yesterday 
	Natural-Language Generation and Summarization 
	Novelty detection 
Temporal Relations in Discourse	 3/24</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Studies of Tense 
(we will not cover today , but see the references)
	Linguistic analysis of tense: 
(Kamp&amp;Reyle, 1993; Moens&amp;Steedman, 1988) 
Annotation Schemes:  
(Setzer and Gaizauskas, 2001) 
Temporal Relations in Discourse	 8/24</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Ordering Events:Features 
	Type of verb: reporting, stative, accomplishment
	Tense and Aspect in the current clause, and shifts 
from previous clauses 
Presence of anchor and TIMEX  
	Syntactic features: type of conjunction, modiers 
and propositions 
Temporal Relations in Discourse	 15/24</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Newspaper Text
NEW YORK (AP) - Martha Stewart thanked viewers of her television 
shows and readers of her magazines and websites for their support 
on Monday after meeting with a probation ofcer following her con
viction for lying about a stock sale. 
She made the remarks as she was getting into the front passenger 
seat of a sport utility vehicle outside a lower Manhattan courthouse, where she met with a probation ofcer for about an hour. I want to thank my readers, my viewers and the Internet users, Stewart 
said just before closing the door behind her. 
The probation meeting is the rst step toward sentencing on 
June 17 . 
Temporal Relations in Discourse 1/24</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Temporal Relations in Discourse 
Regina Barzilay 
March 11, 2003</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Unsupervised Approach
(Lapata&amp;Lascarides, 2004)
Goal: Infer Sentence-internal Temporal Relations
Key Idea: Use connectives as predictors of relations
Leonard Shane, 65, held the post of president before
William Shane, 37, was elected to it last year.
The results were announced after the market closed.
Leaving the party , John walked home.
Temporal Relations in Discourse 17/24</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Assigning Time Values(1)
remove SEMI-TIMEX expressions, such as today in 
USA TODAY 
	Method: Binary Classication 
Features:  
	POS info: word before, word after 
	Lexical info: presence of said, will, year, 
most 
	Temporal markers: presence of a year or a day 
	Context: whether appear in quotations 
Accuracy: 79.8%, Baseline 66.5% 
Temporal Relations in Discourse	 12/24</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Model
Assuming conditional independence of subordinate from 
main clause: 
argmax tjP(tj)P(S M|tj)P(S Stj)|
Assuming feature independence: 
argmax tjP(tj)
P(a(M,i )|tj)P(a(S,i)tj)|
i
Temporal Relations in Discourse 19/24</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Model Features:Temporal
FINITE = {past, present }
NON-FINITE = {innitive, ing-form, en-form }
MODALITY = {NIL, future, ability , possibility , obligation
}
ASPECT = {imperfective, perfective, progressive }
VOICE = {active, passive } NEGATION = {afrmative, 
negative } 
Temporal Relations in Discourse 20/24</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Algorithm for TIMEX tagging 
	Identify TIMEX 
Resolve self-Contained  
	Propagate annotations based on context 
	Track Reference Time based on Temporal Focus 
and Document creation date 
Temporal Relations in Discourse	 11/24</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Annotation Scheme
ISO standard: CC:YY :MM:DD:HH:XX:SS
Time points are treated as primitives (Bennett&amp;Partee,
1972)
	Tuesday , Novembers 2, 2002  20:00:11:02
	From May 1999 to June 1999 is represented as 
two points 
  April is usually wet is not marked as TIMEX 
Only 25% of clauses have time marker 
Temporal Relations in Discourse	 10/24</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Automatic Reference Resolution (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>9</slideno>
          <text>Demonstratives and One Anaphora
	Demonstratives (this, that) capture spatial proximity 
I like this one, better than that 
	One Anaphora evokes a new entity into the 
discourse whose description is dependent of this new entity 
I saw no less that 6 Acuras today . Now I was one. 
Reference Resolution	 10/??</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Clustering Algorithm 
	Initialization: every noun is a singleton
	From right to left, compare each noun to all
proceeding clusters
	Combine close enough clusters unless there exist any incompatible NP 
Example: The chairman spoke with Ms. White. He ... 
Reference Resolution	 29/??</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Troublemakers
	Inferrables: inferential relation to an evoked entity
I almost bought an Acura today , but a door had a dent and the
engine seemed noisy .
Discontinuous Sets: refer to entities that do not  
form a set in a text
John has an Acura, and Mary has a Mazda. They drive them all
the time.
	Generics: refer to general set of entities (in contrast
to a specic set mentioned in text)
I saw no less than six Acuras today . They are the coolest cars.
Reference Resolution	 11/??</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Supervised Learning
(Soon et al.,2001) 
Decision Tree Induction  
	Shallow feature representation (12 features):
	corrective clustering 
	Signicant performance gain over rule-based 
algorithms 
Reference Resolution	 31/??</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Algorithm
1. Remove potential referents that do not agree in 
number or gender with the pronoun 
2. Remove potential referents that do not pass 
intrasentetial syntactic coreference constraints 
3. Update the total salience value of the referent
4. Select the referent with the highest value 
Accuracy on unseen data: 86% 
Reference Resolution 25/??</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Instance Representation
Based noun phrases (automatically computed) are
represented with 11 features:
Individual Words  
Head Word  
Position  
 Pronoun type (nominative, accusative) 
 Semantic Class: Time, City , Animal, Human, Object (WordNet) 
 Gender (WordNet, specied list) 
 Animacy (based on WordNet) 
Reference Resolution 27/??</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Discourse Model Update 
(Lappin&amp;Leass, 1994) 
	Add every new discourse entity to discourse model 
	Update its value based on salience factors 
	Cut in half recency values when process new entity 
(recency enforcement) 
Reference Resolution	 22/??</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Salience Factors 
Sentence Recency 100 
Subject Emphasis 80 
Existential Emphasis 70 
Accusative 50 
Indirect Object 40 
Non-adverbial Emphasis 50 
Head-noun Emphasis 80 
Reference Resolution 23/??</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Reference Resolution: Example 
The Salesgirl (Burns and Allen) 
Gracie: And then Mr. and Mrs. Jones were having matrimonal
trouble, and my brother was hired to watch Mrs. Jones.
George: Well, I am imagine she was a very attractive woman.
Gracie: She was, and my brother watched her day and night for
six month.
George: Well, what happened?
Gracie: She nally got a divorce.
George: Mrs. Jones?
Gracie: No, my brothers wife.
Reference Resolution 1/??</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Distance Metric 
dist(NP i,NP j)=
wf incomp f(NP i,NP j)
f
Reference Resolution 28/??</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Reference Resolution: Trends
	Knowledge-Rich Approaches vs Knowledge-Lean 
Approaches 
	Semi-automatic Fully-Automatic Preprocessing 
	Small-scale vs Large-Scale Evaluation 
Reference Resolution	 20/??</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Syntactic Factors
subject &gt;existential predicate nominal &gt;object &gt;
indirect object &gt;demarcated adverbial PP
1. An Acura Integra is parked on the lot. (subject) 
2. There is an Acura Integra parked in the lot. 
3 . ... 
4. Inside his Acura Integra, John kissed Mary . (demarcated
adverbial PP)
Penalty for non-head occurrences
Score for equivalence classes
Reference Resolution 24/??</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>When something goes wrong
In the past decade almost all Islamic revivalist movements 
have been labeled fundamentalists, whether they be of extremist or moderate origin. The widespread impact of the term is obvious from the following quotation from one 
of the most inuential Encyclopedias under the title 
Fundamentalist: The term fundamentalist has. . . been used to describe members of militant Islamic groups. Why would the media use this specic word, so often with relation to Muslims? Most of them are radical Baptist, 
Lutheran and Presbyterian groups. 
Reference Resolution 6/??</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Preferences in Pronoun Interpretation
	Recency: Entities introduced in recent utterances 
are more salient than those introduced further back 
John has an Integra. Bill has a Legend. Mary likes to drive it. 
	Repeated mention: Entities that have been focus on in the prior discourse are more likely to continue to be focused on in subsequent discourse 
John needed a car to get his new job. He decided that he wanted 
something sporty . Bill went to the Acura dealership with him. He 
bought an Integra. 
Reference Resolution	 15/??</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Syntactic Constraints
	Gender Agreement 
John has an Acura. It is attractive. 
	Syntactic Agreement 
John bought himself a new Acura. 
John bought him a new Acura. 
Reference Resolution	 13/??</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Knowledge-Lean Multi-strategy Approach 
(Lappin&amp;Leass, 1994) 
	Integrates the effects of the recency and
syntactically-based preferences
	Doesnt rely on semantic or pragmatic knowledge
	Follows greedy strategy 
	Two stages: discourse model update and pronoun resolution 
Reference Resolution	 21/??</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Generic Algorithm 
Identication of Discourse Entities
 
Identify nouns and pronouns in text
Characterization of Discourse Entities

Compute for each discourse entity NP ia set of values from
{Ki1,...,k im} from mknowledge sources
	Anaphoricity Determination
Eliminate non-anaphoric expressions to cut search space
Generation of Candidate Antecedents  
Compute for each anaphoric NP ja list of candidate antecedents 
Cj
Reference Resolution	 18/??</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Types of referential expressions: Nouns
Indenite Noun Phrases:  
I saw an Acura Integra today .
Some Acura Integras were being unloaded.
I saw this awesome Acura Integra today .
Denite Noun Phrases  
I saw an Acura Integra today . The Integra was white and needed
to be washed.
The fastest car in the Indianapolis 500 was an Integra.
Reference Resolution 8/??</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Semantic Constraints
 Selectional restrictions of the verb on its arguments
(1) John parked his Acura in the garage. He had driven it around 
for hours. 
(2) John parked his Acura in the garage. It is incredibly messy , 
with old bike and car parts lying around everywhere. 
(3) John parked his Acura in downtown Beverly Hills. It is
incredibly messy , with old bike and car parts lying around
everywhere.
Reference Resolution 14/??</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Todays Topics 
Motivation 
 Types of referential expressions 
 Syntactic and semantic constraints on coreference 
 Preferences in coreference interpretation2 
 Algorithms for coreference resolution 
Reference Resolution 4/??</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Clustering for Coreference 
(Cardie&amp;Wagstaff:1999) 
	Each group of coreferent noun phrases denes an 
equivalence class 
	Distance measure incorporates linguistic intuition 
about similarity of noun phrases 
	Hard constraints enforce clustering construction 
Reference Resolution	 26/??</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Preferences in Pronoun Interpretation
	Grammatical Role: Hierarchy of candidate entities 
based on their grammatical role 
John went to the Acura dealership with Bill. He bought an 
Integra. 
Bill went to the Acura dealership with John. He bought an 
Integra. 
Parallelism:  
Mary went with Sue to the Acura dealership. Sally went with her to the Mazda dealership. 
Reference Resolution	 16/??</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Motivation 
Information extraction  
	Question-Answering 
Machine-Translation 
pronoun in the Malay language is translated by its antecedent 
(Mitkov, 1999) 
Summarization 
Reference Resolution	 5/??</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Reference Resolution
	Task: determine which noun phrases refer to each 
real-world entity mentioned in a document 
	Goal: partition noun phrases in a text into coreference equivalence classes, with one cluster for each set of coreferent NPs 
	Difference between anaphora and coreference
In the previous example: {Mrs. J2ones, she, she, Mrs. Jones }, 
{my brother, my brother }, {my brothers wife } 
Reference Resolution	 3/??</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Pronouns
Stronger constrains on using pronouns than on noun 
phrase references. 
 Require a high degree of activation from a referent 
 Have short activation span 
a. John went to Bobs party , and parked next to a Acura Integra. 
b. He went inside and talked to Bob for more than an hour. 
a. Bob told him that he recently got engaged. 
b. ??He also said that he bought it yesterday . 
Reference Resolution 9/??</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>When something goes wrong
Why would the media use this specic word, so often with 
relation to Muslims? 
Before the term fundamentalist was branded for Muslims, 
it was, and still is, being used by certain Christian denominations. Most of them are radical Baptist, 
Lutheran and Presbyterian groups. 
Reference Resolution 7/??</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Generic Algorithm(cont.)
	Filtering 
Remove all the members of C jthat violate reference constraints 
	Scoring/Ranking 
Order the candidates based on preferences and soft constraints 
	Searching/Clustering 
Clustering of instances with the same antecedent 
Reference Resolution	 19/??</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Syntactic Constraints on Coreference 
 Number Agreement 
* John has a new Acura. They are red.
John has three New Acuras. It is red.
 Person and Case Agreement
* John and Mary have Acuras. We love them. 
You and I have Acuras. We love them. 
Reference Resolution 12/??</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Preferences in Pronoun Interpretation 
Verb Semantics: emphasis on one of verbs arguments 
	implicit causality of a verb causes change in
salience of verb arguments
John telephoned Bill. He lost the pamphlet on Acuras.
John criticized Bill. He lost the pamphlet on Acuras.
	thematic roles (Goal, Source) cause change in 
salience of verb arguments 
John seized the Acura pamphlet from Bill. He loves reading 
about cars. 
John passed the Acura pamphlet to Bill. He loves reading about 
cars. 
Reference Resolution	 17/??</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Reference Resolution 
Regina Barzilay 
February 18, 2004</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Adding Linguistic Knowledge
Rich Linguistic representation for learning (Ng&amp;Cardie 
2002) 
53 features  
manual feature selection  
	signicant gain in performance over (Soon et al., 2001) 
Reference Resolution	 32/??</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Results
MUC-6 (30 documents): Recall 48.8*%, Precision
57.4%, F-measure 52.8%
Baseline: 34.6%, 69.3%, 46.1% Types of Mistakes:
	Parsing mistakes
	Coarse entity representation and mistakes in feature computation 
	Greedy nature of the algorithm 
Reference Resolution	 30/??</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Introduction (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>25</slideno>
          <text>Does it work?
A day after a suicide bomber killed 10 people in a terror attack on a Jerusalem 
bus, Israeli forces conducted operations Friday in the West Bank and Gaza, 
killing three Palestinians the Israeli army had identied as terrorists. The 
leader of Hamas said Friday that his group is making every effort to seize 
Israeli soldiers as bargaining chips for the release of Palestinians in Israeli 
jails. At least 45 people were wounded in the terror attack, which Israeli of
cials said proved the need for what Israel calls a security fence intended to block terrorists from entering the country. 
Computational Models of Discourse: Overview 25/25</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Example: Information Retrieval 
Show me the p roof of Cantor theorem 
Computational Models of Discourse: Overview 6/25</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Natural Language Processing
Goal: Computers using natural language as input or 
output 
language languag e 
understanding computer 
generation 
NLU example: convert an utterance into a sequence of computer instructions 
NLG example: produce a summary of a patients record 
Computational Models of Discourse: Overview 2/25</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Today
Overview of the course:
	Discourse processing: motivation and background 
Discourse theories  
	Applications 
Administration: 
	Requirements 
Computational Models of Discourse: Overview	 1/25</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Example: Text Generation
(Duboue&amp;McKeown, 2003): verbalization of semantic 
data from movie databases 
Actor, born Thomas Connery on August 25, 1930, in Ed
inburgh, Scotland. He has a brother, Neil, born in 1938. 
Connery dropped out of school to join the British Navy. 
Connery is best known for his portrayal of the British spy, 
James Bond, in the 1960s. 
Computational Models of Discourse: Overview 7/25</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Applications 
Summarization 
 Anaphora resolution 
 Essay grading 
 Segmentation 
 Information ordering 
 Dailog processing 
... 
Computational Models of Discourse: Overview 22/25</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Rhetorical Structure Theory
Assumption: Clauses in well-formed text are related via 
predened rhetorical relations 
Evidence: a claim information intended to  
increase the readers belief in the claim 
... 
Computational Models of Discourse: Overview 17/25</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Challenges
Sentences cannot be processed in isolation 
Coreference 
 Ordering
 Segmentation 
We need to model text and dialog structure 
Computational Models of Discourse: Overview 8/25</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Models of Discourse Structure
	Investigation of lexical connectivity patterns as the 
reection of discourse structure 
	Specication of a small set of rhetorical relation among discourse segments 
	Adaption of the notion of grammar
 Examination of intentions and relations among
them as the foundation of discourse structure
Computational Models of Discourse: Overview	 13/25</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Cohesion 
1. A: Im going camping next week. Do you have a two person tent I could borrow? 
2. B: Sure. I have a two-person backpacking tent. 
3. A: The last trip I was on there was a huge storm. 
4. A: It poured for two hours. 
5. A: I had a tent, but I got soaked anyway. 
6. B: What kind of tent was it? 
7. A: A tube tent. 
8. B: Tube tents dont stand up well in a real storm. 
9. A: True. 
10.B: Where are you going on this trip? 
11.A: Up in the Minarets. 
12.B: Do you need any other equipment? 
13.A: No. 
14.B: Okay. Ill bring the tent tomorrow. 
Computational Models of Discourse: Overview 16/25</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Example: User Interfaces
SHRDLU (Winograd, 1972): language interface for 
block manipulation 
Person: PICK UP A BIG RED BLOCK.
Computer: OK. (does it)
Person: PUT IT NEAR THE PYRAMID.
Computational Models of Discourse: Overview 4/25</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>What is Discourse 
Example by Charles Fillmore: 
Please use the toilets, not the pool. 
The pool for members only. 
Computational Models of Discourse: Overview 9/25</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Inference in Discourse Processing
	There are several possible ways to interpret an 
utterance in context 
	We need to nd the most likely interpretation
	Discourse model provides a computational 
framework for this search 
Computational Models of Discourse: Overview	 11/25</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Cohesion
Assumption: Well-formed text exhibits strong lexical 
connectivity via use of: 
	Repetitions
	Synonyms
Coreference
 
Computational Models of Discourse: Overview	 15/25</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Example: Question-Answering(1) 
Q: Who is the president of First Union Corp? 
First Union Corp is continuing to wrestle with sever prob
lems. According to industry insiders at Pine Webber, their 
president, John R. Georgius, is planning to retire soon. 
Computational Models of Discourse: Overview 5/25</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Rhetorical Structure Theory 
3. The last trip I was on there was a huge storm. 
4. It poured for two hours. 
5. I had a tent, but I got soaked anyway. 
result 
3 4 5 evidence 
Computational Models of Discourse: Overview 18/25</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Types of Discourse 
 Monologue (narrative, lecture) 
 Human-human dialog 
 Human-machine dialogue 
Computational Models of Discourse: Overview 23/25</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Discourse Phenomena
 a word, phrase, and utterance whose interpretation 
is shaped by the discourse or dialogue context 
John arrived at an oasis. He saw the camels around the
water hole ...
John arrived at an oasis. He left the camels around the
water hole ...
 a sequence of utterances whose interpretation is 
more than sum of its component parts 
Computational Models of Discourse: Overview 10/25</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Example 
1. A: Im going camping next week. Do you have a two person tent I could borrow? 
2. B: Sure. I have a two-person backpacking tent. 
3. A: The last trip I was on there was a huge storm. 
4. A: It poured for two hours. 
5. A: I had a tent, but I got soaked anyway. 
6. B: What kind of tent was it? 
7. A: A tube tent. 
8. B: Tube tents dont stand up well in a real storm. 
9. A: True. 
10.B: Where are you going on this trip? 
11.A: Up in the Minarets. 
12.B: Do you need any other equipment? 
13.A: No. 
14.B: Okay. Ill bring the tent tomorrow. 
Computational Models of Discourse: Overview 14/25</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Does it work? 
	Summarization: F-scores 70% (DUC 2003)
	Anaphora resolution: F-scores 60-70%
(Ng&amp;Cardie:2002)
	RST parsing: 47% (compare with th accuracy of syntactic parsers  high 80th!) 
Discourse processing is hard! 
Computational Models of Discourse: Overview	 24/25</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Why NLP?
Lots of information is in natural language format 
Documents 
News broadcasts  
User Utterances  
Lots of users want to communicate in natural language.
DO what I mean!  
Now we are betting the company on these natural
interface technologies Bill Gates, 1997 
Computational Models of Discourse: Overview 3/25</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Text Grammars
Assumption: existence of text grammar in limited 
domains (analogous to sentence grammar) 
 Tale grammar (31 terminals) V .Propp(1920s) 
 Scientic articles: introduction, conclusions, ... . 
Computational Models of Discourse: Overview 19/25</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Discourse Exhibits Structure!
	Discourse can be partition into segments, which can 
be connected in a limited number of ways 
	Speakers use linguistic devices to make this 
structure explicit 
cue phrases, intonation, gesture 
	Listeners comprehend discourse by recognizing this 
structure 
	Kintsch, 1974: experiments with recall 
	Haviland&amp;Clark, 1974: reading time for given/new information 
Computational Models of Discourse: Overview	 12/25</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Computational Models of Discourse:
Overview
Regina Barzilay 
February 4, 2004</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Intention-based Approaches 
Dialogue as collaborative activity: 
 Intention of A: to get a tent 
 To achieve this goal, A: 
 Requests a tent from B 
 Convinces B in the importance of this request 
Computational Models of Discourse: Overview 20/25</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Computational Approaches
	Rule-based approaches: manually encode all the 
required domain and common knowledge 
	Machine-learning approaches: learn all the required knowledge from a corpus 
	Supervised classication 
	Hidden-Markov Models 
	Clustering 
	Reinforcement learning 
Computational Models of Discourse: Overview	 21/25</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Evaluation for Discourse Processing (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides/>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Discourse Structure in Text Summarization; Alignment (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>Design Choices in Alignment 
Determined by a Corpus Type 
 Matching predicate 
 Search strategy 
Summarization 20/39</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Alignment 
Champollion 1822 
Find pairs 
of corresponding elements 
Summarization 14/39  
 
 
 
To see this image, go to 
http://images.google.com/images?q= rosetta_stone.jpg&amp;imgsz=large</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Supervised Approaches
	Alignment (trivial for extraction, hard for 
generation) 
Feature Selection  
	Classication (standard classiers  Naive Bayes, SVM, maximum entropy , Boostexter) 
Summarization	 7/39</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Feature Selection
Deep Features 
Rhetorical structure based  
	RST (Marcu, 2000) 
	Domain-dependent argumentative structure 
(Teufel&amp;Moens, 2000) 
 Content-based (Barzilay&amp;Lee, 2003) 
Around 10% improvement 
Summarization	 13/39</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Supervised Approaches 
++ 
++ 
+ 
 
 
 
Summarization 5/39</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Feature Selection 
Shallow Features: 
	Locational Features (in the newspaper genre, the 
rst paragraph is a summary) 
	Presence of cue words (e.g., in conclusion) 
	Sentence length 
	Number of highly weighted words in a sentence 
Summarization	 8/39</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Word Frequency vs Resolving Power
(from van Rijsbergen, 1979) The most frequent words 
are not the most descriptive 
freq Upper cutoff Lower Cutoff 
discriminative power of significant words 
significant words 
rank 
Summarization 10/39</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Corpus Type
	Language Proximity (Monolingual vs Bilingual, 
technical vs lay) 
	Content Proximity (comparable vs parallel) 
	Matching Granularity (1:1 vs 1:5) 
Summarization	 21/39</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Alignment Input
Amsterdam is the largest city in The Netherlands and the countrys economic center. It is the ofcial capital 
of The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams 
historic attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old 
houses. Amsterdam is also famous for its atmosphere of freedom and tolerance. 
City and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the capital and the principal commercial and nancial centre of The Netherlands. To the scores of tourists who 
visit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the 
distinctive colour and avour of its old sections, which have been so well preserved. However, visitors to the city also see a crowded metropolis beset by environmental pollution, trafc congestion, and housing 
shortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a 
bygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and 
the atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing urban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of government, which is The Hague. The royal family , for example, is only occasionally in residence at the 
Royal Palace, on the square known as the Dam, in Amsterdam. 
Summarization 15/39</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Methods for Overall Alignment 
 Dynamic programming 
 Methods based on Computational Geometry 
 Signal processing Methods 
Summarization 23/39</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Zipf Distribution
The product of the frequency of words (f) and their 
rank(r) is approximately constant: f R=C(where C 
is around N/10) 
Freq 
Rank 100 200 300 400 500 600 700 
12345678 
Rank = order of words frequency of occurrence 
Summarization 9/39</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>What is Summarizer 
	Find important information in a text
	Learn transformation rules based on training 
instances 
	Extract certain facts from a text, and combine them into a text 
Summarization	 2/39</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Length-based Alignment
	Matching Predicate: Long sentences will be 
translated as long sentences, short sentences 
translated as short sentences 
	Method: Dynamic programming 
Summarization	 18/39</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Alignment in MT
	Alignment task: Given bitext, identify units which 
are translations of each other. 
	Units: paragraphs, sentences, phrases, words.
	Usage: rst step for full translation(Brown et al), 
lexicography(Dagan &amp; Church, Fung &amp; McKeown), 
aid for human transaltors(Shemtov), multi-lingual 
IR. 
Summarization	 17/39</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Macro-Alignment
For unseens pair of texts applied a trained classier to 
generate possible mappings 
Text 1 Text 2 
Par. 2 
Cluster B 
Par. 2 Par. 7 
Cluster E Cluster 3 
Par. 13 
Cluster G 
Summarization 35/39</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Domain-Dependent Structure-Based
Alignment
(Barzilay&amp;Elhadad, 2003) Assumption: Weak similarity 
function augmented with structural information 
Content Structure Induction  
 Learning of Structural Mapping Rules 
 Macro Alignment 
 Micro Alignment 
Summarization 31/39</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Summarization Evaluation
	Precision/Recall or their weighted version are used
	As a baseline, people use a lead summary 
	Human agreement is computed using Kappa 
	When evaluation results matter, it is done manually 
(DUC competition) 
	Provides large collection of human-generated 
summaries 
	Outputs are evaluated manually 
Summarization	 38/39</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Key Questions 
Content selection  
	Content organization and linguistic realization 
Evaluation 
Summarization	 4/39</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Signal Processing Methods 
(Fung, 1995) 
Summarization 26/39</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Content Structure Induction 
Automatically induced topic labeling via clustering
Lisbon has a mild and equable climate, with a mean annual temperature of 63 degree F (17 degree C). The 
proximity of the Atlantic and the frequency of sea fogs keep the atmosphere humid, and summers can be 
somewhat oppressive, although the city has been esteemed as a winter health resort since the 18th century . 
Average annual rainfall is 26.6 inches (666 millimetres). 
Jakarta is a tropical, humid city , with annual temperatures ranging between the extremes of 75 and 93 degree F (24 and 34 degree C) and a relative humidity between 75 and 85 percent. The average mean temperatures 
are 79 degree F (26 degree C) in January and 82 degree F (28 degree C) in October. The annual rainfall is 
more than 67 inches (1,700 mm). Temperatures are often modied by sea winds. Jakarta, like any other large 
city , also has its share of air and noise pollution. 
Summarization 32/39</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Supervised Approaches 
Training Data 
Generation via 
Alignment Feature 
Selection Classification 
Summarization 6/39</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Weak Similarity Function 
Petersburg served as the capital of Russia for 200  
years.(A) For two centuries Petersburg was the capital of the  
Russian Empire.
The city is also the countrys leading port and center
  
of commerce. (B) And yet, as with so much of the city , the port facili-  
ties are old and inefcient. 
Summarization 29/39</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Assigning Weights
	Raw frequencies (typically with the list of 
stop-words) 
	TF*IDF  a way to deal with the problem of the Zipf distribution 
	TF - Term frequency 
	IDF - Inverse term frequency 
Summarization	 11/39</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Patterns of Mapping 
Summarization 30/39</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Types of Summarization 
	Input: speech/text, single-/multi-document
	Output: generic/query-oriented 
	Approach: domain dependent/independent, 
extraction/generation 
Summarization	 3/39</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Learning of Structural Mapping Rules
Classication on cluster level
Features: words, cluster type
j2 j1 Corpus2 Corpus1 
i1 i2 
Par.2 Par.1 Par.3 Par.1 
Par.2 
Par.1 Text Text 
Text Text ClusterB
ClusterE
Cluster1 
Summarization 34/39</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>TF*IDF
wik=Tfik log(N/n k)
wikT e r m kin document Di
Tfik Frequency of term kin document Di
N total number of documents in the collection C
nk total number of documents in the collection C
that contain Tk
Summarization 12/39</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Semantic-based Summarization
Assumption: In a limited domain, we know what is 
important (Radev&amp;McKeown, 1995, 
Elhadad&amp;McKeown, 2001) 
	Use an information extraction system to select 
important information 
	Use a semantics-to-text generation system to 
generate a new text 
Summarization	 39/39</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Computational Geometry Methods 
discovered TPC 
undiscovered TPC noise 
main 
diagonal next 
TPC chain 
search 
frontier 
search 
frontier search 
rectangle 
previous chain 
Summarization 25/39</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Length-based Alignment
Let D(i, j )be the lowest cost alignment between
sentences s1,...,s iand t1,...,t j.
Base: D(0,0) = 0.

D(i, j 1) + cost( 0:1 align, t j) D(i 1,j ) + cost( 1:0 alignsi,) D(i 1,j 1) + cost(1:1 alignsi,tj)D(i, j )=m i nD(i 1,j 2) + cost(1:2 alignsi,tj1,tj)D(i 2,j 1) + cost(2:1 alignsi1,si,tj)D(i 2,j 2) + cost(2:2 alignsi1,si,tj1,tj)
Summarization 19/39</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Learning of Structural Mapping Rules 
j2 j1 Corpus2 Corpus1 
i1 i2 
Par.2 Par.1 Par.3 Par.1 
Par.2 
Par.1 Text Text 
Text Text ClusterB
ClusterE
Cluster1 
Summarization 33/39</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Evaluation 
Range Struct Cos. 
Prec. Rec. Prec. Rec. 
0%40% 50% 25% 23% 15% 
40%70% 85% 73% 66% 86% 
70%100% 95% 95% 90% 95% 
Summarization 37/39</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Today 
 Summarization (content selection, evaluation) 
 Techniques: alignment, classication, rewriting 
Summarization 1/39</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Micro-Alignment 

s(i, j 1) skip penalty s(i1,j ) skip penalty s(i1,j1) +sim(i, j )s(i, j )=maxs(i1,j2) +sim(i, j )+sim(i, j 1)
s(i2,j1) +sim(i, j )+sim(i 1,j )
s(i2,j2) +sim(i, j 1) +sim(i 1,j )
Summarization 36/39</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Summarization 
Regina Barzilay 
March 8, 2003</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Computational Geometry Methods
(Melamed, 1997) Assumption: Distribution of true points of
correspondence (TPC) satises certain geometric properties
	Generate all the matching points satisfying the matching
predicate (over-generation)
	Find a subset of matching points that satises a pattern of
TPC:
	Linearity 
	Injectivity 
	Low variance of slope 
Various heuristics are used to minimize the search space 
Summarization	 24/39</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Alignment Output
Amsterdam is the largest city in The Netherlands and the countrys economic center. It is the ofcial capital 
of The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams 
historic attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old 
houses. Amsterdam is also famous for its atmosphere of freedom and tolerance. 
City and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the capital and the principal commercial and nancial centre of The Netherlands. To the scores of tourists who 
visit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the 
distinctive colour and avour of its old sections, which have been so well preserved. However, visitors to the city also see a crowded metropolis beset by environmental pollution, trafc congestion, and housing 
shortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a 
bygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and 
the atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing urban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of government, which is The Hague. The royal family , for example, is only occasionally in residence at the 
Royal Palace, on the square known as the Dam, in Amsterdam. 
Summarization 16/39</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Matching Predicate 
	Length similarity . (Gale &amp; Church, Brown et al)
	Lexical similarity: 
	Bilingual dictionary (Wu) 
	Words with the same distribution. (Kay &amp; 
Roscheisen, Fung &amp; McKeown) 
	Cognates (Simard et al, Church, Melamed) 
Summarization	 22/39</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Alignment for Summarization 
 Always monolingual 
 Seems to be trivial (use word intersection!) 
Summarization 27/39</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>It is hard! 
 Insertions, deletions, reodering 
 Weak similarity function 
Summarization 28/39</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Cohesion and Local Coherence: Lexical Chains, Centering Theory; Applications to Automated Essay Scoring (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>12</slideno>
          <text>Lexical Chains: Example 
Lexical Cohesion and Coherence 12/34</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Text cohesion 
Hobbs Example(1982) 
The concept of cohesion refers to relations of meaning that exist within the 
text, and that denes it as a text. Cohesion occurs where the interpretation of 
some element in the discourse dependent on that of another. 
Lexical Cohesion and Coherence 6/34</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Coherence in Automatically Generated Text
DUC results: most of automatic summaries exhibit  
lack of coherence 
	Is it possible to automatically compute text 
coherence? 
	text representation 
	inference procedure 
Lexical Cohesion and Coherence	 3/34</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Lexical Chains: Accuracy
Example: Entertainment-service 1 auto-maker 1 enterprise 1 
massachusetts-institute 1 technology-microsoft 1 microsoft 10 concern 
1 company 6 
	The accuracy bounded by the quality of a lexical
resource
	The need in disambiguation makes the task harder 
Disambiguation accuracy around 60% 
For more examples see: 
http://www.cs.columbia.edu/nlp/summarization-test/index.html 
Lexical Cohesion and Coherence	 13/34</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Lack of Coherence 
Hobbs Example(1982) 
When Teddy Kennedy paid a courtesy call on Ronald Reagan recently, he 
made only one Cabinet suggestion. Western surveillance satellites conrmed 
huge Soviet troop concentrations virtually encircling Poland. 
Lexical Cohesion and Coherence 2/34</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Analysis
	The same content, different realization 
Variation in coherence arises from choice of  
syntactic expressions and syntactic forms 
Lexical Cohesion and Coherence	 23/34</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Coherent Discourse 
Coherence is established via center continuation
John went to his favorite music 
store to buy a piano. 
He had frequented the store for 
many years. 
He was excited that he could 
nally buy a piano. 
He arrived just as the store was 
closing for the day. John went to his favorite music 
store to buy a piano. 
It was a store John had fre
quented for many years. 
He was excited that he could nally buy a piano. 
It was closing just as John ar
rived. 
Lexical Cohesion and Coherence 32/34</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Application to Essay Grading
(Miltsakaki&amp;Kukich00) 
Framework: GMAT e-rater 
	Implementation: manual annotation of coreference 
information 
	Grading: based on ratio of shifts 
	Data: GMAT essays 
Lexical Cohesion and Coherence	 33/34</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Study results
	Correlation between shifts and low grades 
(established using t-test) 
	Improvement of score prediction in 57% 
Lexical Cohesion and Coherence	 34/34</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Understanding the Results
	No signicant difference between LSA and the 
baseline model in this experiment 
	Other experiments showed that LSA may perform better, but note need in parameter estimation 
	Neither model is used for prediction 
Lexical Cohesion and Coherence	 20/34</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Automatic Measurement of Text Coherence
	Cohesive ties reect the degree of text coherence 
	First attempts to (semi-) automate cohesion 
judgments rely on: 
	propositional modeling of text structure (Kintsch&amp;van Dijk78) 
time consuming and requires training 
	readability measures (Flesch48) 
weak correlation with comprehension measures 
Lexical Cohesion and Coherence	 14/34</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Example
John went to his favorite music store to buy a piano.
It was a store John had frequented for many years.
He was excited that he could nally buy a piano.
It was closing just as John arrived.
Lexical Cohesion and Coherence 27/34</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Center Typology 
	Types: 
 Forward-looking Centers C f(U, DS) 
 Backward-looking Centers C b(U, DS) 
	Connection: C b(U n) connects with one of C f
(U n1) 
Lexical Cohesion and Coherence	 26/34</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Text Representation
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Lexical Cohesion and Coherence 4/34</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Center Continuation
Continuation of the center from one utterance not only 
to the next, but also to subsequent utterances 
	Cb(U n+1)=C b(U n) 
	Cb(U n+1) is the most highly ranked element of
Cf(U n+1) (thus, likely to be C b(U n+2)
Lexical Cohesion and Coherence	 29/34</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Lexical Chains: Applications 
Summarization 
 Segmentation 
 Malapropism Detection
Information Retrieval
 
Lexical Cohesion and Coherence 10/34</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Term similarity 
Latent Semantic Analysis (Deerwester90)
	Goal: identication of semantically similar words 
birth, born, baby 
 Assumption: the context surrounding a given word
provides important information about its meaning
	Method: Singular Vector Decomposition 
Lexical Cohesion and Coherence	 16/34</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Lexical Cohesion and Coherence 
Regina Barzilay 
February 17, 2004</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Example 
Halliday&amp;Hasan(1982) 
Time ies.
- You cant; they y too quickly.
Find three cohesive ties! 
Lexical Cohesion and Coherence 8/34</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Lexical Chains: Computation 
Associanist text models 
	Dene word similarity function 
	Dene insertion conict strategy (greedy vs. 
dynamic strategy) 
Lexical Cohesion and Coherence	 11/34</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Experimental Set-Up 
Data from (Britton&amp; Gulgoz88) 
Source: text on the airwar in Vietnam from an Air  
Force training textbook 
	Various revision methods to improve text
readability:
	Principled (based on propositional model) 
	Heuristic (based on readers intuition) 
	Readability (based on readability index) 
Lexical Cohesion and Coherence	 17/34</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Constraints on Distribution of Centers 
	Cfis determined only by U; 
	Cfare partially ordered in terms of salience 
	The most highly ranked element of C f(U n1)i s
realized as C b(U n)
	Syntax plays role in ambiguity resolution: subj &gt;
ind obj &gt;obj &gt;o t h e r s
	Types of transitions: center continuation, center
retaining, center shifting
Lexical Cohesion and Coherence	 28/34</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Center Retaining 
Retention of the center from one utterance to the next
	Cb(U n+1)=C b(U n) 
	Cb(U n+1) is not the most highly ranked element of 
Cf(U n+1) (thus, unlikely to be C b(U n+2) 
Lexical Cohesion and Coherence	 30/34</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Centering Theory: Basics 
	Unit of analysis: centers 
	Afliation of a center: utterance (U) and discourse 
segment (DS) 
 Function of a center: to link between a given
utterance and other utterances in discourse
Lexical Cohesion and Coherence	 25/34</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Another Example
John really goofs sometimes.
Yesterday was a beautiful day and he was excited about
trying out his new sailboat.
He wanted Tony to join him on a sailing trip.
He called him at 6am.
He was sick and furious at being woken up so early.
Lexical Cohesion and Coherence 24/34</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Centering Theory 
(Grozs&amp;Joshi&amp;Weinstein95) 
	Goal: to account for differences in perceived 
discourse 
Focus: local coherence  
global vs immediate focusing in discourse (Grosz77) 
	Method: analysis of reference structure 
Lexical Cohesion and Coherence	 21/34</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Center Shifting 
Shifting the center, if it is neither retained no continued 
 Cb(U n+1) &lt;&gt; Cb(U n) 
Lexical Cohesion and Coherence 31/34</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Phenomena to be Explained
Johh went to his favorite music 
store to buy a piano. 
He had frequented the store for 
many years. 
He was excited that he could 
nally buy a piano. 
He arrived just as the store was closing for the day. John went to his favorite music store to buy a piano. 
It was a store John had frequented for many years. 
He was excited that he could 
nally buy a piano. 
It was closing just as John ar
rived. 
Lexical Cohesion and Coherence 22/34</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Leftovers from Last Time 
Input Type CSegfor ABC 
ASR 0.1723 
Closed Captions 0.1515 
Transcripts 0.1356 
Note the impact for ASR! 
Lexical Cohesion and Coherence 1/34</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Experimental Set-Up 
Data from (Britton&amp; Gulgoz88) 
	Evaluation: based on recall, efciency recall and 
scores on a multiple choice 
	Assessment: Principled and Heuristic is better than Readability and Original 
Lexical Cohesion and Coherence	 18/34</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Text Cohesion
Cohesion captures devices that link sentences into a text 
Lexical cohesion  
References  
 Ellipsis 
 Conjunctions 
Lexical Cohesion and Coherence 7/34</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Lexical Chains: Example 
1. There was once a little girl and a little boy and a dog 
2. And the sailor was their daddy 
3. And the little doggy was white 
4. And they like the little doggy 
5. And they stroke it 
6. And they fed it 
7. And they ran away 
8 . A n dt h e nd a d d y h a dt og oo nas h i p 
9. And the children misssed em 
10. And they began to cry 
Lexical Cohesion and Coherence 9/34</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Vector-Based Coherence Assessment
	Each sentence is represented as a weighted vector of
its terms SENTENCE 1: 1000110
SENTENCE 2: 1 111001
	Distance between two adjacent sentences is
measured using cosine
wy,b1wt,b2sim(b 1,b2)=t
n 2 2w	 wt	 t,b1 t=1 t,b2
	Lexical continuity is measured as average distance
between sentences in a paragraph
Lexical Cohesion and Coherence	 15/34</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Todays Topics 
 Two linguistic theories of text connectivity 
 Text Cohesion (Halliday&amp;Hasan76) 
 Centering Theory (Grosz&amp;Joshi&amp;Weinstein83) 
 Application to automatic essay scoring 
Lexical Cohesion and Coherence 5/34</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Results 
Weighted No. Inference 
LSA word props Efciency mult. 
Text coherence overlap recalled (props/min) choice 
Original 0.192 0.047 35.5 3.44 37.11 
Readability rev. 0.193 0.073 32.8 3.57 29.74 
Principled rev. 0.347 0.204 58.6 5.24 46.44 
Heuristic rev. 0.403 0.225 56.2 6.01 48.23 
Lexical Cohesion and Coherence 19/34</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Dialogue Systems (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides/>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Intentions and the Structure of Discourse (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Intention Hierarchy
Understanding Structural Relations among relations is 
key! 
Dominance: 
DSP 1dominates DSP 2if satisfying DSP 2is intended 
to provide part of the satisfaction of DSP 1
	Satisfaction-precedence: 
DSP 1satisfaction-precedes DSP 2if DSP 1must be 
satised before DSP 2
Intentions and The Structure of Discourse	 16/25</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Informational Approach
Understanding Linguistic Structure is sufcient for 
Discourse Processing 
	Lexical cohesion  patterns of sentence connectivity 
Rhetorical relations  content-based relations  
between sentences 
Intentions and The Structure of Discourse	 3/25</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Intention Hierarchy
	Dominance: I1 dominates I2, I3, I4, I5
I1(Intend C (Intend A (A nd a ight for C))) 
I3(Intend A (Intend C (Tell C A destination city))) 
	Satisfaction-precedence: I2, I3 satisfaction-precedes I5 I
3(Intend A (Intend C (Tell C A destination city))) 
I5(Intent C (Intend A (A nd a nonstop ight for C))) 
Intentions and The Structure of Discourse	 19/25</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Attentional Structure 
Abstraction of participants focus of attention
	Attentional Structure is modeled by focus spaces: 
objects and relations in focus of participants 
attention 
	Changes in Attentional Structure are modeled by a 
set of insertion and deletion rules 
Intentions and The Structure of Discourse	 9/25</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Examples of Intention Types
 Intend that some agent intend to identify an object
Intend that Ruth intend to identify my dress. 
 Intend that some agent know some property of an
object Intend that Ruth know that my dress is red
Intentions and The Structure of Discourse 15/25</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Examples of Intention Types
	Intend that some agent intend to perform some 
physical task 
Intend that Ruth intend to x the at tire. 
	Intend that some agent believe some fact 
Intend that Ruth believe that campre has started. 
	Intend that some agent believe that one fact supports another 
Intend that Ruth believe the smell of smoke provides evidence that the campre is started. 
Intentions and The Structure of Discourse	 14/25</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Transitions
DISCOURSE SEGMENTS FOCUS SPACE STACK DOMINANCE HIERARCHY 
DS1 
DS2 
DS3 Properties 
objects relatetions 
Properties 
objects relations DS1 FS1 DS3 FS3 DSP1 DOMINATES DSP3 
DSP1 DOMINATES DSP2 
Intentions and The Structure of Discourse 12/25</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Informational vs. Intentional Coherence
(Moore&amp;Pollack, 1992): informational and intentional
levels of discourse analysis cannot be separated
Youll want to book your reservation before the end of 
the day . Proposition 143 goes into effect tomorrow. 
Intentional structure: convince the caller to book  
her reservation until the end of the day 
	Information structure: explanation relation between 
two sentences 
Intentions and The Structure of Discourse	 22/25</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Attentions, Intentions, and the Structure of
Discourse
Abstract Model of Discourse Structure as a composite of 
three interacting constituents: 
	Linguistic Structure
Discourse Utterances
Intentional Structure  
Intentions organized in hierarchical discourse structure 
Attentional Structure 
Dynamically-changing model of objects, properties and 
relations that are salient at each point of discourse 
Intentions and The Structure of Discourse	 6/25</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Intention Hierarchy 
DS1 
C1 DS2 DS3 DS4 DS 5 
A1C1 A2C2 A3 C4C7 
Intentions and The Structure of Discourse 20/25</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Intentions and The Structure of
Discourse
Regina Barzilay 
March 29, 2004</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Informational Approach 
Observations: 
	Amenable for computational approaches (esp. 
corpus-based techniques) 
	Shown to be useful in some natural language processing tasks 
	Independent of how humans process discourse 
	Limited expressive and predictive power 
Intentions and The Structure of Discourse	 4/25</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Patterns of Entity Distribution
Scholars 





S 
Balance


 
 
Fate



 
 1 SSSXX 1 
Hearing
 2 2 
Spaniards


 

 3 SXX O 3 
Thousands
4 4 
Judge
 
  
OO 
5 OX X 5 
Warrant
 
 
 6 6 
Extradition

 

 
Arrest

 


O 
Surgery

X



 
October


 
London


 
Pinochet
S

SS
 
Augusto


 
Dictator


 
Intentions and The Structure of Discourse 10/25</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Coherent Discourse
	Overall plan of the speaker ties intentions together
	Interaction between the speakers plan and the 
hearers intention comprehension 
	Intention determination  complete
specication of what is intended by whom
	Intention recognition  the processing that leads a discourse participant to identify what the 
intention is 
Intentions and The Structure of Discourse	 21/25</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Intentional Structure
Discourse Purpose(DP) is an underlying purpose that is 
held by the person who initiates discourse 
Discourse Segment Purpose(DSP) species how this 
segment contributes to achieving the overall discourse 
purpose 
	Assumption: one DP per discourse
	No Taxonomy of Intentions (not the difference with 
the RST) 
Intentions and The Structure of Discourse	 13/25</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Intentional Approach
Understanding Speaker Intentions is central to 
Discourse Processing 
Attention, Intentions, and the Structure of Discourse(Grosz&amp;Sidner:1986) 
Utterances are considered as actions  
	The hearers understanding of the plan-based speaker intentions is the basis of discourse 
coherence 
Intentions and The Structure of Discourse	 5/25</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Transitions 
DISCOURSE SEGMENTS FOCUS SPACE STACK DOMINANCE HIERARCHY 
DS1 
DS2 
DS3 Properties 
objects relatetions 
Properties 
objects relations DS1 DS2 FS2 
FS1 DSP1 DOMINATES DSP2 
Intentions and The Structure of Discourse 11/25</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>C1: I need to travel in May .
A1: And, what day in May you want to travel?
C2: OK uh I need to be there from the 12th to the 15th
A2: And youre ying into what city?
C3: Seattle
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I dont think theres many options for non-stop
A4: Right. Theres three non-stops today .
C5: What are they?
A5: The rst one departs PGH at 10:00 ...
C6: OK Ill take the 5ish ight on the night before on the 11th
A6: On the 11th? OK. Departs at 5:55 pm
C7: OK
Intentions and The Structure of Discourse 17/25</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Intention Hierarchy
I1(Intend C (Intend A (A nd a ight for C)))
I2(Intend A (Intend C (Tell C A departure date)))
I3(Intend A (Intend C (Tell C A destination city)))
I4(Intend A (Intend C (Tell C A departure time)))
I5(Intent C (Intend A (A nd a nonstop ight for C)))
Intentions and The Structure of Discourse 18/25</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Linguistic Structure
	(Para)-Linguistic expressions reect discourse 
structure 
	Cue phrases (For example, In the rst place)
	Change in aspect and tense 
	Change in intonation and gesture 
	Discourse structure constraints the interpretation of expressions 
	Pronoun resolution 
Intentions and The Structure of Discourse	 8/25</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Determining Intentional Structure 
Intentions and The Structure of Discourse 25/25</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Linguistic Structure 
Constituents: 
	Discourse segments 
	Embedding relations that can hold between them 
	Interaction between linguistic structure and the 
discourse utterances 
	Linguistic expressions reect discourse structure
	Discourse structure constraints the interpretation of expressions 
Intentions and The Structure of Discourse	 7/25</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Discourse Processing: Key Questions 
What individuates a discourse?  
What makes it coherent?  
Travelers left and entered our car at every stopping of 
the train. We began to recite our lessons. Similar facts 
were observed on the 23rd of July in the same year, in 
the Pacic Ocean, by the Columbus, of the West India 
and Pacic Steam Navigation Company . 
Intentions and The Structure of Discourse 1/25</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Plan Representation 
REQUEST-INFO (A,C,F) 
Constraints: Agent(A)  Client (C)
Precondition: Know (C,I)
Effect: Know (A, I)
Body: B(C, W (A, Know (A, I)))
Intentions and The Structure of Discourse 24/25</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Plan Representation 
STRIPS BOOK-FLIGHT (A,C,F) 
Constraints: Agent(A)  Flight (F)  Client (C)
Precondition: Know (A,departure-date(F))  Know
(A,departure-time(F))  Know (A, origin-city(F)) 
Know (A, destination-city(F))  Has-Seats (F)  ...
Effect: Flight-Booked (A, C, F)
Body: Make-Reservation (A, F , C)
Intentions and The Structure of Discourse 23/25</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Two Views on Discourse 
Is the speaker in the loop? 
	Informational Approach: Understanding Linguistic 
Structure is sufcient for Discourse Processing 
	Intentional Approach: Understanding Speaker
Intentions is required for Discourse Processing
Intentions and The Structure of Discourse	 2/25</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Domain-dependent Models of Text Structure (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec08/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>Pattern Detection
Analogous to motif detection 
T1:A B C D FAABFD 
T2: F CABDD FF 
 Scanning 
 Generalizing 
 Filtering 
Domain-dependent Text Structures 20/44</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>What is Text? 
A product of structural relations (coherence)
S1: A strong earthquake shook the Aegean Sea island of 
Crete on Sunday 
S2: but caused no injuries or damage. 
S3: The quake had a preliminary magnitude of 5.2 
Domain-dependent Text Structures 2/44</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Narrative Grammars
	Propp (1928): fairy tales follow a story grammar
	Barlett (1932): formulaic text structure facilities 
readers comprehension 
	Wray (2002): texts in multiple domains exhibit 
signicant structural similarity 
Domain-dependent Text Structures	 26/44</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Evaluation
Pattern condence: 84.62%
Constraint accuracy: 89.45%
Domain-dependent Text Structures 22/44</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Argumentative Zoning
BACKGROUND 
Many of the recent advances in Question Answering have followed 
from the insight that systems can benet from by exploiting the 
redundancy ...
OTHER WORK 
Brill et al. (2001) describe using the vast amount of data available on 
the WWW to achieve impressive performance ...
WEAKNESS 
The Web, while nearly innite in content, is not a complete repository 
of useful information ...
OWN CONTRIBUTION 
In order to combat these inadequacies, we propose a strategy in which 
in information is extracted from ...
Domain-dependent Text Structures 10/44</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Supervised Content Modeling 
(Duboue&amp; McKeown, 2001) 
	Goal: Find types of semantic information 
characteristic to a domain and ordering constraints 
on their presentation 
	Approach: nd patterns in a set of transcripts
manually annotated with semantic units
Domain: Patients records  
Domain-dependent Text Structures	 17/44</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Similarity in Domain Texts
TOKYO (AP) A moderately strong earthquake with a preliminary magni
tude reading of 5.1 rattled northern Japan early Wednesday , the Central Meteorological Agency said. There were no immediate reports of casual
ties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers 
(36 miles) beneath the Pacic Ocean near the northern tip of the main island of Honshu
. . . . 
ATHENS, Greece (AP) A strong earthquake shook the Aegean Sea island of Crete on Sunday but caused no injuries or damage. The quake had 
a preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT) 
on the sea oor 70 kilometers (44 miles) south of the Cretan port of 
Chania. The Athens seismological institute said the temblors epicenter 
was located 380 k ilometers (238 miles) south of the capital. No injuries or damage were reported. 
Domain-dependent Text Structures 24/44</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Baselines for Ordering 
	Straw baseline: Bigram Language model
	State-of-the-art baseline: (Lapata:2003)
	represent a sentence using lexico-syntactic 
features 
	compute pairwise ordering preferences 
	nd optimally global order 
Domain-dependent Text Structures	 39/44</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Features 
Position  
Verb Tense and Voice  
 History 
 Lexical Features (other researchers claim that) 
Domain-dependent Text Structures 15/44</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Two Approaches to Text Structure 
 Domain-dependent models (Today) 
 Content-based models 
 Rhetorical models 
 Domain-independent models 
 Rhetorical Structure Theory (Next Class) 
Domain-dependent Text Structures 6/44</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Ordering: Learning Curve 
100
Ordering Accuracy 90
80
70
60
50
40
30
20
10
0
earthquake 
finance 
clashes 
accidents 
drugs 
0 20 40 60 80 100
Training set size
Domain-dependent Text Structures 42/44</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Semantic Sequence
age, gender, pmh, pmh, pmh, pmh, med-preop,
med-preop, med-preop, drip-preop, med-preop,
ekg-preop, echo-preop, hct-preop, procedure, . . .
Domain-dependent Text Structures 19/44</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Argumentative Zoning
Many of the recent advances in Question Answering have
followed from the insight that systems can benet from
by exploiting the redundancy in large corpora.
Brill et al. (2001) describe using the vast amount of
data available on the WWW to achieve impressive per 
formance ...
The Web, while nearly innite in content, is not a com 
plete repository of useful information ...
In order to combat these inadequacies, we propose a
strategy in which in information is extracted from ...
Domain-dependent Text Structures 9/44</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Summarization: Learning Curve 
90 Summarization Accuracy 85 
80 
75 
70 
65 
60 hmm-based 
word+loc 
lead 
10 15 20 25 30 
Summary/source training set size 
Domain-dependent Text Structures 44/44</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Summarization: Algorithm
Input: source text
Training data: parallel corpus of summaries and source
texts (aligned)
	Employ Viterbi on source texts and summaries
	Compute state likelihood to generate summary
sentences:
p(s summary s source )=summary count(s)
|	source count(s),
	Given a new text, decode it and extract sentences
corresponding to summary states
Domain-dependent Text Structures	 37/44</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Examples 
Category Realization 
Aim We have proposed a method of clustering words 
based on large corpus data 
Textual Section 2 describes three parsers which are ...
Contrast However, no method for extracting the relation
ship from supercial linguistic expressions was 
described in their paper. 
Domain-dependent Text Structures 13/44</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Approach
	Goal: Rhetorical segmentation with labeling 
Annotation Scheme:  
	Own work: aim, own, textual 
	Background 
	Other Work: contrast, basis, other 
	Implementation: Classication 
Domain-dependent Text Structures	 12/44</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Motivation 
Summarization 
Extract a representative subsequence from a set of
sentences
	Question-Answering Find an answer to a question in natural language 
	Text Ordering 
Order a set of information-bearing items into a coherent 
text 
Machine Translation  
Find the best translation taking context into account 
Domain-dependent Text Structures	 7/44</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Content Models 
(Barzilay&amp;Lee, 2004) 
	Content models represent topics and their ordering 
in text. 
Domain: newspaper articles on earthquake 
Topics: strength, location, casualties, . . . 
Order: casualties prior to rescue efforts 
 Assumption: Patterns in content organization are 
recurrent 
Domain-dependent Text Structures	 23/44</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Domain-dependent Text Structures 
Regina Barzilay 
March 1, 2003</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Annotated Transcript 
He is 58-year-old male . History is signicant for Hodgkins disease , 
age gender pmh 
t r e a t e d w i t h ...t o h i s n e c k , b a c k a n d c h e s t . Hyperspadias , BPH , 
pmh pmh 
hiatal hernia and proliferative lymph edema in his right arm .N o I V  s 
pmh pmh 
or blood pressure down in the left arm. Medications  Inderal , Lopid , 
med-preop med-preop 
Pepcid , nitroglycerine and heparin . EKG has PACs . ... 
med-preop drip-preop med-preop ekg-preop 
Domain-dependent Text Structures 18/44</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Viterbi re-estimation 
Goal: incorporate ordering information 
 Decode the training data with Viterbi decoding 
 Use the new clustering as the input to the parameter 
estimation procedure 
Domain-dependent Text Structures 33/44</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Evaluation: Data 
Domain Average 
Length Vocabulary 
Size Token/ 
type 
Earthquake 10.4 1182 13.158 
Clashes 14 1302 4.464 
Drugs 10.3 1566 4.098 
Finance 13.7 1378 12.821 
Accidents 11.5 2003 5.556 
Domain-dependent Text Structures 38/44</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Application: Information Ordering 
	Input: set of sentences 
	Applications: 
	Text summarization 
	Natural Language Generation 
	Goal: Recover most likely sequences 
get marry prior to give birth (in some domains) 
Domain-dependent Text Structures	 34/44</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Results: Summarization 
Summarizer Extraction accuracy 
Content-based 88% 
Sentence classier 76% 
(words + location) 
Leading nsentences 69% 
Domain-dependent Text Structures 43/44</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Rhetorical Structure 
elaboration 
contrast 
S S2 S 
1 3
Domain-dependent Text Structures 4/44</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Analogy with Syntax 
Domain-independent Theory of Sentence Structure
 Fixed set of word categories (nouns, verbs, ...) 
 Fixed set of relations (subject, object, ...) 
P( A is sentence this weird) 
Domain-dependent Text Structures 5/44</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Content-based Structure
	Describe the strength and the impact of an 
earthquake 
	Specify its magnitude 
	Specify its location 
... 
Domain-dependent Text Structures	 3/44</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Today: Domain-Specic Models 
Rhetorical Models:  
	Argumentative Zoning of Scientic Articles 
(Teufel, 1999) 
Content-based Models:  
	Supervised (Duboue&amp;McKeown, 2001) 
	Unsupervised (Barzilay&amp;Lee, 2004) 
Domain-dependent Text Structures	 8/44</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Results 
 Classication accuracy is above 70% 
 Zoning improves classication 
Domain-dependent Text Structures 16/44</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Kappa Statistics
(Siegal&amp;Castellan, 1998; Carletta, 1999)
Kappa controls agreement P(A)for chance agreement
P(E)
P(A) p(E )K=1 p(E )
Kappa from Argumentative Zoning: 
 Stability: 0.83 
 Reproducibility: 0.79 
Domain-dependent Text Structures 14/44</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>From Clusters to States 
 Each large cluster constitutes a state 
 Agglomerate small clusters into an insert state 
Domain-dependent Text Structures 30/44</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Estimating Emission Probabilities
State siemission probability: 
npsi(w0, ..., w n)=/producttext
j=0psi(w jwj1) |
Estimation for a normal state:  
deffci(ww/prime)+1psi(w/prime|w) =fci(w)+1|V,| 
Estimation for the insertion state:  
def 1 max i&lt;mpsi(w/primew). psm(w/prime|w) =/summationtext
uV(1 max i&lt;mpsi|
(u w ))|
Domain-dependent Text Structures 31/44</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Similarity in Domain Texts
TOKYO (AP) A moderately strong earthquake with a preliminary magni
tude reading of 5.1 rattled northern Japan early Wednesday , the Central Meteorological Agency said. There were no immediate reports of casual
ties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers 
(36 miles) beneath the Pacic Ocean near the northern tip of the main island of Honshu
. . . . 
ATHENS, Greece (Ap) A strong earthquake shook the Aegean Sea island of Crete on Sunday but caused no injuries or damage. The quake had 
a preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT) 
on the sea oor 70 kilometers (44 miles) south of the Cretan port of 
Chania. The Athens seismological institute said the temblors epicenter 
was located 380 k ilometers (238 miles) south of the capital
. . . . 
Domain-dependent Text Structures 25/44</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Initial Topic Induction
Agglomerative clustering with cosine similarity measure
(Iyer&amp;Ostendorf:1996,Florian&amp;Yarowsky:1999, Barzilay&amp;Elhadad:2003) 
The Athens seismological institute said the temblors epicenter was lo
cated 380 kilometers (238 miles) south of the capital. 
Seismologists in Pakistans Northwest Frontier Province said the temblors 
epicenter was about 250 kilometers (155 miles) north of the provincial capital Peshawar. 
The temblor was centered 60 kilometers (35 miles) northwest of the provincial capital of Kunming, about 2,200 kilometers (1,300 miles) southwest of Beijing, a bureau seismologist said. 
Domain-dependent Text Structures 29/44</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Estimating Transition Probabilities
3/6 
1/5 3/4 
ci,cj)+ 2p(s j|si)=g(
g(ci)+ 2m
g(ci,cj)is a number of adjacent sentences ( ci, cj) 
g(ci)is a number of sentences in ci
Domain-dependent Text Structures 32/44</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Application: Summarization
	Domain-dependent summarization:
(Radev&amp;McKeown:1998)
	specify types of important information
(manually)
	use information extraction to identify this
information (automatically)
	Domain-independent summarization: (Kupiec et
al:1995)
	represent a sentence using shallow features 
	use a classier 
Domain-dependent Text Structures	 36/44</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Example of Learned Pattern
intraop-problems
intraop-problems

operation 11.11%
drip 33.33%
intraop-problems 33.33%
total-meds-anesthetics 22.22%


drip
Domain-dependent Text Structures 21/44</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Computing Content Model 
Implementation: Hidden Markov Model 
 States represent topics 
 State-transitions represent ordering constraints 
Rescue Strength Location History 
efforts Casualties 
Domain-dependent Text Structures 27/44</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Information Ordering: Algorithm 
Input: set of sentences 
	Produce all permutations of the set 
Rank them based on the content model  
Domain-dependent Text Structures	 35/44</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Model Construction 
	Initial topic induction 
	Determining states, emission and transition 
probabilities 
Viterbi re-estimation  
Domain-dependent Text Structures	 28/44</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Results: Ordering 
Domain Algorithm Prediction 
Accuracy Rank 
Content 72% 2.67 0.81 
Earthquake Lapata 03 24% (N/A) 0.48 
Bigram 4% 485.16 0.27 
Content 48% 3.05 0.64 
Clashes Lapata 03 27% (N/A) 0.41 
Bigram 12% 635.15 0.25 
Content 38% 15.38 0.45 
Drugs Lapata 03 27% (N/A) 0.49 
Bigram 11% 712.03 0.24 
Content 96% 0.05 0.98 
Finance Lapata 03 17% (N/A) 0.44 
Bigram 66% 7.44 0.74 
Content 41% 10.96 0.44 
Accidents Lapata 03 10% (N/A) 0.07 
Bigram 2% 973.75 0.19 
Domain-dependent Text Structures 40/44</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Baselines for Summarization 
	Straw baseline: nleading sentences 
	State-of-the-art Kupiec-style classier: 
	Sentence representation: lexical features and 
location 
	Classier: BoosTexter 
Domain-dependent Text Structures	 41/44</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>What is Text? 
A product of cohesive ties (cohesion)
ATHENS, Greece (Ap) A strong earthquake shook the 
Aegean Sea island of Crete on Sunday but caused no in
juries or damage. The quake had a preliminary magni
tude of 5.2 and occurred at 5:28 am (0328 GMT) on the 
sea oor 70 kilometers (44 miles) south of the Cretan 
port of Chania. The Athens seismological institute said the temblors epicenter was located 380 kilometers (238 
miles) south of the capital. No injuries or damage were 
reported. 
Domain-dependent Text Structures 1/44</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Motivation
	Scientic articles exhibit (consistent across
domains) similarity in structure
	BACKGROUND 
	OWN CONTRIBUTION 
	RELATION TO OTHER WORK 
	Automatic structure analysis can benet: 
	Q&amp;A 
	summarization 
	citation analysis 
Domain-dependent Text Structures	 11/44</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Automatic Reference Resolution and Reference Generation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Symbolic Approach 
Pronominal Anaphora Resolution (Byron, 2002) 
 Mentioned Entities  referents nouns phrases 
Activated Entities  entire sentences and nominals  
 Discourse Entity attributes: 
 Input: The surface linguistic constituent 
 Type: ENGINE, PERSON, ... 
 Composition: hetero- or homogeneous 
 Specicity: individual or kind 
Reference Resolution 16/30</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Last Time
	Symbolic Multi-Strategy Anaphora Resolution 
(Lappin&amp;Leass, 1994) 
	Clustering-based Coreference Resolution (Cardie&amp;Wagstaff, 1999) 
	Supervised ML Coreference Resolution + Clustering 
(Soon et al, 2001), (Ng&amp;Cardie, 2002) 
Reference Resolution	 3/30</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Features 
F-measure: 
 Fem&amp;Masc Pronoun: 17.4% baseline, 17.25% 
 Third Person Neuter Pronoun: 14.68%, 19.26% 
 Third Person Plural: 28.30%, 28.70% 
Reference Resolution 24/30</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Algorithm
	Check Success: see if the contracted description 
picks up one entity from the context 
	Choose Property: determine which properties of the referent would rule out the largest number of 
entities 
	Extend Description: add the chosen properties to 
the description being constructed and remove 
relevant entities from the discourse. 
Reference Resolution	 29/30</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Abstract Referents
(Webber, 1988)
(A0) Each Fall, penguins migrate to Fiji.
(A1) Thats where they wait out the winter.
(A2) Thats when its cold even for them.
(A3) Thats why Im going there next month.
(A4) It happens just before the eggs hutch.
Reference Resolution 14/30</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Statistical Generation 
 (Radev,1998): classication-based 
 (Nenkova&amp;McKeown,2003): HMM-based 
Reference Resolution 30/30</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Co-training 
(Blum&amp;Mitchell, 1998) 
1. Given a small amount of training data, train two
classiers based on orthogonal set of features
2. Add to training set ninstances on which both
classiers agree
3. Retrain both classiers on the extended set 
4. Return to step 2 
Reference Resolution 9/30</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Observations 
(Ng&amp;Cardie2002) 
0,76,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y, 
Y,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-. 0,75,83,C,D,C,D,D,D,C,D,I,I,C,I,I,C,N,N,D,C,D,D,N,N,N,N,N,C,Y, Y,D,D,D,C,0,D,D,D,D,D,D,C,1,D,D,C,Y,Y,D,D,D,20,20,D,D,+. 0,74,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y, 
Y,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-. 
Reference Resolution 5/30</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Co-training for Coreference
Coreference does not support natural split of features
Algorithm for feature splitting
	Train a classier on each feature separately
	Select the best feature and assign it to the rst view,
and the second best feature assign to the second
view
	Iterate over the remaining feature, and add them to
one of the views
Separate training for each reference type (personal 
pronouns, possessives,... )
Reference Resolution	 10/30</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Features
Features induced for spoken dialogue: ante-exp-type [type
of antecedent (NP ,S ,V P ) ]
ana-np-pref [preference for NP arguments]
mdist-3mf3p [ the number of NP-markables between anaphora
and potential antecedent ]
ante-tdf [the relative importance of the expression in the
dialogues]
average-ic [ information content: neg. log of the total
frequency of the word divided by number of words ]
Reference Resolution 23/30</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Semantic Constraints 
Heavily-typed system 
	Verb Senses (selectional restrictions) 
Load them into the boxcar ( them has to be 
CARGO) 
Predicate NPs  
Thats a good route  ( that has to be a ROUTE) 
	Predicate Adjectives
Its right (it has to be a proposition)
Reference Resolution	 19/30</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Anaphora Generation 
(Reiter&amp;Dale1995) 
	Application: Lexical choice for generation 
Framework:
Context Set C=a1,a2,...,a n
Properties: pk1,pk2,...,p km
 Goal: Distinguish Referent from the Rest 
Reference Resolution	 28/30</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Abstract Referents
	Webber (1990): each discourse unit produces a 
pseudo discourse entity  proxy for its 
propositional content 
	Abstract Pronoun interpretation: requires 
presentation of fact referents 
	Walker&amp;Whittaker (1990): in problem-solving 
dialogs, people refer to aspects of the solution that 
were not explicitly mentioned 
(Byron, 2002) 
A1 Send engine to Elmira. 
A2 Thats six hours. 
Reference Resolution	 15/30</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Features (Soon et al, 2001) 
	distance in sentences between anaphora and antecedent? 
	antecedent in a pronoun? 
	weak string identity between anaphora and antecedent? 
	anaphora is a denite noun phrase? 
	anaphora is a demonstrative pronoun? 
	number agreement between anaphora and antecedent 
	semantic class agreement anaphora and antecedent 
	gender agreement between anaphora and antecedent 
	anaphora and antecedent are both proper names? 
an alias feature  
	an appositive feature 
Reference Resolution	 4/30</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Types of Speech Acts 
Tell, Request, Wh-Questions, YN-Question, Conrm 
(1) The highway is closed (Tell) 
(2) Is the highway closed? (Y/N Question) 
(3) Thats right. 
(4) Why is the highway closed? (WH-Q) 
(5) *Thats right. 
Reference Resolution 18/30</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Results 
Improvements for some types of references
	Denite noun phrases: from 19% to 28% (2000
training instances)
	No improvements for possessives, proper names and
possessive pronouns
Study of learning curves
 Personal and possessive pronoun can be trained
from very small training data (100 instances)
	Other types of references require large amounts of
training data
Reference Resolution	 11/30</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Example of Dialog
A1: ..[ he]is nine months old ...
A2: ..[ He] ilikes to dig around a little bit.
A3: ..[ His mother] imother comes in and says, why
did you let [ him] i[plays in the dirt] j.
A4: I guess [[he]is enjoying himself] k.
B5: [That] ks right.
B6: [It]js healthy ...
Reference Resolution 13/30</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Activated Entities 
Generation of Multiple Proxies 
	To load the boxcars/Loading them takes an hour 
(innitive or gerund phrase) 
	I think he that hes an alien (the entire clause) 
	I think that hes an alien (sentential) 
	If hes an alien (Subordinate clause) 
Reference Resolution	 17/30</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Evaluation
10 dialogues, 557 utterances, 180 test pronouns 
Salience-based resolution: 37%  
	Adding Semantic constraints: 43%
	Adding Abstract referents: 67%
Smart Search order: 72%
 
	Domain Independent Semantics: 51% 
Reference Resolution	 21/30</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Announcements
3/3  rst part of the projects 
Example topics 
	Segmentation 
Identication of discourse structure  
Summarization 
	Anaphora resolution 
	Cue phrase selection 
Reference Resolution	 1/30</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Reference Resolution
Captain Farragut was a good seaman, worthy of the 
frigate he commanded. His vessel and he were one. He 
was the soul of it. 
 Coreference resolution: {the frigate, his vessel, it }
 Anaphora resolution: {his vessel, it } 
Coreference is a harder task! 
Reference Resolution 2/30</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Today 
 Minimizing amounts of training data: 
 Co-training 
 Weakly-supervised learning 
 Hobbs algorithm 
 Anaphora resolution in dialogs 
Reference Resolution 8/30</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Classication Rules 
+ 786 59 IF SOON-WORDS-STR = C 
+ 73 10 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM &lt;= 1P R O 
RESOLVE = C ANIMACY = C 
+ 40 8 IF WNCLASS = C CONSTRAINTS = D PARANUM &lt;= 0 PRO-RESOLVE = C 
+ 16 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM &lt;= 1 BOTH-IN-QUOTES = I 
APPOSITIVE = C 
+ 17 0 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C PARANUM &lt;= 1 
BPRONOUN-1 = Y AGREEMENT = C CONSTRAINTS = C BOTH-PRONOUNS = C 
+ 38 24 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM &lt;= 2B O T H 
PRONOUNS = D AGREEMENT = C SUBJECT-2 = Y 
+ 36 8 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C BOTH-PROPER-NOUNS = 
C 
+ 11 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM &lt;= 3 SUBJECT-1 = Y SUBJECT
2 = Y SUBCLASS = D IN-QUOTE-2 = N BOTH-DEFINITES = I 
Reference Resolution 6/30</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Knowledge-Lean Approach 
(Strube&amp;Muller2003) 
	Switchboard: 3275 sentences, 1771 turns, 16601 
markables 
 Data annotated with disuency information
Problematic utterances were discarded

	Approach: ML combines standard features with 
dialogue specic features 
Reference Resolution	 22/30</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Observations 
 Coreference for speech processing is hard!
 New features for dialogue are required
Prosodic featires seems to be useful
 
Reference Resolution 25/30</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Example
Engine 1 goes to Avon to get the oranges.
(TELL (MOVE :theme x :dest y :reason (LOAD :theme w)))
(the x (refers-to x ENG1))
(the y (refers-to y AVON))
(the w (refers-to w ORANGES))
So itll get there at 3 p.m.
(ARRIVE :theme x :dest: y :time z)
get there requires MOVABLE-OBJECT
Reference Resolution 20/30</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Anaphora In Spoken Dialogue 
Differences between spoken and written text 
	High frequency of anaphora 
	Presence of Vague anaphora
(Eckert&amp;Strube2000) 33%
Presence of non-NP-antecedents  
(Byron&amp;Allen1998) TRAINS93: 50% 
(Eckert&amp;Strube2000) SwitchBoard: 22% 
	Presence of repairs, disuences, abandoned 
utterances and so on... 
Reference Resolution	 12/30</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Example 
U1: Lyns mother is a gardener. U2: Craige likes her. 
Reference Resolution 27/30</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Reference Resolution 
Regina Barzilay 
February 23, 2004</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Observations
	Feature selection plays an important role in
classication accuracy: MUC-6 62.6% (Soon et al.,
2001)  Ng&amp;Cardie, 2002) 69.1%
	Clustering operates over the results of hard
clustering, which may negatively inuence the nal
results
	Machine learning techniques rely on large amounts
of annotated data: 30 texts
	All the methods are developed on the same corpus
of newspaper articles
Reference Resolution	 7/30</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Hobbs Algorithm 
Task: Pronoun resolution  
 Features: Fully Syntactic 
 Accuracy: 82% 
Reference Resolution 26/30</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>MIT Dialogue Systems (PDF) (Courtesy of Stephanie Seneff. Used with permission.)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSome other Spoken Dialogue Systems
Canon TARSAN (Japanese)
Info retrieval from CD-ROM
InfoTalk (Cantonese)
Transit fare
KDD ACTIS (Japanese)
Area-codes, country-codes 
and time-difference 
NEC (Japanese)
Ticket reservation
NTT (Japanese)
Directory assistance
SpeechWorks (Chinese)
Stock quotes
Toshiba TOSBURG (Japanese)
Fast food orderingCanon TARSAN (Japanese)
Info retrieval from CD-ROM
InfoTalk (Cantonese)
Transit fare
KDD ACTIS (Japanese)
Area-codes, country-codes 
and time-difference 
NEC (Japanese)
Ticket reservation
NTT (Japanese)
Directory assistance
SpeechWorks (Chinese)
Stock quotes
Toshiba TOSBURG (Japanese)
Fast food orderingAsia U.S.
AT&amp;T How May I Help You?,...
BBN Call Routing
CMU Movieline, Travel,...
Colorado U Travel
IBM Mutual funds, Travel
Lucent Movies, Call Routing,...
MIT Jupiter, Voyager, Pegasus,..
Weather, navigation, flight info
Nuance Finance, Travel,
OGI CSLU Toolkit
SpeechWorks Finance, Travel,...
UC-Berkeley BERP
Restaurant information
U Rochester TRAINS
Scheduling trainsAT&amp;T How May I Help You?,...
BBN Call Routing
CMU Movieline, Travel,...
Colorado U Travel
IBM Mutual funds, Travel
Lucent Movies, Call Routing,...
MIT Jupiter, Voyager, Pegasus,..
Weather, navigation, flight info
Nuance Finance, Travel,
OGI CSLU Toolkit
SpeechWorks Finance, Travel,...
UC-Berkeley BERP
Restaurant information
U Rochester TRAINS
Scheduling trainsEurope
CSELT (Italian)
Train schedules
KTH WAXHOLM (Swedish)
Ferry schedule
LIMSI (French)
Flight/train schedules
Nijmegen (Dutch)
Train schedule
Philips (Dutch,Fr.,German)
Flight/Train schedules
Vocalis VOCALIST (English)
Flight schedulesCSELT (Italian)
Train schedules
KTH WAXHOLM (Swedish)
Ferry schedule
LIMSI (French)
Flight/train schedules
Nijmegen (Dutch)
Train schedule
Philips (Dutch,Fr.,German)
Flight/Train schedules
Vocalis VOCALIST (English)
Flight schedules
Large-scale deployment of some dialogue systems
 e.g., CSELT, Nuance, Philips, ScanSoft (SpeechWorks)
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Flexible Vocabulary 
Impractical to support all proper names all the time
 Several hundred thousand hotel names in the U.S.
 Issues of recognition accuracy and computational load
Solution is two-fold
 Support the ability for the user to explicitly enter names when 
appropriate
 Adapt the systems working voc abulary to dynamically reflect 
information it presents to the user
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>78</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Short Video Clip
First two turns illustrate different summarizations of 
database results for two different cities         (automatically determined)
Third turn shows multi-modal interaction               
(speech plus pen)
In last turn, user refers to restaurant by name, but the 
name was unknown to the recognizer at the beginning 
of the dialogue (flexible vocabulary)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
MITs TINA NL System
TINA was designed for speech understanding
 Grammar rules intermix syntax and semantics  
 Probabilities are trained from user utterances Parse tree converted to a semantic frame that encapsulates 
meaning
TINA enhances its coverage through robust parsing
 Full parse hypotheses are preferred
 Backs off to parsing fragments and skipping unimportant words  
 Fragments are combined into a full semantic frame
 When all else fails, system ba cks off to phrase spotting
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Word Networks: 
Efficient Representation of N-best Lists
show me flights from boston to denver and
show me flights from boston to denvershow me flights from boston to denver on
show me flight from boston to denver and
show me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver andshow me a flight from boston to denvershow me a flight from boston to denver onshow me flights from boston to denver andshow me flights from boston to denvershow me flights from boston to denver on
show me flight from boston to denver and
show me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver andshow me a flight from boston to denvershow me a flight from boston to denver onanswer
If the parser can propose probabilities for next-word 
theories in the network, then it can be used to adjust 
theory scores in second pass through the network
show
ameflights
flight boston from denver toand
on
in# #
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
ESPRIT
SPEECHSome Speech-Related Government Programs
DARPA SC ARPA SURBBN, CMU, Lincoln 
SDC, SRI, ...  
HWIM, Harpy, Hearsay
DARPA SLSATT, BBN, CMU, CRIM, 
MIT, SRI, Unisys, ...ATIS, Banking, DART, 
OM, VOYAGER, ...
ESPRIT
SUNDIAL
CNET, CSELT, 
DaimlerBenz, LogicaAir and Train TravelLE
ARISE
CSELT, IRIT, KPN, 
LIMSI, U. Nijmegen..Train Travel1970 1990 1980 2000DARPA WSJ/BN D.C.ATT, BBN, CMU, CU, IBM, 
MIT, MITRE, SpeechWorks,SRI, +Affiliates, ...
Complex Travel
ESPRIT
MASK
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>79</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Short Video Clip
Video Clip
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>74</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMonolingual Multilingual
Language transparent design: 
 it is crucial that we seek solution s that will easily port to other 
languages besides English 
Can we develop tools to automatically derive linguistic 
structure (e.g., parsing rules) from aligned corpora?
Mokusei
MuXing
Speech
UnderstandingSpeech
UnderstandingSpeech
GenerationSpeech
GenerationSpeech
GenerationSpeech
Generation
Common
Semantic
FrameCommon
Semantic
Frame
DATABASESpeech
UnderstandingSpeech
Understanding
Speech
GenerationSpeech
Generation
Speech
UnderstandingSpeech
UnderstandingCommon
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>60</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Vocabulary growth is unbounded across a wide variety of tasksLearning New Words
Many new words are important content words (i.e., 75% nouns)
NAME
NOUN
VERB
ADJECTIVE
ADVERBConversational
InterfacesConversational interfaces must be able to learn new words
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSAn Attractive Strategy
Conduct R&amp;D of human language technologies within the
context of real (and useful) application domains
 Flight schedules and status, weat her, restaurant or hotel guide,
calendar management, city navigation,  traffic reports, sports updates, 
etc.
Forces us to confront critical technical issues (e.g., error 
recovery, new word problem)
Provides a rich and continuing source of useful data
Demonstrates the usefulness of the technology
Facilitates technology transfer
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Database
Server
UserDialogue Manager
Context
TrackingHubDialogue Control Table
User Model
. . .
Source: BOS
Class: Coach
. . .Dialogue State
Source: BOS
Destination: DFW
Day: Tomorrow
Time: 9 a.m.
I HAVE AN AMERICAN FLIGHT THAT 
LEAVES AT 9:30.  WOULD THAT WORK?An Illustrative Example
CAN YOU PROVIDE A DEPARTURE TIME 
OR AIRLINE PREFERENCE?I WANT TO GO TO DALLAS 
TOMORROWSource: BOS
DEPARTING 9 A.M.
9 A.M.DepartTime: 9 a.m.
SELECTED FLIGHTBOOK ITDestination: DFW
Day: TomorrowSource: BOS
Day: TomorrowDate: Jan 21, 2000
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Understanding Words in Context 
Subtle differences in phrasing can lead to completely 
different interpretations
 Is there a six A.M. flight?
 Are there six A.A. flights?
 Is there a flight six?
 Is there a flight at sixsix could mean:
A  t i m e A count A flight number 
The possibility of recognition errors makes it hard to rely 
on features like the article a or the plurality of flights.
Yet insufficient syntactic/semantic analysis can lead to 
gross misinterpretations
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
:clause requestkeypad KeypadDate
:week | :day | :reldate ResolveRelativeDate
!:destination NeedDestination
:clause book &amp; :numfound =1 AddFlightToItinerary
:nonstops &amp; :arrivaltime SpeakArrivalTimesRepresentative Entries from Flight Domain   
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Some Numeric Evaluation Metrics 
in Flight Domain 
68.0%0.0%25.0%6.3%
66.2%16.9%4.2%12.7%MIT Data
(66 calls and 1,150 utterances)NIST Data
(55 calls and 826 utterances)
NIST MIT
94.1 89.7 Transcription Parse Coverage (%)10.6 11.0 Concept Error Rate (%)14.4 15.3 Word Error Rate (%)353.3 299.3 Task Completion Time (s.)Task Completion Rate
Itinerary + Pricing
Incomplete Itinerary No ItineraryComplete Itinerary
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSData vs.Performance (Weather Domain)
Longitudinal evaluations show improvements
Collecting real data improves performance:
 Enables increased complexity and improved robustness for acoustic 
and language models
 Better match than laboratory recording conditions
Users come in all kinds
Intro || NLU || Dialogue || Evaluation || Portability || Future051015202530354045
Apr May Jun Jul Aug Nov Apr Nov MayError Rate (%)
110100
Training Data (x1000)Word
DataWER
Data</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSData Collection
System development is chicken &amp; egg problem
Data collection has evolved considerably
 Wizard-based system-based data collection
 Laboratory deployment public deployment 
 100s of users thousands millions
Data from realusers solving realproblems accelerates 
technology development
 Significantly different from laboratory environment
 Highlights weaknesses, allows continuous evaluation But, requires systems providing realinformation!
Expanding corpora will require unsupervised training or 
adaptation to unlabelled data
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>76</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Research Issues: Multimodal Interactions
What is the unifying linguistic framework that can 
adequately describe multi-modal interactions?
What is the optimal design for system configuration?
 E.g., timing constraints less stri ngent when signals are more robust
What are the appropriate integration and delivery 
strategies?
 How are modalities affected by presence of alternative modes? 
* Graphical interface alters para meters of response decisions
*T e r s e  vs.verbose spoken responses depend on existence of 
ancillary graphics
 When to utilize which modality
 trans-modal interfaces (e.g ., read email over the phone)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Choose AlternateHypothesis Selection Process
Involves recognizer, parser, and dialogue manager
Control specified in hub programCollect N-best 
Candidate FramesParse Probabilities
Select Preferred 
Hypothesis
Pragmatic 
EvaluationWord 
Graph
Word Confidence 
Scores
Dialogue Context 
Filter
Implicit 
ConfirmationExplicit 
ConfirmationParse Status
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
..
Customer: Yeah, [um] I'm looking for the Buford Cinema.
Agent: OK, and you're wanting to know whats showing 
there or ... 
Customer: Yes, please. 
Agent: Are you looking for a particular movie?
Customer: [um] What's showing.
Agent: OK, one moment.
..
Agent: They're showing A Troll In Central Park .
Customer: No.
Agent: Frankenstein .
Customer: What time is that on?
Agent: Seven twenty and nine fifty.
Customer: OK, and the others?The Nature of Mixed Initiative Interactions
(A Human-Human Example)
disfluency
interruption, overlap
confirmation
clarification
back channel
inference
ellipsis
co-reference
complex quantifierclarification subdialogue
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS The U.S. DARPA Communicator Program 
(1998-2003)
Still focusing on flight scheduling domain
 Sites were tasked with finding their own realflight database
Emphasis on common architecture with plug-and-play 
capabilities: Galaxy Communicator
Generally much larger set of cities than in ATIS (&gt;500), 
and covering at least major airports world-wide
Sites were free to organize dialogue interaction in any 
way they chose
 Encouraged mixed-initiative dialogue development
Evaluation was conducted on per-site basis and 
depended critically on user exit polls
 Users were frequent traveler s booking their real travel 
arrangements
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>70</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSResearch Issues: Speech Understanding 
We need to do more than just understand the words
 Confidence scoring (utterance &amp; word levels) 
 Utilizing timing information Modeling non-speech events and disfluencies
 Out-of-vocabulary word detection &amp; addition
Acquisition of linguistic knowledge
 Still dont know how to rapidly deve lop effective language models that 
yield high coverage of within-domain linguistic space
Robustness to environments and speakers
 Adaptation and personalization
Other challenges:
 Detecting and utilizing non-linguis tic information such as speaker 
identity and emotional state
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
The Premise:
Everybody
wants
InformationEverybody
wants
Information
Need new
interfaces
Speech is It!
For North America
CommerceNet
Research Center (1999)
Even when
they are
on the moveEven when
they are
on the move
The interface
must be
easy to useThe interface
must be
easy to useDevicesmust besmall
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Human ComputerInitiative
Human takes 
complete control
Computer is totally 
passiveHuman takes 
complete control
Computer is totally 
passive
H:I want to visit my grandmother.Computer maintains 
tight control
Human is highly 
restrictedComputer maintains 
tight control
Human is highly 
restricted
C:Please say the departure city.Dialogue Interaction Modes
Conversational systems differ in the degree with which 
human or computer takes the initiative
Mixed Initiative
Dialogue
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Typical Discourse Phenomena 
in Conversational Systems
Deictic (verbal pointing) and anaphoric (e.g., pronominal) 
reference:
1. Show me the restaurants in Cambridge.
2. What is the phone number of the third one ?
3. How do I get there from the nearest subway stop?
Ellipsis:
1. When does flight twenty two arrive in Dallas?
2. What is the departure time ()?
Fragments:
1. What is the weather today in Denver?
2. How about Salt Lake City?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSWhat Makes Parsing Hard?
Must realize high coverage of well-formed sentences 
within domain
Should disallow ill-formed sentences, e.g.,
 the flight that arriving in the morning
 what restaurants do you know about any banks?
Avoid parse ambiguity (redundant parses)
Maintain efficiency
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Word GraphFinal Hypothesis Selection Process
Parse 
Status
Confidence 
ScoresParse 
Scores
Discourse 
ContextPragmatic 
ConsiderationParsed Candidates
Final Selection
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>61</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Acquiring New Words: Proper Names
Initial research based on acquiring unknown user names
 User is asked to speak and spell their first and last names
&lt;unknown&gt;  j o a n n e Processed jh ow ae nJoanne   j o a n n e Spoken
Obtains both pronunciation and spelling of unknown word
Integrated sound-to-letter constraint s reduce letter error rate by 44%
Used in enrollment phase of a task delegation domain (Orion)
New users can register over the telephone
System automatically incorporates information for subsequent useJoanne  j o a n n e 
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Discourse and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSpoken Language Understanding  
Spoken input differs significantly from text
 False starts
 Filled pauses
 Agrammatical constructs Recognition errors
We need to design natural language components that can 
both constrain the recognizer's search space and respond 
appropriately even when the input speech is not fully 
understood
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Cuisine: 4 American, 2 Brazilian, 2 Indian, 
Price: 7 inexpensive, 4 medium, 
Street: 4 Hancock St., 2 Adams St., Summarization
Summarize information returned from database query
 Use clustering methods to dynamically determine response
 Guides user towards appropriat e disambiguation constraints
Are there any 
restaurants in Quincy?
Intro || NLU || Dialogue || Evaluation || Portability || FutureThere are 14 restaurants.  Some of the 
options are American, Brazilian,
Indian, and Italian. None of them are 
expensive. Many of them are on 
Hancock Street, Adams Street, Billings 
Road, and Franklin Street.</text>
        </slide>
        <slide>
          <slideno>68</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample Interaction in Restaurant Domain
System knows NO restaurants by name upon start-up
Two-pass processing recognizes Royal East in first query
Level of detail in summaries dependent on data sets  
Database retrievals license more restaurant names
Bollywood Caf recognized in first pass
This approach can in principal handle an unlimited number 
of restaurant names, worldwide</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>SPOKEN LANGUAGE SYSTEMS
MIT Computer Science and Artificial Intellligence Laboratory  Spoken Computer 
Conversational Systems 
Stephanie Seneff
CSAIL, MIT
April 12, 2004</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Dialogue
ManagementDialogue
Management
Database
ServerDatabase
Server HubLanguage
GenerationLanguage
Generation
Speech
RecognitionSpeech
RecognitionContext
ResolutionContext
ResolutionSpeech
SynthesisSpeech
Synthesis
Language 
UnderstandingLanguage 
UnderstandingAudio
ServerAudio
ServerGALAXY 
Architecture HotelsHotels
Air TravelAir TravelGeneric Dialogue Modeling
Dialogue management is abstracted into set of utility 
functions that serve as component building blocks  
 Allows extensive sharing of softw are among broad class of domains
 Will enable rapid portability to new domains 
SportsSportsCanonicalizerCanonicalizerGeneric 
Dialogue 
ManagerGeneric 
Dialogue 
Manager
SummarizerSummarizerWeatherWeather
Summarize 
database 
resultsPlan system 
responses
Regularize 
common 
concepts
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>73</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
The Role of Simulated Dialogues?
We cannot rely solely on live human-computer dialogue 
to stress test our systems  
Somewhat effective strategy:
 Batch mode reprocessing of previously recorded dialogues
However, prior mixed initiative dialogues quickly 
become incoherent as systems evolve
Proposal:  simulate user half of the conversation
 Randomly generate an appropriate re sponse in reaction to each 
system turn
Simulated Input as text strings or spoken utterances
 selection from library of user utterances or 
 through speech synthesis
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample Dialogue in Restaurant Domain
Tell me about restaurants in Boston.
I have found 404 restaurants in Boston. They are located in Boston and South 
Boston.  There are 29 choices for cuisine.  I also have information about price 
range.
How about a high end restaurant in Back Bay?
I have found 10 high end restaurants in  Back Bay. Some of the options are 
Nouvelle Cuisine, American and Italian.
Italian
There are 2 high end Italian restaurants in Back Bay.  Cafe Louis on Berkeley 
Street and Via Matta in the Park Plaza.
What restaurants do you know in Somerville?
There are 35 restaurants in Somerville. There are 13 choices for cuisine. Many of 
them are on Broadway, Washington St., Elm St. and Holland St.
Holland Street
There are four restaurants in Somerville on Holland Street:  Orleans, Mr. Crepe, 
Johnny D's Uptown and Sabur.
What kind of food do they serve?
Orleans serves American food, Mr. Crepe serves bakery items, Johnny D's 
Uptown serves barbecue and Sabur serves Mediterranean food.
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Simulated Dialogue InteractionSimulating the User 
Im interested in a Chinese restaurant
What is the phone number of Royal EastStructured 
Database
Entities 
Properties 
Relationships 
Ontologies
Stylized 
Queries
Natural   
QueriesWeb 
sourceBoston Restaurant Guide
NAME  : Royal East  
PHONE: 617 222 4444CUISINE: Chinese
Restaurants SERVE CUISINERestaurants    IN         CITY
I want CUISINE ChineseTell me PHONE for NAME royal east
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Simple, directed dialogue systems are being deployed 
commercially
Application-specific, mixed-initiative spoken dialogue 
systems are emerging from universities and other research institutions
Research on multi-modal mixed-initiative dialogue 
systems is beginningCurrent Landscape
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Backing off to Suggestions
clause: locate
category: restaurant
cuisine: chinesecity: cambridge
street: mainMeaning 
Representation
DB
0 entries
12 entriescountry
state
city
neighborhood
street
restaurant
lat/lon
cuisinecountry
state
city
neighborhood
street
restaurant
lat/lon
cuisinecountry
state
city
neighborhood
street
restaurant
lat/lon
cuisineI dont know of any Chinese 
restaurants on Main Street in 
Cambridge.  However, I know of 
12 Chinese restaurants in Harvard Square and Central 
Square.Im looking for a Chinese restaurant 
on Main Street in Cambridge
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
To achieve this flexibility requires progress in many areas:
Dialogue manager needs to be domain-independent
Responses should reflect distributed properties of database retrievals 
Systems must be able to acquire and use new words and conceptsWhere is Atasca in 
CambridgeId like to 
hear some 
rock musicConversational interfaces will be much more effective if they 
can adapt to user requests and changing database contentFlexible Conversational Interaction
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Stochastic Approaches to 
Language Understanding
Semantic
ModelLexical
Model
what to say how to say itmeaning sentence
MS
Choose among all possible meanings the one that 
maximizes:
( | )( )(| )()PS MPMPM SPS=
HMM techniques have been used to determine the meaning 
of utterances (ATT, BBN, IBM. CU)
Encouraging results have been achieved, but a large body of 
annotated data is needed for training
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>75</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSpeech only Multimodal Interactions
Typing, pointing, clicking can augment/complement speech 
A picture (or a map) is worth a thousand words?
LANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGmeaningSPEECH
RECOGNITIONSPEECH
RECOGNITION
GESTURE
RECOGNITIONGESTURE
RECOGNITIONHANDWRITING
RECOGNITIONHANDWRITING
RECOGNITION
MOUTH &amp; EYES
TRACKINGMOUTH &amp; EYES
TRACKING
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTighter SR/NL Integration
Natural language analysis can provide long distance 
constraints that n-grams cannot
Examples:
 What is the flight serves dinner?
 What meals does flight two serve dinner?
Question: How can we design systems that will take 
advantage of such constraints?
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSContext Free Rules for Example
sentence  full_parse [robust_parse]
full_parse  (command question statement )
command  display object
object  [determiner]  topic [predicate] [predicate]
predicate  (source destination depart_time )
source  from  (city airport)
destination  to (city airport)
display  show me
city  (boston dallas denver )
determiner  (a the )
...
Context free : left hand side of rule is single symbol
brackets [ ]: optional
Parentheses ( ): alternates .    
Terminal words in italicsShow me flights from Boston to Denver
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSGalaxy Communicator Architecture
Language
GenerationLanguage
Generation
Speech
RecognitionSpeech
RecognitionDiscourse
ResolutionDiscourse
ResolutionText-to-Speech
ConversionText-to-Speech
ConversionDialogue
ManagementDialogue
Management
Language
UnderstandingLanguage
UnderstandingApplication
Back-endApplication
Back-endAudio
ServerAudio
ServerAudio
ServerAudio
ServerI/O
ServersI/O
ServersApplication
Back-endApplication
Back-endApplication
Back-endApplication
Back-endHub
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSComponents of a Conversational System
DISCOURSE 
CONTEXTDISCOURSE 
CONTEXTDIALOGUE
MANAGEMENTDIALOGUE
MANAGEMENTDATABASEGraphs
&amp; Tables
LANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGMeaning
RepresentationMeaning
Representation
MeaningLANGUAGE
GENERATIONLANGUAGE
GENERATIONSPEECH
SYNTHESISSPEECH
SYNTHESISSentence
SPEECH
RECOGNITIONSPEECH
RECOGNITION
Words
Intro || NLU || Dialogue || Eval uation || Portability || FutureSpeech</text>
        </slide>
        <slide>
          <slideno>80</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMITs Discourse Module Internals
DISCOURSE
MODULEInput Frame 
Displayed List
Resolve 
Deixis
Incorporate 
Fragments
Interpreted FrameResolve 
PronounsResolve 
Definite NP
Fill 
Obligatory 
RolesUpdate 
History 
Elements
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSDialogue Management Strategies
Directed dialogues can be implemented as a directed graph 
between dialogue states
 Connections between states are predefined
 User is guided through the graph by the machine Directed dialogues have been succ essfully deployed commercially
Mixed-initiative dialogues are possible when state transitions 
determined dynamically
 User has flexibility to specify constraints in any order
 System can back off to a directed  dialogue under certain circumstances
 Mixed-initiative dialogue systems are mainly research prototypes
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
By introducing NL constraints early, one can potentially 
improve performance
NL Re-Sortparsable
sentences
SRbest 
scoring
hypothesisspeechbest
partial theory
next word
extensionsAlternatives to N-Best Interface
 can also reduce the need for a statistical language model, which may 
be hard to obtain for some applications
However, NL parsing is generally very slow and memory 
intensive
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>66</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Dynamic Vocabulary Recognition
Recognizer search space represented as a finite-state 
transducer containing static and dynamic components
Dynamic word classes are pre-specified (e.g., CITY)
New vocabulary words determined by dialogue (e.g., Nice)
Graph splices determined by phonological constraints
Phantom word-class marker not used for recognition</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
ESPRIT
SPEECHSome Speech-Related Government Programs
DARPA SC ARPA SURBBN, CMU, Lincoln 
SDC, SRI, ...  
HWIM, Harpy, Hearsay
DARPA SLSATT, BBN, CMU, CRIM, 
MIT, SRI, Unisys, ...ATIS, Banking, DART, 
OM, VOYAGER, ...
ESPRIT
SUNDIAL
CNET, CSELT, 
DaimlerBenz, LogicaAir and Train TravelLE
ARISE
CSELT, IRIT, KPN, 
LIMSI, U. Nijmegen..Train Travel1970 1990 1980 2000DARPA WSJ/BN D.C.ATT, BBN, CMU, CU, IBM, 
MIT, MITRE, SpeechWorks,SRI, +Affiliates, ...
Complex Travel
ESPRIT
MASK
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTable-Driven Dialogue Control
Set of operations perform specialized tasks
Ordered set of rules specify active operations  
Dynamic set of state variables drive rule execution
A rule fires when conditions are met:
 Simple arithmetic, string, and Boolean tests on state variables
Operations typically alter state variables
Operations specify one of three possible moves :      
Continue, Stop, Start Over
Several rules apply in a single turn
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
We cannot expect any natural language system to be able 
to fully parse and understand all such sentencesWe cannot expect any natural language system to be able 
to fully parse and understand all such sentencesUsers can be very creative with 
language,  especially when frustrated
Examples from ATIS domain:
I would like to find a flight from Pittsburgh to Boston on 
Wednesday and I have to be in Boston by one so I would like a flight out of here no later than 11 a.m.
I'll repeat what I said before Im on scenario 3 I would like a 
727 flight from Washington DC to Atlanta Georgia I would like 
it during the hours of from 9 a.m. till 2 p.m. if I can get a flight within that- that time frame and if .. I would like it for Friday
Intro || NLU || Dialogue || Evaluati on || Portability || Future[laughter] [throat clear] Some database &lt;um&gt; Im inquiring 
about a first class flight originating city Atlanta destination city Boston any class fare will be alright</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
0 1 02 03 04 05 06 07 0Entire SetIn Domain (ID)Male (ID)Female (ID)Child (ID)Non-native (ID)Out of DomainExpert
% Error RateSentence
Word
Male ERs are better than females (1.5x) and children (2x)
Strong foreign accents and out-of-domain queries are hard
Experienced users are 5x better than novices
Understanding error rate is consistently lower than SERASR Error Analysis (Weather Domain)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Typical Context Resolution Tasks
Input Semantic Frame
Resolve Deixis
Incorporate FragmentsResolve Pronouns
Inherit Predicates
Fill Obligatory Roles
Interpreted FrameUpdate HistoryWhat does this one serve?
What is their phone number?
Are there any on Main Street?
What about Mass Ave?
Give me directions from MIT.Show me restaurants in Cambridge.
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>62</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Illustration of User Name Enrollment
Enrollment dialogue has been simplified for illustrative 
purposes
 User prompted for name, ce ll phone number, and time zone
If user confirms spellings of first and last names, 
vocabulary is automatically augmented to support them
(Not illustrated) System backs off to keypad entry when 
spoken information incorrectly interpreted
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>64</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSDynamic Vocabulary Understanding
Dynamically alter vocabulary based on dialogue context
Tell me about restaurants 
in Arlington.There are 11 restaurants 
in Arlington.  Some of the 
options are
Arlington Diner
Blue Plate Express
Tea Tray in the Sky
Asiana Grille
Bagels etc
Flora
.HubNLGNLG
ASRASRContextContextTTSTTSDialogDialog
NLUNLUAudioAudioDBDB
Whats the phone 
number for Flora?The telephone number 
for Flora is</text>
        </slide>
        <slide>
          <slideno>65</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Whats the phone number of Flora in Arlington ???? Whats the phone number of Flora in ArlingtonDynamically alter vocabulary within a single utterance
Whats the phone number 
for Flora in Arlington.Dynamic Vocabulary Understanding: II
Arlington Diner
Blue Plate Express
Tea Tray in the Sky
Asiana Grille
Bagels etc
Flora
.HubNLGNLG
ASRASRContextContextTTSTTSDialogDialog
NLUNLUAudioAudioDBDB
The telephone number 
for Flora is Clause: wh_question
Property: phoneTopic: restaurantName: ????
City: ArlingtonClause: wh_questionProperty: phoneTopic: restaurantName: Flora
City: Arlington</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>69</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
Introduction and historical context
Speech understanding
Context resolution and dialogue modeling
Data collection and evaluation
Rapid development of new domains 
Flexibility and personalization
Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSWhat Are Conversational Systems?
Systems that can communicate with users through a 
conversational paradigm, i.e., they can:
Understand verbal input, using
*Speech recognition
*Language understanding (in context)
Verbalize response, using
* Language generation
* Speech synthesis
 Engage in dialogue with a user during the interaction
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>67</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Audio Clip: Rapid Development and 
Flexible Vocabulary
Domain-independent dialogue manager
 Domain-specific information encoded in external tables
No real user data available for training
 Generate thousands of utteranc es through dialogue simulation
 Train recognizer and NL com ponents on simulated utterances
Responses tailored to properties of  retrieved 
database entries
 Enumerate short lists
 Provide succinct summary of long lists
Recognizer vocabulary of restaurant names 
dynamically adjusted as dialogue unfolds</text>
        </slide>
        <slide>
          <slideno>72</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Rapid Development of Flexible Systems
A Challenge:
 Given an unstructured knowledge sour ce, how long would it take to 
create a dialogue system capable of providing access to that 
information space through natural spoken interaction?
 Which aspects of system developm ent consume the most resources?
Speech Understanding:
 How to obtain language models ade quately capturing the linguistic 
space of the domain?
 How to exploit dialogue contex t to adjust the vocabulary and 
language models  
Dialogue Management/Response Planning:
 How to efficiently encode all the appropriate system responses, 
including help mechanisms and error recovery?
 How to separate out domain-specific aspects so that new domains 
can leverage code devel oped for other applications?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>63</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSEnvisioned Future Extension
USER: Can you tell me the phone number of the  
Thaiku Restaurant in Seattle?
SYSTEM: I may not know the name of the restaurant.   
Can you spell it for me?
USER: T H A I K USYSTEM: The phone number of Thaiku is 206 706 7807
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Example of MITs Mercury 
Travel Planning System
New user calling into Mercury flight planning system
Illustrated technical issues:
Back-off to directed dialogue when necessary (e.g., password)
Understanding mid-stream corrections (e.g., no Wednesday)
Soliciting necessary information from user
Confirming understood concepts to user
Summarizing multiple database results
Allowing negotiation with user
Articulating pertinent information
Understanding fragments in  context (e.g., 4:45)
Quantifying user satisfaction (e.g., questionnaire)
Intro || NLU || Dialogue || Eval uation || Portability || Future
Understanding relative dates (e .g., the following Tuesday)</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
System RefinementLimited
NL
Capabilities
Data
Collection
(Wizard)
Performance
EvaluationExpanded
NL
CapabilitiesSpeech
Recognition
Data
Collection
(Wizard-less)System Development Cycle
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSThe U.S. DARPA SLS Program (1990-1995)
The Community adopted a common task (Air Travel 
Information Service, or ATIS) to spur technology 
development
Users could verbally query a static database for air travel 
information
 11 cities in North America (ATIS-2)
 Expanded to 46 cities in 1993 (ATIS-3)
All systems could handle continuous speech from unknown 
speakers (~2,000 word vocabulary)
Research driven by five annual common evaluations  
 CAS  evaluation methodology  heavy  dependence on strict manually 
provided correct answers
 restricted system design to passive mode of  interaction shifted focus away from user interface
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMultiple Roles of Dialogue Modeling
For each turn, prepare the systems side of the 
conversation, including responses and clarifications
Resolve ambiguities
 Ambiguous database retrieval (e.g. London, England or London, 
Kentucky)
 Pragmatic considerations (e.g ., too many flights to speak)
Inform and guide user  
 Suggest subsequent sub-goals (e.g., what time?)
 Offer dialogue-context depen dent assistance upon request
 Provide plausible alternatives when requested information 
unavailable
 Initiate clarification sub-dialogues for confirmation
Influence other system components
 Adjust language model due to dialogue context
 Update discourse context
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Natural Language Processing Components
Understanding:
 Parse input query into a meaning repr esentation, to be interpreted for 
appropriate action by application domain
 Select best candidate from proposed recognizer hypotheses
Discourse Context Resolution
 Interpret each query in context of preceding dialogue
Dialogue Management
 Plan course of action under both ex pected and unexpected conditions; 
compose response frames.
Generation
 Paraphrase user queries into same or different language.
 Compose well-formed sentences to  speak the (sequence of) response 
frames prepared by the dialogue manager.
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Input Processing: Understanding
LANGUAGE
UNDERSTANDINGSemantic
RepresentationSPEECH
RECOGNITIONSpeech
Waveform
Sentence
Hypotheses
Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: "Boston"
Predicate: TO
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: " Boston "
Predicate: TO
Topic: CITY
Name: " Denver "FLIGHT
FLIGHTSDENVER SHOW TO BOSTON FROM ME
ONAND
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>77</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Conclusions
Spoken dialogue systems are needed, due to
 Miniaturization of computers
 Increased connectivity Human desire to communicate
To be truly useful, these interfaces must behave naturally
 Embody linguistic competence, both input and output
 Help people solve real problems efficiently
Conversational interfaces must be able to learn from user 
interaction and database content
To achieve this flexibility requires progress in key areas:
 Response planning needs to be flexible and content-driven 
 New concepts must be acquire d naturally during interaction
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Incorporating Soft NL Constraints into 
Recognizer
Class n-gram can be automatically derived from NL Grammar
to        idaho_falls &lt;cty&gt; on           may &lt;mth&gt; twenty_third &lt;dy&gt;SENTENCE
CLARIFIER
DESTINATION DATE
TO CITY_NAME MONTH ON DAY
CARDINAL_DATE
to           idaho falls               on             may       twenty thirdCITY_NAME MONTH
CARDINAL_DATE
Developer identifies parse categories for class n-gram
System tags words with associated class labels
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Constraint
Coverage100%100%speechMultiple Roles for Natural Language 
Parsing in Spoken Language Context 
Understanding100%
textspoken
language
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample of Natural Language Understanding
show me flights from boston to denverflight destination sourcetopicdisplay object
predicatefull_parse
commandsentence
predicate
city city to from flight_listdestination source flightdisplaySome parse nodes 
carry semantic tags for
creating semantic frameClause: DISPLAY
Topic: FLIGHT
Predicate: SOURCE
Topic: CITY
Name: "Boston"
Predicate: DESTINATION
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: SOURCE
Topic: CITY
Name: " Boston "
Predicate: DESTINATION
Topic: CITY
Name: " Denver "
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
U: I need a flight from Boston to San Francisco
C: Did you say Boston or Austin?U: Boston, MassachusettsC: What date will you be traveling?
U: Tomorrow
C: Hold on while I retrieve the flights for you
C: I have found 10 flights meeting your sp ecification.             
When would you like to leave?
U: In the morning.
C: Do you have a preferred airline?
U: United
C: I found two non-stop United fl ights leaving in the morning Help the user narrow
down the choicesClarification
(insufficient info)Clarification
(recognition errors)
Post-Retrieval: Multiple DB Retrievals =&gt; Unique ResponseStages of Dialogue Interaction
Pre-Retrieval: Ambiguous Input =&gt; Unique Query to DB
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTypical Steps in Transforming User Query
Parsing 
 Establishes syntactic organization and 
semantic content 
Generate Semantic Frame
 Produces meaning representation 
identifying relevant constituents and 
their relationships
Incorporation of discourse context
 Deals with fragments, pronominal 
references, etc.
Transformation to database query
 Produces SQL formatted string for 
database retrievalGenerate 
Frame
Incorporate 
Context
Produce     
DB QueryProduce 
Parse TreeRecognizer Hypotheses
Parse Tree
Semantic Frame
Frame in Context
SQL
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>71</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSResearch Issues: Dialogue Modeling
Modeling human-human conversations
 Are human-human dialogues a good model for systems?
 If so, how do we structure our systems to enable the same kinds of 
interaction found in human-human conversations?
Dialogue strategies
 When to use explicit vs.implicit vs.no confirmation?
 When to back off to alternatives such as typing or keypad entry?
 How to model help mechanisms to in form users of system capabilities?
 How to recognize and recover from errors?
 How to enable the above capabilities across diverse domains
Producing and responding to back-channel
 Would likely have striking e ffect if properly implemented
How can system learn user preferences through repeated 
interactions?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
NL Re-SortN Complete
"sentence"
hypothesesparsable
sentences
SRbest 
scoring
hypothesisspeech
show me flights from boston to denver and
show me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onshow me flights from boston to denver andshow me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onAnswerSR/NL Integration via N-Best Interface
N-Best resorting has also been used as a mechanism for 
applying computationall y expensive constraints
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Plan Recognition (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>'
&amp;$
%Example
withoutplanrecognition withplanrecognition
1USER: "Let's workonemail." USER: "Let's work onemail."
2AGENT: "Youhave5unread messages." AGENT: "You have5unread messages."
3USER: "Let's readamessage."
4USER: &lt;Select message from Chuck&gt; USER: &lt;Select message from Chuck&gt;
5USER: "Let's readmessage initsownwindow."
6USER: &lt;Open message&gt; USER: &lt;Open message&gt;
7USER: "Let's dosomething using message."
8USER: &lt;Visit http://foo.bar.com&gt; USER: &lt;Visit http://foo.bar.com&gt;
9 ...long lunch... ...long lunch...
10USER: "What wasIdoing?" USER: "What wasIdoing?"
11AGENT: "Working onemail. AGENT: "Working onemail.
12 Reading message from Chuck. Reading message fromChuck.
13 Reading message initsownwindow. Reading message initsown...
14 Donesomething using message. Donesomething using message.
15 &lt;Visit http://foo.bar.com&gt; &lt;Visit http://foo.bar.com&gt;
16 Expecting toreact tomessage." Expecting toreact tomessage."
17USER: "Let's react tomessage."
18USER: "Let's sendemail."
19USER: &lt;Start newmessage&gt; USER: &lt;Start newmessage&gt;
20USER: "Let's fillinmessage."
21USER: &lt;Address message toDan&gt; USER: &lt;Address message toDan&gt;
22 AGENT: "Why yousending email toDan?
23 (a)reacting tomsgfromChuck
24 (b)asanewgoal"
25 USER: "I'm reacting tomsgfromChuck."
26 ...additional editing actions... ...additional editing actions...
27USER: "Done filling inmessage." USER: "Done filling inmessage."
28AGENT: "Let's send, cancel orsavemsg." AGENT: "Let's send, cancel orsave msg."
29USER: "Please savemessage." USER: "Please save message."
30AGENT: &lt;Save message&gt; AGENT: &lt;Save message&gt;
Key: "utterance" &lt;primitive action&gt;
DialogueSystems 16/16</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Learning Dialogue Strategies via Reinforcement Learning (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides/>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Automatic Interpretation of Dialogue Acts (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Automatic Interpretation of
Dialogue Acts
Regina Barzilay 
April 4, 2004</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Last Time
DAMSL (Dialogue Act Markup in Several Layers) 
(Allen&amp;Core1997; Walker et al1996; Carletta1997) 
	Driven by the needs of dialog system developers 
	Contains several levels, including forward looking
function and backward Looking function
Hierarchical in structure  
	Domain-independent, but tailored towards task-oriented 
dialogs 
Automatic Interpretation of Dialogue Acts	 1/26</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>INFORM Denition
Grice: A speaker informs the hearer of something
merely by causing the hearer to believe that the speaker
wants them to know something
INFORM (S,H,P) 
Constraints: Speaker (S)  Hearer (H)  Preposi
tion (P)
Precondition: Know (S,P)  W(S, INFORM (S, H,
P))
Effect: Know (H, P)
Body: B (H, W (S, Know (H, P)))
Automatic Interpretation of Dialogue Acts 17/26</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Example of Inference 
Can you give me a list of ights from Atlanta?
1. S.REQUESTS
(S,H,InformIf(H,S,CanDo(H,Give(H,S,LIST))))
2.	PI.AE: 
B(H,W(S,InformIf(H,S,CanDo(H,Give(H,S,LIST))))) 
3.	PI.AE/EI: B(H,W(S,KnowIF(H,S,CanDo(H,Give(H,S,LIST))))) 
4.	PI.KP/EI: B(H,W(S,CanDo(H,Give(H,S,LIST)))) 
5.	PI.KP/EI: B(H,W(S, Give(H,S,LIST))) 
6.	PI.BA: REQUEST(H,S,GIVE(H,S,LIST)) 
Automatic Interpretation of Dialogue Acts	 22/26</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Example
YES-NO-QUESTION: Will breakfast be served on 
USAir 1557? 
STATEMENT: I dont care about lunch 
COMMAND: Show me ights from Milwaukee to 
Orlando on Thursday night 
Automatic Interpretation of Dialogue Acts 4/26</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>BDI Models
Belief, Desire and Intention Models (Cohen &amp; Perrault, 
1979; Perrault &amp; Allen, 1980; Allen, 1995) 
	Used for generation and understanding 
Based on formalization of Searles Inference Chains  
via planning techniques 
Automatic Interpretation of Dialogue Acts	 13/26</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Plan Inference Rules (PI)
	(PI. AE) Action-Effect Rule: For all agents S and H,
if Y is an effect of action X and if H believes that S
wants X to be done, then it is plausible that H
believes that S wants Y to obtain
	(PI. PA) Precondition-Action Rule: For all agents S
and H, if X is a precondition of actions Y and if H
believes S wants X to obtain, then it is plausible that
H believes that S wants Y to be done
	(PI. BA) Body-Action Rule: For all agents S and H,
if X is part of the body of Y and if H believes that S
wants X done, then it is plausible that H believes
that S wants Y done
Automatic Interpretation of Dialogue Acts	 20/26</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Plan Inference Rules (PI)
	(PI. KP) Know-Desire Rule: For all agents S and H,
if H believes S wants to KNOWIF(P), then H
believes S wants P to be true:
plausibleB(H, W (S, KNOW IF (S, P ))) =
B(H, W (S, P ))
	(EI. 1) Extended Inference Rule: B(H, W (S))can
be added to any plan inference rules: 
plausibleB(H, W (S, Y ))is a PI If	B (H, W (S, X )) =
rule, then 
plausibleB(H, W (S, B (H, (W (S, X ))))) = 
B(H, W (S, B (H, (W (S, Y )))))
Automatic Interpretation of Dialogue Acts	 21/26</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>BDI Axiomatization 
Action Schemas (similar to STRIPS) 
	Constraints: Variable types 
	Preconditions: Conditions that must already be 
true to successfully perform the action 
	Effects: A set of partially ordered goal states that 
must be achieved in performing the actions 
	Body: A set of partially ordered goal states that 
must be achieved in performing the action 
Automatic Interpretation of Dialogue Acts	 15/26</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Plan-Inferential Interpretation
	The speaker can mention the hearers doing the 
action 
Would you please repeat this information?
Will you tell me the departure time on American ight?
	The speaker can question the speakers having 
permission to receive results of the action 
May I get lunch instead of breakfast?
Could I have a listing of ights leaving Boston?
Automatic Interpretation of Dialogue Acts	 9/26</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Interpretation is Easy! 
YES-NO-QUESTIONS have aux-inversion
STATEMENTS have declarative syntax (no aux-
inversion)
COMMANDS have imperative syntax (commands
with no syntactic subject)
Automatic Interpretation of Dialogue Acts 5/26</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>BDI Basics 
Belief: 
B(S, P )
B(A, P ) B(A, Q ) B(A, P  Q)
Know 
KNOW (S, P ) P B(S, P )
Know Whether  
KNOWIF (S, P )
KNOW (S, P ) KNOW (S,P)
Want  
W(S, P ), W (S, S (ACT (H))
Automatic Interpretation of Dialogue Acts 14/26</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Pros/Cons of Inference Approaches 
Automatic Interpretation of Dialogue Acts 23/26</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Interpretation is NOT Easy!
The mapping between surface form and speech act is 
not obvious and not one-to-one 
QUESTION vs REQUEST: Can you give me a list of
ights from Atlanta to Boston?
QUESTION vs STATEMENT:... And you said you
want to travel next week?
Automatic Interpretation of Dialogue Acts 6/26</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Today 
Automatic Interpretation of Dialogue Acts 
 Plan-Inferential Interpretation 
 Cue-based Interpretation 
Automatic Interpretation of Dialogue Acts 3/26</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Cue-based Interpretation 
Microgrammar of dialogue (Goodwin, 1996) 
1. Words and Collocations (please for REQUEST)
2. Prosody (rising pitch for YES-NO-QUESTIONS
3. Conversational Structure (yeah after PROPOSAL 
likely to be an agreement; after AGREEMENT likely 
to be a backchannel) 
Automatic Interpretation of Dialogue Acts 24/26</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Dialogue MicroGrammar
(Sumh&amp;Weibel, 1994; Mast et al, 1996; Stolcke et al, 
2001) 
d =argmax dP(d|W)=argmax dP(d)P (W d)|
Example: Ngrams with high predictive power for reformulation: 
so you, you mean, so they, so its 
Automatic Interpretation of Dialogue Acts 25/26</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Incorporating Dialogue Structure 
E- observable evidence consisting of prosodic features
(F) and lexical features (W) 
D={d1,d2,...,d N} 
P(D)P(E|D)D =argmax DP(d|E)=argmax D =P(E)
argmax DP(D)P(E|D)
P(E|D)=P (F D )P(W D )= | |
argmax DP(D)P(F|D)P(W D )|
Automatic Interpretation of Dialogue Acts 26/26</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>DAMSL 
 STATEMENT: a claim made by a speaker 
 INFO-REQUEST: a question by the speaker 
 CHECK: question for conrming information 
 ACCEPT: a claim made by a speaker 
 ANSWER: answering a question 
 UNDERSTANDING: whether speaker understands 
Automatic Interpretation of Dialogue Acts 2/26</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Plan-Inferential Interpretation
(Gordon&amp;Lakoff, 1971; Searle, 1975): A speaker could
mention or question various properties of the desired 
activity to make an indirect request
	The speaker can mention the hearers ability to
perform the action
Can you give me the list of the ights from Atlanta to Boston? 
Would you be able to put me on the ight with Delta? 
	The speaker can mention speakers wish or desire
about the activity
I want to y from Boston to San Francisco
I would like to stop somewhere else in between
Automatic Interpretation of Dialogue Acts	 8/26</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>REQUEST Denition 
REQUEST (S,H,ACT) 
Constraints: Speaker (S)  Hearer (H)  ACT (A)
Preconditions: W(S, ACT(H))
Effect: W (H, ACT (H))
Body: B (H, W (S, ACT (H)))
Automatic Interpretation of Dialogue Acts 18/26</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Searles Inference Chain
5. A preparatory condition for a directive is that the 
hearer have the ability to perform the directed 
action 
6. Therefore X has asked me a question about my 
preparedness for the action of giving X a list of ights 
7. Furthermore, X and I are in a conversational 
situation in which giving ights is expected and 
common activity 
Automatic Interpretation of Dialogue Acts 11/26</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Surface Acts
	Surface-level acts correspond to the literal
meaning of the imperative, interrogative and
declarative structures:
S.REQUEST (S,H,ACT)
	B (H, W(S, ACT(H)))
	They trigger the start of the hearers inference chain
(by matching the body of a request)
Speaker: S. REQUEST (S, H, InformIf (H, S,
CanDo(H,Give (H, S, LIST)))))
Hearer: REQUEST (H, S, GIVE,(H, S, LIST))
Automatic Interpretation of Dialogue Acts	 19/26</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Example of Action Schema 
BOOK-FLIGHT (A,C,F) 
Constraints: Agent(A)  Flight (F)  Client (C)
Precondition: Know (A,departure-date(F))  Know
(A,departure-time(F))  Know (A, origin-city(F)) 
Know (A, destination-city(F))  Has-Seats (F)  ...
Effect: Flight-Booked (A, C, F)
Body: Make-Reservation (A, F , C)
Automatic Interpretation of Dialogue Acts 16/26</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>8. Therefore, in the absence of any other plausible act,
X is probably requesting me to give him a list of 
ights 
Automatic Interpretation of Dialogue Acts 12/26</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Two Solutions
	Shallow Processing: act classication based on 
lexical, prosodic and structural cues 
	Deep Understanding: inference-based interpretation 
Automatic Interpretation of Dialogue Acts	 7/26</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Searles Inference Chain
1. X asked me a question about whether I have the
ability to give a list of ights
2. I assume that X is being cooperative in the
conversation and that his utterance therefore has
some plan
3. X knows that I have the ability to give such a list ,
and therefore is no alternative reason why X should
have purely theoretical interest in my list-giving
ability
4. Therefore Xs utterance has some illocutionary
point. What can it be?
Automatic Interpretation of Dialogue Acts 10/26</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Discourse in Psycholinguistics (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides/>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Topic Segmentation: Agreement, Evaluation, Automatic Text Segmentation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>24</slideno>
          <text>More Results 
	High sensitivity to change in parameter values
	Thesaural information does not help 
Most of the mistakes are close misses  
Topic Segmentation	 24/34</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Simple Algorithm 
Passanneau&amp;Litman93 
Recall Precision Error 
Cue 72% 15% 50% 
Pause 92% 18% 49% 
Humans 74% 55% 11% 
Topic Segmentation 8/34</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Boundary Detection
Based on changes in sequence of similarity scores: 
Depth Scores: relative depth (in comparison to the 
closest maximum) 
Number of segments: s /2
Topic Segmentation 20/34</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Flow model of discourse 
Chafe76: 
Our data ... suggest that as a speaker moves from 
focus to focus (or from thought to thought) there 
are certain points at which they may be a more or 
less radical change in space, time, character conguration, event structure, or even world ... At 
points where all these change in a maximal way, 
an episode boundary is strongly present. 
Topic Segmentation 11/34</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Examples of Derived Rules 
Condition Decision Conf. 
LC0.67, CUE1, 
OVR1.20, SIL3.42 yes 94.1 
LC0.35, SIL &gt;3.42, 
OVR4.55 yes 92.2 
CUE1, ACT&gt;0.1768, 
OVR1.20, LC0.67 yes 91.6 
... 
default no 
Topic Segmentation 33/34</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>What is Segmentation?
Segmentation: determining the positions at which topics 
change in a stream of text or speech. 
SEGMENT 1: OKAY
tsk Theres a farmer,
he looks like ay uh Chicano American,
he is picking pears.
A-nd u-m hes just picking them,
he comes off the ladder,
a-nd he- u-h puts his pears into the basket.
SEGMENT 2: U-h a number of people are going by,
and one of them is um I dont know,
I cant remember the rst ...t h e  r s t p e r s o n t h a t g o e s b y
Topic Segmentation 1/34</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Example
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Topic Segmentation 13/34</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Skorochodkos Text Types
Chaine d 
Ringed 
Monolit h 
Piecewi se 
Topic Segmentation 10/34</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Meeting Segmentation 
Motivation: Facilitate information Access  
 Challenges: 
 High error rate in transcription 
 Multi-thread structure 
Topic Segmentation 25/34</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>0 Speaker Change 
10
 20
 30
Topic Segmentation 31/34</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Agreement on Segmentation 
Topic Segmentation 22/34</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Evaluation Results 
Methods Precision Recall 
Baseline 33% 0.44 0.37 
Baseline 41% 0.43 0.42 
Chains 0.64 0.58 
Blocks 0.66 0.61 
Judges 0.81 0.71 
Topic Segmentation 23/34</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Silences
	Pauses  speaker silence in the middle of her 
speech 
 Gap  silences not attributable to any party
Topic boundaries are typically preceeded by gaps
Topic Segmentation	 29/34</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Segmentation: Agreement
Percent agreement  ratio between observed 
agreements and possible agreements 
A BC
  
  
+ 
++ 
 
+++ 
   
 
22
= 91%8 3
Topic Segmentation 4/34</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Cue Word Selection 
Automatic computation of cue words: 
	Compute word probability to appear in boundary 
position 
	Select words with the highest probability
Remove non-cues.
 
Topic Segmentation	 27/34</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Similarity Computation: Output 
0.22 
0.33 
Topic Segmentation 18/34</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Preprocessing and Initial Segmentation 
Tokenization  
 Morphological analysis 
 Token-sequence division 
Topic Segmentation 15/34</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Text Segmentation 
Hearst94 
	Goal: divide text into coherent segments 
	Main Idea: change in lexical connectivity patterns 
signals topic change 
	Linguistic Theory: Text Cohesion 
Topic Segmentation	 9/34</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Motivation 
Information Retrieval  
Summarization 
	Question-Answering 
	Word-sense disambiguation and anaphora 
resolution 
Topic Segmentation	 2/34</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Similarity Computation: Representation 
Vector-Space Representation 
SENTENCE 1: I like apples 
SENTENCE 2: Apples are good for you 
Vocabulary Apples Are For Good I Like you
Sentence 1 1 0 0 0 1 1 0
Sentence 2 1 1 1 1 0 0 1
Topic Segmentation 16/34</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Algorithm for Feature Segmentation
Supervised ML 
(Galley&amp;McKeown&amp;Fosler-Lussier&amp;Jing03) 
 Combines multiple knowledge source:
 cue phrases 
 silences
 overlaps
 speaker change 
 lexical cohesion 
 Uses probabilistic classier (decision tree) to 
combine them 
Topic Segmentation 26/34</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Results 
Method Pk WD 
Feature-based 23.00 25.47 
Cohesion-based 31.91 35.88 
Topic Segmentation 34/34</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Selected Cue Words 
OKAY 93.05 
shall 0.44 
anyway 0.43 
alright 0.64 
lets 0.66 
good 0.81 
Topic Segmentation 28/34</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Segmentation Algorithm 
 Preprocessing and Initial segmentation 
 Similarity Computation 
 Boundary Detection 
Topic Segmentation 14/34</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Todays Topics
	Human Agreement on Segmentation and Evaluation
	Segmentation Algorithms: 
	Features: word distribution, cue words, speaker, 
change,...
	Methods: classication, clustering, HMMs, ...
	Segmentation for different genres: text, meetings, broadcasts, 
Topic Segmentation	 3/34</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Cochrans Test
Estimate the null hypothesis that the number of subjects 
assigning a boundary at any position is randomly 
distributed 
Topic Segmentation 6/34</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Example 
Stargazers Text(from Hearst, 1994) 
 Intro - the search for life in space 
 The moons chemical composition 
 How early proximity of the moon shaped it 
 How the moon helped the life evolve on earth 
 Improbability of the earth-moon system 
Topic Segmentation 12/34</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Gap Plot 
1
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
20 40 60 80 100 120 140 160 180 200 220 240 26 0
Topic Segmentation 19/34</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Overlaps
 Average overlap rate within some window 
Little overlap in the beginning of segments 
Topic Segmentation 30/34</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Results on Agreement 
Grosz&amp;Hirschbergberg92 newspaper text 74-95% 
Hearst93 expository text 80% 
Passanneau&amp;Litman93 monologues 82-92% 
Topic Segmentation 5/34</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Determination of Window Size 
Feature Tag Size(sec) Side 
Cue phrases CUE 5 both 
Silence (gaps) SIL 30 left 
Overlap OVR 30 right 
Speaker activity ACT 5 both 
Lexical cohesion LC 30 both 
Topic Segmentation 32/34</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Similarity Computation: Cosine Measure
Cosine of angle between two vectors in n-dimensional
space
tsim (b1,b2)=wy,b1wt,b2
n 2 2wt t,b1 t=1wt,b2
SENTENCE 1: 1000110
SENTENCE 2: 1111001
sim(S 1,S2)=
10+01+01+01+10+10+01
 
(12+02+02+02+12+12+02)(12+12+12+12+02+02+12=0.26
Topic Segmentation 17/34</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Evaluation Measures 
Boundary Non-boundary 
Alg. Boundary a b 
Alg. Non-boundary c d 
aRecall a+c
aPrecision
a+b
b+cError a+b +c+d
Topic Segmentation 7/34</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Segmentation Evaluation
Comparison with human-annotated 
segments(Hearst94): 
	13 articles (1800 and 2500 words)
	7 judges 
	boundary if three judges agree on the same segmentation point 
Topic Segmentation	 21/34</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Topic Segmentation 
Regina Barzilay 
February 8, 2004</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Studies of Dialogues; Taxonomy of Speech Acts (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>Transition-taking Rules 
Mechanisms for the next speaker selection:
	adjacency pairs (Schelgoff, 1968):
GREETING GREETING
 
COMPLIMENT DOWNPLAYER
 
REQUEST  GRANT
utterance boundaries
 
Studies of Dialogs	 7/22</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Dialogs 
What is a connection between the picture and our topic? 
Studies of Dialogs 1/22  
 
 
 
To see this image, go to 
http://images.google.com/images?q=false_maria.jpg</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Dialogue Acts
	Austin (1962): Utterance in a dialogue is a kind of 
action performed by the speaker 
	Types of acts:
	locutionary act: the utterance of a sentence 
with a particular meaning 
 illocutionary act: the act of asking, promising,
etc., in uttering a sentence (you cant do that!)
	perlocutionary act: the (often intentional) 
production of certain effects upon the feelings, 
thoughts, or actions of the addressee in uttering 
a sentence 
Studies of Dialogs	 17/22</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Dialogue Act Markup
DAMSL (Dialogue Act Markup in Several Layers) 
(Allen&amp;Core1997; Walker et al1996; Carletta1997) 
	Driven by the needs of dialog system developers 
	Contains several levels, including forward looking
function and backward Looking function
Hierarchical in structure  
	Domain-independent, but tailored towards task-oriented 
dialogs 
Studies of Dialogs	 20/22</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Today 
	Properties of human dialogs
	Formalization of these properties required for 
system building 
	Taxonomies of Speech Acts 
Studies of Dialogs	 2/22</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Turn Taking
People can quickly gure out who should talk next, and 
when they should speak 
	Little overlap (Levinson1983 smaller than 5%)
	The amount of time between two turns is low (less than few hundred milliseconds) 
Studies of Dialogs	 5/22</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Conversational Implicature
	The interpretation of an utterance relies on more 
than just the literal meaning of the sentence 
	The speaker seems to expect the hearer to draw certain inferences 
Studies of Dialogs	 14/22</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Continuers
Devices the hearer uses to indicate that she believes she
understands what the speaker means (Clark&amp;Schaefer,
1989)
	Continued attention: B shows she is continuing to
attend and therefore remains satised with As
presentation
Relevant next contribution : B starts in on the next  
relevant contribution 
 Acknowledgment: B nods or says a continuer like
uh-huh, yeah,o ra n assessment like thats great
Studies of Dialogs	 11/22</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Utterance Segmentation 
 Cue words (well, and, so) 
 N-grams and POS sequences 
 Prosody (pitch, accent, pause duration) 
Studies of Dialogs 9/22</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>What Makes Dialogue Different? 
 Turn taking 
 Grounding 
 Conversational Implicature 
Studies of Dialogs 3/22</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Transition-taking Rules 
Conversation Analysis (1974) 
	If during this turn the current speaker has selected A 
as the next speaker then A must speak next 
	If the current speaker does not select the next speaker, any other speaker may take the next turn 
	If no one else takes a turn, the current speaker may take the next turn 
Studies of Dialogs	 6/22</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Grice Maxims 
Maxim of Relevance: Be relevant  
 Maxim of Quality: Be perspicuous 
 Avoid obscurity of expression 
 Avoid ambiguity 
 Be brief (avoid unnecessary prolixity) 
 Be orderly 
Studies of Dialogs 16/22</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Searles Taxonomy
	Expressives: expressing the psychological state of 
the speaker about a state of affairs ( thinking, 
apologizing, welcoming, deploring ) 
	Declarations: bringing about a different state of the 
world via the utterance (I resign, Youre red) 
Studies of Dialogs	 19/22</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Backward-looking Functions 
 ACCEPT: a claim made by a speaker 
 ANSWER: answering a question 
 UNDERSTANDING: whether speaker understands 
Studies of Dialogs 22/22</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Transition-taking Rules 
Silence Duration 
A: Is there something bothering you or not? 
(1.0) 
A: Yes or no? 
(1.5) 
A: Eh? 
B: No. 
Studies of Dialogs 8/22</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Searles Taxonomy
Searle (1975): Rened taxonomy of (illocutionary)
speech acts
	Assertive: committing the speaker to somethings
being the case ( suggesting, swearing, boasting,
concluding)
	Directives: attempts by the speaker to get the
addressee to so something (asking, ordering,
requesting, inviting, begging)
	Commissives: committing the speaker to some
future course of actions ( promising, planning,
vowing, betting, opposing)
Studies of Dialogs	 18/22</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Dialogue Example
C1: I need to travel in May.
A1: And, what day in May you want to travel?
C2: OK uh I need to be there from the 12th to the 15th
A2: And youre ying into what city?
C3: Seattle
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I dont think theres many options for non-stop
A4: Right. Theres three non-stops today.
C5: What are they?
Studies of Dialogs 4/22</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Studies of Dialogs 
Regina Barzilay 
March 31, 2004</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Conversational Implicature
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting thats from the 12th
to the 15th
A4: ...Theres three non-stops today. 
Studies of Dialogs 13/22</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Grounding
The speaker and the hearer must establish common 
ground (Stalnaker, 1978) 
A: ...returning on U.S. ight one one eight 
B: Mm hmm 
Mm is a back-channel 
Studies of Dialogs 10/22</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Continuers
	Demonstration: B demonstrates all or part of what 
she has understood A to mean, for example by 
paraphrasing or reformulating As utterance, or by 
collaboratively completing As utterance 
A: OK Ill take the 5ish ight on the night before on the 11th 
B: On the 11th? 
	Display: B displays verbatim all or part of As 
presentation 
Grounding is expressed in different modalities 
Note the impact of modality (face2face vs phone 
conversation) 
Studies of Dialogs	 12/22</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Grice Maxims
	Maxim of Quantity: Be exactly as informative as 
required 
	Make your contribution as informative as 
required (for the current purposes of exchange) 
	Do not make your contribution more informative 
that is required 
	Maxim of Quality: Try to make your contribution 
one that is true: 
	Do not say what you believe to be false 
	Do not say that for which you lack adequate 
evidence 
Studies of Dialogs	 15/22</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Forward-looking Functions 
 STATEMENT: a claim made by a speaker 
 INFO-REQUEST: a question by the speaker 
 CHECK: question for conrming information 
 OPENING: greeting 
Studies of Dialogs 21/22</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Rhetorical Structure Theory (PDF)
Guest Speaker: Florian Wolf</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Example
[No matter how much one wants to stay a non-smoker, A
], [the truth is that the pressure to smoke in junior high is 
Bgreater than it will be any other time of ones life. ]. [We 
know that 3,000 teens start smoking each day , C][although 
it is a fact that 90% of them once thought that smoking was 
Dsomething that theyll never do. ]
Domain-independent Models of Text Structure 6/16</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Accuracy 
	Compared against manually constructed trees
	Tested against human-constructed trees 
	Automatically constructed trees exhibit high 
similarity with human-constructed trees 
	However, see (Marcu&amp;Echihabi, 2002) CONTRAST vs ELABORATION: only 61 from 238 have a 
discourse marker (26%) 
Domain-independent Models of Text Structure	 14/16</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Binary Relations 
 (JUSTIFICATION, A, B) 
 (JUSTIFICATION, D, B) 
 (EVIDENCE, C, B) 
 (CONCESSION, C, D) 
 (RESTATEMENT , D, A) 
Domain-independent Models of Text Structure 7/16</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Automatic Computation
(Marcu, 1997; Marcu&amp;Echihabi, 2002) 
Surface cues for discourse relations: 
I like vegetables, but I hate tomatoes. 
Domain-independent Models of Text Structure 12/16</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Domain-independent Models of Text
Structure
Regina Barzilay 
March 3, 2003</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Domain-Dependent Rhetorical Model 
Domain: Scientic Articles 
	Human exhibit high agreement on the annotation 
scheme 
	The scheme covers only a small fraction of discourse relations 
Domain-independent Models of Text Structure	 2/16</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Domain-Dependent Content Models 
 Capture topics and their distribution 
 Based on pattern matching techniques 
 Motifs of semantic units 
 Distributional model 
 Useful in generation and summarization 
Domain-independent Models of Text Structure 1/16</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Is it Realistic 
Domain-independent Models of Text Structure 3/16</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Other Words Also Count!
(Marcu&amp;Echihabi, 2002)
Surface cues for discourse relations:
I like vegetables, but I hate tomatoes. 
Domain-independent Models of Text Structure 15/16</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Rhetorical Structure Theory
(Mann&amp;Thompson:1988, Matthessen&amp;Thompson:1988)
	Developed in the framework of natural language 
generation 
	Aims to describe building blocks of text structure
	Nucleus vs Satellites 
	Binary Relations between Discourse Units 
	Compositionality principle dene how to build a
tree from binary relations
Domain-independent Models of Text Structure	 5/16</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Domain-Independent Rhetorical Model
Model elements:  
	Binary Relations 
	Compositionality Principle 
	Requirements: 
	Stability and Reproducibility of an Annotation 
Scheme 
	Expressive Power of a Model 
Domain-independent Models of Text Structure	 4/16</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Compositionality
Whenever two large text spans are connected through a 
rhetorical relation, that rhetorical relation holds 
between the most important parts of the constituent 
spans. 
Marcu (1997): used constraint-satisfaction approach to 
build discourse trees given a set of binary relations 
Domain-independent Models of Text Structure 9/16</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>RST tree
JUSTIFICATION 
A B C D 
JUSTIFICATION CONCESSION 
Domain-independent Models of Text Structure 8/16</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Automatic Computation of RST Relations 
(Marcu, 1997) 
	Aggregate discourse relations to a few stable
groups: (contrast, elaboration, condition,
cause-explanatuin-evidence)
	Establish deterministic correspondence between cue 
phrases and discourse relations: 
	{But, However } Contrast 
	{In addition, Moreover } Elaboration 
Domain-independent Models of Text Structure	 13/16</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Relations 
Relation Nucleus Satellite 
Background text whose understanding 
is being facilitated text whose understanding 
is being facilitated 
Elaboration basic information additional information 
Preparation text to be presented text which prepares the 
reader to expect and in
terpret the text to be pre
sented 
Domain-independent Models of Text Structure 10/16</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Ambiguity
John can open the safe.
He knows the combination.
Domain-independent Models of Text Structure 11/16  
 
To see this image, go to 
http://images.google.com/images?q=yolady.gif</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Method
	Assume that certain markers unambiguously predict
discourse relations
	Create Cartesian product of words located on two
sides of a discourse marker
	For each pair of words, compute its likelihood to
predict a discourse relations
	argmax rkP(rk|(s1,s2)) =
argmax rkP((s1,s2)|rk) P(rk), where
P((s1,s2)rk)=
i,js1,s2P((w i,w j)rk)
 |	 |
Domain-independent Models of Text Structure	 16/16</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Topic Segmentation (cont.): Hierarchical Text Segmentation, Meeting Segmentation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>22</slideno>
          <text>Examples of Derived Rules 
Condition Decision Conf. 
LC0.67, CUE1, 
OVR1.20, SIL3.42 yes 94.1 
LC0.35, SIL &gt;3.42, 
OVR4.55 yes 92.2 
CUE1, ACT&gt;0.1768, 
OVR1.20, LC0.67 yes 91.6 
... 
default no 
Topic Segmentation 22/23</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Agglomerative Clustering
4
1 2 3 4 5
 5 
	First, each data point is a singleton cluster
	Next, closest points are merged until all points are 
combined 1 3 
2
Topic Segmentation	 6/23</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Cue Word Selection 
Automatic computation of cue words: 
	Compute word probability to appear in boundary 
position 
	Select words with the highest probability
Remove non-cues.
 
Topic Segmentation	 16/23</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>More Results 
	High sensitivity to change in parameter values
	Thesaural information does not help 
Most of the mistakes are close misses  
Topic Segmentation	 4/23</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Topic Segmentation 
Regina Barzilay 
February 11, 2004</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Hierarchical Segmentation 
(Yaari, 1997) 
	Partition the text into elementary segments 
	While more than one segment left do 
	Find closest adjacent segments s i,si+1(based on 
cosine measure) 
	Merge si,si+1into one segment 
Topic Segmentation	 8/23</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Selected Cue Words 
OKAY 93.05 
shall 0.44 
anyway 0.43 
alright 0.64 
lets 0.66 
good 0.81 
Topic Segmentation 17/23</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>HMM-based Segmentation: Construction 
van Mulbregt&amp;Carp&amp;Gillick&amp;Lowe99: 
	Each state of HMM represents a topic 
	Topics are derived via story clustering 
	Emission probabilities for a state are computed 
based on a unigram language model 
Topic Segmentation	 10/23</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Evaluation Results 
Methods Precision Recall 
Baseline 33% 0.44 0.37 
Baseline 41% 0.43 0.42 
Chains 0.64 0.58 
Blocks 0.66 0.61 
Judges 0.81 0.71 
Topic Segmentation 3/23</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Example
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Topic Segmentation 1/23</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Algorithm for Feature Segmentation
Supervised ML 
(Galley&amp;McKeown&amp;Fosler-Lussier&amp;Jing03) 
 Combines multiple knowledge source:
 cue phrases 
 silences
 overlaps
 speaker change 
 lexical cohesion 
 Uses probabilistic classier (decision tree) to 
combine them 
Topic Segmentation 15/23</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Determination of Window Size 
Feature Tag Size(sec) Side 
Cue phrases CUE 5 both 
Silence (gaps) SIL 30 left 
Overlap OVR 30 right 
Speaker activity ACT 5 both 
Lexical cohesion LC 30 both 
Topic Segmentation 21/23</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>HMM-based Segmentation: Decoding 
 Transitions are controlled by switch penalty 
 Segmentation via Viterbi-style decoding 
Topic Segmentation 11/23</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Silences
	Pauses  speaker silence in the middle of her 
speech 
 Gap  silences not attributable to any party
Topic boundaries are typically preceeded by gaps
Topic Segmentation	 18/23</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>TDT Segmentation Results
	Data: 384 shows, 6,000 stories and 2.2 million 
words 
	Sources: ABC, CNN, . . . 
TDT Evaluation Measure:  
CSeg = PMi s s +( 1 ) PF alseAlarm
Topic Segmentation	 12/23</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>0 Speaker Change 
10
 20
 30
Topic Segmentation 20/23</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Segmentation Algorithm 
 Preprocessing and Initial segmentation 
 Similarity Computation 
 Boundary Detection 
Topic Segmentation 2/23</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Meeting Segmentation 
Motivation: Facilitate information Access  
 Challenges: 
 High error rate in transcription 
 Multi-thread structure 
Topic Segmentation 14/23</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Broadcast News Segmentation 
Goal: divide news stream into stories  
	Assumption: news stories typically belong to one of 
several categories (sports, politics, ... ) 
Topic Segmentation	 9/23</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>TDT Performance 
Input Type CSegfor ABC 
ASR 0.1723 
Closed Captions 0.1515 
Transcripts 0.1356 Note the impact for 
ASR! 
Topic Segmentation 13/23</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Todays Topics 
 Hierarchical segmentation 
 HMM-based segmentation 
 Supervised segmentation 
Topic Segmentation 5/23</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Overlaps
 Average overlap rate within some window 
Little overlap in the beginning of segments 
Topic Segmentation 19/23</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Results 
Method Pk WD 
Feature-based 23.00 25.47 
Cohesion-based 31.91 35.88 
Topic Segmentation 23/23</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Agglomerative Clustering
	Complete-link  merge the two clusters whose 
merger has the smallest diameter 
	Single-link  merge the two clusters whose two closest members have the smallest distance 
	Average-link  merges in each iteration the pair of clusters with the highest cohesion. 
Topic Segmentation	 7/23</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Generation of Referring Expressions (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>Entity Prole
Collection of entity descriptions (automatically
constructed)
Example: Prole of Ung Huot
a senior member, Cambodias, Cambodian foreign minister, co-
premier, rst prime minister, foreign minister, MR., new co-
premier, new rst prime minister, newly-appointed prime minis
ter, premier 
Generation of Referring Expression 23/34</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Key Idea
	Semantic constraints imposed on lexical choice are 
reected in contextual indicators 
	This correlation can be learned automatically from a 
large collections of texts, given a feature vector and 
a referent 
Generation of Referring Expression	 22/34</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Example
Many years ago, there was an Emperor, who was so excessively 
fond of new clothes, that he spent all his money in dress. He did 
not care to go either to the theatre or the chase, except for the 
opportunities then afforded him for displaying his new clothes. He had a different suit for each hour of the day; and as of any 
other king or emperor, one is accustomed to say, he is sitting 
in council, it was always said of him, The Emperor is sitting in his wardrobe. One day, two rogues, calling themselves weavers, 
made their appearance. They gave out that they knew how to 
weave stuffs of the most beautiful colors and elaborate patterns, the clothes manufactured from which should have the wonder
ful property of remaining invisible to everyone who was extraor
dinarily simple in character. These must, indeed, be splendid clothes! thought the Emperor. 
Generation of Referring Expression 29/34</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Basics
	A noun phrase is considered to be a referring
expression iff its only communicative purpose is to
identify an object too the hearer
Context is the set of entities that the hearer is  
currently assumed to be attending to 
	Referring expression satises the referential
communicative goal if it is a distinguishing
description in the given context
Example: small black dog, large white dog, small black
cat
Generation of Referring Expression	 12/34</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Lexicalization + Syntactic Realization
 This stage determines the particular words to be
used to express domain concepts and relations.
	Constraints: discourse focus, style constraints,
syntactic environment.
 Implementation: decision tree.
Example: predicate: win(X, Y)
Verb: X defeated Y, Y was defeated X, X won in the game against X, X won the game 
Noun:: victory of X over Y, victory of X, defeat of Y 
 
Generation of Referring Expression 10/34</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Experimental Results 
	Training: 10, 353 Testing: 1,511 
	Precision: 88.87%, Recall 63.39% 
	Steep learning curve 
(500: 64.29%, 2.86%  50,000: 88.87%, 63.39%) 
	Positive impact of WordNet extension  10% 
increase on average 
Generation of Referring Expression	 26/34</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Target NP Features 
Premodiers: 
	Titles President George W . Bush 
	Name-external modiers Irish Flutist James 
Galway 
Postmodiers:  
	Apposition 
	Relative Clause 
	Prepositional Phrase Modiciation 
Generation of Referring Expression	 30/34</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Distributional Properties of Prole 
 11, 504 entities from 178 MB of newswire 
 9, 053 have a single description 
 2, 451 very from 2 to 24 descriptions 
Generation of Referring Expression 24/34</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Inappropriate Modiers
Overly specic or unexpected classiers violates 
principles of Conversation Implicature 
Look at the dog. 
Look at the pitt bull. 
Generation of Referring Expression 14/34</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Example: Summary of basketball games
scoring((Shaquille, ONeal), 37)
time(Friday, night)
team((Shaquille, ONeal), (Orlando, Magic))
win(Orlando, Magic), (Toronto, Raptors)
score(101,89)
... 
Generation of Referring Expression 4/34</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Sentence Planning
	A one-to-one mapping from predicates to messages 
is bad. 
	Sentence planning groups information into
sentences (aggregation).
Before aggregation: Shaquille ONeal scored 37
points. The game was on Friday night.
Orlando Magic defeated Toronto Raptors.
Raptors lost seven games in a row.
After aggregation: Shaquille ONeal scored 37
points Friday night powering the Orlando Magic
to a 101 89 victory over the Toronto Raptors,
losers of seven in a row.
Generation of Referring Expression	 9/34</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Realization task 
Insert function words.  
 Choose correct specication of content words.
Order words.  
FUF/SURGE input for the sentence  John likes Mary 
now. 
((cat clause)
(proc ((type mental) (tense present) (lex "like")))
(partic ((processor ((cat proper) (lex "John")))
(phenomenon ((cat proper) (lex "Mary")))))
(circum ((time ((cat adv) (lex "now"))))))
Generation of Referring Expression 11/34</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Features
	Context  a bag of words surrounding the entity
	Length of the article  an integer 
	Name of the entity  e.g., Bill Clinton 
	Prole  set of all the descriptions 
Wordnet  WordNet extension for Prole members  
Generation of Referring Expression	 25/34</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>NLG System Architecture 
Content Determination  
	Discourse Planning 
	Sentence Aggregation 
Lexicalization 
	Syntactic and morphological realization 
Generation of Referring Expression	 6/34</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Rule Examples
IF ination IN CONTEXT , THEN politician
IF detective IN PROFILE AND agency in CONTEXT ,
THEN policeman
IF celine IN CONTEXT , THEN north american
Generation of Referring Expression 27/34</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Generation of Referring Expression 
Regina Barzilay 
February 25, 2004</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Evaluation
Rewriting rules based on HMM improve the 
performance of summarization system: 
Preferences: 89% rewrite, 9% original, 2% no 
preference 
Generation of Referring Expression 34/34</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Why to Use NLG?
	Important information is stored in ways which are 
not comprehensible to the end users  databases, 
expert system, log les. 
	NLG systems can present this information to users in 
an accessible way. 
	Data presented in textual form can be searched by 
IR systems. 
Generation of Referring Expression	 3/34</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Transcript Analysis
(Reiter&amp;Dale,1992) (assembly task dialog) 
Which attribute should be used?  
gender vs color vs shape 
	Is it preferable to use modier or to use a more 
specic head noun? 
the small dog vs the chihuahua 
	Should relative or absolute adjectives be used? 
the small dog vs the one foot high dog 
Generation of Referring Expression	 16/34</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Content determination
	Input: Knowledge base.
Schemata and Inference mechanism.

 Output: predicates to be conveyed in the text. 
Example: 
Game statistics: win(Orlando, Magic), (Toronto, Raptors) 
Players records: team((Shaquille, ONeal), (Orlando, Magic)) 
Teams record: lost(7,(Toronto, Raptors)) 
Generation of Referring Expression	 7/34</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Discourse Planning
	Text have an underlying structure in which parts are 
related together. 
	Rhetorical relationships. 
	Conceptual grouping. 
Generation of Referring Expression	 8/34</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Text-to-Text Generation 
	Input: text (lack of semantic information)
	Applications: summarization, question-answering, 
machine translation 
Generation of Referring Expression	 20/34</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Fragment of HMM 
Modication No Modication 
Initial 0.76 0.24 
Modication 0.44 0.56 
No modication 0.24 0.75 
Generation of Referring Expression 33/34</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Types of Referents
	Is the target named entity the head of the phrase or 
not? 
	If it is the head what kind of pre- and post- modier 
dies it have? 
How was the name itself realized in the NP?  
Generation of Referring Expression	 31/34</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Algorithm
	Check Success: see if the contracted description 
picks up one entity from the context 
	Choose Property: determine which properties of the referent would rule out the largest number of 
entities 
	Extend Description: add the chosen properties to 
the description being constructed and remove 
relevant entities from the discourse. 
Generation of Referring Expression	 19/34</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Results of Transcript Analysis
	Preference for adjectives that communicate size, 
shape or color? 
the black dog vs the male dog 
	The use of specic head nouns depends on audience expertise 
the small dog vs the chihuahua 
	Preference for relative adjectives in speech, and for absolute adjectives in writing 
the small dog vs the one foot high dog 
Generation of Referring Expression	 17/34</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Impact of Conversational Implicature
Redundant Information causes violations of 
Conversation Implicature (Grice, 1975) 
Sit by the table.
Sit by the brown wood table.
Generation of Referring Expression 13/34</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Unsupervised Referent Selection
Statistical Model of Syntactic Realization 
(Nenkova&amp;McKeown, 2003) 
Hypothesis: There is a regularity in lexical realization of referent chain 
Generation of Referring Expression 28/34</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>What is NLG?
	Program which produces texts in natural language.
	Input: some underlying non-linguistic 
representation of information. 
	Output: documents, reports, help messages and other types of texts. 
	Knowledge sources required: knowledge of 
language and of the domain. 
Generation of Referring Expression	 2/34</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>HHM Construction
	Each state of HMM corresponds to one syntactic 
realization 
	Transitions are estimated based on corpus counts (anaphoric expressions are not resolved!) 
Generation of Referring Expression	 32/34</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Psychological Data
(Levelt, 1989) 
People do include unnecessary modiers in referring 
expressions 
a white bird, a black cup and a white cup 
 Incremental Processing Helps in Understanding 
 Redundancy Helps in Understanding 
Generation of Referring Expression 15/34</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Supervised Approach to Referent Selection
Goal: Select the best entity description in a given corpus 
(Radev, 1998) 
Elections (1996): Bill Clinton, the democratic presiden
tial candidate
False bomb alert in Little Rock, Ark (1997): Bill Clinton,
an Arkansas native
Generation of Referring Expression 21/34</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Algorithm: Representation 
(Reiter&amp;Dale,1992) 
 Input is organized in (attribute, value) pairs 
 Type is one of the attributes 
 Attributes are organized in taxonomy 
Object 1: (type, chihuahua), (size, small), (color, black). 
Object 2: (type, chihuahua), (size, large), (color, white). 
Object 3: (type, cat), (size, small), (color, black). 
 
Generation of Referring Expression 18/34</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Output
Orlando, FL  Shaquille ONeal scored 37 points Friday 
night powering the Orlando Magic to a 101 89 victory 
over the Toronto Raptors, losers of seven in a row. 
Generation of Referring Expression 5/34</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Todays Topics 
	Overview of Natural Language Generation
	Psychological Evidence regarding Generation of 
Referring Expressions 
	Selection of Referring Expressions 
	Symbolic Approaches (Dale&amp;Reiter) 
	Corpus-Based Approaches (Radev,
Nenkova&amp;McKeown)
Generation of Referring Expression	 1/34</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
