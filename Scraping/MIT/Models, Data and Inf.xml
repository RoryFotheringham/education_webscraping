<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/</course_url>
    <course_title>Models, Data and Inference for Socio-Technical Systems</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Mathematics </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Analyzing a probability problem (PDF)
Probability mass functions</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec2/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>Subway Interviews
You are standing outside of the Park Street subway 
(T) station, with clipboard, and you want to 
interview only registered Republicans.  For 
modeling purposes we will say that the 
probability that a random T-rider who passes 
you is a Republican is 0.20. (Actual is 0.13.)  You have infinite patience, as the stream of 
riders continues all day long.
(a) Let R= the number of T-riders you question 
until you find the 1st Republican.  Find the 
probability mass function of R.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Here we used the z-transform for 
the probability mass function 
(PMF), defined as</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>R=1R=2R=3 R=4R=5 R=6R=7 0.2 0.20.2 0.20.2 0.20.20.80.8 0.80.80.80.80.8 Sample Space with Probability Assignment
Geometric Probability Mass Function</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Earthquakes
 Richter Scale:  logarithmic.  Each whole 
number step in the magnitude scale 
corresponds to the release of about 31 
times more energy than the amount 
associated with the preceding whole 
number value.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>12AB
Let T12= event successful Transmission from node 1 to node 2.
Assume all links function independently. 
Then,
P{T12}=P{E or (AB) or (AC) or (DB) or (DC)}
P{T12}=P{E + E[(AB) + (AC) + (DB) + (DC)]} 
P{T12}=P{E + E[A(B+C) + D(B +C)]}  
P{T12}=P{E + E[(A+D)(B+C)]}= P{E + E[(A+AD)(B+BC)]}
P{T12}=pE + (1-pE){[pA+(1- pA) pD] [pB+(1- pB) pC]}
Generalization o f this could be a term project.CAdd two more redundant arcs to increase Reliability
DE</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Indicator random variables.
Suppose
Then E[ Xi] = 1* pi+ 0*(1- pi) = pi.
Example 1: Flip a coin N times, with P{Heads} = p.  Assume 
independent flips.
Define the random variable NH= number of Heads in Nflips.
Let the set indicator R.V. Xibe 1 if the ithcoin flip is Heads. 
Then,</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Example 2.  Baseball Hats.
Suppose that one player from each of the 30 Major League Baseball
teams attends a party at MIT, and each arrives wearing his teams baseball cap.  Each tosses his hat into a closet upon arrival.The host, at the end of the party, gives a random hat to each 
departing player.  What is the ex pected number of hats that are
returned to their rightful owners?
Solution:  Define indicator r.v.
Let H = the number of hats returned to the correct owners.  Then,
Answer independent of the number of players or teams!</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Great Expectations.
Suppose you flip a coin until you get the first
Heads, then stop.  If the 1stflip is Heads, you win $2.
If the 2nd is the 1stHeads, you win $4.
If the 3rd flip is the 1stHeads, you win $8. If the 
nthflip is the 1stHeads, you win $2n.The bank is 
Donald Trump, so this game can go on for a long 
long time!
(a) How much would you be willing to pay for 
playing this game?
(b) What is the expected dollar value of winnings in 
the game?</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Set indicator random variables.
Suppose
Then E[ Xi] = 1* pi+ 0*(1- pi) = pi.
Example 1: Flip a coin N times, with P{Heads} = p.  Assume 
independent flips.
Define the random variable NH= number of Heads in Nflips.
Let the set indicator R.V. Xibe 1 if the ithcoin flip is Heads. 
Then,
E[NH]=E[Xi
i=1N
 ]= E[Xi
i=1N
 ]= p=Np
i=1N</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Both discrete and continuous (Laplace) transforms will be 
reviewed in tutorial on Friday.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Jacob (James) Bernoulli 
(16541705)For Bernoulli Trials
Source: Wikipedia
LIBE
1994RTY
Figure by MIT OCW.
ESD.86
Class #2
February 12, 2007</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Does the St. Petersburg Paradox Occur in Nature?
A possible term project!
Size and frequency of occurrence of Earthquakes.
Small earthquakes occur every day all around the world, 
.Large earthquakes occur less frequently, the 
relationship being exponential ; namely, roughly ten times as 
many earthquakes larger t han magnitude 4 occur in a 
particular time period than earthquakes larger than 
magnitude 5. In the (low seismicity) United Kingdom , 
for example, it has been calculated that the average 
recurrences are:an earthquake of 3.7 or larger every 
Year, an earthquake of 4.7 or larger every 10 years, an 
earthquake of 5.6 or larger every 100 years.  
The USGS estimates that, since 1900, there 
have been an average of 18 major earthquakes (magnitude 7.0-7.9) 
and one great earthquake (magnitude 8.0 or greater) per year, and 
that this average has been relatively stable. [5
http://en.wikipedia.org/wiki/Earthquake#Size_and_frequen cy_of_occurrence</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Richter Magnitude
Seismic Energy Yield Example 0.5
5.6 kg (12.4 lb) Hand grenade 1.0
32 kg (70 lb) Construction site blast 1.5
178 kg (392 lb) WWII  conventional
bombs2.0
1 metric ton late WWII conventional
bombs2.5
5.6 metric tons WWII blockbuster bomb 3.0
32 metric tons Massive Ordnance Air
Blast bomb3.5
178 metric tons Chernobyl nu clear
disaster , 19864.0
1 kiloton Small atomic bomb 4.5
5.6 kilotons Average tornado  (total
energy)5.0
32 kiloton Nagasaki atomic bomb 5.5
178 kilotons Little Skull Mtn., NV
Quake, 19926.0
1 megaton Double Spring Flat, NVQuake, 19946.5
5.6 megatons Northridge quake, 1994 7.0
50 megatons Tsar Bomba, largest
thermonuclear weapon
ever tested7.5
178 megatons Landers, CA Quake, 1992 8.0
1 gigaton San Francisco, CA
Quake, 19068.5
5.6 gigatons Anchorage, AK Quake,
19649.0
32 gigatons 2004 Indian Ocean
earthquake10.0
http://www.answers .com/topic/richter-magnitude-scaleDoes this fit the idea of the St. Petersburg Paradox?</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>How many people go by you until you 
have completed your kthinterview?
Let Y=number of people who pass by up to and 
including the one who is the kthperson interviewed.
P{Y=y}=P{exactly k-1 interviews occur in ( y-1) 
people passing AND the ythperson passing is 
interviewed}Negative Binomial Probability Mass Function</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Set indicator random variables.
Suppose
Then E[ Xi] = 1* pi+ 0*(1- pi) = pi.
Example 1: Flip a coin N times, with P{Heads} = p.  Assume 
independent flips.
Define the random variable NH= number of Heads in Nflips.
Let the set indicator R.V. Xibe 1 if the ithcoin flip is Heads. 
Then,</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Analyzing a Probability Problem 
Four Steps to Happiness
1.  Define the Random Variable(s)
2.  Identify the (joint) sample space
3.  Determine the probability law over the          
sample space
4.  Carefully work in the sample space to 
answer any question of interest</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Example 2.  Baseball Hats.
Suppose that one player from each of the 30 Major League Baseball
teams attends a party at MIT, and each arrives wearing his teams baseball cap.  Each tosses his hat into a closet upon arrival.The host, at the end of the party, gives a random hat to each 
departing player.  What is the ex pected number of hats that are
returned to their rightful owners?
Solution:  Define indicator r.v.
Let H = the number of hats returned to the correct owners.  Then,
Answer independent of the number of players or teams!</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>ilities:  Reliability, Robustness
12AB
Let T12= event successful Transmission from node 1 to node 2.
pA=P{Link A works properly}=P{A}
pB= P{Link B works properly}=P{B}
Assume links A and B function independently. 
Then,
P{T12}=P{A and B}=P{AB}= pA pB</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Example #3.  Winning Streaks.
Suppose the Boston Celtics were much better, that they had a 
50% chance of winning any game, and that the outcomes of all games
were mutually independent.
If there were 100 games in a season, how many 7 game 
winning streaks might we expect?
How do we interpret this?
(1) Game nis a game which they win and for which they have won
the previous 6. Here (1/2)7= 1/128.  Use indicator r.v.s.
(2) Game nis a game which they win and for which they have won
the previous 6, andthey lose the next game.   Here we need 
(1/2)7(1/2)= 1/256.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>12AB
Let T12= event successful Transmission from node 1 to node 2.
pA=P{Link A works properly}=P{A}
pB= P{Link B works properly}=P{B}
pC= P{Link C works properly}=P{C}
Assume links A, B and C function independently. 
Then,
P{T12}=P{A and (B or C)}=P{AB+AC}= P{AB+ACB}=
P{T12}=pA pB +pA pC (1- pB)CAdd one redundant arc to increase Reliability</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>This is the St.Petersburg Paradox
Daniel Bernoulli (1738; English trans. 1954)
Google this &amp; find many interesting articles, such as
http://plato.stanford.edu/en tries/paradox-stpetersburg/</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>If Npeople go by, how many did 
you interview?
X=number interviewed, X=0,1,2,
p=0.2.
Why?  What is this called?  What is the sample space?
What are the probability assignments over the sample space?
Binomial Probability Mass Function</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Hypothesis testing (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Test for "Goodness of Fit"
as Conducted in Weibull's Paper
 Calculates the degrees of freedom
10 (bins) -1  3 (parameters of the df) = 6
 Calculates the statistic
 States the P-value 
 Comparison to alternative=estimatedestimated observed2
2 ) (

Note: Table is cumulative, 
2test requires frequency in binP=0.49 
for Weibull
01 0 2 000.10.2
dchisq x 6,()
0
0.2
0
0.2
x5.40
5.40,18.1718.17, 
P=0.008
for NormalFrom the previous lecture
Copyright  2006 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Fisher's Objections to the 
Neyman-Pearson Framework
 The following cocepts apply in acceptance 
sampling but generally are not meaningful in 
scientific investigations 
 The distinction between type Iand II error
 The notion of repeated sampling from the same 
population 
 Inductive behaviour (GG on NP "...accepting a 
hypothesis does not mean that you believe in it, 
but only that you act as if it were true. ")
Fisher, R. A., 1955, "Statistical Me thods and Scientific Induction," 
Journal of the Royal Statistical Society (B) 17:69-77.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Fisher's Suggestion of How to Think 
about Results of Hypothesis Tests
"...whenever a test of significance gives us no strong 
reason for rejecting it (the null hypothesis)... the worker's real attitude in such as case might be, according to the crcumstances:"
a) "The possible deviation from truth of my ...(null) hypothesis 
... seems not to be of sufficient magnitude to warrant any immediate modification."
b) "The deviation is in the direction expected for certain 
influences... and to this extent my suspicion has been 
confirmed; but the body of data available so far is not by 
itself sufficient to demonstrate their reality."
Fisher, R. A., 1955, "Statistical Methods and Scientific 
Induction," Journal of the Royal Statistical Society (B) 17:69-77.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Fisher on Smoking
 ~1950 a study at the London School of 
Hygiene states that smoking is an 
important cause of lung cancer
 Fisher writes 
 an error has been made of an old kind, 
in arguing from correlation to causation
 For my part, I think it is more likely that a 
common cause supplies the explanation
R. A. Fisher, 1958, The Centennial Review, vol. II, no. 2, pp. 151-166.
R. A. Fisher, 1958, Letter to the Editor of Nature , vol. 182, p. 596.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Problem Set #5
1. Parameter estimation
Make a probability plot
Make an estimate by regression
Make an MLE estimate
Estimate yet another way
Comment on "goodness of fit"
2. Hypothesis testing
Find a journal paper us ing the "null ritual"
Suggest improvements (valid ity, insight, communication)</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Randomization 
Distribution
 This code reorders 
the data from the 
cotton experiment
 Does the ANOVA
 Repeats 5000 
times
 Plots the pdf of the 
F ratiotrials=5000;bins=trials/100;
X=[7 7 15 11 9 12 17 12 18 18 14 18 1819 19 19 25 22 19 23 7 10 11 15 11];
group=ceil([1:25]/5);
[p,table,stats] = anova1(X, group,'off');F=cell2mat(table(2,5))
for i=1:trials
r=rand(1,25);
[B,INDEX] = sort(r);
Xr(1:25)=X(INDEX);
[p,table,stats] = anova1(Xr, group,'off');
Fratio(i)=cell2mat(table(2,5));
end
hold off
[n,x] = hist(Fratio,bins);
n=n/(trials*(x(2)-x(1)));colormap hsvbar(x,n)hold on
xmax=max(Fratio);
x=0:(xmax/100):xmax;y = fpdf(x,4,20);plot(x,y,'LineWidth',2)</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Next Steps
 Between now and next Monday
 Read Tufte "Visual and Statistical Thinking: Displays of 
Evidence for Making Decisions"
 Friday, 6 April
 Recitation to support PS#5
 Monday, 9 April
 Session on descriptive statistics and graphics
 PS#5 due, NO NEW HW ASSIGNED
 Wednesday, 11 April
 Session on Regression (no pre-read)
 Friday, 13 April
 Session to support the term project
 Be prepared to stand up and talk  for 5 minutes about your 
ideas and your progress</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Gigerenzer's Paper 
 "Mindless Statistics"
 Journal of Socio-Economics
 What are its key points?
H i s t o r y
 Research practices
 Statistics per se
Gigerenzer, G., 2004,Mindless Statistics, J. of Socio-Economics 33:587-606.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Simpon's
Paradox
Bickel, P. J. , et al ., 1975,  "Sex Bias in Graduate Admissions: Data from Berkeley," Science 187.(4175):398  404.Correcting for the 
tendency of womento apply to graduate departments thatare more difficult for 
applicants of
either sex to enter, there is a smallbut statistically significant bias in favorof women.
BUT why the tendency?Graph removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>What if Assumptions are Violated?
 This Matlab code generates data with a 
notreatment effect on mean
 But dispersion is affected by treatment 
T y p e  Ierror rate rises
for i=1:1000
control=random('Normal',0, 1,1,20);
trt=random('Normal',0, 2,1,20);
reject_null(i) = ttest2(control,trt,0.01);
end
mean(reject_null)</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Simpon's Paradox
It is decisively rejected that male and female applicant had the 
same probability of being acc epted to Berkeley in 1973. 
BUT The probability of finding that a department that was biased 
against women ... is about 57 times in 1000.
Bickel, P. J. , et al., 1975,  "Sex Bias in Graduate Admissions: Data from 
Berkeley," Science 187.(4175):398  404.Table 1 removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Type- IIIerror
 "At issue here is the importance of good 
descriptive and exploratory statistics 
rather than mechanical hypothesis testing with yes-no answers...The attempt to give an "optimal" answer to the wrong question has been called 
"Type-III error".  The statistician John 
Tukey (e.g., 1969) argued for a change in perspective..."
Gigerenzer, G., 2004,Mindless Statistics, J. of Socio-Economics 33:587-606.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Similar Issues in Research Today
 Correlation demonstrated in the Table
 Causal link implied in the conclusion What was the design of the experiment
?
 What mechanisms were proposed?
Chusilp, P., and Y. Jin, 2006,  "Impact of Mental Iterati on on Concept Generation," ASME J. of Mech. Design 128:14-25.
The paper's conclusion section states: 
"The results sggested that (1) increas ing number of iteration has positive 
impact on quality, variety, and quantit y, but mixed effect on novelty..."
Copyright  2006 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Matlab's Description of the
Two-sample t -test
h = ttest2(x,y) performs a t-test of the null 
hypothesis that data in the vectors x and y 
are independent random samples from normal distributions with equal means and equal but unknown variances, against the alternative that the means are not equal. The result of the test is returned in h. h = 1 indicates a rejection of the null hypothesis at the 5% significance level. h = 0 indicates a failure to reject the null hypothesis at the 5% significance level.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Concept Question
 How do effect and "alpha" affect the rate 
at which the t-test rejects H0 ?
a)effect,rejects
b)effect,rejects
c)alpha,  rejects
d)alpha,  rejects
1) a &amp; c
2) a &amp; d3) b &amp; c4) b &amp; deffect=1;alpha=0.01;
for i=1:1000
control=random('Normal', 0,1,1,20);
trt=       random('Normal', effect,1,1,20);
reject_null(i) = ttest2(control,trt, alpha );
endmean(reject_null)</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Hypothesis Testing
Dan Frey
Associate Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Concept Question
 This Matlab code repreatedly generates and tests 
simulated "data" 
 20 "subjects" in the control and treatment groups
 Both normally distributed with the same mean
 How often will the t -test reject H0(=0.01)?
for i=1:1000
control=random('Normal',0,1,1,20);
trt=random('Normal',0,1,1,20);reject_null(i) = ttest2(control,trt,0.01);
endmean(reject_null)1) ~99% of the time
2) ~1% of the time3) ~50% of the time4) None of the above</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Fisher's Null Hypothesis Testing
1. Set up a statistical null hypothesis. The null 
need not be a nil hypothesis (i.e., zero 
difference).
2. Report the exact level of significance ... Do 
not use a conventional 5% level, and do not 
talk about accepting or rejecting hypotheses.
3. Use this procedure only if you know very 
little about the problem at hand.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>The Null Ritual
1. Set up a statistical null hypothesis of "no mean 
difference" or "zero correlation." Don't specify 
predictions of your research hypothesis or any 
other substantive hypothesis
2. Use a 5% convention for rejecting the null.  If 
significant, accept your hypothesis.  Report the 
result as p&lt;0.005, p&lt;0.01, or p&lt;0.001 
(whichever comes next to the obtained p-value)
3. Always perform this procedure</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Neyman-Pearson "Decision Theory"
1. Set up two statistical hypotheses, H1and H2, and decide 
about , , and sample size before the experiment, 
based on subjective cost-benefit considerations. These 
define a rejection region for each hypothesis.
2. If the data falls into the rejection region of H1, accept H2; 
otherwise accept H1. Note that accepting a hypothesis 
does not mean that you believe in it, but only that you act 
as if it were true.
3. The usefulness of the procedure is limited among others 
to situations where you have a disjunction of hypotheses ... and where you can make meaningful cost-benefit 
trade-offs for choosing and .
Gigerenzer, G., 2004,Mindless Statistics, J. of Socio-Economics 33:587-606.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Gigernezer's Quiz
1. You have absolutely disproved the null hypothesis
2. You have found the probability of the null hypothesis being true.
3. You have absolutely proved your experimental hypothesis (that
there is a difference between the population means).
4. You can deduce the probability of the experimental hypothesis
being true.
5. You know, if you decide to reject the null hypothesis, the 
probability that you are making the wrong decision.
6. You have a reliable experimenta l finding in the sense that if, 
hypothetically, the experiment we re repeated a great number of 
times, you would obtain a signific ant result on 99% of occasions.Suppose you have a treatment that you suspect may alter 
performance on a certain task.  You compare the means of your 
control and experimental groups (say 20 subjects in each sample).  
Further, suppose you use a simple independent means t-test and 
your result is significant ( t= 2.7, d.f.= 18, p= 0.01). Please mark 
each of the statements below as true or false. ...</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Concept Question
 This Matlab code repreatedly generates and tests 
simulated "data" 
 20 "subjects" in the control and treatment groups
 Both normally distributed with the different means
 How often will the t -test reject H0(=0.01)?
for i=1:1000
control=random('Normal', 0,1,1,200);
trt=       random('Normal', 1,1,1,200);
reject_null(i) = ttest2(control,trt,0.01);
end
mean(reject_null)1) ~99% of the time
2) ~1% of the time
3) ~50% of the time
4) None of the above</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Model Adequacy Checking
 Normality
 normal probability plot of residuals 
 Constant variance
 plot residuals versus time sequence
 plot residuals versus treatments or fitted 
values
 Bartletts Test [ndim, prob] = barttest(x,0.05)
j i HH
j ia
,  one least at for  ::
2 2
12 2
22
1 0   
= = =K</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>A Modern View on the N-P Approach
"The Neyman Pearson approach rests on the idea 
that, of the two errors, one can be thought of as 
more important.  By convention this is chosen to 
be the type Ierror ... In the medical setting, this 
asymmetry appears reasonable... It has also been argued that, generally in science, announcing a new phenomenon has been observed when in fact nothing has happened is more serious than 
missing something new that has in fact occurred.  
We do not find this persuasive..."  
Bickel and Docksum, 2001, Mathematical Statistics, Prentice Hall.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Quiz Results
The percentages 
of participants in each group who 
endorsed one or 
more of the six false statements reagrding the meaning of  p = 0.01.
Gigerenzer, G., 2004,Mindless Statistics, J. of Socio-Economics 33:587-606.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Randomization Distribution
 The null hypothesis implies that the observations 
are not a function of the treatments
 If that were true, the allocation of the data to 
treatments (rows) shouldnt affect the test statistic
 How likely is the statistic observed under re-
ordering? Observations
Cotton 
weight 
percentage 12345
1 577 1 5 1 19
20 12 17 12 18 1825 14 18 18 19 19
30 19 25 22 19 23
3 5 71 01 11 51 1</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>NP Framework and Two Types of Error
 Set a critical value cof a test statistic T or else set the 
desired confidence level or "size"
 Observe data X
R e j e c t  H1if the test statistic T (X)c
 Probability of Type IError  The probability of T(X)&lt;c H1
 (i.e. the probability of rejecting H1given H1is true 
 Probability of Type IIError  The probability of T(X)cH2
 (i.e. the probability of not rejecting H1given H2is true 
T h e  power of a test is 1 - probability of Type IIError
 In the N-P framework, power is maximized subject to Type 
Ierror being set to a fixed critical value c or of or other confidence 
region (e.g. for "two-
tailed" tests)</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>What if Assumptions are Violated?
 This Matlab code generates data with a 
notreatment effect on mean
 The two poulations are uniformly dist. 
T y p e  Ierror rate rises
for i=1:1000
control=random('Uniform',0,1,1,20);
trt=random('Uniform',0,1,1,20);
reject_null(i) = ttest2(control,trt,0.01);
endmean(reject_null)</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Is This Really Being Done?
 Quick check  Compendex search for "significant" in 
abstract and the title of a research journal I read, 
publish in, review for
 11 results in 2006, 12 in 2005, etc.
Chusilp, P., and Y. Jin, 2006, "Impact of Mental Iteration on 
Concept Generation," ASME J. of Mech. Design 128:14-25.
The paper's conclusion 
section states: "The results suggested that 
(1) increasing number of 
iteration has positive impact on quality, variety, and quantity, but mixed effect on novelty..."
Copyright  2006 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>More of Gigerenzer's Concerns
 Meehl's Conjecture
 In non-experimental settings with large sample sizes, 
the probability of rejecting the null hypothesis of nil 
group differences in favor of a directional alternative is 
about 0.50.
 Statistical significance practical significance
 Feynman's Conjecture
 To report a significant result and reject the null in favor 
of an alternative hypothesis is meaningless unless the alternative hypothesis has been stated before the data 
were obtained.
 (My opinion) Yes, such a result requires independent 
confirmation, but you should still report it! 
Gigerenzer, G., 2004,Mindless Statistics, J. of Socio-Economics 33:587-606.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>The Neyman-Pearson Lemma
 When performing a hypothesis test 
between two point hypotheses H1and 
H2 ,the most powerful test of size is
=    =  ) ) ( Pr(   where) () () (1
10H c X cx Lx Lx</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Descriptive statistics and statistical graphics (PDF - 1.1 MB)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>30</slideno>
          <text>What about 
Feynman's Demonstration?
 It did show the 
phenomenon at issue
 But it also confounds 
many factors
 It doesn't show 
consistency with most of 
the data (non-failures)Photo and diagrams removed 
due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Aggregation of Data
Aggregation in time Spatial AggregationFigures removed due copyright restrictions. 
Source: Tufte, Edward. Chapter 2 in Visual Explanations: 
Images and Quantities, Evidence and Narrative . Cheshire, 
CT: Graphics Press, 1997.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Neocortex
 2mm thick
 6 layers
 About the size of a newspaper (unfolded) 
 So it has to be wrinkled up to fit in the skull
 About 3x1010neurons
 About 1000 synapses per neuron
 Each neuron capable of 200 cycles / sec
 5 million times slower than a computer</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Visual Cortex
About  of the cerebral cortex.
Retina  a 2 -D data stream
~ a million nerve fibers
Figure by MIT OCW.
The fiber pathways are two-way They carry as 
much information down from higher conceptual 
areas as from lower sensory areas...
Pinker, Steven, 1997, How the Mind Works , Norton &amp; Co, New York.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Concept Question
 This Matlab code repreatedly generates 
simulated "data" (5 samples) from a normally 
distributed population with a known mean and computes a 95% confidence interval
 How often will the 95% confidence interval 
include the mean of the distribution from which the samples came?
pm=1;
for i=1:1000
sample=random('Normal',pm,1,1,5);
[h,p,ci] = ttest(sample);in_int(i)=(ci(1)&lt;=pm)*(ci(2)&gt;=pm);
endmean(in_int)1) 95% of the time
2) a little less than 95%3) a little more than 95%
4) Not enough information</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Florence Nightingale's Polar-Area Diagrams
Source: Nightingale, F. Notes on Matters Affecting the Health, Efficiency and Hospital Administration of the British Army. 185 8.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Face Recognition 
 saccade = a fast motion of 
the eye (~3 per second)
 Your eye can be tracked as 
it looks at faces
 Your eyes are apparently 
very active in seeing a face
 The activity depends on 
familiarityImages of eye 
tracking during face 
recognition removed 
due to copyright 
restrictions.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Asch Conformity Experiments
 Subjects asked to participate in a "vision test" 
 In reality, all but one of the participants were confederates of the 
experimenter
 A high proportion (~33%) conformed to the erroneous majority view
 Many subjects showed extreme discomfort, but conform anyway
Subject
ConfederatesA
A
APresented on screen</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>What about the 
Exceptions?
V e r y  f e w  
instances in the 
work house and 
brewery despite 
proximity to the 
suspected pump
 Also some 
instances far 
from the pumpMap removed due copyright restrictions. 
Source: Tufte, Edward. Chapter 2 in Visual 
Explanations: Images and Quantities, Evidence 
and Narrative. Cheshire, CT: Graphics Press, 
1997.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>The General Argument
1. (Tufte) "An essential analytic task in making decisions 
based on evidence is to understand how things work 
mechanism, trade-offs, process and dynamics, cause 
and effect.  That is, intervention-thinking and policy-
thinking demand causality-thinking."
2. (Tufte) "Making decisions bas ed on evidence requires 
the appropriate display of that evidence.  Good displays 
of that data help to reveal knowledge relevant to 
understanding mechanism, process and dynamics, 
cause and effect.  That is, displays of statistical data 
should directly serve the analytic task at hand."
3. (Frey) Replace the instances of the term "making 
decisions" with "doing high quality ESD research."  The 
sentences above are still correct yet have different 
implications.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Histograms
 A graph of continuous data
 Approximates a pdf in the limit of large n
05Histogram of Crankpin Diameters
Diameter, Pin #1Frequency</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Session
 Descriptive statistics
 Visual perception / cognition
 Tufte's paper 
 Statistical graphics
 Statistical thinking</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>A Hypothesis
 Facts about the brain provide some 
insight into problems in:
 statistical inference
 research methods
 decision making
 Cognitive sciences and psychology 
also provide insights into the 
remedies</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Skewness and Kurtosis
 Skewness
K u r t o s i s) )) ( ((3x E x E
) )) ( ((4x E x E
positively skewed distribution
positive kurtosis</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Three Brains in One
 Reptilian Complex
 digestion, reproduction, 
circulation, breathing, 
"fight or flight" response 
 Limbic System
 houses primary centers 
of emotion 
 hippocampus -- important 
aspects of long term 
memory
 Neocortex
 processing senses
 logic language
 motor control
Cerebrum
(Neo cortex or new brain)
Limbic system
(Mammalian or mid brain)
Reptilian complex
(Old brain)
Brain stemCerebellumCorpus callosum
Figure by MIT OCW .</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Correlation Coefficient
 Sample
 Which is an estimate of ()()
()Y Xn
ii i
S S nY Y X X
r11
 
=
=

=n
ii X X XnS
12 2) (11
y xy E y x E x E
 ))) ( ))( ( ((</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Bayesian Credible Interval
 In Bayesian statistics, a credible interval is a 
posterior probability interval...
 For example, a statement such as "following 
the experiment, a 90% credible interval for 
the parameter tis 35-45" means that the 
posterior probability that tlies in the interval 
from 35 to 45 is 0.9.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Scatter Plots
load carsmall
gscatter(Weight,MPG,Model_Year,'','xos')
figure(2)
xvars = [Weight Displacement Horsepower];yvars = [MPG Acceleration];gplotmatrix(xvars,yvars,Model_Year,'','xos')</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Confidence Intervals
 Assuming a given distribution and a sample 
size n and a given value of the parameter 
the 95% confidence interval from U to Vis s.t. 
the estimate of the parameter
 The confidence interval depends on the 
confidence level, the sample variance, and 
the sample size% 95 )  Pr( = &lt; &lt;
 V U</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Edward R. Tufte's Paper 
 "Visual and Statistical Thinking: Displays 
of Evidence for Making Decisions"
 What are its key points?
H i s t o r y
 Research practices
 Statistics per se
Tufte, 1997,"Visual and Statistical Thinking: Displays of Evidence for Making Decisions" 
Graphics Press LLC, Cheshire CT.</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Lower segment of 
rocket casing 
Inside of rocket (filled with500 tons of propellent) Upon ignition, smoke leaked from this joint. A flame burned through 59 seconds later. Primary O-ring 
Secondary O-ring Upper segment of 
rocket casing 
Exterior wall of rocket 
Figure by MIT OCW.
 Figure by MIT OCW.
Courtesy of NASA.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Right Conclusion, 
Poor Communication
Source: NASA</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Confusion about 
Evidence, Burden of Proof, and Significance
Absence of evidence 
evidence of absencereport from engineers:VP decision:
Source: NASASource: NASA</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Concept Question
 Here is a sample of data
 This Matlab code computes a 95% confidence interval 
for the mean
 Then it runs a t-test on the data and the hypothesis that 
it was from a normal distribution with a mean greater 
than or equal to the lower confidence bound
 What p-value will the test return?
sample=[1.2 3.1 4.5 2.3 0.8 7.2];
[h,p,ci] = ttest(sample);
[h2,p2] =ttest(sample,ci(1),0.05,'left')1) exactly 0.95
2) exactly 0.9753) more than 0.975
4) less than 0.95
5) none of the above</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Descriptive Statistics and 
Statistical Graphics
Dan Frey
Associate Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Measures of Central Tendency
 Arithmetic mean
 an unbiased estimate of
 Median
 Mode  The most frequently 
observed value in the sample                       = =
Sx x x xf x Ed ) ( ) (

==n
iiXnX
11
 
even  is   if  21odd  is   if  
21221



+=
++
n X Xn X
n nn</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Change Blindness
 focused attention is needed to detect 
change*
 You cannot assume that information flowing 
into your brain will be attended to; it may be 
ignored completely
 Is it also true of graduate students?
 Is it true of a professors?
 Is it possible to do good research when key 
information fails to be acknowledged?
*Rensink, R. A., 2002, Change Detection,
Annual Review of Psychology, 53:4245-277.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Would this Have Been Convincing?
 Continuous measure of degradation
 Plotted versus the suspected variable
 Shows the range over which the prediction is needed
 The lack of data in that range is the most striking featureImage removed due to copyright restrictions.
Plot of O-ring damage index vs. Temperature at time of launch. 
Source: Tufte, Edward. Visual Explanations: Images an d Quantities, Evidence and 
Narrative . Cheshire, CT: Graphics Press, February 1997.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Box and 
Whisker Plot
load fisheririss1 = meas(51:100,3);s2 = meas(101:150,3);
boxplot([s1 
s2],'notch','on','labels',{'versicolor','virginica'})</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Evolution and the Brain
 The mind is a system of organs of 
computation, designed by natural 
selection to solve the kinds of problems 
our ancestors faced
Pinker, Steven, 1997, How the Mind Works , Norton &amp; Co, New York.Image removed due to copyright restrictions.
Diagram showing three stages in skull &amp; brain evolution.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Measures of Dispersion
 Population Variance
 Sample variance
 an unbiased estimate of
nthcentral moment
nthmoment about m) )) ( ((2 2x E x E =

=n
iiX XnS
12 2) (11
) )) ( ((nx E x E
) ) ((nm x E()
= =n
iiX XnX VAR
12 1) (</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Next Steps
 Wednesday, 11 April
 Session on Regression (no pre-read)
 Friday, 13 April
 Session to support the term project
 Be prepared to stand up and talk for 5 
minutes about your ideas and your 
progress
 16-17 April, No classes (Patriot's Day)
 Wednesday 18 April, ANOVA</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>The Cholera Epidemic in London, 1854
E.W. Gilbert, "Pioneer 
Maps of Health and 
Disease in England," 
Geographical Journal , 124 
(1958), 172-183.  Dot plot of 
cholera 
instances 
suggests a 
spatial pattern
 Centered on a 
water pumpMap removed due to 
copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>The Human Brain
 The human brain is tremendously complex
 It possesses many highly specialized component 
parts each associated with specific tasks
 The functioning of the human brain gives rise to
 Consciousness 
 Intelligence (ability to learn, plan...)
 Emotions
 Decision making
 The mind  accomplishes 
remarkable feats 
no engineer can duplicate
Pinker, Steven, 1997, How the Mind Works , Norton &amp; Co, New York.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>But What 
Does it Mean?
Courtesy of American Statistical As sociation. Used  with permission.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Visual 
Cognition
 Humans are much
Kanwisher, Nancy, 2003, Imaging Visual Cognition .better than computers 
at classifying objects based on complex, 
noisy, ambiguous images
 Certain classes of things have specialized 
areas
 The same areas involved in seeing a 
face or place are active in thinking about itPlace area
Face area
Body area
Figure by MIT OCW .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Analysis of variance, with discussion of Bayesian and frequentist statistics (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>37</slideno>
          <text>Confidence Intervals
Between Treatment Means
 For a treatment mean 
 For a difference between treatment 
meansnMSt ynMSt yE
a N i iE
a N i   +   , 2 / . , 2 / .

nMSt y ynMSt y yE
a N j i j iE
a N j i2 2
, 2 / . . , 2 / . .   +      
 
Note the factor of two</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Balance
 When the number of samples at each 
treatment is equal across all treatments, the 
design is said to be balanced
 Unbalance causes no difficulty in the 
computation of ANOVA tables
 BUT a balanced design provides 
 More robustness to violation of the 
homoscedascity assumption
 The greatest possible power of the hypothesis test</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>The Effect of Heteroscedascity
 This Matlab code generates data with a 
notreatment effect on location
 But dispersion is affected by group 
~N(0,group)
T y p e  Ierror rate rises substantially
for i=1:1000
group=ceil([1:50]/10);
X=group.*random('Normal',0,1,1,50);[p,table,stats] = anova1(X, group,'off');reject_null(i)=p&lt;0.05;
end
plot(group,X,'+'); mean(reject_null)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Today
 Efron, 2004
 Bayesians, Frequentists, and Scientists
 Analysis of Variance (ANOVA)
 Single factor experiments
 The model Analysis of the sum of squares Hypothesis testing
 Confidence intervals</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Next Steps
 Friday 20 April, recitation (by Frey)
 Monday 23 April
 PS#6 due
 Multiple regression
 Wenesday 25 April
 Design of Experiments</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>The 
Bootstrap
Image removed due to copyright restrictions.
Figures 4 and 5 in Efron, Bradley. "Modern Science and the Bayesian-Frequentist Controversy."
http://www-stat.stanford.edu/~brad/papers/NEW-ModSci_2005.pdf</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Bayes' Theorem
) Pr() Pr( ) Pr() Pr(BA B AB A=) Pr() Pr() Pr(BB AB A
BU
) Pr() Pr() Pr(AB AA B
with a bit of algebraAAB</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Example 
of ANOVA
 What do I get if I compute 
the mean of these 
values?
 What is the variance of 
these values related to?
 If this data were taken in 
the presence of time trend, how would the tables change if the experimental procedure were altered to eliminate the trend? 
Montgomery, D. C. 1997, Design and Analysis of Experiments</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Wilcoxon "rank sum" test
(what processing is carried out)
BRCA1=[-1.29 -1.41 -0.55 -1.04 1.28 -0.27 -0.57];
BRCA2=[ -0.70 1.33 1.14 4.67 0.21 0.65 1.02 0.16 ];
p = ranksum(BRCA1,BRCA2,'alpha',0.05)
-1.41
-1.29
-1.04
-0.55-0.57
-0.27
1.28-0.70
1.140.21
0.65
1.020.16
1.33
4.67#4
#8 thru
#12
#14 and
#15rank sum of BRCA2 is
83 
largest possible is 92
smallest possible is 36
network algorithms applied 
to find p-value is ~2%Form ranks 
of combined 
data sets</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>False Discovery Rates
Courtesy of Brad ley Efron.  Used  with permission.
Source: "Modern Science and the Bayesian-Frequentist Controversy."
http://www-stat.stanford.edu/~ brad/papers/NEW- ModSci_2005.pdfImage removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Wisdom Regarding the 
Foundations of Statistics
It is often argued academically that no science can be more 
secure than its foundations, and th at, if there is controversy 
about the foundations, there must  be even more controversy 
about the higher parts of the scienc e.  As a matter of fact, the
foundations are the most controve rsial part of many, if not all,
sciences As in other scienc es, controversies about the 
foundations of statistics reflect themselves to some extent in 
everyday practice, but not nearly so catastrophically as one might imagine.  I believe that her e, as elsewhere, catastrophe is 
avoided, primarily because in practical situations common sense generally saves all but the most  pedantic of us from flagrant 
error Although study of the foun dations of a science does not 
have the role that would be assigne d to it by nave first-things-
firstism, it certainly has a cont inuing importance as the science 
develops, influencing, and being influenced by, the more immediately practical parts of the science. 
Savage, L. J., 1954, The Foundations of Statistics , Dover Publications, Inc., New York.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Empirical Bayes
 "... the prior quantities are estimated 
frequentistically in order to carry out Bayesian 
calculations."
 "... if we had only one gene's data ... we would 
have to use the Wilcoxon null, but with thousands 
of genes to consider at once, most of which are probably null, we can empirically estimate the null distribution itself.  Doing so gives far fewer 
significant genes in this case." 
 "... Estimating the null hypothesis itself from the 
data sounds a little crazy, but that's what I mean 
about huge data sets presenting new opportunities..."
Efron, Bradley, 2005, Efron, B. "Bayesians, Frequentists, and Scientists,"  Journal of the American Statistical Association , 100, (469):1-5.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>How Important Is 
Normality?
 This code includes 
uniformly
distributed variates
 The randomization 
distribution is computed
 Plots the pdf of the 
F ratio
 Looks like the F 
distribution!trials=5000;bins=trials/100;
X=random('Uniform',0,1,1,25);group=ceil([1:25]/5);
for i=1:trials
r=rand(1,25);
[B,INDEX] = sort(r);
Xr(1:25)=X(INDEX);
[p,table,stats] = anova1(Xr, group,'off');
Fratio(i)=cell2mat(table(2,5));
end
hold off
[n,x] = hist(Fratio,bins);n=n/(trials*(x(2)-x(1)));colormap hsv
bar(x,n)
hold on
xmax=max(Fratio);
x=0:(xmax/100):xmax;y = fpdf(x,4,20);
plot(x,y,'LineWidth',2)</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Determining Sample Size from 
Confidence Intervals
 State the desired width  ci(e.g. on a 
difference between treatment means)
 Estimate  of the experimental error
 Solve for nsuch that
nt cia N2
, 2 /2
  
design  balanced  a for     a n N =
s=3;    % estimated experimental error
a=5;     %number of treatmentsn=2:20;y=tinv(0.95,n*a-a).*sqrt(s./n);plot(n,y,'+')</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Bayes' Theorem and Hypotheses
) Pr() Pr( ) Pr() Pr(DH D HD H=

==n
ii i H H D D
1) Pr( ) Pr( ) Pr("prior" probabilitythis is the p-value 
statistical tests provide</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Breakdown of DOF
N
number of yvalues
1
due to the meanN-1
total sum of squares
a-1 
for the treatments N-a
for error</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Breakdown of Sum Squares
SSdue to mean
===a
in
jijy GTSS
112
2
..y n=
== =a
in
jij T y y SS
112
..) (
ESS 
= =a
ii Treatments y y n SS
12
.. .) (Grand Total 
Sum of Squares Total Sum of 
Squares</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Discovering Dispersion Effects
 Earlier we considered non-constant variance 
as a difficulty  a violation of our model 
assumptions
 BUT sometimes we are interested in studying 
and exploiting these dispersion effects
(a.k.a. robust design) 
 Analysis can proceed as usual (ANOVA, etc)
 Best to use log(s )rather than sas the 
response</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Wilcoxon Null Distribution
Wilcoxon, F., 1945, "Individual Co mparisons by Ranking Methods," Biometrics Bulletin 1(6): 80-83. Assign ranks to two sets of unpaired data
 The probability of occurence of any total or a lesser total by chance under the assumption that the group
means are drawn from the same population:</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Determining Sample Size
 Can be done on the basis of Type II
error probability 
 BUT this requires an estimate of treatment 
effects compared to error
 OR the experimenter can specify 
desired width in the confidence interval
 This only requires an estimate of 
experimental error</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Treatment Effects Model
ij i ijy

 ++=
Each cell 
is a yij
Each row 
is 
treatment i
Replicates in columnsObservations
Cotton 
weight 
percentage 12345
15 7 7 15 11 9
20 12 17 12 18 18
25 14 18 18 19 19
30 19 25 22 19 233 5 71 01 11 51 1</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Assumptions of the Model
 Error is normally distributed
 With equal variance 
 across treatments and 
 over time
 Effects of other factors do not bias the 
resultsij i ijy

 ++=</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Plan for Today
 Efron, 2004
 Bayesians, Frequentists, and Scientists
 Analysis of Variance (ANOVA)
 Single factor experiments
 The model Analysis of the sum of squares Hypothesis testing
 Confidence intervals</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Randomization 
Distribution
 This code reorders 
the data from the 
cotton experiment
 Does the ANOVA
 Repeats 5000 
times
 Plots the pdf of the 
F ratiotrials=5000;bins=trials/100;
X=[7 7 15 11 9 12 17 12 18 18 14 18 1819 19 19 25 22 19 23 7 10 11 15 11];group=ceil([1:25]/5);
for i=1:trials
r=rand(1,25);
[B,INDEX] = sort(r);
Xr(1:25)=X(INDEX);
[p,table,stats] = anova1(Xr, group,'off');
Fratio(i)=cell2mat(table(2,5));
end
hold off
[n,x] = hist(Fratio,bins);n=n/(trials*(x(2)-x(1)));colormap hsv
bar(x,n)
hold on
xmax=max(Fratio);
x=0:(xmax/100):xmax;y = fpdf(x,4,20);
plot(x,y,'LineWidth',2)</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Probabilistic formulation: The probability 
that a woman has breast cancer is 0.8%.  If 
she has breast cancer, the probability that a 
mammogram will show a positive result is 
90%.  If a woman does not have breast cancer, the probability of a positive result is 7%.  
Take for example, a woman who has a 
positive result.  What is the probability that 
she actually has breast cancer?Frequency format: Eight out of every 1000 
women have breast cancer.  Of these eight 
women with breast cance r seven will have a 
positive result with mammography.  Of the 992 women who do not have breast cancer some 70 
will have a positive mamm ogram.  Take for 
example, a sample of women who have positive 
mammograms.  What pr oportion of these women 
actually have breast cancer?Figure removed due to copyright restrictions. 
Figure 1 in G. Gigerenzer, and A. Edwards. Simple tools for 
understanding risks: from  innumeracy to insight. British Medical 
Journal 327 (2003), 741-744.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Model Adequacy Checking
 Normality
 normal probability plot of residuals 
 Independence / constant variance
 plot residuals versus time sequence
 plot residuals versus fitted values
 Bartletts Test [ndim, prob] = barttest(x,0.05)
j i HH
j ia
,  one least at for  ::
2 2
12 2
22
1 0   
= = =K</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Confidence Intervals
 One-at-a-time confidence intervals 
apply to each treatment mean
 If you want  to apply to all the 
treatment means ( rof them) 
simultaneously just replace /2 with / 2rnMSt ynMSt yE
a N i iE
a N i   +   , 2 / . , 2 / .

Bonferroni method</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>What is a "Degree of Freedom"?
 How many scalar values are needed to 
unambiguously describe the outcome o this 
experiment?
 What if I were to tell you      ?
 What if I were to tell you                    ?Observations
Cotton 
weight 
percentage 12345
15 7 7 15 11 9
20 12 17 12 18 1825 14 18 18 19 19
30 19 25 22 19 23
3 5 71 01 11 51 1
..y
4 ... 1     .=i yi</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Single Factor Experiments
 A single experimental factor is varied
 The parameter takes on various levels
Observations
Cotton 
weight 
percentage 12345
15 7 7 15 11 9
20 12 17 12 18 18
25 14 18 18 19 19
30 19 25 22 19 233 5 71 01 11 51 1
Fiber strength in lb/in2experimental 
factorEach cell 
is a yij
Each row 
is a 
treatment ia=5replicates</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Bayes and Breast Cancer
 The probability that a woman randomly selected from 
all women in the US has breast cancer is 0.8%.  
 If a woman has breast cancer, the probability that a 
mammogram will show a positive result is 90%.  
 If a woman does not have breast cancer, the 
probability of a positive result is 7%.  
 Take for example, a woman from the US who has a 
single positive mamogram.  What is the probability 
that she actually has breast cancer? 1) ~90%
2) ~70%3) ~9%4) &lt;1%5) none of the above</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Analysis of Variance
(and discussion of Bayesian and frequentist statistics)
Dan Frey
Assistant Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Bayesians, Frequentists, and Scientists
by Brad Efron
 How does the paper characterize the 
differences between the two 
approaches?
 What is currently driving a modern 
combination of these ideas?
 What lessons did you take away from 
the examples given? 
Efron, B., 2005, "Bayesians, Fre quentists, and Scientists,"  Journal of the 
American Statistical Association , 100, (469):1-5.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Brad Efrons Biographical Information
 Professor of Statistics at Stanford University
 Member of the National Academy of Sciences
 President of the American Statistical Association 
 Winner of the Wilks Medal 
 "... renowned internationally for his pioneering work in 
computationally intensive statistical methods that substitute 
computer power for mathematic al formulas, particularly the 
bootstrap method. The goal of th is research is to extend 
statistical methodology in ways that make analysis more 
realistic and applicable for complicated problems. He 
consults actively in the applicat ion of statistical analyses to 
a wide array of health care evaluations."</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Testing Equality of 
Treatment Means
 Hypotheses
 Test statistic
 Criterion for rejecting H0i HH
ia
 one least at for  0 :0 :
12 1 0
= = = =
  K
ETreatments
MSMSF=0
a N a F F  &gt;, 1 , 0</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Example 3-12 
Smelting Experiment
X=[0.05 0.04 0.05 0.06 0.03 0.05
0.04 0.02 0.03 0.05 0.03 0.02
0.09 0.13 0.11 0.15 0.08 0.12
0.03 0.04 0.05 0.05 0.03 0.02];
logX=log(X);
[p,table,stats] = anova1(logX');Standard 
deviations of 
the voltage with 
4 control 
algorithms</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>What is a "Degree of Freedom"?
 How many scalar values are needed to 
unambiguously describe the state of this object?
 What if I were to fix the xposition of a corner?</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Randomization Distribution
 The null hypothesis implies that these 
observations are not a function of the treatments
 If that were true, the allocation of the data to 
treatments (rows) shouldnt affect the test statistic
 How likely is the statistic observed under re-
ordering? Observations
Cotton 
weight 
percentage 12345
1 577 1 5 1 19
20 12 17 12 18 1825 14 18 18 19 19
30 19 25 22 19 23
3 5 71 01 11 51 1</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Wisdom Regarding the 
Foundations of Statistics
It is often argued academically that no science can be more 
secure than its foundations, and th at, if there is controversy 
about the foundations, there must  be even more controversy 
about the higher parts of the scienc e.  As a matter of fact, the
foundations are the most controve rsial part of many, if not all,
sciences As in other scienc es, controversies about the 
foundations of statistics reflect themselves to some extent in 
everyday practice, but not nearly so catastrophically as one might imagine.  I believe that her e, as elsewhere, catastrophe is 
avoided, primarily because in practical situations common sense generally saves all but the most  pedantic of us from flagrant 
error Although study of the foun dations of a science does not 
have the role that would be assigne d to it by nave first-things-
firstism, it certainly has a cont inuing importance as the science 
develops, influencing, and being influenced by, the more immediately practical parts of the science. 
Savage, L. J., 1954, The Foundations of Statistics , Dover Publications, Inc., New York.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Bayes' Theorem and Hypotheses
% 3 . 9
% 72 . 0 % 9 . 6% 72 . 0
) Pr() Pr( ) Pr() Pr( =
+= =PC P CP C
) Pr( ) Pr( ) Pr(~ ) ~ Pr( ) Pr(C C P C C P P + ="base rate" in the population 0.8%power of the test to detect 
the disease 90% 
1- "base rate"  
99.2%"false alarm" rate of 
the test 7%</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Same Basic Ideas in My Research
B D
AB AD
ABCABD
ABCDA
ACmain effects
two-factor interactions
BC BD CDC
three-factor interactions
ACD BCD
four-factor interactionsB D
AB AD
ABCABD
ABCDABCDA
ACmain effects
two-factor interactions
BC BD CDC
three-factor interactions
ACD BCD
four-factor interactionsA notion of structure in a problem domain.
Quantified by a huge body of data.Encoded in a probabilistic model.
Used to from "bootstrap" estimates of statistics (in this case, performance of techniques for design of computer experiments).
===1   if   ) , 0 (0    if    ) 1 , 0 () (
i2i
 c NNfi i

= += += +
= =
2    if    1    if    0    if    
) , 1 Pr(
j i 11j i 01j i 00   
  
pppj i ij</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Cochrans Theorem
 Let Zibe NID(0,1) for i =1,2,and
Where s &lt;and Qihas idegrees of freedom.  
Then Q1,Q2,  Qsare independent chi-square 
random variables with 1, 2, sdegrees of 
freedom respectively iff=1+2++s
 Implies that                is Fa-1,N-as
ii Q Q Q Z+ + + = 
=K2 1
12

ETreatments
MSMS</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Wilcoxon "rank sum" test
(as described in the Matlab "help" system)
p = ranksum(x,y,'alpha',alpha)
Description -- performs a two-sided rank sum 
test of the null hypothesis that data in the 
vectors x and y are independent samples 
from identical continuous distributions with equal medians, against the alternative that 
they do not have equal medians. x and y can 
have different lengths. ...The test is 
equivalent to a Mann-Whitney U-test.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Queueing and transitions: Sampling from distributions, Gauss (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>15</slideno>
          <text>Central Limit Theorem
Consider the sum Snof niid random 
variables Xi, where  
Then, as n gets large, Sntends to a 
Gaussian or Normal distribution with 
mean equal to nmXand variance equal 
to          .E[Xi]=mX&lt;
VAR[Xi]=X2&lt;
Sn=X1+X2+...+Xn=Xi
i=1n

nX2</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>1/4 1/41/2pX(y)
0y
FX(y)
0y11 2 3
12 3R=r1</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>x axis1Point of flashlight
illumination

x
1.  R.V.':  X,</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>FR()P{R}=1e2/22,  0
R1sample from a uniform pdf over [0,1]
R1=1e2/22, which implies that
=2ln(1 R1)
=2R2
X=cos=2ln(1 R1)cos(2R2)
Y=sin=2ln(1 R1)sin(2R2)
Here we have 2 exact samples 
from the Gaussian pdf, with no 
approximation from the CLT!</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Apply Littles Law to Service Facility
=A(average service time)
=average number in service facility =A/
A==(1e/)
W=LA=/
(1e/)=
2(1e/)http://www.fdcw.unimaas.nl/cwsiot/shopwindow/music/Floor%20Manschot%20&amp;%20Michiel%20Stoter/plaatjes/happiness.jpg</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>1 0 100.20.40.60.81
p2x()
1
2nexn()2
2n()2

x 
Sum of 2 iid Uniformly Distributed Random Variables</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>1 0 100.511.5
p1x()
1
2nexn()2
2n()2

x 
One Uniformly Distributed Random Variable</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>2 0 200.20.40.60.8
p5x()
1
2nexn()2
2n()2

x 
Sum of 5 iid Uniformly Distributed Random Variables</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>1 23 4 01/41/2fX(y)
y
1 23 4 01/43/4FX(y)
y1
R=r2</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Lets talk about Monte Carlo 
sampling: Inverse Method.
Uses CDF, and is Never Fail!</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>1 23 4 01/41/2fX(y)
y
1 23 4 01/43/4FX(y)
y1
R=r3</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>1/4 1/41/2pX(y)
0y
FX(y)
0y11 2 3
12 3R=r4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Outline
One More Time:  Markov Birth and Death 
Queueing Systems
Central Limit Theorem
Monte Carlo Sampling from Distributions
Q&amp;A</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Normalizing Random Variables
Suppose we have a r.v. W  having 
Mean =E[W]=a   and
Variance =E[(Wa)2]=W2
Define a new r.v.
XWa.  Then
E[X]=E[Wa]=E[W]a=aa=0
VAR[X]=VAR[W]=W2
Or suppose we define
YW.  Then
E[Y]=E[W]=a
Y2=E[(Wa)2]=2E[(Wa)2]=2W2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86. Queueing &amp; Transitions
Sampling from Distributions, Gauss
Richard C. Larson
March 12, 2007
http://simulationtutorials.com/images/CLT.gif
Courtesy of Dr. Sam Savage, www.AnalyCorp.com.  Used with permission.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>FR(r)=1
22  e(x+y)2/22dxdy
           circle of
           radius r
fR()d= dd
=021
22e2/22=2e2/22d,  0
fR()=2e2/22,  0A Rayleigh pdf
With parameter 1/</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>2 1 0 1 200.20.40.60.8
p3x()
1
2nexn()2
2n()2

x 
Sum of 3 iid Uniformly Distributed Random Variables</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>x axis1Point of flashlight
illumination

x
1.  R.V.':  X, 
2.  Sample space for :  [-/2, /2]
3.  uniform over  [- /2, /2]
4. (a) FX(x) = P{X &lt;x} = P{tan&lt;x}=P{ &lt; tan-1(x)}= 1/2 + (1/) tan-1(x)
(b) fX(x) =(d/dx) FX(x) = 1/( )(1 + x2)    all x
Cauchy pdf Mean =?, Variance = ????</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>1 23 4 01/41/2fX(y)
y
1 23 4 01/43/4FX(y)
y1
R=r1</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Its Movie Time!</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Buy one, get the other 3 for free!
W=1+Wq
L=Lq+LSF=Lq+
L=W</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Central Limit Theorem Demo
Thanks to Prof. Dan Frey! :) 
ESD.86</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>fSn(y)=1
X2ne{(ynmX)2/(2nX2)}  -&lt;y&lt;Sn=X1+X2+...+Xn=Xi
i=1n

nmxfsn (y)
nsX
y
Figure by MIT OCW .</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>1/4 1/41/2pX(y)
0y
FX(y)
0y11 2 3
12 3R=r2</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Normalizing Random Variables
Suppose we have a r.v. W  having 
Mean =E[W]=a   and
Variance =E[(Wa)2]=W2
Define a new r.v.
XWa.  Then
E[X]=E[Wa]=E[W]a=aa=0
VAR[X]=VAR[W]=W2</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>1/4 1/41/2pX(y)
0y
FX(y)
0y11 2 3
12 3R=r3</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Time to 
Buckle your 
Seatbelts!
http://www.census .gov/pubinfo/www/multimedi a/img/seatbelt-lo.jpg</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>x axis1Point of flashlight
illumination

x
1.  R.V.':  X, 
2.  Sample space for :  [-/2, /2]
3.  uniform over  [- /2, /2]</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Optional Exercise:
Is it better to enter a single 
server queue with service rate 
or a 2-server queue each with rate /2?
Can someone draw one or both of the 
state-rate-transition diagrams?  
Then what do you do?</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>1 23 4 01/41/2fX(y)
y
1 23 4 01/43/4FX(y)
y1
R=r4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>P0=(e/)1=e/&gt;0
=utilization factor =1 P0=1e/&lt;1.
Pk=(/)k
k!e/,   k=0,1,2,... Poisson Distribution!
L=time - average number in syste m=/ How?
L=AW      Little's Law, where
Aaverage rate of accepted arrivals into system</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Normalizing Random Variables
Suppose we have a r.v. W  having 
Mean =E[W]=a   and
Variance =E[(Wa)2]=W2
Thus, if we define
Z(Wa)/W, then
E[Z]=0
Z2=1
Z is called a normalized r.v.Most table 
lookups of the 
Gaussian are via 
the CDF, with a 
normalized r.v.</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Spin the Flashlight
And, so, finite variance is just a 
professors oral exam trick 
question? :)</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>The Gaussian or Normal PDF
1fY(y)=Y2e{(yE[y])2/(22
Y)}  -&lt;y&lt;
http:/ /coding.yonsei.ac.kr/images/f10.gifmf(x)
x
f (x) =s
2s2(x - m)2{ {exp1
s 2p
Figure by MIT OCW .</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>x axis1Point of flashlight
illumination

x</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>2 1 0 1 200.20.40.60.8
p4x()
1
2nexn()2
2n()2

x 
Sum of 4 iid Uniformly Distributed Random Variables</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>x axis1Point of flashlight
illumination

x
1.  R.V.':  X, 
2.  Sample space for :  [-/2, /2]</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Inverse Method Also Works for 
Continuous Random Variables</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Final Example:  
Single Server, Discouraged Arrivals
Pk=1
k!()kP0
P0=[1+()+1
2!()2+1
3!()3+...+1
k!()k+...]1
P0=(e/)1=e/&gt;0
/2/3/4/5
State-Rate-Transition Diagram, Discouraged Arrivals</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Obtaining Samples of the 
Gaussian R.V.
In Monte Carlo simulations, one often 
uses the Central Limit Theorem (CLT) 
to approximate the Gaussian.  
Example 1: Erlang Order N for large 
Nshould be approximately Gaussian
Example 2: Sum and normalize 12 
uniforms over [0,1].  Good idea?</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Example 3:  The Relationships Method
fX(x)=fY(x)=1
2ex2/22  &lt; x&lt;
X and Y are zero - mean independent Gaussian r.v.'s.
RX2+Y2
FR(r)P{Rr}=P{X2+Y2r}
FR(r)=1
22  e(x+y)2/22dxdy
           circle of
           radius r</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>The Weibull distribution and parameter estimation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Weibulls Derivation
) () 1 ( )) ( 1 (x n n ne P x F=  = ) (1 ) (xe x F =
x x
om
u
xx x
e x F) (
1 ) (
 =A cdf can be transformed into the form
This is convenient because
Among simplest functions satisfying the condition isThe function (x)must be positive, non-decreasing, and should 
vanish at some value xuwhich is often zero but not necessarily.
om
u
xx x
x) (
) (
=

So, a reasonable distribution to try is</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Next Steps
 Between now and Weds
 Read Gigerenzer "Mindless Satatistics"
 Wednesday 10:30-noon
 Session on Hypothesis testing
F r i d a y
 Recitation to support PS#5
 Monday 
 PS#5 due
 Session on XXX</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Useful Facts about the 
Two Parameter Weibull Distribution
kx
e x F
 =1 ) (
kx k
ex kx f
=
 1
) (cdf 
pdf 
Mean Variance 
Skew Median Support 
) ; 0 [+x
+ k11

()k/ 12 ln
2 2 21
 + k
33 2 3331 
  + k
How is this slide different for the three parameter distribution?</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Maximum Likelihood Estimates
 Choose the estimate    of the parameter 
to maximize the likelihood function 
 Or, if more convenient, maximize log(L())) ; , , , ( ) (2 1
nX X X f LK =
nobserved valuesparameter value(s) 
to be estimated
Note:  The estimated parameter value is not guaranteed to be 
the most likely one.  It's the data that is made most likely by 
the parameter estimate.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>The Wonderful One-Hoss Shay
by Oliver Wendell Holmes
HAVE you heard of the wonderful one-
hoss-shay, that was built in such a 
logical way it ran a hundred years to a 
day
Now in building of chaises, I tell you what, 
there is always somewhere a weakest spot,-- in hub, tire, felloe, in spring or 
thill, in panel, or crossbar, or floor, or 
sill</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>1041050.0010.0030.01 0.02 0.05 0.10 0.25 0.50 0.75 0.90 0.96 0.99 0.999
Data95% Confidence BoundsUAL Field Failure Weibull Plot</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Reliability Terminology
 Reliability function R(t) -- The probability 
that a product will continue to meet its 
specifications over a time interval
 Mean Time to Failure MTTF -- The 
average time Tbefore a unit fails
 Instantaneous failure rate (t)
=
0) (dt t R MTTF
)   to survives  System   to survives  System Pr( ) ( t dt t t + =

=t
de t R0) () (</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Plot of Weibull's Data
empirical frequency
050100150200250300350400450
30 32 34 36 38 40 42 44
StrengthNumber of samplesWhat P should 
we list for x
value 42?
The convention for 
probabilty plotting is
(i-1/2)/ n</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Problem Set #5
1. Parameter estimation
Make a probability plot
Make an estimate by regression
Make an MLE estimate
Estimate yet another way
Comment on "goodness of fit"
2. Hypothesis testing
Find a journal paper uing the "null ritual"
Suggest improvements (valid ity, insight, communication)</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
The Weibull Distribution and
Parameter Estimation
Dan Frey
Associate Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>0 0.5 1 1.5 2 2.5
x 10400.511.522.533.54Egypt Air Field Data
Hours of OperationNon-Failure
Failure    
0 0.5 1 1.5 2 2.5
x 104024681012UAL Field Data
Hours of OperationNon-Failure
Failure    Field Data on Engine Life
NOTE:  What should we do about the non-failed items? 
What do we lose if we censor the data?</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Looking for Further Evidence of 
Two Populations
No evidence of bi-
modality in fatigue data
Clear evidence of bi-
modality in strength dataCopyright  1951 by ASME. Used with permission.
Copyright  1951 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Weibulls Derivation
n
n P P) 1 ( 1 = x xLets define a cdf for each link meaning the link will fail at a
load Xless than or equal to xasP(Xx)=F(x)Call Pnthe probability that a chain will fail under a load of x
If the chain does not fail , its because all nlinks did not fail
If the nlink strengths are probabilistically independent
Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Simple Distributions
Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .  
Copyright  1951 by ASME. Used with permission.Copyright  1951 by ASME. Used with permission.Copyright  1951 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Influence of the Shape Parameter
s
ot t
e t R



=) (
In reliability, the following
"reliability function" is commonly defined (the 
complement of failure)
constant failure rate</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Eighteen hundred and twenty came;-- Running 
as usual; much the same. Thirty and forty at 
last arrive, And then come fifty, and fifty-five
There are traces of age in the one-hoss-shay
A general flavor of mild decay, But nothing local, as one may say. There couldn't be,--for the Deacon's art had made it so like in every part that there wasn't a chance for one to start
And yet, as a whole, it is past a doubt in another 
hour it will be worn out!</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Complex Distributions
Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .  
Copyright  1951 by ASME. Used with permission. Copyright  1951 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Reliability Growth (Duane Model)
 As newly designed equipment is refined, it 
becomes more reliable
 J. T Duane [1964] published regressions for 
aerospace items 
See Ushakov,1994, Handbook of Reliability Engineering30 Sep
197531 Mar
1976
31 Mar
197431 Mar
1977
31 Mar
197830 Sep
1976
30 Sep
1977y = 2.439 - .127x3
2
1
6 7 8 9 10 11 12
x = Ln (Cumulative Flight Hours)y = LnCumulative Failures
Cumulative Flight Hours
Reliability improvement of the F-15A, an example of Duane model growth.
Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>First a shiver, and then a thrill, Then 
something decidedly like a spill,--
What do you think the parson found, 
when he got up and stared around? The 
poor old chaise in a heap or mound, as 
if it had been to the mill and ground!  You see, of course, if you're not a 
dunce, how it went to pieces all at 
once,-- all at once, and nothing first,--
just as bubbles do when they burst. 
End of the wonderful one-hoss-shay. 
Logic is logic. That's all I say.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>A Discussion Point
What is the probability of failure 
at a load of xu? 
om
u
xx x
e x F) (
1 ) (
 =
Try to think of a situation where 
there is a physical or logical 
reason to suppose a non-zero value of x
uexists.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Concept Question
number_of_trials=10;
number_of_samples=10;
for t=1:number_of_trials
data = wblrnd(2.0,0.8,number_of_samples,1);
[paramhat, paramci] = wblfit(data);
shape(t)=paramhat(1); scale(t)=paramhat(2);
end
mean(shape)
mean(scale)
To make the mean(paramhat) equal to the parameters:
1) raise number_of_trials
2) raise number of samples
3) both must be raised
4) will not converge exactly in the limit even if both</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>But the Deacon swore .. he would build 
one shay to beat the taown 'n' the 
keounty 'n' all the kentry raoun'; It should be so built that it couldn' break daown! --"Fur," said the Deacon, "t 's mighty plain thut the weakes' place mus' stan' the strain; 'n' the way t' fix it, uz I maintain, is only jest t' make that place uz strong uz the rest"
So the Deacon inquired of the village folk 
where he could find the strongest oak, That could n't be split nor bent nor broke,--</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Series and Parallel Networks
C B AR R R R=A B CA
B
C
) 1 )( 1 )( 1 ( ) 1 (C B A R R R R=
But ONLY if statistically independent!</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Test for "Goodness of Fit"
as Conducted in Weibull's Paper
 Calculates the degrees of freedom
10 (bins) -1  3 (parameters of the df) = 6
 Calculates the statistic
 States the P-value 
 Comparison to alternative=estimatedestimated observed2
2 ) (

Note: Table is cumulative, 
2test requires frequency in binP=0.49 
for Weibull
01 0 2 000.10.2
dchisq x 6,()
0
0.2
0
0.2
x5.40
5.40,18.1718.17, 
P=0.008
for NormalCopyright  1951 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>The Bathtub Curve
TimeFailure RateConstant failure rate
R(t)=e-tInfant mortality period
Wear-out period
Failure rate rises 
with time</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>1040.0010.0030.01 0.02 0.05 0.10 0.25 0.50 0.75 0.90 0.96 0.99 0.999
95% Confidence BoundsEgypt Air Field Failure Weibull Plot</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Some Terms Related to Estimation
 Consistent  for any c
 Unbiased 
 Minimum variance()0 lim =  
 c P
n )
 =) ()
E





=2) ( ln1) var(

X fnE)MLEs 
are
MLEs are not always
MLEs are 
pretty close</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Another Good Check on Fit
 Plot the residuals
 Check for patterns
 Check for uniform variancee=y-y_hat; 
plot(y_hat, e, 'or')</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>How was the Struggle Resolved?
The reaction to his paper in the 1950s was 
negative, varying from skepticism to outright rejection... Weibull's claim that the data could select the distribution and fit the parameters seemed too good to be true. However, pioneers in the field like Dorian Shainin and 
Leonard Johnson applied and improved the 
technique. ... Today, Weibull analysis is the leading method in the world for fitting life data.
Abernathy, Robert, 2002, The New Weibull Analysis Handbook</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Point and Interval Estimates
 Up to now, we have discussed point 
estimates only  a single real value for a 
parameter
 These are fine, but sometimes one would like 
to communicate information about degree of 
confidence
 For this, interval estimates are helpful
 e.g., 95% confidence intervals on paramters</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>The Weibull Distribution
om
u
xx x
e x F) (
1 ) (
 =
More common today to see Weibull derived 
kx
e x F
 =
1 ) (Shape
parameterLocation parameter
Scale parameter
If location parameter=0, we call it the two parameter
Weibull distribution 
Weibull reported for Bofors steel m=2.93.  What is k or ?




 =x
e x F1 ) (ORother notations 
also used, be 
careful!</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .  x
[1.275 
kg/mm3]nP log(x-xu)
32 10
33 36
34 84
35 150
36 224
37 291
38 340
39 369
40 383
42 389

P11log log
How do we estimate this probability?  
What are the potential bear-traps?Note: How do we estimate this value?
Recreating Weibull's Bofors Steel Plot
-2.5-2-1.5-1-0.500.51
0.30 .50 .70 .91 .1
Copyright  1951 by ASME. Used with permission.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Constant Failure Rates
When the system operating time is the MTBF, the reliability is 37%
- Blanchard and Fabrycky
R(t)
1.0R(t)=e-t
0.5
tMTBF</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Signs of a Struggle
"The objection has been stated that this 
distribution function has no theoretical basis.  
But ... there are  with very few exceptions 
the same objections against all other df, at 
least in so far as the theoretical basis has 
anything to do with the population in question.  Furthermore, it is utterly hopeless to expect a 
theoretical basis for distribution functions of 
random variables such as ..."
Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>The Procedure Weibull used for 
Parameter Estimation
If we assume



+  = 







01log ) log() ( 11log logxx x mx Fuom
u
xx x
e x F) (
1 ) (
 =
It follows that
1. Starts with a list of values x for strength, size, life ...
2. Assigns observed probabilities PF(x)to the values
3. Transforms the Pand xvalues as indicated above
4. Fits a straight line to the data  Which is in the form of a linear realtionship, so Weibull:</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Maximum Likelihood Estimate
of the Weibull Distribution
 Write the likelihood function
leads to
Note:  But there is no closed form solution in general and 
numerical methods must be used.() 0 ) , ( ln=
k L
=
= =n
iX k
i
nk
i
eX kk X X X f k L
11
2 1 ) , ; , , , ( ) , (
 

 K
kn
ik
iXn/ 1
11

=
=

leads to () 0 ) , ( ln=k Lk1) ln( ) ln(




 =

nX
XX Xki
k
iik
i</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Weibulls 1951 Paper 
 A Statistical Distribution Function of 
Wide Applicability
 Journal of Applied Mechanics
 Key elements
 A simple, but powerful mathematical idea
 A method to reduce the idea to practice
 A wide range of data
 Why study it in this course?
Weibull, W., 1951,A Statistical Distri bution Function of Wide Applicability, J. of Appl. Mech .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Design of experiments, part 1 (PDF)
Design of experiments, part 2 (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Design of Experiments:
Part 1
Dan Frey
Assistant Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Factors Considered Initially
 
Image removed due to copyright restrictions.
TABLE 1:
 in Box and Liu, 1999. Factor Levels Used in Design I: An Initial 2S-4
IVScreening Experiment. 
and FIGURE 1: The Initial Helicopter Design</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Robust Parameter Design  is a 
statistical / engineering methodology that aims at reducing the performance variation of a system (i.e. a product or process) by choosing the setting of its control factors to make it less sensitive to 
noise variation."Robust Parameter Design
Wu, C. F. J. and M. Hamada, 2000, Experiments: Planning, Analysis, and 
Parameter Design Optimization , John Wiley &amp; Sons, NY.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>My Observations of Industry
 Farming equipent company has reliability problems
 Large blocks of robustness experiments had been 
planned at outset of the design work
 More than 50% were not finished
 Reasons given
 Unforseen changes
 Resource pressure
S a t i s f i c i n g Well, in the third experiment, we 
found a solution that met all our 
needs, so we cancelled the rest 
of the experiments and moved on 
to other tasks</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Geometric Growth of 
Experimental Effort
2345678050100150
2n
n23456780100020003000
3n
n</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Interaction plotANOVA table</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Breakdown of DOF
abn
number of yvalues
1
due to the meanabn-1
total sum of squares
a-1 
for factor Aab(n-1)
for error
b-1 
for factor B(a-1)(b-1) 
for interaction AB</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Example 5-1  Battery Life 
FF= fullfact([3 3]);
X=[FF; FF; FF; FF];
Y=[130 150 138 34 136 174  20  25 96 155 188 110 40 122 
120 70 70 104 74 159 168 80 106 150 82 58 82 180 126 160 
75 115 139 58 45 60]';
[p,table,stats]=anovan(Y,{X(:,1),X(:,2)}, 'interaction' );
hold off; hold on
for i=1:3;  for j=1:3; 
intplt(i,j)=(1/4)*sum(Y.*(X(:,1)==j).*(X(:,2)==i)); end
plot([15 70 125],intplt(:,i)); end</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Basic Terms in Factorial DOE
Response  the output of the system you are measuring 
Factor  an input variable that may affect the response 
Level  a specific value a factor may take
Trial  a single instance of the setting of factors and the 
measurement of the response
Replication  repeated instances of the setting of 
factors and the measurement of the response
Effect  what happens to the response when factor 
levels change
Interaction  joint effects of multiple factors</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Statistics as a Catalyst to Learning
 Concerned improvement of a paper 
helicopter
 Screening experiment (16)
 Steepest ascent (5) Full factorial (16) Sequentially assembled CCD (16+14=30) Ridge exploration (16) (16+5+30+16)*4 &gt; 250 experiments Resulted in a 2X increase in flight time vs
the starting point design 4 82
IVcut
fold42
Box, G. E. P. and P. T. Y. Liu, 1999, Statistics as  a Catalyst to Learning by 
Scientific Method: Part 1 , Journal of Quality Technology, 31 (1): 1-15.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Treatment Effects Model (Two Factors)

===
+ + + + =
n kb ja i
yijk ij j i ij
 , , 2  , 1 , , 2  , 1 , , 2  , 1
) (
KKK
    
0= i If factor ahas two levels 02 1=+

2 1
=
22A=
21A =</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Treatment Effects Model (Two Factors)

===
+ + + + =
n kb ja i
yijk ij j i ij
 , , 2  , 1 , , 2  , 1 , , 2  , 1
) (
KKK
    
0 ) ( ) (
1 1= =  
= =b
jija
iij
 a+bequations 
but only a+b-1
are independentinteractions  there are of ab these terms
(a-1)(b-1)DOFthis is not a product</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Hypothesis Tests in Factorial Exp
 Equality of treatment effects due to 
factor A or due to factor B
 Test statistic
 Criterion for rejecting H0i HH
ia
 one least at for  0 :0 :
12 1 0
= = = =
  K
EA
MSMSF=0
) 1 ( , 1 , 0  &gt;n ab a F Fi HHia
 one least at for  0 :0 :
12 1 0
= = = =
  K
EB
MSMSF=0
) 1 ( , 1 , 0  &gt;n ab b F F</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Estimation of the Parameters 
 X y + =
We wish to minimize the 
sum squared errorAssume the model equation
() ( )X y X y    = =T TL
To minimize, we take 
the derivative and set it 
equal to zeroX X y X 2 2
T T L+  =
The solution is() y X X X T T1=
And we define the fitted model X y =Recall from the lecture on multiple regression</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Screening Design
 What is the objective of screening?
 What is special about this matrix of 1s and -1s?Image removed due to copyright restrictions.
TABLE 2: in Box and Liu, 1999.
Design I: Layout and Data for 28 -4
IV Screening D esign</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Thought Questions
 If we optimize this thing, what does 
that mean?
 How were design parameters 
chosen?
 Were important ones missed? 
 What does Box say about variables 
being recombined to make this 
process more efficient?
 Is it reasonable to run 248 
experiments on a simple design?  Under what circumstances?
 What are the key differences 
between the process described here and system design in industry?cut
fold</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Fisher, R. A., 1926, The Arra ngement of Field Experiments,
Journal of the Ministry of Agriculture of Great Britain , 33: 503-513.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Cuboidal Representation
AB
C+
-
++
--This notation 
indicates 
observations made 
with factors at 
particular levels.
(1) (a)(b)
(c)(ab)(abc) (bc)
(ac)
Exhaustive search of the space of 
3 discrete 2-level factors is the
full factorial 23experimental design</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Central Composite Design 
ACB+
-
++
-
-runs  axial  and       points center   with 2n
Enables a model to be fit with 
all second order polynomial 
terms included (i.e. A2, AB, etc.)
by Box  run  2 here  shown  2
43</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Factor Effect Plots 
AB30 52
20 40- +-+
1020304050
A- A+B+
B-
A- +B
-+
A00</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Analysis of Variance
 What would you conclude about lack of fit?
 What is being used as the denominator of F?Image removed due to copyright restrictions.
 TABLE  10:  in Box and Liu, 1999. Design III: Analysis of Variance for Completed Composite Design</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Plan for Today
 Discussion of the reading assignment
 History of DOE
 Full factorial designs
 The design
 The model Analysis of the sum of squares
 Hypothesis testing
 Fractional factorial designs</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Hypothesis Tests in Factorial Exp
 Significance of ABinteractions 
 Test statistic
 Criterion for rejecting H0   0    one least at :,  all for   0 :
10
=
ijij
Hj i H

EAB
MSMSF=0
) 1 ( ), )( 1 ( , 0   &gt;n ab b a F F</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>George Box on Sequential 
Experimentation
Because results are usually known quickly, the natural 
way to experiment is to use information from each group of 
runs to plan the next 
Statistical training unduly emphasizes mathematics at the expense of science.   This has resulted in undue 
emphasis on one-shot statistical procedures e x a m p l e s  
are hypothesis testing and alphabetically optimal designs.
Box, G. E. P. 1999, Statistics as a Cata lyst to Learning by Scientific Method: 
Part 2 , Journal of Quality Technology, 31 (1): 16-29.</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Fractional Factorial Experiments
Tabular Representation
Trial A B C D E F G
1 -1 -1 -1 -1 -1 -1 -1
2 -1 -1 -1 +1 +1 +1 +1
3 -1 +1 +1 -1 -1 +1 +1
4 -1 +1 +1 +1 +1 -1 -1
5 +1 -1 +1 -1 +1 -1 +1
6 +1 -1 +1 +1 -1 +1 -1
7 +1 +1 -1 -1 +1 +1 -1
8 +1 +1 -1 +1 -1 -1 +1
27-4Design 
Resolution II I.FG=-A
+1
+1+1+1-1-1
-1
-1
Two-way interactions are 
aliased with main effects</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Breakdown of Sum Squares
SSdue to mean
===a
ib
jn
kijky
1112
2
...y N=
=== =a
ib
jn
kijk T y y SS
1112
...) (
ESS

= =a
ii A y y bn SS
12
... ..) (Grand Total 
Sum of Squares
Total Sum of 
Squares

= =b
jj B y y an SS
12
... . .) (
==   =a
ib
jj i ij AB y y y y n SS
112
... . . .. .) (</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Concept Test 
AB
- +-+If there are no interactions in 
this system, then the 
factor effect plot from 
this design could look like:
1020304050
AB+
B-
1020304050
AB+
B-
1020304050
AB+
B-
1 2 3
Hold up all cards that apply.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Calculating Interactions
AB
C+
-
++
--
[] ) ( ) ( ) ( ) ( ) 1 ( ) ( ) ( ) (41a c bc ab b ac abc AC     + + + (1) (a)(b)
(c)(ab)(abc) (bc)
(ac)</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>One way of thinking of the great advances 
of the science of experimentation in this 
century is as the final demise of the one 
factor at a time method , although it 
should be said that there are still 
organizations which have never heard of 
factorial experimentation and use up many 
man hours wandering a crooked path.
Logothetis, N., and Wynn, H.P., 1994, Quality Through Design: 
Experimental Design, Off-line Quality  Control and Taguchis Contributions , 
Clarendon Press, Oxford.Majority View on One at a Time</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Fractional Factorial Experiments
Cuboidal Representation
AB
C+
-
++
--
This is the 23-1fractional factorial.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Factors Re-Considered
Wing width w
Wing length l
Wing area A=lw
Wing aspect ratio Q=l/wImage removed due to copyright restrictions.
FIGURE 1:  in Box and Liu, 1999. The Initial Helicopter Design</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Treatment Effects Model versus 
the Regression Model
 If the factors are two level factors
 And they are coded as (-1,+1)
T h e nA n d



 + + + + =2 1 12 2 2 1 1 0x x x x y
1 2
= 1 1
=
12 12 ) (
 =ijk ij j i ijy



 + + ++= ) (</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Estimation of the Parameters  
when X is a 2kdesign
The columns are orthogonal() y X X X T T1=
() j iijT = if   0 X X
() j i nk
ijT= = if   2 X X
() I X XkT
n21 1=[]1y XT
AB
C+
-
++
--</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Calculating Main Effects
AB
C+
-
++
--
[] ) 1 ( ) ( ) ( ) ( ) ( ) ( ) ( ) (41    + + + bc c b a ac ab abc A(1) (a)(b)
(c)(ab)(abc) (bc)
(ac)</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Three Level Factors
AB
C
33Design8 vertices +
12 edges +
6 faces +
1 center =
27 points</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Regression  Battery Life 
FF= fullfact([3 3]);
A=FF(:,1)-2;  B=FF(:,2)-2;  ones(1:3*3)=1;R=[ones' A B A.*A B.*B A.*B ];X=[R; R; R; R];
Y=[130 150 138 34 136 174  20  25 96 155 188 110 40 122 
120 70 70 104 74 159 168 80 106 150 82 58 82 180 126 160 
75 115 139 58 45 60]';
[b,bint,r,rint,stats] = regress(Y,X,0.05);
[t,m] = meshgrid(-1:.1:1,-1:.1:1);
Yhat= b(1)+b(2)*t+b(3)*m+b(4)*t.*t+b(5)*m.*m+b(6)*t.*m;
hold off; h=plot3(t,m,Yhat);hold on; scatter3(X(:,2),X(:,3),Y);</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Fractional Factorial Experiments
Cuboidal Representation
AB
C+
-
++
--
[] ) ( ) 1 ( ) ( ) (21bc ac ab A   + =(1)(ac)(bc)
(ab)</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Effect Estimates
Image removed due to copyright restrictions.
TABLE 3: in Box and Liu, 1999.
Design I: Estimates for a 28 -4
IV    Screening Design</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Tabular Representation
Trial A B C
1 -1 -1 -1
2 -1 -1 -1
3 -1 +1 +1
4 -1 +1 +1
5 +1 -1 +1
6 +1 -1 +1
7 +1 +1 -1
8 +1 +1 -1
23Design A cube has 
eight vertices</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>One at a Time Experiments
AB
C+
-
++
--
Provides resolution of individual factor effects
But the effects may be biased
) 1 ( ) ( a A(1) (a)(b)
(c)If the standard 
deviation of 
(a) and ( 1) is ,
what is the standard 
deviation of A?</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>"Steepest" Ascent
 What does 
"steep" 
mean in this 
context?
Image removed due to copyright restrictions.
FIGURE 4:
 in Box and Liu, 1999.Data for 5 Helicopters on the path of Steepest Ascent Calculated
from Design 1</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Next Steps
 Friday 27 April
 Recitation to support the term project
 Monday 30 April
 Design of Experiments: Part 2
 Wednesday 2 May
 Design of Computer Experiments
 Friday 4 May  
 Exam review
 Monday 7 May  Frey at NSF
 Wednesday 9 May  Exam #2</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Cross (or Product) Arrays
ABCDEFG
1-1 -1 -1 -1 -1 -1 -1
2-1 -1 -1 +1 +1 +1 +1
3-1 +1 +1 -1 -1 +1 +1
4-1 +1 +1 +1 +1 -1 -1
5+1 -1 +1 -1 +1 -1 +1
6+1 -1 +1 +1 -1 +1 -1
7+1 +1 -1 -1 +1 +1 -1
8+1 +1 -1 +1 -1 -1 +1Control Factorsa-1 -1
b-1
c-1 +1 +1 -1+1 +1 -1+1 +11 32
III
4 72
III
1 3 4 72 2 III IIINoise Factors
Taguchi, G., 1976, System of Experimental Design .</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Concept Test
AB
C+
-
++
--
[] ) 1 ( ) ( ) ( ) ( ) ( ) ( ) ( ) (41    + + + bc c b a ac ab abc A(1) (a)(b)
(c)(ab)(abc) (bc)
(ac)If the standard 
deviation of
(a), (ab), et cetera 
is , what is the 
standard deviation of 
the main effect 
estimate A ?
1)  2) Less than  3) More than  4) Not 
enough info</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Plan for Today
 Discussion of the reading assignment
 History of DOE Full factorial designs
 The design
 The model Analysis of the sum of squares Hypothesis testing
 Other designs
 Fractional factorial designs Central composite designs</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Plan for Today
 Discussion of the reading assignment
 History of DOE Full factorial designs
 The design
 The model Analysis of the sum of squares Hypothesis testing
 Other designs
 Fractional factorial designs Central composite designs</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Response Surface 
Methodology
 A method to seek improvements in a 
system by sequential investigation and 
parameter design
 Variable screening
 Steepest ascent
 Fitting polynomial models
 Empirical optimization
Box, G. E. P. and Wilson, K. B. (1951), On the Experimental Attainment of Optimum 
Conditions , Journal of the Royal Statistical Society , B13, 1-38.</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Overview Research
Complex 
SystemsMethodology 
ValidationConcept 
DesignOutreach 
to K-12
Adaptive Experimentation 
and Robust Design
B D
AB AD
ABCABD
ABCDA
ACmain effects
two-factor interactions
BC BD CDC
three-factor interactions
ACD BCD
four-factor interactionsB D
AB AD
ABCABD
ABCDABCDA
ACmain effects
two-factor interactions
BC BD CDC
three-factor interactions
ACD BCD
four-factor interactions() 
+  ++




 
+  +










&gt; &gt; &gt;
01 2
2 2 221) 2 ( 2212
1
12 12
22 2 22
2
22
1
2 1
21) 2 (21
210 Pr dx dx
nexerfnx x
x
INT ME INTnx x n
INT
ijINT MEINT 

 
 


 
PBS show 
Design Squad</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>An experiment is simply a question put to nature 
 The chief requirement is simplicity: only one 
question should be asked at a time.
Fisher, R.A., 1921, Studies in Crop Variation. I. An Examination of the Yield of Dressed Grain from 
Broadbalk, Journal of Agricultural Science 11:107-135.Russell, E. J., 1926, Field experiments: How they are made and what 
they are, Journal of the Ministry of Agriculture 32:989-1001.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Concept Question
[] ) 1 ( ) ( ) ( ) ( ) ( ) ( ) ( ) (41    + + + bc c b a ac ab abc AAB
C+
-
++
--(1) (a)(b)
(c)(ab)(abc) (bc)
(ac)Say the independent 
experimental error of observations
(a), (ab), et cetera is .
We define the main effect 
estimate  to be 221  ) 1=AWhat is the standard deviation of the main effect estimate A? 
 41  ) 2=A 8   ) 3=A
=A  ) 4</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Creating and 
Randomizing Full Factorials 
in Matlab11
213141
12
223242
13
233343
X = fullfact([4 3]);
r=rand(1,4*3);
[B,INDEX] = sort(r);
Xr(1:4*3,:)=X(INDEX,:);32
11
312221
12
431333
41
4223X Xr</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>To call in the statistician after the 
experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
- Fisher, R. A., Indian Statistical Congress, Sankhya, 1938.</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Efficiency
 The variance for OFAT is  
using 4 experiments
 The standard deviation for 23-1was 
using 4 experiments
 The inverse ratio of variance per unit is 
considered a measure of relative efficiency
 The 23-1is considered 2 times more efficient 
than the OFAT2
[][]24 4222
=</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Today
 Discussion of the reading assignment
 History of DOE
 Full factorial designs
 The design
 The model
 Analysis of the sum of squares
 Hypothesis testing
 Other designs
 Fractional factorial designs
 Central composite designs</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Normal Probability Plots
 What's the purpose of these graphs?Image removed due to copyright restrictions.
FIGURE 2: . in Box and Liu, 1999. Design I - Normal Plots for: (a) Location effects from y  and (b) Dispersion Effects from 100 log(s)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Derived distributions to statistics (PDF) (Courtesy of Aman Chawla. Used with permission.)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>6/20/2007 3Features of Bridge
Chi-squared random variable
---- central role in ----
Chi-Square statistical test</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6/20/2007 9The Data
No. of Dice with 5 or 
6 pointsObserved
0 185
1 1149
2 3265
3 5475
4 6114
5 5194
6 3067
7 1331
8 403
9 105
10 14
11 4
12 0
Total: 26306</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6/20/2007 10Illustration (contd.)
Under the fairness hypothesis, compute 
probabilities
Pr(No die with 5 or 6 points in a throw of 12 dice) = (2/3)^12
Pr(k dice with 5 or 6 points in a throw of 12 dice) = 
k k
kC




12
12
32
31</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6/20/2007 13Illustration (contd.)
Under fairness hypothesis, each 
observed value is a multinomial r.v.
By Central Limit Theorem, since n=26306 is large, this can be thought of as a Normal random variable.
Subtracting the expected value, squaring and dividing by the expected value gives a standard normal random variable
( )8724 1 . 432
2==
k pected epected e observed</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6/20/2007 14Chi-test
Thus Chi-statistic has Chi-squared 
distribution of order 12.
Using knowledge of Chi-squared 
distribution, compute the Probability that 
a Chi-squared order 12 r.v. takes on a 
value greater than the observed value.
If this Probability is large, it implies that 
the observed value of Chi-stat is typical
Since the statistic measures the 
deviations between observed data and 
values expected under fairness 
hypothesis, this implies that the fairness hypothesis is not unwarranted.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6/20/2007 8Pearsons Illustration
12 dice are thrown
Number of dice that show up with a 
5 or 6 is counted.
This experiment is repeated a total of 26,306 times 
Motivation: Determine fairness of dice.
Fair die has equal probability of landing on any one of its 6 faces.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6/20/2007 4The Chi-Squared 
Random Variable
Defined as the sum of the squares of n 
independent standard normal random variables. 
Derive the distribution for n=2 today.
For general n,
()
() )! 1 ( ) () (
21) (
0122 22
22
) (
 =  = =



n n dt e t xz U e z z f
t xnz n
n
n</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6/20/2007 1Derived 
Distributions to 
Statistics
Aman Chawla
LIDS</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6/20/2007 50 1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.912 pdf with 1 through 5 degrees of freedom
1 dof
2 dof
3 dof
4 dof
5 dof
Figure 1: Probability Density Function of the Chi-Squared Distribution with 1 through 5 degrees of freedom.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6/20/2007 12Data
No. of Dice with 5 or 
6 pointsObserved Expected Deviation
0 185 203 -18
1 1149 1217 -68
2 3265 3345 -80
3 5475 5576 -101
4 6114 6273 -159
5 5194 5018 +176
6 3067 2927 +140
7 1331 1254 +77
8 403 392 +11
9 105 87 +18
10 14 13 +1
11 41+ 3
12 000
Total: 26306 26306</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6/20/2007 60 1 2 3 4 5 6 7 800.10.20.30.40.50.60.70.80.912 cdf with 1 through 5 degrees of freedom
1 dof
2 dof
3 dof
4 dof
5 dof
Figure 2: Cumulative Distribution Function of the Chi-Squared Distribution with 1 through 5 degrees of f reedom.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6/20/2007 2Introduction
Seen Applied Probability in course 
so far
Next part of course: Statistics
Today: A Bridge</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6/20/2007 7Chi-Squared Test
One of the most widely used 
statistical tests
Derived in 1900 by Karl Pearson
An Illustration, used by Pearson 
himself, serves well to elucidate.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6/20/2007 11Illustration (contd.) 
Expected number of trials yielding 
0 dice with 5 or 6 points = 26306 * 
(2/3)^12 = 202.7495, etc.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Multiple regression (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>Why Hypothesis Testing is 
Important in Multiple Regression
 Say there are 10 regressor variables
 Then there are 11 coefficients in a linear 
model
 To make a fully 2ndorder model requires 
 10 curvature terms in each variable
 10 choose 2 = 45 interactions
 Youd need 68 samples just to get the matrix 
XTXto be invertible
 You need a way to discard insignificant terms</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>See What Happens if We 
Remove Variables
 Remove weight &amp; temp
 Do the regression (CO2 vs TAS &amp; alt)
 Examine the betas and their intervals
[b,bint,r,rint,stats] = regress(y,X(:,1:3),0.05);</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Studentized Residuals
The residuals are defined as
So the covariance matrix
of the residuals isy y e=
) 1 ( 2
iii
iher
=The studentized residuals
are defined as) ( Cov ) ( Cov2H I e  =
y H I Hy y e) (== therefore
If these elements were z-scores then with probability 99.7%
3 3&lt;&lt;ir ????</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>X(:,5)=X(:,2).*X(:,3);This line will add a  
interactionAdding Interactions
Whats the effect 
on the 
regression?</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The Model Equation  X y+=




=
nyyy
M21
y




=
nk n nkk
x x xx x xx x x
LM O M M MLL
2 12 22 211 12 11
111
X




=
n
M21
Each row of X
is paired with 
an observation
There are n
observations of 
the responseEach observation 
is affected by an 
independent 
homoscedastic
normal variates



=
k
M10
 There are kcoefficientsEach column of X
is paired with a 
coefficient0 ) (=iE
2) ( =i Var</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Estimation of the Error Variance 2
 X y+= Remember the the model equation
If assumptions of the 
model equation hold, then
So an unbiased
estimate of 2is) 1 ( 2  =k n SSE
( )2) 1 (=   k n SS EE) , 0 ( ~ N</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Adding h.o.t. to the Model Equation




=
nyyy
M21
y




= 2
1 2 1 2 12
21 22 21 22 212
11 12 11 12 11
111
n n n n nx x x x xx x x x xx x x x x
M O M M MXEach row of X
is paired with 
an observation
There are n
observations of 
the responseYou can add 
interactions You can add 
curvature 



=1112210</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Phase One
 Open a Matlab window
 Load the data (load FAAcase3.mat)
 Explore the data</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Case Wrap-Up
 What were the recommendations?
 What other analysis might be done?
 What were the key lessons?</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>dims=size(X);
i=2:dims(1)-1;
climb(1)=1;
climb(dims(1))=0;
des(1)=0;
des(dims(1))=1;
climb(i)=(alt(i)&gt;(alt(i-1)+ 100))|(alt(i+1)&gt;(alt(i)+100));
des(i)=(alt(i)&lt;(alt(i-1)-100))|(alt(i+1)&lt;(alt(i)-100));
for i=dims(1):-1:1
if climb(i)|des(i) 
y(i,:)=[]; X(i,:)=[]; yhat(i,:)=[]; r(i,:)=[];
end
end
hold off
plot(yhat,r,'or')
This code will remove the 
points at which the aircraft 
is climbing or descending</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>R2 and Adjusted R2
TE
TR
SSSS
SSSSR  = 12
) 1 (11) 1 () (12 2Rp nn
n SSp n SSR
TE
adj 



 = 
can rise or drop as parameters are added2
adjRWhat fraction of the total sum of squares (SST) 
is accounted for jointly by all the parameters 
in the fitted model?a.k.a. coefficient of 
multiple determination
R2can only rise as 
parameters are 
added</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Scenario
 The FAA and EPA are interested in reducing 
CO2 emissions
 Some parameters of airline operations are 
thought to effect CO2 (e.g., Speed, Altitude, 
Temperature, Weight)
 Imagine flights have been made with special 
equipment that allowed CO2 emission to be measured (data provided)
 You will report to the FAA and EPA on your 
analysis of the data and make some recommendations</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Concept Test
 You perform a linear regression of 100 data points 
(n=100).  There are two independent variables x1
and x2.  The regression R2is 0.72.  Both 1and 2
pass a ttest for significance.  You decide to add the 
interaction x1x2to the model.  Select all the things 
that cannot happen:
1) Absolute value of 1decreases 
2) 1changes sign 
3) R2decreases 
4) 1fails the ttest for significance</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>The Model Equation
 X y+=For a single variable
For multiple variables

++= x Y




=
nk n nkk
x x xx x xx x x
LM O M M MLL
2 12 22 211 12 11
111
X




=
k
M10





=
n
M21





=
nyyy
M21
yis renamed 0
These 1s allow 0to enter the equation without being mult by xs1+=k p</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Standardized Residuals
The residuals are defined as
So an unbiased 
estimate of 2is ) ( 2p n SSE =
y y e=ed= The standardized residuals are defined as
If these elements were z-scores then with probability 99.7%
3 3&lt;&lt;id</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Try The Regression Again on 
Cruise Only Portions
 What were the effects on the residuals?
 What were the effects on the betas?
hold off
[b,bint,r,rint,stats] = regress(y,X,0.05);
yhat=X*b;plot(yhat,r,'+')</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Factorial Experiments
Cuboidal Representation
x1+
-
+-+
-
Exhaustive search of the space of discrete 2-level factors is the
full factorial 23experimental designx2
x3</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Adding Center Points
x1+
-
+-+
-
Center points allow an experimenter to check for curvature 
and, if replicated, allow for an estimate of 
pure experimental errorx2
x3</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Plan for Today
M u d  c a r d s
 Multiple Regression
 Estimation of the parameters
 Hypothesis testing Regression diagnostics Testing lack of fit 
 Case study
 Next steps</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Breakdown of Sum Squares
SSdue to mean
==n
iiy GTSS
12
2y n=
= =n
ii T y y SS
12) (

==n
ii ESS
12e 
= =n
ii R y SS
12) (y
PESSLOFSSGrand Total 
Sum of Squares</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Estimation of the Parameters 
 X y+=
We wish to minimize the 
sum squared errorAssume the model equation
()()X y X y    = =T TL
To minimize, we take 
the derivative and set it 
equal to zeroX X y X 2 2
T T L+  =
The solution is() y X X X T T1=
And we define the fitted model X y =</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Next Steps
 Wenesday 25 April
 Design of Experiments
 Please read "Statistics as a Catalyst to Learning"
 Friday 27 April
 Recitation to support the term project
 Monday 30 April
 Design of Experiments
 Wednesday 2 May
 Design of Computer Experiments
 Friday 4 May??  Exam review??
 Monday 7 May  Frey at NSF
 Wednesday 9 May  Exam #2</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MathCad
Demo
Montgomery 
Example 10-1
Montgomery, D. C., 2001, Design 
and Analysis of Experiments , John 
Wiley &amp; Sons.Done in MathCad:</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>The Hat Matrix
() y X X X T T1=
So we defineX y =
()T TX X X X H1() y X X X X yT T1=
Hy y=Which maps from 
observations y to 
predictions  ySince
and
therefore</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Test for Significance 
Groups of Coefficients
) () (2 1
0p n SSr SSF
ER
= Reject H0ifp n r F F&gt;, , 0
  X y+=2 2 Reduced model
2
2 2) ( y n SST
R  =y H y The regression sum of squares 
for the reduced model is
) ( ) ( ) (2 2 1    R R R SS SS SS  Define the sum squares of the removed set given the other coefficients are in the model
The partial 
Ftest</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Multiple Regression 
Dan Frey
Associate Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Excel Demo -- Montgomery Ex10-2
Montgomery, D. C., 2001, Design and Analysis of Experiments , John Wiley &amp; Sons.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Back to 
MathCad
Demo
Montgomery 
Example 10-1
Montgomery, D. C., 2001, Design 
and Analysis of Experiments , John 
Wiley &amp; Sons.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Test for Significance 
Individual Coefficients
jjj
Ct
20=
Reject H0if1 , 2 / 0 &gt;k n t tThe hypotheses are
 0 :0 :
10
=
jj
HH
The test statistic is()1= X XTC
Standard error
jjC2</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Influence Diagnostics
 The relative disposition of points in xspace 
determines their effect on the coefficients
 The hat matrix H gives us an ability to check for 
leverage points
hijis the amount of leverage exerted by point yjon 
 Usually the diagonal elements ~ p/nand it is good to 
check whether the diagonal elements within 2X of 
that iy</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Testing for Lack of Fit
(Assuming a Central Composite Design)
 Compute the standard deviation of the 
center points and assume that 
represents the MSPE
PELOF
MSMSF=0()
1
=
Ci
PEny y
MSpoints  center
PE PE MS n SS) 1 ( =
E LOF PE SS SS SS= +pSSMSLOF
LOF=</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Breakdown of DOF
n
number of yvalues
1
due to the meann-1
total sum of squares
k
for the regression n-k-1
for error</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Test for Significance of 
Regression
The hypotheses are
j HH
jk
 one  least  at  for  0 :0 :
12 1 0
====


 K
The test statistic is
) 1 (0 =k n SSk SSF
ER
Reject H0if 1 , , 0 &gt;k n k F F</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Phase Three
 Try different data (flight34.mat)
 Do the regression
 Examine the betas and their intervals
 Plot the residuals
y=[fuel_burn];
ones(1:34)=1;
X=[ones' TAS alt temp];
[b,bint,r,rint,stats] = regress(y,X,0.05);yhat=X*b;plot(yhat,r,'+')</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Phase Two
 Do the regression
 Examine the betas and their intervals
 Plot the residuals
y=[CO2./ground_speed];
ones(1:3538)=1;
X=[ones' TAS alt temp weight];[b,bint,r,rint,stats] = regress(y,X,0.05);yhat=X*b;plot(yhat,r,'+')</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>MathCad Demo
on Distribution of 
Samples and Its 
Effect on 
Regression</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Concept Question
Which of these is a valid Xmatrix?




=
sec 4 . 0 4 . 5 1sec 7 . 0 2 . 3 1sec 2 . 0 1 . 7 1sec 3 . 0 0 . 5 1
mmmm
X




=
A AV Vm m
4 . 0 4 . 5 1sec 7 . 0 sec 2 . 3 12 . 0 1 . 7 13 . 0 0 . 5 1
X
=sec 3 . 0 1 . 7 1sec 1 . 0 0 . 5 1
mmX
A C B
1) A only
2) B only
3) C only4) A and B
5) B and C
6) A and C7) A, B, &amp; C
8) None
9) I dont know</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Plan for Today
M u d  c a r d s
 Multiple Regression
 Estimation of the parameters
 Hypothesis testing
 Regression diagnostics
 Testing lack of fit 
 Case study
 Next steps</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Today
 Multiple Regression
 Estimation of the parameters
 Hypothesis testing
 Regression diagnostics
 Testing lack of fit 
 Case study
N e x t  s t e p s</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Accounting for Indices
 X y+=




=
nyyy
M21
y




=
nk n nkk
x x xx x xx x x
LM O M M MLL
2 12 22 211 12 11
111
X




=
k
M10
1




=
n
M21
+=k pnx1 nxp px1 nx1</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Test for Significance of 
Groups of Coefficients

=
21
 Partition the coefficients into two groupsto be removed
to remain
  X y+=2 2Reduced model




=
nk n nkk
x x xx x xx x x
LM O M M MLL
2 12 22 211 12 11
111
X
Basically, you form X2by removing the columns associated 
with the coefficients you are testing for significance 2X00
=
1 11 0
::
HH</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Markov processes and their application to queueing, part 1 (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec8/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>Littles Law for Queues
a(t)=cumulative # arrivals to s ystem in (0, t]
d(t)=cumulative #  departures from system in (0, t]
L(t)=a(t)d(t)
L(t)=number of customers in the system 
           (in queue and in service) at time td(t)a(t)
L(t)
Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>t= timeCumulative # of Arrivals
0L(t)FCFS
SJF
LSJF(t)FCFS=First Come, First Served
SJF=Shortest Job First
What about LJF,
Longest Job 1st?</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Markov Processes and their 
Application to Queueing
Richard C. Larson
March 5, 2007
Photo courtesy of Johnathan Boek e. http://www.flickr.com/photos/boeke/134030512/</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Nearest Neighbors:  Euclidean
Define D2= distance from a random point
to nearest Poisson entity
Want to derive fD2(r).
r
Random 
PointHappiness:
FD2(r)P{D2r}=1P{D2&gt;r}
FD2(r)=1Prob{no Poisson entities in circle of radius r}
FD2(r)=1er2  r0
fD2(r)=d
drFD2(r)=2rer2  r0
Rayleigh pdf with parameter 2</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>System is 
General
Our results apply to entire queue 
system, queue plus service facility
But they could apply to queue only!
Or to service facility only!L=W
S.F. Lq=Wq
LSF=WSF=/
1/=mean service time</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Lets Get an expression for Each of 3 Quantitiestavera ge customer arrival rate =a(t)/t
Wtavera ge time that an arrived customer has spent in the s ystem
Wt=(t)/a(t)
Lt=time average #  customers in system during (0,t]
Lt=1
tL()d=(t)/t
0t
Lt=(t)
t=a(t)
t(t)
a(t)=tWt
In the limit,
L=W,    Little's Law</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Utilization Factor 
Similar logic for Nidentical parallel 
servers gives
Here, /corresponds to the time-
average number of servers busy=(
N)1=
N &lt;1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Nearest Neighbors:  Euclidean
Define D2= distance from a random point
to nearest Poisson entity
Want to derive fD2(r).
r
Random 
PointfD2(r)=d
drFD2(r)=2rer2  r0
Rayleigh pdf with parameter 2E[D2]=(1 / 2)1   "Square Root Law"
D22=(2/2)1
2</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Queue of Waiting CustomersQueueing System
Arriving Customers
Departing CustomersService 
Facility
Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>How Might you Derive the PDF 
fo the kthNearest Neighbor?
Blackboard exercise!</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>To Queue or Not to Queue,
That May be a Question!</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Utilization Factor 
Single Server.  Set
E[Y] is time-average number of 
customers in the SF
Buy Littles Law, Y={1 if server is busy
0 if server is idle
E[Y]=1*P{server is busy} +0*P{server is idle}
E[Y]=1*+0==E[#  customers in SF] =?
=/&lt;1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>http://zappa.nku.edu/~longa/geome d/modules/ss1/lec/poisson.gifSpatial
Poisson
Processes
Courtesy of Andy Long. Used with permission.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>More Issues
Littles Law is general. It does not 
depend on
 Arrival process
 Service process
#  s e r v e r s
 Queue discipline
 Renewal assumptions, etc.
It just requires that the 3 limits exist.L=W</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Balance of Flow Equations
To be continued..
0P0=1P1
(n+n)Pn=n1Pn1+n+1Pn+1 for n=1,2,3,...</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>All of this means,
You buy one, you get the other 3 for free!
W=1+Wq
L=Lq+LSF=Lq+
L=W</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Outline
Spatial Poisson Processes, one more time
Introduction to Queueing Systems
Littles Law
Markov Processes</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Littles Law for Queues
a(t)=cumulative # arrivals to s ystem in (0, t]
d(t)=cumulative #  departures from system in (0, t]
L(t)=a(t)d(t)
L(t)=number of customers in the system 
           (in queue and in service) at time t
(t)=[a()d()]d
0t =L()d
0t
(t)=total number of customer minutes spent in the system</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Key Issues
Lin a time-average.  Explain
is average of arrival rate of customers 
who actually enter the system
Wis average time in system (in queue 
and in service) for actual customers 
who enter the systemL=W</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Markov Queues
Markov here means, No Memory</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Finite or 
Infinite?Finite or Infinite?
Queue 
Discipline:How queuers
Are selected 
for serviceServers:
Statistical Clones?
Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>What Kinds of Queues Occur in 
Systems of Interest to ESD?
ESD 
Queues?
Photos courtesy, from top left, clockw ise: U.S. FAA: F lickr user *keng http://www.flickr.com/p hotos/kengz/67187556/ ; 
Luke Hoersten http://www.flickr.com /photos/lukehoersten/532375235/)</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Spatial Poisson Processes
Entities distributed in space (Examples?)
Follow postulates of the (time) Poisson 
process
dt = Probability of a Poisson event in dt 
History not relevant
What happens in disjoint time intervals is 
independent, one from the other
The probability of a two or more Possion events in dt is second order in dt  and can be ignored 
Lets fill in the spatial analogue..</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>SSet S has area A (S).
Poisson intensity is 
Poisson entities/(unit area).
X(S) is a random variable 
X(S) = number of Poisson
entities in S
P{X(S)=k}=(A(S))k
k!eA(S), k=0,1 ,2,...</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Nearest Neighbor:  Taxi Metric
rFD1(r)P{D1r}
FD1(r)=1Pr{no Poisson entities in diamond}
FD1(r)=1e2r2
fD1(r)=d
drFD1(r)=4re2r2</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Still More 
Issues
What about balking?  Reneging? Finite 
capacity?
Do we need iid service times? Iid inter-arrival times?
Do we need each busy period to behave statistically identically?
Look at role of (t).  Can change queue 
statistics by changing queue discipline.L=W</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Pedestrian crossing problem (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec4/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>Problem Framing, Formulation 
and Solution</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>2. Probability that zero pedestrians 
cross left to right on any dump</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Pedestrian Crossing Problem
Richard C. Larson
February 20, 2007</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>For each decision rule, determine:
1.Expected number of pedestrians 
crossing left to right on any dump
2.Probability that zero pedestrians cross left to right on any dump
3.The pdf for time between dumps
4.Expected time that a randomly arriving customer must wait until crossing
5.Expected time that a randomly arriving observer, who is not a pedestrian, will wait until the next dump</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>A Rough Model of 77 Massachusetts Avenue
Image: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Learning Objectives
Problem Framing, Formulation and Solution
Review of conditional probability
Review of Poisson Processes
Introduction to Random Incidence
Reference:  Urban Operations Research,
Chapter 2, Sec. 2.14 
http://web.mit.edu/urban_or_book /www/book/chapter2/2.14.html</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>"To shape, fashion or form"
"To put together the parts of"
"To enclose in a border"Frame:
New World Dictionary</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Rule A:  Dump Every T Minutes (open loop control)</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>A Rough Model of 77 Massachusetts AvenueTwo independent 
Poisson Processes</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>A Rough Model of 77 Massachusetts Avenue= waiting pedestrian
= queuer</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>4. Expected time that randomly arriving 
customer must wait until crossing</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>5. Expected time that a randomly 
arriving observer, who is not a 
pedestrian, will wait until next dump</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Today we work out 
the answers 
together on the 
blackboard!</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>3. The pdf for time between dumps</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Rule C:  Dump Whenever Longest Wait = ToMin.
(again closed loop control)</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>1.  Expected number of pedestrians 
crossing left to right on any dump</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Rule B:  Dump When Pedestrian Count = No
(closed loop control)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Regression (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>15</slideno>
          <text>The Method of Least Squares

=n
iie
12
i i iy y e=(x17,y17)e17Given a set of ndata 
points (x ,y) pairs
There exists a unique
line 
that minimizes the 
residual sum of squaresbx a y+ = 

==n
ii e ens
12 2
21</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>What can we do with Regression?
Linear force Applied by Prosthetic Hand versus 
Input Cable Tension
012345678
0 5 10 15 20 25 30 35
cable tension, lbslinear force, lbsCalibrate a measuring device
Characterize a Product
Evaluate a Conjecture
The Effect of Coupling and Scale on Completion Time 
01020304050
123456
Number of VariablesNormalized Completion 
Time
Full Matrix Uncoupled Matrix</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Regression Toward the Mean
) 1 ( 22
222 2
1 21) , (
  
=y xy x
e y x fConsider the joint pdf of two standard normal variates
Now, let's say you make an observation xand condition 
your marginal distribution of y
2 ) 1 ( 22
22
22 2
21
1 21
) () , () (x y xy x
xe ex fy x fx y f 
= =
 
) 1 , ( ~2 x N ythe mean of yis less far from the 
mean than the observed value X</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Inferences Based on the Least 
Squares Estimators
xx
eSsbt) (=
is a random variable having the tdistribution 
with n-2 degrees of freedom</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Next Steps
 Friday, 13 April
 Session to support the term project
 Be prepared to stand up and talk for 5 minutes 
about your ideas and your progress
 16-17 April, No classes (Patriot's Day)
 Wednesday 18 April 
 Efron, "Bayesians, Frequentists, and Scientists"
 Analysis of Variance
 Friday 20 April, recitation (by Frey)
 Monday 23 April, PS#6 due</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>The Bootstrap
 A random smaple of size nis observed 
from a completely unspecified probability 
distribution F
 Given a random variable R(X,F), 
estimate the sampling distribution on R
on the basis of he observed data x
Efron, B., 1979, "Bootstrap Methods: Another Look at the 
Jackknife," Annals of Statistics 7:1-26. F X x Xi i iind~    ,=</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Assumptions Required for 
Inferences to be Discussed
 The Yiare 
 independent
 normally distributed with means  and common variance (homoscedastic)i i i X Y

++=
iX
+</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Plan for Today
 Regression 
 History / Motivation
 The method of least squares Inferences based on the least squares 
estimators
 Checking the adequacy of the model  The Bootstrap Non-linear regression</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>What is Linear Regression?

++= x Yrandomtheoretical 
parametersindependent 
variable1. Form a probabilistic model
2. Get a sample of data in pairs (Xi,Yi), i=1...n
3. Estimate the parameters of the model from the datarandom
E()=0</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Concept Question
In hospital (A), 45 babies ar e born each day (on average) 
and in the smaller hospital (B) about 15 babies are born 
each day (on average). 
Let's model births as a Bernoulli process and both hospitals 
have p=0.5 (baby boys and girls equally probable).
For a period of a year, each hospital recorded the days on 
which more than 60% of the babies were boys. 
1) Hospital A probably recorded  more days with &gt;60% boys
2) Hospital B probably recorded  more days with &gt;60% boys
3) Hospital A and B are probably about the same</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Regression Curve of Yon x


++= x Yrandomtheoretical 
parameters
independent 
variablerandom
E()=0</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Checking the Assumptions
 Plot the residuals
 Check for patterns
 Check for uniform variancehold off
e=y-y_hat; plot(y_hat, e, 'or')</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>The Bootstrap
 Sampling with 
replacement
load lawdata
figure(1); plot(lsat,gpa,'+')
lslinerhohat = corr(lsat,gpa);rhos1000 = bootstrp(1000,'corr',lsat,gpa);[n,xout]=hist(rhos1000,30); 
figure(2); bar(xout,n); hold on;
plot([rhohat rhohat],[0 max(n)],'-rx');540 560 580 600 620 640 660 6802.72.82.933.13.23.33.43.5
LSAT scoresLaw school GPAs
0 0.2 0.4 0.6 0.8 1020406080100120</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Plan for Today
 Regression 
 History / Motivation
 The method of least squares Inferences based on the least squares 
estimators
 Checking the adequacy of the model  The Bootstrap Non-linear regression</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Regression Curve vs Prediction Equation

++= x YrandomRandom
E()=0theoretical 
parameters
independent variable
bx a y+=
computed estimates of 
theoretical parameters and estimated E(Y X)Regression Curve
Prediction Equation</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Concept Question
You are seeking to calibrate a load cell.  You wish to 
determine the regression line relating voltage (in Volts) to force (in Newtons).  What are the units of 
a, b, Sxx, and Sxyrespectively?
1) N, N, N, and N 
2) V, V, V2, and V2
3) V, V/N, N2, and VN
4) V/N, N, VN, and V2
5) None of the variables have units</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Matlab Code for Regression
p = polyfit(x,Y,1)
y_hat=polyval(p,x);
plot(x,y_hat,'-','Color', 'g')</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Evaporation vs Air Velocity
Confidence Intervals for Prediction
Air vel
(cm/sec)Evap coeff. 
(mm2/sec)
20 0.18
60 0.37
100 0.35
140 0.78
180 0.56
220 0.75
260 1.18
300 1.36
340 1.17
380 1.65[p,S] = polyfit(x,y,1);
alpha=0.05;
[y_hat,del]=polyconf(p,x,S,alpha); plot(x,y,'+',x,y_hat,'g')hold onplot(x,y_hat+del,'r:')plot(x,y_hat-del,'r:')</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Regression Curve vs Prediction Equation

++= x YrandomRandom
E()=0theoretical 
parameters
independent variable
bx a y+=
computed estimates of 
theoretical parameters and estimated E(Y x)Regression Curve
Prediction Equation</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>The Bootstrap
 In the Acknowledgements:
...I also wish to thank the many freinds who 
suggested names more colorful than Bootstrap , 
including Swiss Army Knife , Meat Axe , Swan-Dive , 
Jack-Rabbit, and my personal favorite, the 
Shotgun , which, to paraphrase Tukey, "can blow 
the head off any problem if the statistician can stand the resulting mess."
Efron, B., 1979, "Bootstrap Methods: Another Look at the 
Jackknife," Annals of Statistics 7:1-26.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>What can we do with Regression?
ExtrapolateSuggest a Trend Diagnose a Problem
Source: Wikipedia. Courtesy of globalwarmingart.com.Photo and screen shot removed due to copyright restrictions.
Calibration apparatus by Renishaw plc.</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Concept Question
When you carry out an exponential regression by 
transforming the dependent variable, the resulting regression curve minimizes the sum squared error of the residuals as plotted here.
1) TRUE 
2) FALSE</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Evaporation vs Air Velocity
Hypothesis Tests
Air vel
(cm/sec)Evap coeff. 
(mm2/sec)
20 0.18
60 0.37
100 0.35
140 0.78
180 0.56
220 0.75
260 1.18
300 1.36
340 1.17
380 1.65Air vel (c mEvap coeff. (mm2/sec)
20 0.18 SUMMARY OUTPUT
60 0.37
100 0.35 Regression Statistics
140 0.78 Multiple R 0.934165
180 0.56 R Square 0.872665
220 0.75 Adjusted R Square 0.854474
260 1.18 Standard Error 0.159551
300 1.36 Observations 9
340 1.17
ANOVA
df SS MS F ignificance F
Regression 1 1.221227 1.221227 47.97306 0.000226
Residual 7 0.178196 0.025457
Total 8 1.399422
Coefficient sandard Er rot Stat P-value Lower 95 %Upper 95 %ower 95.0 %Upper 95.0%
Intercept 0.102444 0.106865 0.958637 0.369673 -0.15025 0.355139 -0.15025 0.355139
X Variable 1 0.003567 0.000515 6.926259 0.000226 0.002349 0.004784 0.002349 0.004784
RESIDUAL OUTPUT PROBABILITY OUTPUT
Observation Predicted YResiduals dard Residuals Percentile Y
1 0.173778 0.006222 0.041691 5.555556 0.18
2 0.316444 0.053556 0.35884 16.66667 0.35
3 0.459111 -0.10911 -0.73108 27.77778 0.37
4 0.601778 0.178222 1.194149 38.88889 0.56
5 0.744444 -0.18444 -1.23584 50 0.75
6 0.887111 -0.13711 -0.91869 61.11111 0.78
7 1.029778 0.150222 1.006539 72.22222 1.17
8 1.172444 0.187556 1.256685 83.33333 1.18
9 1.315111 -0.14511 -0.97229 94.44444 1.36X Variable 1  Residual Plot
-0.4-0.200.20.4
0 100 200 300 400
X Variable 1Residuals
X Variable 1 Line Fit  Plot
00.511.5
0 100 200 300 400
X Variable 1YY
Predicted 
Normal Probability Plot
00.20.40.60.811.21.41.6
0 2 04 06 08 0
Sample PercentileY</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>... while attempting to teach flight instructors that 
praise is more effective than punishment for promoting skill-learning...one of the most seasoned 
instructors in the audience raised his hand and made 
his own short speech..."On many occasions I have praised flight cadets for clean execution of some 
aerobatic maneuver, and in general when they try it 
again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general 
they do better the next time. So please don't tell us 
that reinforcement works and punishment does not, because the opposite is the case." ...because we 
tend to reward others wh en they do well and punish 
them when they do badly, and because there is 
regression to the mean, it is part of the human 
condition that we are statistically punished for 
rewarding others and rewarded for punishing them.Regression Toward the Mean
Kahneman, D., 2002, Bank of Sweden "Nobel" Prize Lecture</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Exponential Curve Fitting
Theta T/W
01
30 1.06708
60 1.1396690 1.215042
120 1.296548150 1.38352180 1.436327
210 1.57536
240 1.701036270 1.7938
300 1.914129
330 2.002529360 2.179542TWe=
WTThe Capstan 
Equation
lTW= log(TW);
p = polyfit(theta,lTW,1);
lTW_hat=polyval(p,theta);
TW_hat=exp(lTW_hat);
plot(theta,TW,'+r',theta,TW
_hat,'-g')</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Plan for Today
 Regression 
 History / Motivation
 The method of least squares Inferences based on the least squares 
estimators
 Checking the adequacy of the model  The Bootstrap Non-linear regression</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Regression
(multiple Regression to come later)
Dan Frey
Assistant Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Plan for Today
 Regression 
 History / Motivation
 The method of least squares Inferences based on the least squares 
estimators
 Checking the adequacy of the model  The Bootstrap Non-linear regression</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Plan for Today
 Regression 
 History / Motivation
 The method of least squares Inferences based on the least squares 
estimators
 Checking the adequacy of the model  The Bootstrap Non-linear regression</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Why is the Least Squares 
Approach Important?
There are other criteria that also provide reasonable 
fits to data (e.g. minimize the max error)
BUT, if the data arise from the model below, then least squares method provides an unbiased , 
minimum variance estimate of and 

++= x Yrandom
randomtheoretical 
parametersindependent 
variable</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Matlab Code Simulating the 
Probability model
hold on
alpha=2;beta=3;eps_std=1;for trial=1:100x(trial) = random('Uniform',0,1,1,1);
eps= random('Normal',0, eps_std,1,1);
Y(trial)=alpha+beta*x(trial)+eps;endplot(x,Y,'+')hold onplot(x,alpha+beta*x,'-','Color','r')</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>The Bootstrap
 Construct a sample probability distribution    , 
putting mass 1/ nat each point x1, x2, ..., xn
 With     fixed, draw a random sample X* of 
size nfrom
 Approximate the sampling distribution as the 
bootstrap distribution
 "...shown to work satistfactorily on a variety of 
estimation problems." ), (* *F R RX=
Efron, B., 1979, "Bootstrap Methods: Another Look at the 
Jackknife," Annals of Statistics 7:1-26. F
F</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Polynomial Regression

++= x Y    + + + + + =p
px x x Y ...2
2 1 0Linear 
regression curve
Polynomial 
regression curve
h.o.t+  +  + 
= =0022
2
0 0 0dd) (dd) ( ) ( ) (
x x x x xfx xxfx x x f x fOften used for locally approximating well behaved
functions because of Taylors series approximation</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Beware of Over Fitting
Current 
(Amps) Efficiency
1.18 49.0%
1.22 48.5%
1.27 48.6%
1.3 52.5%
1.4 54.2%
1.49 54.7%
1.56 51.0%
1.69 52.7%
2.02 48.8%2.36 42.4%
2.78 39.4%
3.26 38.1%p2= polyfit(I,e,2)
p4 = polyfit(I,e,4)I2=1:0.1:3.5;e_hat2=polyval(p2,I2);e_hat4=polyval(p4,I2);
plot(I,e,'+r',I2,e_hat2,'-g', I2,e_hat4,'-b')</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>The Binomial Distribution
n=10; 
x = 0:n;
y = binopdf(x,n,0.5);
subplot(3,1,1); bar(x,y,0.1)
n=100; 
x = 0:n;
y = binopdf(x,n,0.5);
subplot(3,1,2); bar(x,y,0.1)
n=1000; 
x = 0:n;
y = binopdf(x,n,0.5);
subplot(3,1,3); bar(x,y,0.1)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Regression Toward the Mean
Galton, Francis, 1886, 
"'Regression towards mediocrity 
in hereditary stature," J ournal of 
the Anthropological Institute15:246-63.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Matlab Code Simulating the 
Probability model
hold on
alpha=2;beta=3;eps_std=1;for trial=1:100x(trial) = random('Uniform',0,1,1,1);
eps= random('Normal',0, eps_std,1,1);
Y(trial)=alpha+beta*x(trial)+eps;endplot(x,Y,'+')hold onplot(x,alpha+beta*x,'-','Color','r')</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Computing Least Squares 
Estimators
()
= =n
ii xx x x S
12()
= =n
ii yy y y S
12What are the 
values of a
and bfor the 
regression 
line?
xxxy
SSb=
x b y a=
) , (y x
()()
=  =n
ii i xy y y x x S
1()()0&gt; y y x xi i
()()0&lt; y y x xi i()()0&lt; y y x xi i
()()0&gt; y y x xi i</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>The Bootstrap for Regression
load lawdata; [n m]=size(lsat);
[b bint]=regress(gpa, [ones(n,1) lsat]);for t=1:1000
samp_repl=floor(n*rand(size(lsat)))+1;x=[ones(n,1) lsat(samp_repl)]; 
y=gpa(samp_repl);
b_boot = regress(y,x); int(t)=b_boot(1); slope(t)=b_boot(2);
end[bin_n,xout]=hist(slope,30); figure(3); bar(xout,bin_n); hold on;plot([bint(2,1) bint(2,1) bint(2,2) bint(2,2)],[max(bin_n) 0 0 max(bin_n)],'-rx');
figure(4); hold on;for t=1:1000; 
plot(lsat,slope(t)*lsat+int(t),'m'); 
endplot(lsat,gpa,'+'); 0 1 2 3 4 5 6 7 8
x 10-3020406080100120
540 560 580 600 620 640 660 6802.62.833.23.43.63.84</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Interpolation is Different
Kriging Fits a curve to a set of points
 But assume no error in the points themselves
 Enable estimates at values other than the observed ones
x = 0:10;
y = sin(x);xx = 0:.25:10;yy = spline(x,y,xx);plot(x,y,'o',xx,yy)
spline interpolation
Courtesy of Prof. Emmanuel Vazq uez.  Used with permission.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Example 
Evaporation vs Air Velocity
Air vel
(cm/sec)Evap coeff. 
(mm2/sec)
20 0.18
60 0.37
100 0.35
140 0.78
180 0.56
220 0.75
260 1.18
300 1.36
340 1.17
380 1.65</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>0 5 10 15 20 25 GRIP FORCE (lbs)
0 10 20 30 40 50 
INPUT CABLE TENSION (lbs)
CLEAN SOAPY WET DRY DEGREASEDVMA I PERFORMANCE
UNDER VARYING ENVIRONMENTAL CONDITIONSWhat about a system like this?</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Checking the Assumptions
 Normal scores-plot 
of the residuals
 Check for linearity
 If there are outliers
 check sensitivity of 
results 
 try to identify special 
causeshold off
normplot(e)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Markov processes and their application to queueing, part 2 (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec9/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>Balance of Flow Equations
0P0=1P1
(n+n)Pn=n1Pn1+n+1Pn+1 for n=1,2,3,...
Another way to balance the flow:nPn=n+1Pn+1n=0,1 ,2,...Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>P0=(e/)1=e/&gt;0
=utilization factor =1 P0=1e/&lt;1.
Pk=(/)k
k!e/,   k=0,1,2,... Poisson Distribution!
L=time - average number in syste m=/ How?
L=AW      Little's Law, where
Aaverage rate of accepted arrivals into system</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Outline
Littles Law, one more time
PASTA treat
Markov Birth and Death Queueing Systems</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>PASTA:  Poisson Arrivals See Time Averages</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>t= timeCumulative # of Arrivals
0L(t)FCFS
SJF
LSJF(t)FCFS=First Come, First Served
SJF=Shortest Job First
What about LJF,
Longest Job 1st?</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>The M/M/1 Queue
P0{1 +(/)+(2/2)+...+(n+1/n+1)+...}=1
{1+(/)+(2/2)+...+(n+1/n+1)+...}=1/[ 1(/)]
For /&lt; 1.P0=1/   for /&lt;1 .
Pn=(/)nP0=(/)n(1/) for n=1 ,2,3,...
Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Queue of Waiting CustomersQueueing System
Arriving Customers
Departing CustomersSERVICE 
FACILITY
Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Time to 
Buckle your 
Seatbelts!
http://www.census .gov/pubinfo/www/multimedi a/img/seatbelt-lo.jpg</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Apply Littles Law to Service Facility
=A(average service time)
=average number in service facility =A/
A==(1e/)
W=LA=/
(1e/)=
2(1e/)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>System is 
General
Our results apply to entire queue 
system, queue plus service facility
But they could apply to queue only!
Or to service facility only!L=W
S.F. Lq=Wq
LSF=WSF=/
1/=mean service time</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>The M/M/1 Queue
P0+(0/1)P0+(01/[12])P0+...+(01...n/[12...n+1])P0+...=1
P0{1+(0/1)+(01/[12])+...+(01...n/[12...n+1])+...}=1
P0{1 +(/)+(2/2)+...+(n+1/n+1)+...}=1
{1+(/)+(2/2)+...+(n+1/n+1)+...}=1/[ 1(/)]
For /&lt; 1.Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>nPn=n+1Pn+1n=0,1 ,2,...
0P0=1P1 
1P1=2P2 ...
nPn=n+1Pn+1 P1=(0/1)P0
P2=(1/2)P1=(0/1)(1/2)P0=(01/[12])P0
Pn+1=(n/n+1)Pn=(01...n/[12...n+1])P0
Telescoping!
Source: Larson and Odoni, Urban Operations Research</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>All of this means,
You buy one, you get the other 3 for free!
W=1+Wq
L=Lq+LSF=Lq+
L=W</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Final Example:  
Single Server, Discouraged Arrivals/2/3/4/5
State-Rate-Transition Diagram, Discouraged Arrivals
Pk=1
k!()kP0
P0=[1+()+1
2!()2+1
3!()3+...+1
k!()k+...]1
P0=(e/)1=e/</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Blackboard Modeling
3 server zero line capacity
3 server capacity for 4 in queue
Same as above, but 50% of queuers
balk due to having to wait in queue
Single server who slows down to half 
service rate when nobody is in queue
More?? .</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>The M/M/1 Queue
P0=1/   for /&lt;1 .
Pn=(/)nP0=(/)n(1/) for n=1 ,2,3,...PT(z) Pn
n=0
 zn= (/)n(1/)
n=0
 zn=1
1z
d
dzPT(z) 
  
z=1 nPn
n=0
 =L=(1)()
(1z)2 
  =
1 for &lt;1
L=W=/(1)
implies W=(1/)/(1)=(1/)/ ( 1)
Lq=Wq etc.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86.  Markov Processes and their 
Application to Queueing II
Richard C. Larson
March 7, 2007
Photo: US National Archives</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Markov Queues
Markov here means, No Memory</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>P
0P0=1P1 
1P1=2P2 ...
nPn=n+1Pn+1 P1=(0/1)P0
P2=(1/2)P1=(0/1)(1/2)P0=(01/[12])P0
Pn+1=(n/n+1)Pn=(01...n/[12...n+1])P0
Telescoping!
P0+P1+P2+...= Pn
n=0
 =1
P0+(0/1)P0+(01/[12])P0+...+(01...n/[12...n+1])P0+...=1
P0{1+(0/1)+(01/[12])+...+(01...n/[12...n+1])+...}=1
Now, you easily solve for P0and then for
All other Pns.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>More on M/M/1 Queue
Let w(t) = pdf for time in the system 
(including queue and service)
Assume First-Come, First-Served (FCFS) 
Queue Discipline
w(t)= w(t|k)Pk
k=0
 =k+1tket
k!k
k=0
 (1)
w(t)=et(1)(t)k
k!k=0
 =(1)etet
w(t)=(1)e(1)t  t0Exercise:  Do the same for Time in queue</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Optional Exercise:
Is it better to enter a single 
server queue with service rate 
or a 2-server queue each with rate /2?
Can someone draw one or both of the 
state-rate-transition diagrams?  
Then what do you do?</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Mean Wait vs. Rho
0510152025
0 0.2 0.4 0.6 0.8 1
RhoSeries1
Note the Elbow!</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>0,0
 0,1
1,0
 1,1About the cut between states to  
write the balance of flow equations</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Introduction and overview (PDF)
3-door problem (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec1_intro/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>21</slideno>
          <text>The End!The End!</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>ESD.86 Models, Data, Inference 
for Socio-Technical Systems 
(New) Prereq: ESD.83, 6.041
G (Spring)  3-0-9
Use data and systems knowledge to build models of complex socio-
technical systems for improved sys tem design and decision-making. 
Enhance model-building skills, incl uding: review and extension of 
functions of random variables,  Poisson processes, and Markov 
processes. Move from applied probabili ty to statistics via Chi-squared 
t and f tests, derived as functi ons of random variables. Review 
classical statistics, hypothesis te sts, regression, correlation and 
causation, simple data mining techniques, and Bayesian vs. classical 
statistics. Class project.
Enrollment limited to 25 students. Preference given to ESD Ph.D.
Students.
Richard C. Larson, Daniel D. Frey</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>7
This is a 
Knowledge 
Requirement
&gt;For students whose academic plan is to take MIT 
subjects that go much deeper than   this subject (in 
statistics, probability, quantitative research methods), 
the requirement for taking this subject can be 
waived.
&gt;This subject represents a 'k nowledge requirement' that 
will be assumed on doctoral general exams.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19
In a 162 game season, 
we should not be surprised to see
&gt;At least one 7 game loosing streak. :(
&gt;At least one 7 game winning streak.  :)
&gt;All within the null hypothesis that each game is an 
independent fair coin flip.
&gt;But imagine the press coverage of these two events.
&gt;Generalize to more important topics.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6
More Overview
&gt;In ESD tradition, the math part would be augmented with reading 
assignments and discussions tracing the history and application of 
each of the major concepts discussed and developed.  
&gt;There would be a term project for each student.  
&gt;There would be computer-based as well as paper-based homework 
assignments.  The subject would be rigorous.  
&gt;While a byproduct would be continued class bonding of the first year 
doctoral students, the primary focus is on intellectual content.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>2
Engineering Systems: At the intersection 
of 
Engineering, Management &amp; Social Sciences
EngineeringSocial 
SciencesManagement
ESD</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>For the new Methods 
subject
We want to educate doctoral students to conduct 
research on these types of large scale engineering 
projects, which fall at the intersection of traditional 
engineering, management and social sciences</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>13
Go Deep, 
Use all Available 
Subjects
&gt;We cannot think that ESD is so unique that no other MIT subjects
can contribute to ESD students' kn owledge of 'methods.'  In the 
course of an ESD doctoral student 's studies, she/he will rely most 
often on existing subjects at MIT or perhaps Harvard to go deep 
in the required methods.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>20
On-Going 
Student Project
&gt;Track media weekly to find media mis-interpretation or 
misuse of data.
Statistical significance of the m edia phrase, If it bleeds, it 
leads.
Making inferences based solely  on sampling from extremes.
http://news.csumb.edu/site/Images/news/headlines.jpg</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>18
LIB
E199
4RTY
LIB
E199
4RTY
LIB
E199
4RTYLIB
E199
4RTYLIB
E199
4RTYLIB
E199
4RTYLIB
E199
4RTY
LIB
E199
4RTY
LIB
E199
4RTYLIB
E199
4RTY
A 4-Game Winning Streak!
Streak
Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>11
Fundamentals, via 
Sample Space Approach
&gt;Start with constructing probabilistic model s using functions of random variables 
with a sample space approach.  
&gt;Then we slide into statistics via experimental design with threats to validity, 
saying in essence that all designs are co mpromised by one or more of these.  
&gt;Then, the statistical part should be a continuum of the sample space applied 
probability treatment so everything is f undamental -- no memorization of weird 
stat formulas, just for t he sake of memorization.  
&gt;We will do more with less in the stats area.  
&gt;We cover Bayesian as well as classica l statistics, highlighting the philosophy, 
strengths and weaknesses of each.    
&gt;If they want a true stats course , that would follow this course.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>8
Building from
&gt;Subject will have an enforceable prerequisite:  6.041 or 
equivalent.
&gt;It will leverage all the fine work  that Dan Frey has done with 
SOE curriculum development grant support -- funded in 
response to the oft-cited Odoni report on the lack of a good 
solid engineering-focused statistics subject in the SOE.  
&gt;But, this is NOT a statistics subject!</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>12
Want students to be able to work with blank 
sheets of paper.
They know fundamentals and can derive results.
They are not just users of computer routines.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>15
Introduce New Ideas in 
Homework
&gt;Stochastic Dynamic Programming, Real Options, via 
Sequential Decision Trees
&gt;Shannon measure of information, Entropy (the element of surprise)
&gt;Derivation of certain          
     
 
WaitStorm
No Storm
HarvestMold
No Mold$67,200
$12,000
$42,000
$36,000
$30,00025%
20%
&lt;19%
$34,200$41,280
$39,240
$39,240
$34,200or $24,000.4
.4
.4.5
.5.6
.2$37,200$34,080
$35,640
$35,640statistical tests                   
(F, T, Chi-Squared)
Figure by MIT OCW. After example by Akinc.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>17
A Real Null Hypothesis:  A Sports .500 Team
&gt;Each game is essentially decided by an independent flip of a 
fair coin
&gt;Track the media coverage as certain expected  streaks
during the year. 
Wide use of derived distributi ons of Max and Min random 
variables.
Random incidence, potential fallacies in sampling</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>16
Linkages to ilities
&gt;Reliability
Measures of..
Systems designs with 
redundancy
&gt;Robustness
&gt;Predictability
&gt;Stability
http://www.mathpages.com/home/kmath336/kmath336_files/image001.gif
1
2
3
n0,1
1,0
0,21n+1
2n+1
3,n+1
4,n+12,0
0,3
3,0
0,n
n,00,n+1
n+1,00 n+1
:
Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>9
An ESD Service Subject
&gt;If we are successful, we should attract students from elsewhere 
in the SOE who are not associated with ESD.  
&gt;This should be ESD's first 'service subject.
&gt;Tentatively, the 2007 spring seme ster new subject will be team-
taught by Dan Frey and yours truly, with a cameo by Roy Welsch.</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Class Class 
ProjectsProjects</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>14
Model Building
&gt;Model building , based on empirical  eviden ce and axiomatic conjectures, 
should be the emphasis for the new subject.   
&gt;We are not creating a new subject in applied probability nor are we 
creating one in statistics.  But we us e both to obtain our objective.  
&gt;The focus is more on model synthesis, not data analysis per se .  
&gt;It is an active model creating focus, not a passive critical social science 
focus.  
&gt;Axiomatic models would be emphasize d more than data inferred models, 
inferred from curve fitting -- where causation and correlation can become 
confused.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>5
Overview
&gt;A new QUANTITATIVE methods subject, with 
aspects of each of three disciplinary areas: Engineering, Management and Social Science.
&gt;To be required of 1st year ESD Ph.D. students, spring semester
&gt;There will be another new subject on QUALITATIVE methods.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>10
Lectures, Problem Sets, Tutorials
Readings:  Historical Context, including Cases
Computer-Based Exercises
Media Project
Term ProjectSubject Operates Along Parallel Tracks</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Design of an ESD Design of an ESD 
Core Methodology Core Methodology 
SubjectSubject
Dick Larson, Dan Frey, with Roy WelschFebruary 7, 2007</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Random incidence: A major source of selection bias (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec5/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>The Inter-Arrival Times
fY(x) = fY1(x) = fY2(x) = ...
If the Yis are mutually independent then 
we have a renewal process.
But the Random Incidence results we are 
about to obtain do not require that we 
have a renewal process.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>y1y2 y3 y4 y5y6 ...</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>The Gap We Fall Into by Random Incidence
fW(w)dw =P{length of gap is between wand w+dw}
fW(w)dw is proportional to two things:
(1) the relative frequency of gaps [w, w+dw]
(2) the length of the gap w (!!).
Thus, normalizing so we have a proper pdf,
We can write
fW(w)dw= w fY(w)dw/E[ Y], or
fW(w)=wfY(w)/E[Y]</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Doctoral Exam Question
You arrive at a bus stop where busses arrive 
according to a Poisson Process with rate 
per unit time.
Use no-memory property of Poisson 
processes. Time until next bus arrives has 
negative exponential density with mean 1/ .
Looking backwards, time since last bus was 
at the bus stop has negative exponential 
density with mean 1/ .
Thus, mean time between buses is 2/ , not 
1/ .  Whats wrong here?</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4
Key result:
E[V] = E[Y]2(1+2)/(2E[Y])
where =coefficient of variation of the R.V. Y</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4
Definitions of the random variables:
Yi= time interval between the ithand i+ 1starrival event
W= length of the inter-arrival gap in which you fall 
V= time remaining in the gap in which you fall</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Lets Visit Several Examples, 
Including that Bus Stop Doctoral 
Exam Question!</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Random Incidence
A Major Source of Selection Bias
Richard C. Larson
February 21, 2007</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Examples
Waiting for a bus at 77 Mass. Avenue.
 Clumping
Interview passengers disembarking 
from an airplane.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Photo courtesy of Kevin King. http://www. flickr.com/photos/divemasterking2000/541537501/</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4
All 3 random variables have probability density functions:
fY(x) = fY1(x) = fY2(x) = ...
fW(w)
fV(y)</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Mean Time Until Next Arrival
E[V]= E[V|w]fW0 (w)dw
E[V]= (w/2)wfY(w)
E[Y]0 dw
E[V]=E[Y2]/(2E[Y])=Y2+E2[Y]
2E[Y]
E[V]=E2[Y]1+Y2/E2[Y]
2E[Y]=E2[Y]1+2
2E[Y],
where  coefficient of variation of Y=Y/E[Y].</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Random Incidence:  Tending to Land in  Bigger Gaps
Photo courtesy of Kevin King. http://www. flickr.com/photos/divemasterking2000/541537501/</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Time Remaining in the Gap Until Next Arrival
fV(y)
Consider fV|W(y|w)
We can argue that fV|W(y|w)=(1/w) for 0&lt; y&lt;w.
So we can write
fV(y)dy=dy fV|Wy (y|w)fW(w)dw
fV(y)dy=dy (1 /w)
ywfY(w)
E[Y]dw
fV(y)dy=dy(1P{Yy}) /E[Y]</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>The Queue Inference Engine and the psychology of queueing (PDF)
Beyond the physics of queueing (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec12a/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>2.  Maximum Queue Length
Set s =s*Ksuch that
si*K=t(iK)     for all i =1,2,..., N;K=1,2,..., N,
where a nonpositive subscript on  t implies a value of zero.
These values for simply that each arriving customer imust arrive
after the departure time of departing customer i-Kduring the 
congestion period.  Now we can write
P(QK|t)=Pr{queue length did not exceed Kduring
                           the congestion period, given observed
                           departure time data}
                   =(s*K,t)/(0,t).</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>A bank having the QIE for its ATMs 
could mail you your personal queue 
delay pdf each month!</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>X1 1 2Not allowed
since at least onearrival must bein [0,1].X2
012
A priori probability of this event = 3/4 = master probability</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>[Photo of ATM with a visually impaired user removed 
due to copyright restrictions.]
Source: Ed Roberts Campus</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>The Applications
 Servi:  Air Phone
 Larson:  Human server queues @ Logan 
Airport, Post Offices and Banks</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>The QIE Algorithms
Order statistics (Larson,      
1989; Larson &amp; Jones, 1994)                              OP 6
OP 5
OP 4 OP 3 OP 2 OP 1fb-lev fb-lev fb-levLevel
Level
Level Level Level Level7
7
Integration (Bertsimas and Servi, 1990)
Markov processes with taboo states
(Daley and Servi, 1991+)
 All O(N3) in computational complexityFigure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>X1 1 2X2
012
X1and X2are our two unordered arrival times</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>The Data-set
 The QIE works on one congestion period at 
a time.  C=Congestion period; I=Idle period
 A congestion period commences the instant 
that all servers become busy, thereby 
requiring subsequent arrivals to be delayed 
in queue, and terminates the first moment that one of the servers becomes idle after a 
service completion because the queue is 
empty.C C C II I ITime</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>We have the vector tof departure times (and of service 
initiation times):  t= (t1, t2, ..., tN), where t(i+1)&gt;ti.
Also consider a vector  s= (s1, s2, ..., sN), where s(i+1)&gt;si, and 
where si&lt;ti.
We have determined how to efficiently and recursively
compute the following important probability:
This is the probability that the order statistics fall within
a prescribed N-rectangle (in N-dimensional space).
If s= 0, then (0, t) = 'master probability.'</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Probability density functions of arrival ti mes and queueing delays of queued customers.Arrival time pdf of a 
randomly queued customer
PDF's for the first andsecond arrival timesof the queued customers
PDF's for the queue waitsfor the first and secondqueued customers (FIFO)
See: Larson, Richard C. QUEUE INFERENCE ENGINE. In Encyclopedia of Operations Research and Management Science , 
Centennial Edition, Saul I. Gass and Carl M.  Harris (eds.). Boston, MA: Kluwer, 2001, pp.674-679fA(a)
1 0
0 01/32/3
2/34/3
2/34/3a 2
1 1 2a a2PDF of 1st
Arrival Time X(1)PDF of 2nd
Arrival Time X(2)
PDF of Queue Delay for 1st Queued Customer
2/34/3
1 0 2a2/34/3
2 1 0PDF of Queue Delay for 2nd Queued Customer
a
Figure by MIT OCW .</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>We assume a congestion period scaled to [0,1].  We assume that
there are Ncustomers queued (and Ncustomers who depart
during the congestion period). Here tiis the departure time for 
the ithserved customer, and it is also the time of initiation of service
for the ithcustomer to leave the queue.
The set of r.v.'s { X1, X2, ..., XN} are the i.i.d. uniformly distributed
unordered arrival times.
The set of r.v.'s { X(1), X(2), ..., X(N)} are the corresponding 
order statistics, e.g., X(2) is the arrival time of the second 
customer to enter the queue; X(2) is the second smallest 
from the set { X1, X2, ..., XN}.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>For additional reading see
 Larson, R.C., "The Queue Infere nce Engine:  Deducing Queue Statis tics From Transactional Data."  
Management Science 36(5): 586-601, May 1990.
 Jones, Lee K. and Richard C. Lars on, "Efficient Computa tion of Probabilities of Events Described by 
Order Statistics and Applicatio ns to Queue Inference."  ORSA Journal on Computation ., vol. 7, no. 1, 
Winter 1995, pp. 89-100.
 See the following for an overview of the QIE including a summary of published research by 
Servi, Bertsimas, Daley and othe rs on queue inferencing:  Larson, Richard C., QUEUE 
INFERENCE ENGINE, chapter in Encyclopedia of Operations Re search and Management Science , 
Centennial Edition, Saul I. Gass and Carl M. Harri s (eds.), Kluwer, Boston, 2001, pp.674-679.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Queue Inference Engine (QIE)
 Boston area ATMs:  reams of data
 Standard approach first
 Then the notion that there may be more 
information in the transactional data
http://europe.cnn.com/SPECIALS/2001/euro/st ories/security.fears/s tory.france.atm.jpghttp://news.bbc.co.uk/olmedia/210000/ images/_214989_bank_atm_queue150.jpg[Photos of people waiting in line at ATMs 
removed due to copyright restrictions.]</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Illustrative Performance Measures
1.  Maximum Experienced Queue Delay (FCFS).
Interested in the cdf for the max of Nnon-independent r.v.s, 
the Nqueue delays.
Define D(|t) = conditional probability that none of the Nqueued 
customers waited in queue or more time units, given the observed 
departure time data.
Set si= si*= max{ ti-, 0}.
Then 
D(|t) = (s*,t)/(0,t)</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>The Queue Inference Engine
and the Psychology of Queueing
ESD.86
Spring 2007
Richard C. Larson</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>QIE:  Assumptions
 (1) A priori, arriving customers are generated by a 
homogeneous (or slowly time varying) Poisson 
process; 
 (2) The signature of a queue from the transactional 
data is a service stop time followed very shortly by 
a service start time; 
 (3) Any balking that occurs is dependent only on 
whether a positive queue delay will be experienced by the prospective customer, not on 
the line length.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Example:  M/D/1 Queue
 Service time = 1.0 minute
 Congestion period contains three customers
I m p l i e s  N= 2 queued customers
 Busy period = 3 minutes, starting say @ t=0
 We know that zero customers arrived during 
[2,3].  Why?
 So our 2 queued customers arrived during 
[0,2], with at least one in [0,1]</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>There are many more quantities we could 
compute within the QIE framework
 Mean queue delay
 Mean value function of queue length
 CDF of queue delay
 PDF of queue delay for each customer 
served (FCFS)
 PMF of queue length experienced by a 
random customer
 And more.....</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>The Performance Measures
.For each congestion period the QIE computes, 
conditioned on the transactional data set, the 
following quantities:
1.  The time-dependent mean number of customers in queue;
2.  The mean queue delay experienced by a random customer;3.  The time average number of customers in queue over the 
duration of the congestion period;
4.  The probability that a randomly arriving customer during 
the congestion period experiences icustomers ahead of her 
in line.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Order Statistics:  
The NUnordered Arrivals in [0, T] of a Poisson 
Process are Mutually Independent and 
Uniformly Distributed (Urban OR Text, Sec. 2.12.3)
Pr{N(t)=k}=N
k 
   
 t
T 
  
 kTt
T 
  
 NkE[ N(t) ] = ( t/T)N
VAR [ N(t) ] = N(t/T) ( [T-t]/ T)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Sometimes the queue is not so orderly!
But the QIE does not care!  FCFS is not a requirement for the basic QIE performance measures.[Photo of a crowd of people waiting outside a bank 
removed due to copyright restrictions.]</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Part 1
The Queue Inference Engine
QIE</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Can have single or multiple servers
 Service times need not be i.i.d.
 Unless otherwise specified, queue discipline 
need not be FCFS
 There is no parameter estimation!  The rate 
parameter for the Poisson process does 
not appear in the analysis.QIE:  Assumptions - continued</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Spatial models (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec7/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>18</slideno>
          <text>Spatial Poisson Processes
Entities distributed in space (Examples?)
Follow postulates of the (time) Poisson 
process
dt = Probability of a Poisson event in dt 
History not relevant
What happens in disjoint time intervals is 
independent, one from the other
The probability of a two or more Possion events in dt is second order in dt  and can be ignored 
Lets fill in the spatial analogue..</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>0 /22/ f()</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Work within the sample space
yz
11
fY,Z(y,z)=2
yzFY,Z(y,z)=y(2y)=2,  0yz10Height of pdf = 2
What do we do if we do
not have the simple square
symmetry of this problem?
What do we do if the pdf is
not uniform?</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>1
r/21/2
1/21/2
0 /2 /4cos(-/4)
g()</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86 Spatial Models
Richard C. Larson
February 28, 2007Richard C. Larson
February 28, 2007</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Work within the sample space
x1x2
11Use CDF
FY,Z(y,z)P{Yy,Zz}
FY,Z(y,z)=P{Min[X1,X2]y,Max[X1,X2]z}
FY,Z(y,z)=2yzy2, 0yz1
yy
zz
fY,Z(y,z)=2
yzFY,Z(y,z)=y(2y)=2,  0yz1</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>What Have We Learned?
Within a spatial context, how to use the 
Four Steps to Happiness to derive joint 
distributions
Within a spatial context, how to derive a difficult distribution involving geometry
Spatial Poisson Processes, with nearest 
neighbor applications</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Nearest Neighbors:  Euclidean
Define D2= distance from a random point
to nearest Poisson entity
Want to derive fD2(r).
r
Random 
PointHappiness:
FD2(r)P{D2r}=1P{D2&gt;r}
FD2(r)=1Prob{no Poisson entities in circle of radius r}
FD2(r)=1er2  r0
fD2(r)=d
drFD2(r)=2rer2  r0
Rayleigh pdf with parameter 2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Min, Max.  Deriving a Joint PDF
Suppose X1and X2are i.i.d. uniformly 
distributed over [0, 1]. They could, for 
instance, be the locations of 2 police cars.
We seek to derive the joint pdf of Yand Z, 
where
Y=Min(X1, X2)
Z=Max( X1, X2) 
That is, we seek 
fY,Z(y,z)dydz =P{y&lt;Y&lt;y+dy,z&lt;Z&lt;z+dz}</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>(R| ) = cos  +  sin = 2 1/2 cos(-/4)
2.  Identify Sample Space
3.  Probability Law over Sample Space:
Invoke isotropy implying uniformity of 
angle0 /2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Ratio of Manhattan to Euclidean 
Distance Metrics
1.  Define R.V.s
D1= |X1-X2| + |Y1-Y2|
D2=  (X1-X2)2+ (Y1-Y2)2
 Ratio = R = D1/ D2
= angle of directions of travel wrt straight 
line connecting (X1, Y1) &amp; (X2, Y2)</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>1
r/21/2
1/21/2
0 /2 /4
cos -1 (r/21/2) + /4 -cos -1 (r/21/2) + /4cos(-/4)
g()</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>2/1
0 /2 /4
cos -1 (r/21/2) + /4 -cos -1 (r/21/2) + /4pdf for Probability of 'red event' = 2*(2/ )*{-cos -1 (r/21/2) + /4}</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Nearest Neighbors:  Euclidean
Define D2= distance from a random point
to nearest Poisson entity
Want to derive fD2(r).
r
Random 
PointfD2(r)=d
drFD2(r)=2rer2  r0
Rayleigh pdf with parameter 2E[D2]=(1 / 2)1   "Square Root Law"
D22=(2/2)1
2</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>http://zappa.nku.edu/~longa/geome d/modules/ss1/lec/poisson.gifSpatial
Poisson
Processes
Courtesy of Andy Long. Used with permission.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>(X1, Y1) (X2, Y2)
Directions of TravelOne possible minimum distance path=Cos()D2sin()D2</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Outline
Min, Max
Ratio of Urban Distance to Airplane Dist.
Spatial Poisson Processes
Facility Location</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>4.  Find CDF
FR(r) = P{R &lt; r} = P{21/2 cos(-/4) &lt; r}
FR(r) = P{R &lt; r} = P{cos( -/4) &lt; r/ 21/2 }</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Four Steps to Happiness
1. Define the random variables
Y=Min(X1, X2)
Z=Max( X1, X2)
2. Define the joint sample space:
Unit square in positive quadrant
3. Identify the probability measure over
the joint sample space:  Uniform.
4. Work within the sample space to answer
any questions of interest.
http://www.schwimmerlegal.com/smiley.jpg</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>(X1, Y1) (X2, Y2)
Directions of TravelOne possible minimum distance path</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>SSet S has area A (S).
Poisson intensity is 
Poisson entities/(unit area).
X(S) is a random variable 
X(S) = number of Poisson
entities in S
P{X(S)=k}=(A(S))k
k!eA(S), k=0,1 ,2,...</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>(X1, Y1) (X2, Y2)
Directions of Travel</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Nearest Neighbor:  Taxi Metric
rFD1(r)P{D1r}
FD1(r)=1Pr{no Poisson entities in diamond}
FD1(r)=1e2r2
fD1(r)=d
drFD1(r)=4re2r2</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>And finally...
After all the computing is done, we find:
FR(r) = 1 - (4/ )cos -1 (r/21/2),  1&lt; r &lt;21/2
fR(r) = d[FR(r) ]/dr = (4/ ) {1/(2 - r2)1/2 }
Median R = 1.306
E[R] = 4/  = 1.273
R/E[R] = 0.098,  implies very robust</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Random incidence and more (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec6/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>Photo courtesy of Tom Karlo. http ://www.flickr.com/photos/karlo/10746148/Pretend you are a chocolate chip, and you 
wake up to find yourself in a cookie..</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Time Remaining in the Gap Until Next Arrival
1. Deterministic: Y = T with Probability 1.0
Then fv(y)=1/T, for 0&lt; y&lt;T.
Suppose T= 10 minutes and event A is:
A = {V&gt;5}.
Then fv|A(y|A)= fv(y)/P{A} for all yin A.
fv|A(y|A)=(1/10)/(1/2)=1/5 for 5&lt; y&lt;10.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4
All 3 random variables have probability density functions:
fY(x) = fY1(x) = fY2(x) = ...
fW(w)
fV(y)</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Mean Time Until Next Arrival
E[V]= E[V|w]fW0 (w)dw
E[V]= (w/2)wfY(w)
E[Y]0 dw
E[V]=E[Y2]/(2E[Y])=Y2+E2[Y]
2E[Y]
E[V]=E2[Y]1+Y2/E2[Y]
2E[Y]=E[Y]
2(1+2),
where  (coefficient of variation of Y)Y/E[Y].</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The Inter-Arrival Times
fY(x) = fY1(x) = fY2(x) = ...
If the Yis are mutually independent then 
we have a renewal process.
But the Random Incidence results we are 
about to obtain do not require that we 
have a renewal process.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>E[V] = (E[Y]/2)(1+ 2)
Pedestrian Traffic Light Problem
1st ped. to arrive pushes button.
T0minutes later the next Dump occurs.
We are dealing here with a random observer.
E[Y] = (1/) + T0
VAR[Y]= (1/)2
E[V] = (E[Y]/2)(1+ 2)
E[V] = (1/2) [(1/) + T0]{1+ (1/)2/[(1/) + T0 ]2}
E[V] =  1/2+ T0 /2 + 1 /{2[+ 2T0 ]}
Photo courtesy of Austin Tolin. http: //www.flickr.com/photos/austintolin/396264013/</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>The Gap We Fall Into by Random Incidence
fW(w)dw =P{length of gap is between wand w+dw}
fW(w)dw is proportional to two things:
(1) the relative frequency of gaps [w, w+dw]
(2) the length of the gap w (!!).
Thus, normalizing so we have a proper pdf,
We can write
fW(w)dw= w fY(w)dw/E[ Y], or
fW(w)=wfY(w)/E[Y]</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>PMF
00.020.040.060.080.10.120.140.160.18
123456789 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 2 1Series1Distribution of Chips in Cookies, 
By Sampling Random Cookies
Mean = 6.52</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Random Incidence PM F
00.020.040.060.080.10.120.14
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21Series1Distribution of Chips in Cookies, 
As Measured by Chips within the Cookies
Mean = 8.62</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>E[V] = (E[Y]/2)(1+ 2)
4.  Suppose Y = 1.0 with Probability 0.99
Y=100.0 with Probability 0.01
Then E[ Y]=1(0.99) + 100(0.01)= 1.99=2
E[Y2]=1(0.99) + 10000(0.01)=100.99=101
VAR[Y ]= E[ Y2]-E2[Y]=101-4=97
2 =97/4=24.25.
E[V] = (E[Y]/2)(1+ 2)=(2/2)(1+24.25)
E[V] =25.25     Intuition??</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>fW(w)=wfY(w)/E[Y]
Becomes:
PW(w)=wPY(w)/E[Y],
whereY=Number of chips in a random cookie
E[Y] = mean number of chips in a 
random cookie
P
W(w) = P{ wchips in a cookie as seen
by a random chip within a cookie}</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Time Remaining in the Gap Until Next Arrival
fV(y)
Consider fV|W(y|w)
We can argue that fV|W(y|w)=(1/w) for 0&lt; y&lt;w.
So we can write
fV(y)dy=dy fV|Wy (y|w)fW(w)dw
fV(y)dy=dy (1 /w)
ywfY(w)
E[Y]dw
fV(y)dy=dy(1P{Yy}) /E[Y]</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>E[V] = (E[Y]/2)(1+ 2)
1. Deterministic Inter-arrivals:
E[V] = E[Y]/2 = T/2.
2. Negative exponential inter-arrivals:E[Y]=T,Y2=0
E[Y]=1/,Y2=1/2,=1.  E[V]=1/(2)+1/(2)=1/
3. Y=1.0, with Prob. 1/2; Y=9.0, with Prob. 1/2.
E[Y]=(1/2)[1 + 9] = 5. 
Variance[Y ]=E[(Y -E [Y])2]=E[Y2-2YE[Y]+ E[ Y])2]
Variance[ Y]=E[Y2] - E[Y ]2 
Variance[ Y]=(1/2){12+ 92} - 52 =41-25=16
=4/5. E[V]=(5/2)(1+16/25)=2.5(1.64)=4.1</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Time Remaining in the Gap Until Next Arrival
2. Yhas negative exponential pdf, mean .
We know that fv(y)= e(-y)for y&gt;0.
Suppose =1/10, so that E[ Y]=10.
Suppose event A:{ Y&gt;5}
Then fv|A(y|A)= e(-y)/P{A} for all y&gt;5.
P{A} = e(-5)
fv|A(y|A)= e(-[y-5]) for y&gt;5.  
Proves No Memory Property</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>fV(y)dy=dy(1P{Yy}) /E[Y]
1.  For deterministic gaps, P{Yy}={0 for y&lt;T
1 for yT
fV(y)dy=dy(1P{Yy})/E[Y]
fV(y)dy=dy/T for 0 y&lt;T
2.  For negative exponential gaps
fV(y)dy=dy(1P{Yy}) /E[Y]
fV(y)dy=dy(1[1ey]) /(1/)
fV(y)dy=dyey for y0</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>How About an 
Infinite Jogging Trail?
Photo courtesy of Carles Corbi. h ttp://www.flickr.com/photos/bioman/101773602/</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86
Random Incidence
and More
Richard C. Larson
February 26, 2007</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4Key result:
E[V] = (E[Y]/2)(1+ 2)
where =coefficient of variation of the R.V. Y</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>y1y2 y3 y4 y5y6 ...V=v
W=y4
Definitions of the random variables:
Yi= time interval between the ithand i+ 1starrival event
W= length of the inter-arrival gap in which you fall 
V= time remaining in the gap in which you fall</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Where does the chips-in-cookies 
sampling problem arise in real life?</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Design of computer experiments (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec21/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>Monte Carlo Simulations
What are They Good at?
 Above formulae apply regardless of dimension
 So, Monte Carlo is good for:
 Rough approximations or
 Simulations that run quickly
E v e n  i f the system has many random variablesAccuracyN1N#Trials
Fishman, George S., 1996, Monte Carlo: Concepts, Algorithms, and Application s, Springer.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Definitions
Accuracy  The ability of a model to faithfully represent 
the real world 
Resolution  The ability of a model to distinguish 
properly between alternative cases
Validation  The process of determining the degree to 
which a model is an accurate representation of the real world from the perspective of the intended uses of the model. (AIAA, 1998)
Verification  The process of determining that a model implementation accurately represents the developers 
conceptual description of the model and the solution to 
the model. (AIAA, 1998)</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Results of Model-Based and Case-
Based Evaluations</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Model Validation in Engineering
 A model of an engineering system can be 
validated using data to some degree within 
some degree of confidence
 Physical data on that specific system 
cannot be gathered until the system is designed and built
 Models used for design are never fully 
validated at the time design decisions 
must be made</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Concept Test
 A bracket holds a component as shown.  The 
dimensions are strongly correlated random 
variables with standard deviations as noted.  
Approximately what is the standard deviation of the gap?
A) 0.011
B) 0.01
C) 0.009
D) not enough info" 001 . 0=
" 01 . 0=

gap</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Next Steps
 Friday 4 May  
 Exam review
 Monday 7 May  Frey at NSF
 Wednesday 9 May  Exam #2
 Wed and Fri, May 14 and 16
 Final project presentations</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Errors in Scientific Software
 Experiment T1 
 Statically measured errors in code
 Cases drawn from many industries
 ~10 serious faults per 1000 lines of commercially 
available code
 Experiment T2 
 Several independent implementations of the same 
code on the same input data
 One application studied in depth (seismic data 
processing)
 Agreement of 1 or 2 si gnificant figures on average
Hatton, Les, 1997, The T Experiments: Errors in Scientific Software, IEEE 
Computational Science and Engineering .</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Five-point Gaussian Quadrature Integration
2.8570z
0 1.3556y(0)y(1.3556)y(2.8570)
y(-1.3556)y(-2.8570)
-1.3556 -2.8570
[] [ ]
[] [ ]

  + +   + + =
 
) 0 ( ) 8570 . 2 ( ) 0 ( ) 8570 . 2 () 0 ( ) 3556 . 1 ( ) 0 ( ) 3556 . 1 ( 1) 0 () (
21)) ( (
2 21 1212
y y A y y Ay y A y y Aydz z y e z y Ez
()() ()
[]
() ( )
() ( ) 



  + +   + +   = 
 
2
22
22
12
12221
2
)) ( ( ) 8570 . 2 ( )) ( ( ) 8570 . 2 ()) ( ( ) 3556 . 1 ( )) ( ( ) 3556 . 1 ( 1) ( ( ) 0 ()) ( ( ) (
21)) ( ( ) (2
z y E y A z y E y Az y E y A z y E y Az y E ydz z y E z y e z y E z y Ez

A1=0.39362, and A2=0.019953Mean of Response Variance of ResponseResponse Function, y(z)
PDF of Uncertain
Variable, z
Five point formula gives exact calculation of the mean of the 
response for the family of all 8th order polynomials</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Importance Sampling Example
X1X2The variables X1and X2 are uniformly distributed within the 
indicated rectangle. 
The physics of the 
problem suggests that the failure mode boundary is more likely 
somewhere in the right 
hand region.
Sample only on the right 
but weight them to correct for this.failurecorrect 
operation
()  3 . 01 1
1) (
=

n
iiynX()  0=Xy() 1=Xy
01 0.7
) (  from  sampledxx+f</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Resource Demands of System Design
 The resources 
for system 
design typically 
scale as the 
product of the 
iterations in the 
optimization and 
sampling loopsOptimizerInitial Values Optimal Design
Objective Function 
&amp;
ConstraintsUncertain 
VariablesStochastic 
Modeler
ModelSAMPLING 
LOOPDecision VariablesProbabilistic 
Objective Function
&amp; Constrains
Adapted from Diwekar U.M., 2003, A novel sampling approach to co mbinatorial optimization under 
uncertainty Computational Optimization and Applications 24 (2-3): 335-371.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Need for Computer Experiments
 There are properties of engineering systems we 
want to affect via our design / policy
 Let's call these properties a function y(x)where x
is a vector random variables 
O f t e n  yis a estimated by a computer simulation 
of a system
 We may want to know some things such as 
E(y(x)) or  (y(x))
 We often want to improve upon those same 
things
 This is deceptively complex</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Classification of Models
Physical
or IconicAnalogMathematical
or Symbolic
 = xh
thp
xd
dd
d3
12Real Worldadvantages / disadvantages? advantages / disadvantages?
Photo of 
model aircraft in windtunnel. Photo of lab 
apparatus.Computer displayed model of propeller.
Images removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Concept Test
 A bracket holds a component as shown.  
The dimensions are independent
random variables with standard 
deviations as noted.  Approximately what is the standard deviation of the gap?
A) 0.011
B) 0.01C) 0.001D) not enough info
" 001 . 0=
" 01 . 0=

gap</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Design of Computer Experiments
Dan Frey
Associate Professor of Mechanical Engineering and Engineering Systems</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Monte Carlo vs Importance Sampling
()
= x x x xxd f y y E) ( ) ( ) (
() ()) (  of estimator  unbiased an  is  1
1) (x X y E ynn
ii
=) (  from  ampled   vectors random t independen  denote  ,...,) ( ) 1 (x X Xxf sn
()
+
+= x xxx xx x
xxd fff yy E ) () () ( ) () (
) (  from  ampled   vectors random t independen  denote  ,...,) ( ) 1 (x X X x+f sn
() ()
()()) (  of estimator  unbiased an  is  1
1) () ( ) (
xXX X
xxy Eff y
nn
iii i

=+Monte Carlo
Importance Sampling</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Outline
 Motivation &amp; context
 Techniques for computer experiments
 Monte Carlo
 Importance sampling
 Latin hypercube sampling
 Hammersley sequence sampling
 Quadrature and cubature
 Some cautions</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Why Models Can Go Wrong
 Right model Inaccurate answer 
 Rounding error 
 Truncation error
Ill conditioning
 Right model Misleading answer
 Chaotic systems
 Right model No answer whatsoever
 Failure to converge
 Algorithmic complexity
 Not-so right model Inaccurate answer
 Unmodeled effects
 Bugs in coding the model</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Computational Complexity and 
Moores Law
 Consider a problem that requires 3nflops
 Worlds fastest computer ~ 36 Teraflops/sec 
 In a week, you can solve a problem where
n=log(60*60*24*7*36*1012)/log(3)=40
 If Moores Law continues for 10 more years
n=log(210/1.5*60*60*24*7*36*1012)/log(3)=44
 We will probably not reach n=60 in my lifetime</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Hammersley Sequence Sampling
 A sampling scheme design for low discrepancy
 Demonstrated to converge to 1% accuracy 3 to 40 times 
more quickly than LHS  [Kalagnanam and Diwekar, 1997]
   
Monte Carlo Latin Hypercube Hammersley</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>z1z2
z31.35562.8570
-1.3556
-2.8570My New Technique: Based on Partial 
Separability of the Response
15 6+ 15 6 () ()() [ ] ()() [ ]i d i d i d i dy y y y y z y E+ + + + + ++++ +
++ =1 2 1 3 1 2
15 2811 15 3
15 24 603 15
74)) ( ( D D D D D





+++
+
=
IIII
D
15 24 603 1515 2811 15 30 0 015 2811 15 315 24 603 15
L()[]()
()
()
()
()()
()
()
()
()
=
+ ++ +++
+ ++ +++














+ = d
i
i di ddi diT
i di ddi di
yyyyy
yyyyy
y E y E
1
1 31 21 2
1 31 21 2 2) 1 ( )) ( ( ) (
DDDDD
W
DDDDD
z z

==
+ + + + + + + +=d
iiiiii iiiii iii iiiii i iii jjj ii iii iii i ii id
iiii
12 2 2 2 212
945 210 30 96 24 15 6 2            





=
004877409 . 0 005583293 . 0 0 00376481 . 0 000013884 . 0005583293 . 0 223202195 . 0 0 018541911 . 0 00376481 . 00 0 24178391 . 0 0 000376481 . 0 018541911 . 0 0 223202195 . 0 005583293 . 0000013884 . 0 00376481 . 0 0 005583293 . 0 004877409 . 0
W




===
===




+



+ +
+




+



+ +
=05
12
3
13
13 3
6
05
13
13
12
3 3
6 2d
2 12 1 2 1
d
2 12 1 2 1
2 ) ( t t
tt t
r t t
tt t
r
jjij j ij i
jjij j ij i
 P P
 P P
 




=5 4 34 34 3 23 23 2
945 0 105 0 150 96 0 12 0105 0 15 0 30 12 0 2 015 0 3 0
r r rr rr r rr rr r r
M
Used to estimate transmitted variance, its very accurate up to fifth degree.15 6+
15 6</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Mathematical Models Are
Rapidly Growing in Power
 Moores Law  density 2X / 18 months
 Better algorithms being developed
Source: NSF</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Outline
 Motivation &amp; context
 Techniques for computer experiments
 Monte Carlo
 Importance sampling Latin hypercube sampling Hammersley sequence sampling Quadrature and cubature
 Some cautions</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Latin Hypercube Sampling
100 Monte 
Carlo 
Samples
100 Latin 
Hypercube 
Samples
McKay, Beckman, and Conover, [1979, Technometrics ] proved that LHS 
converges more quickly than MCS assuming monotonicity of the response.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Outline
 Motivation &amp; context
 Techniques for computer experiments
 Monte Carlo
 Importance sampling
 Latin hypercube sampling
 Hammersley sequence sampling
 Quadrature and cubature
 Some cautions</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Sampling Techniques for Computer 
Experiments
Random 
SamplingStratified 
SamplingLatin Hypercube 
Samplingclump
gap</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Monte Carlo Method
 Let's say there is a function y(x)where x is a 
vector random variables 
 Create samples x(i)
 Compute corresponding values y(x(i))
 Study the population to obtain estimates and 
make inferences
 Mean of y(x(i)) is an unbiased estimate of E(y(x)) 
 Stdev of y(x(i)) is an unbiased estimate of (y(x))
 Histogram of y(x(i)) approaches the pdf of y(x)
Fishman, George S., 1996, Monte Carlo: Concepts, Algorithms, and Application s, Springer.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Mathematical Models
are Becoming Easier to Use
 A wide range of models are available
 Finite Element Analysis
 Computational fluid dynamics
 Electronics simulations
 Kinematics
 Discrete event simulation
 Sophisticated visualization &amp; animation make 
results easier to communicate
 Many tedious tasks are becoming automated 
(e.g., mesh generation and refinement)</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Example: A Chemical Process
 Objective is to generate chemical species Bat 
a rate of 60 mol/min 
) ( ) (RB B RA A i p H r H r V T T C F Q+ +  =RT E
AAi
AAe kCC/ 01+=
RT EB
BART E
A Bi
Be kC e k CCA
/ 0/ 0
1
++=
ART E
A A C e k rA/ 0= 
ART E
A BRT E
B B C e k C e k rA B / 0 / 0   = 
Adapted from Kalagnanam and Diweka r, 1997, An Efficient Sampling 
Technique for Off-Line Quality Control, Technometrics (39 (3) 308-319.QF  TiCAiCBi
F  T  CACBQF  TiCAiCBi
F  T  CACB</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Cubature
() ()[] ()[]  
+
=+
=
 
 
 
 ++ ++  ++ +++ =
2 / ) 1 (
1) ( ) (
2 22 1
1) ( ) (
2 222 121
2 /
) () 2 ( ) 1 () 1 ( 2) () 2 ( ) 1 () 7 (
22) () 2 (1)) ( (
d d
jj jd
jj jn n
y yd ddy yd d dd dydd d d y e y ET
b b a a 0z z z z zz zK L
0.4 0.2 0 0.2 0.4
0.40.20.20.4 
Sampling pattern for d=9 
projected into a plane 
d2+3d+3=111
Integrates exactly all Gaussian weighted 
multivariate polynomials of degree 5 or less.r ir ir i
r d dr d di d i d dd
r
i
&gt;=&lt;



+ +  ++  + +

0) 2 () 1 )( 1 () 1 )( 2 (1
) (a{} ()

+ = &lt; + 1 , 2 , 1 , :) 1 ( 2) ( ) ( ) (d l l kddl k jK a a b
[Lu and Darmofal, 2003]Used recursively to estimate transmitted 
variance, its exact up to second degree.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Expectation of a Function
xy(x)
E(x)y(E(x))
E(y(x))S
fx(x) fy(y(x))E(y(x))-y(E(x)) E(y(x))y(E(x))</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Broken stick problem (PDF)
Working in sample space</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/esd-86-models-data-and-inference-for-socio-technical-systems-spring-2007/resources/lec3/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>Scaling to Get Expected 
Travel Distance
x(x1, y1)(x2, y2)
D = |X1-X2| + |Y1-Y2|
E[D] = E[|X1-X2| + |Y1-Y2|]
E[D] = (1/3)[Xo+ Yo]XoYo</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>0 1/2 11
1/2
X1X2Step 3:  Probability Uniform over the Square</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>4  Steps: Functions of R.V.s
1.  Define the Random Variables
2.  Identify the joint sample space
3.  Determine the probability law over the         
sample space
4.  Carefully work in the sample space to 
answer any question of interest
4a.  Derive the CDF of the R.V. of interest, working 
in the original sample space whose probability
law you know
4b  Take the derivative to obtain the desired PDF</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Rectangular Response Area
xy
(x1, y1)(x2, y2)
XoYo
D = |X1-X2| + |Y1-Y2|</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>fY(y)={y  0y1
1y  1y2
fY(y)dy= fx1(v)fx2(yv)dvdy
v=0v=1Convolution
v 0 1 y y-1fx1(v) fx2(y-v)</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>fixed location
ambulance
accident 0 1 1/2
2
0 1/2yfD(y)
E[D] = 1/4, a 25% reduction</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>1.  R.V.;s
X1= location of the accident
X2= location of the ambulance
 D = response distance = | X1-X2|
2.  Joint sample space is unit square in 
X1X2plane
3.  PDF over square is uniformResponse Distance of an 
Ambulance
0 1ambulanceaccident</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>1
d 0 1 2 3K= 0 K= 1 K= 2 K= 3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Breaking a Stick
Mark the stick....12 24 36</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Now let
Y=X1+ X2, where X1and X2are iid uniform over [0,1]
x1x2
0 yy
yFY(y)=P{Yy}={y2/2  0 y1
1(2y)2/2  1 y2
fY(y)={y 0y1
2y  1y2
fY(y)dy= fx1(v)fx2(yv)dvdy
v=0v=1
Convolution</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Marine Transfer Station
http://www.dattner.com/html/civic1a.htmlCourtesy of Dattner Architects. Used with permission.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>After Step 4, HAPPINESS!
0 1/2 11
1/2
X1X2http://web.mit.edu/urban_or_book/www/animated-eg/stick/f1.0.html</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Functions of Random Variables
Y=3X-2Z</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>x1x2
yx2= x1-y
yx2= x1+ y
x2&gt; x1
x2&lt; x1
4.a  FD(y) = P{D&lt;y} = 1 - (1-y)2, 0&lt;y&lt;1
4.b  fD(y) = 2(1-y), 0&lt;y&lt;1.Event { D&lt;y}</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Loading
Barge
Loading
Barge
Loading
Barge
Loading
BargeTug
Delivers
LIGHTSTug
Picks Up
HEAVIES
LIGHT and  HEAVY
Barges Stored
Refuse
Inflow
i(t)Barges
Shifted
By Hand
Or TugNYC  Marine Transfer Station</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Break a yardstick in 
two random places
What is the probability that a 
triangle can be formed with the 
resulting three stick pieces?</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>4  Steps:
1.  Define the Random Variables
2.  Identify the joint sample space
3.  Determine the probability law over the         
sample space
4.  Carefully work in the sample space to 
answer any question of interest</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Now suppose
Y=MAX{ X1, X2 ,X3, ...XN}, where Xiare iid uniform over [0,1]
FY(y) = P{ Y&lt;y} = yNWhy?
fY(y) = NyN-1  N=1,2,; 0&lt;y&lt;1
OK, so now we can do Max and Min.</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>1
d 0 1 2 3</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>x1x2
0
4a.  Derive the CDF of the R.V. of interest, working 
in the original sample space whose probability
law you know
4b  Take the derivative to obtain the desired PDF4. Carefully work in the sample space to answer 
any question of interest.
yyFY(y) = P{ Y&lt;y}=1-(1- y)2
fY(y)=d/d y[FY(y)]
fY(y)=2(1-y ), 0&lt;y&lt;1</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>x1x2
yx2= x1-y
yx2= x1+ y
x2&gt; x1
x2&lt; x1Event { D&lt;y}
1
1 0</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Photos of ambulance and a dispatch center 
removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>1
d 02.  The Sample Space</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>1
d 0 1 2 3K= 0 K= 1 K= 2 K= 3
x 1-x1-xx3.  Joint Probability Distribution
a) Dand are independent.
b) is uniformly distributed over [0, 1]
fD,(d, ) = fD(d) f() = fD(d)(1) = fD(d), d &gt; 0, 0 &lt; &lt;1</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>D= barge loads of garbage produced     
on a random day  (continuous r.v.)
= fraction of barge that is filled at 
beginning of day  (0 &lt; &lt; 1)
K= total number of completely filled 
barges produced by a facility on a 
random day  ( Kinteger)
K=  [ D+ ] = integer part of D + 1.  The R.V.s</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>fY(y)={y  0y1
1y  1y2
fY(y)dy= fx1(v)fx2(yv)dvdy
v=0v=1Convolution
v 0 1y y-1fx1(v) fx2(y-v)</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>1.  Random Variables:
X1= location of first mark
X2= location of second mark12 24 36
x1 x2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>ESD.86                      Feb. 14, 2007
Broken stick experiment
D=Min[ X1, X2 ]
D=Max[X1,X2 ]
S=X1 + X2
Convolution
Functions of Random Variables</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>What Have We Learned Today?What Have We Learned Today?
4  Steps: Functions of R.V.s
1.  Define the Random Variables
2.  Identify the joint sample space
3.  Determine the probability law over the         
sample space
4.  Carefully work in the sample space to 
answer any question of interest
4a.  Derive the CDF of the R.V. of interest, working 
in the original sample space whose probability
law you know
4b  Take the derivative to obtain the desired PDF</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Barges in 
Action
Photo courtesy of Eddie Codel. 
http://www.flickr.com/photos/ekai/15899569/</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>fY(y)={y  0y1
1y  1y2
fY(y)dy= fx1(v)fx2(yv)dvdy
v=0v=1Convolution
v 0 1 y y-1fx1(v) fx2(y-v)</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>fD(y) = 2(1- y)
y2
01</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Step 4:  Carefully Work Within 
the Sample Space
What conditions need to be satisfied so 
that a triangle can be formed?
Suppose we consider first the case shown, x
1&gt; x2
12 24 36
x1 x2</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>0 1/2 11
1/2
X1X2Step 2:  Joint Sample Space</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>1. Define the Random Variables
Y=MIN{ X1, X2}, where X1and X2are iid uniform over [0,1]
Identify the joint sample space
x1x2
1
0 1
3. Determine the probability law over the 
sample space - uniform</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>1
d 0 1 2 3K= 0 K= 1 K= 2 K= 3
x 1-x1-xx4.  Working in the Joint Sample Space
Look at E [ K |D = d ]
Let d= i+ x0 &lt; x &lt;1
E [K |D = i + x ] = i(1 -x) + (i+ 1) x= i+ x= d
Implies E [ K] = E [ D]
Data Collection Implications?  Quantized Data?</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Now suppose
Y=MIN{ X1, X2 ,X3, ...XN}, where Xiare iid uniform over [0,1]
FY(y) = P{ Y&lt;y} = 1- P{ Y&gt;y}
FY(y) = 1- (1-y )N
fY(y) = (d/dy) FY(y)  = N(1-y )N-1; N=1,2,
0&lt;y&lt;1</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Digger
Digger
HEAVY BARGESTUG 
DELIVERS
HEAVIES
UNLOADING
BARGE
UNLOADING
BARGE
LIGHT
BARGESTUGS PICK
UP LIGHTSREFUSE
UNLOADEDFresh Kills Landfill
HEAVY BARGESFigure by MIT OCW.
Figure by MIT OCW .
Figure by MIT OCW .
Figure by MIT OCW .</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Sums of Random Variables</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>A Quantization Problem</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>1
d 0 1 2 3K= 0 K= 1 K= 2 K= 33.  Joint Probability Distribution
a) Dand are independent.
b) is uniformly distributed over [0, 1]
fD,(d, ) = fD(d) f() = fD(d)(1) = fD(d), d &gt; 0, 0 &lt; &lt;1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Problem Framing,
Formulation and
Solution</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>But first, we have a winner!
The winning submission for ESD.86, for most 
blatant misuse, abuse or misinterpretation of 
statistics and probability in the media.
Submitted by Roberto Perez-Franco.
Original article New York Times:  
51% of Women Are Now Living Without Spouse, 
New York Times , January 16, 2007, Section A; 
Column 1; National Desk; Pg. 1
Today: HEARING ON 'WARMING OF PLANET
CANCELED BECAUSE OF ICE STORM</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Marking the Results
 
 |                           |
 x2 x112 24 36</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>More Examples of Functions of 
Random Variables</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>fixed location
ambulance
accident 0 1 1/2In previous problem, E[D] = 1/3What if we fix the location of the ambulance at X
2= 1/2?</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>x1x2
11
0</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
