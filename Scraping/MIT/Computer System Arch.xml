<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/</course_url>
    <course_title>Computer System Architecture</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Theory of Computation </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Simple Instruction Pipelining (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l05_singlecycle/</lecture_pdf_url>
      <lectureno>L5</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L5- 3 
Arvind 
Processor Performance
Time = Instructions Cycles Time 
Program  Program  * Instruction * Cycle 
 Instructions per program depends on source code, compiler 
technology, and ISA 
 Cycles per instructions (CPI) depends upon the ISA and the 
microarchitecture 
 Time per cycle depends upon the microarchitecture and the 
base technology 
this lecture
Microarchitecture CPI cycle time 
Microcoded &gt;1 short 
Single-cycle unpipelined 1 long 
Pipelined 1 short 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L5- 2 
Instruction Set Architecture (ISA) Arvind 
versus Implementation 
 ISA is the hardware/software interface 
 Defines set of programmer visible state 
 Defines instruction format (bit encoding) and instruction 
semantics
E x a m p l e s : MIPS, x86, IBM 360, JVM
 Many possible implementations of one ISA
 360 implementations: model 30 (c. 1964), z900 (c. 2001) 
x 8 6  i m p l e m e n t a t i o n s : 8086 (c. 1978), 80186, 286, 386, 486, 
Pentium, Pentium Pro, Pentiu m-4 (c. 2000),  AMD Athlon, 
Transmeta Crusoe, SoftPC 
 MIPS implementations: R2000, R4000, R10000, ...
J V M : HotSpot, PicoJava, ARM Jazelle, ...
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L5- 31 
How to divide the datapath Arvind 
into stages 
Suppose memory is significantly slower than 
other stages. In particular, suppose 
= 10 unitstIM
= 10 units
tDM
= 5 units
tALU 
= 1 unittRF
= 1 unit
tRW 
Since the slowest stage determines the clock, it 
may be possible to combine some stages without 
any loss of performance 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Hardware Elements
 Combinational circuits OpSelect 
 
Result 
Comp? A 
B ALU Sel 
O A0 
A1 
An-1 Mux... A 
Demux... O0 
O1 
On
1 Sel 
A 
Decoder... O0 
O1 
On-1 Mux, Demux, Decoder, ALU, ... - Add, Sub, ... 
- And, Or, Xor, Not, ... 
- GT, LT, EQ, Zero, ... 
lg(n) lg(n) 
lg(n) 
 Synchronous state elements 
 Flipflop, Register, Register file, SRAM, DRAM 
Clk 
D 
Q Enff 
Q D 
Clk En 
ff 
Q0 D0 
Clk En ff 
Q1 D1 
ff 
Q2 D2 
ff 
Qn-1 Dn-1 
... ... 
... register 
Edge-triggered: Data is sampled at the rising edge 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L5- 30 
Arvind 
An Ideal Pipeline 
stage 
1 stage 
2 stage 
3 stage 
4 
 All objects go through the same stages 
 No sharing of resources between any two stages 
 Propagation delay through all pipeline stages is equal 
 The scheduling of an object entering the pipeline 
is not affected by the objects in other stages 
These conditions generally hold for industrial 
assembly lines. 
But can an instruction pipeline satisfy the last condition? 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L5- 10 
Arvind 
Instruction Execution 
Execution of an instruction involves 
1. instruction fetch
2. decode and register fetch
3. ALU operation
4. memory operation (optional)
5. write back
and the computation of the address of the 
next instruction 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L5- 16 
Arvind 
Load/Store Instructions: Harvard Datapath
0x4 
Add 
addr 
inst 
Inst. 
Memory PC RegWrite MemWrite 
clk WBSrc 
ALU / Mem 
base 
disp 
ALU 
Control z ALU clk 
rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext clk 
addr 
wdata rdataData 
Memory we 
GPRs 
OpCode RegDst ExtSel OpSel BSrc 
opcode rs rt displacement6 5 5 16 addressing mode 
(rs) + displacement 
31 26 25 21 20  16 15 0 
rs is the base register 
rt is the destination of a Load or the source for a Store 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L5- 14 
Arvind 
Datapath for ALU Instructions
0x4 
Add 
addr 
inst 
Inst. 
Memory PC RegWrite 
clk 
&lt;31:26&gt;, &lt;5:0&gt; 
BSrc Imm 
Ext z ALU clk 
rd1 rs1 
rs2 
ws 
wd rd2 we&lt;25:21&gt; 
&lt;20:16&gt; 
&lt;15:0&gt; 
ALU 
Control &lt;15:11&gt; GPRs 
OpCode	 RegDst ExtSel OpSel 
rt / rd Reg / Imm
6 5 5  5 5 6
0 rs rt  rd 0 func 
opcode rs rt immediate rd  (rs) func (rt) 
rt  (rs) op immediate 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L5- 25 
Hardwired Control is pure Arvind 
Combinational Logic 
combinational 
logic ExtSel 
BSrc 
OpSelop code 
MemWrite 
WBSrczero? 
RegDst 
RegWrite 
PCSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L5- 12 
Arvind 
Datapath: Reg-Imm ALU Instructions
Imm 
Ext inst&lt;15:0&gt; 0x4 
Add 
clk addr 
inst 
Inst. 
Memory PC 
z ALU RegWrite 
clk 
rd1 rs1 
rs2 
ws 
wd rd2 we 
ALU 
Control GPRs inst&lt;25:21&gt; 
inst&lt;20:16&gt; 
inst&lt;31:26&gt; 
OpCode ExtSel 
655 1 6 
opcode rs rt immediate rt  (rs) op immediate 
31 26 25 2120 16 15 0 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L5- 27 
Arvind 
Hardwired Control Table
Opcode ExtSel BSrc OpSel MemW RegW WBSrc RegDst PCSrc 
ALU * Reg Func no yes ALU rd pc+4 
ALUi sExt16 Imm Op no yes ALU rt pc+4 
ALUiu uExt16 Imm Op no yes ALU rt pc+4 
LW sExt16 Imm + no yes Mem rt pc+4 
SW sExt16 Imm + yes no * * pc+4 
BEQZz=0 sExt16 * 0 ? n o n o * * br 
BEQZz=1 sExt16 * 0 ? n o n o * * pc+4 
J * * * no no * * jabs 
JAL * * * no yes PC R31 jabs 
JR * * * n o n o * * rind 
JALR * * * n o y e s PC R31 r i n d 
BSrc = Reg / Imm WBSrc = ALU / Mem / PC    
RegDst = rt / rd / R31 PCSrc = pc+4 / br / rind / jabs 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L5- 21 
Arvind 
Absolute Jumps (J, JAL) 
RegWrite 
Add 
Add 
clk WBSrc MemWrite 
addr 
wdata rdataData 
Memory we 
z clk 
clk addr 
inst 
Inst. Memory PC rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext ALU 
ALU 
Control 31 PCSrc 
br 
pc+4 rind 
jabs 
0x4 
zero? GPRs 
OpCode RegDst ExtSel OpSel BSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L5- 24 
Arvind Single-Cycle Hardwired Control:
Harvard architecture 
We will assume 
 clock period is sufficiently long for all of 
the following steps to be completed:
1. instruction fetch
2. decode and register fetch
3. ALU operation
4. data fetch if required
5. register write-back setup time
tC &gt; tIFetch + tRFetch + tALU+ tDMem+ tRWB 
 At the rising edge of the following clock, the PC,
the register file and the memory are updated
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>23 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>tC&gt; max {tIM, tRF, tALU, tDM, tRW}  = tDM tC&gt; max {tIM, tRF+tALU, tDM, tRW}  = tDM6.823 L5- 32 
Arvind 
Alternative Pipelining 
write 
-backfetch 
phase execute 
phase decode &amp; Reg-fetch 
phase memory 
phase addr 
wdata Memory we 
ALU 
Imm 
Ext 0x4 
Add 
addr 
Inst. 
Memory rd1 
GPRs rs1 
rs2 
ws 
wd rd2 we 
IRPC 
rdata 
Data rdata 
phase 
tC &gt; max {tIM, tRF+tALU, tDM+tRW} = tDM+ tRW 
increase the critical path by 10% 
Write-back stage takes much less time than other stages. 
Suppose we combined it with the memory phase 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L5- 17 
Arvind 
MIPS Control Instructions 
Conditional (on GPR) PC-relative branch 
6 5 5 16 
opcode rs offset BEQZ, BNEZ 
Unconditional register-indirect jumps
6 5 5  16 
opcode rs JR, JALR 
Unconditional absolute jumps 
6 26 
opcode target J, JAL 
 PC-relative branches add offset 4 to PC+4 to calculate the 
target address (offset is in words): 128 KB range 
 Absolute jumps append target 4 to PC&lt;31:28&gt; to calculate 
the target address: 256 MB range 
 jump-&amp;-link stores PC+4 into the link register (R31) 
 All Control Transfers are delayed by 1 instruction 
we will worry about the branch delay slot later 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L5- 22 
Arvind 
Harvard-Style Datapath for MIPS 
RegWrite 
Add 
Add 
clk WBSrc 
addr 
wdata rdataData 
Memory we 
z clk 
zero? clk addr 
inst 
Inst. Memory PC rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext ALU 
ALU 
Control 31 PCSrc 
br 
rind 
jabs 
pc+4 
0x4 MemWrite 
GPRs 
OpCode RegDst ExtSel OpSel BSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L5- 19 
Arvind 
Register-Indirect Jumps (JR) 
RegWrite 
Add 
Add 
clk WBSrc MemWrite 
addr 
wdata rdataData 
Memory we 
z clk 
clk addr 
inst 
Inst. Memory PC rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext ALU 
ALU 
Control PCSrc 
br 
pc+4 rind 
0x4 
zero? GPRs 
OpCode RegDst ExtSel OpSel BSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L5- 8 
Arvind 
Implementing MIPS:
Single-cycle per instruction
datapath &amp; control logic
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L5- 11 
Arvind 
Datapath: Reg-Reg ALU Instructions
0x4 
Add 
clk addr 
inst 
Inst. 
Memory PC 
inst&lt;5:0&gt; z ALU 
Control RegWrite 
clk 
rd1 rs1 
rs2 
ws 
wd rd2 we inst&lt;25:21&gt; 
inst&lt;20:16&gt; 
inst&lt;15:11&gt; 
OpCode ALU GPRs 
RegWrite Timing?
6 5 5  5 5 6 
0 rd 0 func rs rt  rd  (rs) func (rt) 
31 26 25 21 20 16 15 11 5 0 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L5- 7 
Arvind 
A Simple Memory Model
WriteEnable 
Clock 
Address 
ReadData 
WriteData 
Reads and writes are always completed in one cycle MAGIC 
RAM 
 a Read can be done any time (i.e. combinational) 
 a Write is performed at the rising clock edge 
if it is enabled 
 	the write address and data 
must be stable at the clock edge 
Later in the course we will present a more realistic 
model of memory 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Single-Cycle Processors:
Datapath &amp; Control
Arvind
Computer Science &amp; Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L5- 26 
Arvind 
ALU Control &amp; Immediate Extension
Inst&lt;31:26&gt; (Opcode) 
Decode Map Inst&lt;5:0&gt; (Func) 
ALUop 
0? + 
OpSel 
( Func, Op, +, 0? ) 
ExtSel 
( sExt16, uExt16, 
High16) 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L5- 20 
Arvind 
Register-Indirect Jump-&amp;-Link (JALR) 
RegWrite 
Add 
Add 
clk WBSrc MemWrite 
addr 
wdata rdataData 
Memory we 
z clk 
clk addr 
inst 
Inst. Memory PC rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext ALU 
ALU 
Control 31 PCSrc 
br 
pc+4 rind 
0x4 
zero? GPRs 
OpCode RegDst ExtSel OpSel BSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L5- 15 
Arvind 
Datapath for Memory Instructions
Should program and data memory be separate? 
Harvard style: separate (Aiken and Mark 1 influence) 
- read-only program memory 
- read/write data memory 
at some level the two memories have 
to be the same 
Princeton style: the same (von Neumanns influence) 
- A Load or Store instruction requires 
accessing the memory more than once during its execution 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L5- 4 
Arvind 
Microarchitecture: Implementation of an ISA
Controller 
Data 
path control 
points status 
lines 
Structure: How components are connected. 
Static 
Behavior: How data moves between components 
Dynamic 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L5- 9 
Arvind 
The MIPS ISA
Processor State 
32 32-bit GPRs, R0 always contains a 0
32 single precision FPRs, may also be viewed as
16 double precision FPRs 
FP status register, used for FP compares &amp; exceptions 
PC, the program counter 
some other special registers 
Data types
8-bit byte, 16-bit half word 
32-bit word for integers
32-bit word for single precision floating point
64-bit word for double precision floating point
Load/Store style instruction set
data addressing modes- immediate &amp; indexed 
branch addressing modes- PC relative &amp; register indirect Byte addressable memory- big endian mode 
All instructions are 32 bits 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L5- 29 
Arvind 
Pipelined Datapath
0x4 
Add
addr PC we 
rs1 
rs2 
rd1 we ws addr rdata IR ALUwd rd2 rdata GPRs Data Inst. 
MemoryImm Memory wdata Ext 
writefetch decode &amp; Reg-fetch execute memory -backphase phase phase phase phase 
Clock period can be reduced by dividing the execution of an 
instruction into multiple cycles 
tC &gt; max {tIM, tRF, tALU, tDM, tRW} ( = tDM probably ) 
However, CPI will increase unless instructions are pipelined 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L5- 6 
Arvind 
Register Files 
Clock WE 
ReadData1 ReadSel1 
ReadSel2 
WriteSel Register 
file 
2R+1W ReadData2 
WriteData rd1 rs1 
rs2 
ws 
wd rd2 we 
ws clk 
register 1 wd 
we rs2 rs1 
rd1 
rd2 register 0    
 32 
32 32 
32 
32 32 5 5 
5 
register 31 
	No timing issues in reading a selected register 
	Register files with a large number of ports are difficult 
to design 
	Intels Itanium, GPR File has 128 registers with 8 read ports and 
4 write ports!!! 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L5- 33 
Arvind 
Maximum Speedup by Pipelining 
Assumptions 	 Unpipelined  Pipelined Speedup 
tC 	 tC 
tt1. tIM = tDM = 10, 
ALU = 5, 
RF = tRW= 1
4-stage pipeline 	 27 10 2.7 
2. tIM=tDM = tALU = tRF = tRW = 5 
4-stage pipeline 25 10 2.5 
3. 	tIM=tDM = tALU = tRF= tRW = 5 
5-stage pipeline 25 5 5.0 
It is possible to achieve higher speedup with more 
stages in the pipeline. 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>34 
Thank you !</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L5- 13 
Arvind 
Conflicts in Merging Datapath
0x4 
Add 
addr 
inst 
Inst. 
Memory PC 
clk RegWrite Introduce 
Imm 
Ext z ALU clk 
rd1 rs1 
rs2 
ws 
wd rd2 we 
inst&lt;15:0&gt; 
ALU 
Control inst&lt;5:0&gt; muxes 
GPRs inst&lt;25:21&gt; 
inst&lt;20:16&gt; 
inst&lt;31:26&gt; inst&lt;15:11&gt; 
OpCode ExtSel 
6 5 5  5 5 6 
0 rs rt  rd 0 func 
opcode rs rt immediate rd  (rs) func (rt) 
rt  (rs) op immediate 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L5- 18 
Arvind 
Conditional Branches (BEQZ, BNEZ) 
Add PCSrc 
clk WBSrc MemWrite 
addr 
wdata rdataData 
Memory we 
z clk 
clk addr 
inst 
Inst. Memory PC rd1 rs1 
rs2 
ws 
wd rd2 we 
Imm 
Ext ALU 
ALU 
Control Add br 
pc+4 RegWrite 
0x4 
zero? GPRs 
OpCode RegDst ExtSel OpSel BSrc 
September 26, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L5- 28 
Arvind 
Pipelined MIPS 
To pipeline MIPS: 
	First build MIPS without pipelining with CPI=1 
	Next, add pipeline registers to reduce cycle 
time while maintaining CPI=1 
September 26, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>History of Calculation and Computer Architecture (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l01_earlydev/</lecture_pdf_url>
      <lectureno>L1</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L1-5 
Arvind 
Difference Engine 
1823 
	Babbages paper is published 
1834 
	The paper is read by Scheutz &amp; his son in Sweden 
1842 
	Babbage gives up the idea  of building it;he is onto 
Analytic Engine! 
1855 
	Scheutz displays his machine at the Pari s World Fare 
	Can compute any 6th degree polynomial 
	Speed: 33 to 44 32-digit numbers per minute! 
Now the machine is at the Smithsonian 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L1-15 
Arvind 
Technology Issues 
ENIAC  EDVAC 
18,000 tubes 4,000 tubes 
20 10-digit numbers 2000 word storage 
mercury delay lines 
ENIAC had many asynchronous parallel units 
but only one was active at a time 
BINAC : Two processors that checked each other for reliability. 
Didnt work well because processors never 
agreed 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L1-20 
Arvind 
Software Developments 
up to 1955 Libraries of numerical routines
-Floating point operations 
- Transcendental functions 
-M a t r i x  m a n i p u l a t i o n ,  e q uation solvers, . . . 
1955-60 High level Languages -Fortran 1956 
Operating Systems -
-Assemblers, Loaders, Linkers, Compilers 
- Accounting programs to keep track of 
usage and charges 
Machines required experienced operators 
	Most users could not be expected to understand 
these programs, much less write them 
	Machines had to be sold with a lot of resident software 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L1-23 
Arvind 
Compatibility 
Essential for portability and competition
Its importance increases with the market size 
but it is also the most regressive force 
What does compatibility mean? 
Instruction Set Architecture (ISA) compatibility
The same assembly program can run on an
upward compatible model
then IBM 360/370 ... now Intel x86 (IA32), IA64 
System and application soft ware developers expect 
more than ISA compatibility (APIs) 
applications 
operating system 
proc + mem + I/O Java? 
Wintel 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L1-18 
Arvind 
Commercial Activity: 1948-52 
IBMs SSEC 
Selective Sequence Electronic Calculator 
	150 word store. 
	Instructions, constraints, and tables of data were 
read from paper tapes. 
	66 Tape reading stations! 
	Tapes could be glued together to form a loop! 
	Data could be output in one phase of computation 
and read in the next phase of computation. 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L1-17 
Arvind 
Dominant Problem: Reliability
Mean time between failures (MTBF) 
MITs Whirlwind with an MTBF of 20 min. was perhaps 
the most reliable machine ! 
Reasons for unreliability:
1. Vacuum Tubes 
2. Storage medium
acoustic delay lines
mercury delay lines
Williams tubes
Selections
CORE J. Forrester 1954 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L1-22 
Arvind 
Microprocessors Economics since 1990s
 Huge teams design state-of-the-art 
microprocessors 
PentiumPro ~ 500 engineers
Itanium  ~ 1000 engineers
 Huge investments in fabrication lines and 
technology 
 to improve clock-speeds and yields 
 to build new peripheral chip s (memory controllers, ...) 
 Economics 
 price drops to one tenth in 2-3 years 
 need to sell 2 to 4 milli on units to breakeven 
The cost of launching a new ISA is prohibitive 
and the advantage is dubious! 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L1-8 
Arvind 
The first programmer
Ada Byron aka Lady Lovelace 1815-52
Ada Byron a.k.a "Lady Lovelace" 
Image removed due to copy right restrictions. To 
view image, visit 
http://www.sdsc.edu/ScienceWomen/lovelace.ht 
ml 
Adas tutor was Babbage himself! 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Early Developments:
From Difference Engine to IBM 
701
Arvind
Computer Science &amp; Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L1-2 
ArvindCharles Babbage 1791-1871
Lucasian Professor of Mathematics, 
Cambridge University, 1827-1839
Charles Babbage 
Image removed due to copyright restrictions.
To view image, visit 
http://www.rtpnet.org/robroy/Babbage/hawks.html
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L1-13 
Electronic Discrete Variable Arvind 
Automatic Computer (EDVAC) 
	ENIACs programming system was external 
 Sequences of instructions were executed 
independently of the results of the calculation 
 Human intervention required to take instructions 
out of order 
	Eckert, Mauchly, John von Neumann and others 
designed EDVAC (1944) to solve this problem 
 Solution was the stored program computer 
 program can be manipulated as data 
	First Draft of a report on EDVAC was published in 
1945, but just had von Neumanns signature! 
 In 1973 the court of Minneapolis attributed the 
honor of inventing the computer to John Atanasoff 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L1-7 
Arvind 
Analytic Engine
The first conception of a general purpose computer
1.	The store in which all variables to be operated 
upon, as well as all those quantities which have 
arisen from the results of the operations are 
placed. 
2.	The mill into which the quantities about to be 
operated upon are always brought. 
The program 
Operation variable1 variable2 variable3 
An operation in the mill required feeding two punched 
cards and producing a new punched card for the store. 
An operation to alter the sequence was also provided! 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L1-12 
Electronic Numerical IntegratorArvind 
and Computer (ENIAC) 
	Inspired by Atanasoff and Berry, Eckert and 
Mauchly designed and built ENIAC (1943-45) at 
the University of Pennsylvania 
	The first, completely electronic, operational, 
general-purpose analytical calculator! 
 30 tons, 72 square meters, 200KW 
P e r f o r m a n c e 
 Read in 120 cards per minute 
 Addition took 200 s, Division 6 ms 
 1000 times faster than Mark I 
	Not very reliable! 
Application: Ballistic calculations 
angle = f (location, ta il wind, cross wind,  
air density, temperature, weight of shell, 
propellant charge, ... ) 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L1-14 
Arvind 
Stored Program Computer
Program = A sequence of instructions 
How to control instruction sequencing? 
manual control calculators 
automatic control 
external ( paper tape) Harvard Mark I , 1944 
Zuses Z1, WW2 
internal 
plug board ENIAC 1946 
read-only memory ENIAC 1948 
read-write memory EDVAC 1947 (concept ) 
 The same storage can be used to store program 
and data 
September 7, 2005 
EDSAC 1950 Maurice Wilkes</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L1-21 
Factors that Influence Arvind 
Computer Architecture 
Technology 
Applications Software Computer Architecture 
Compatibility 
Software played almost no  role in defining an 
architecture before mid fifties. 
special-purpose versus general-purpose 
machines 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L1-11 
Arvind 
Linear Equation Solver
John Atanasoff, Iowa State University 
1930s: 
 Atanasoff built the Linear Equation Solver. 
 It had 300 tubes! 
Application: 
 Linear and Integral differential equations 
Background: 
	Vannevar Bushs Differential Analyzer 
---an analog computer 
Technology: 
 Tubes and Electromechanical relays 
Atanasoff decided that the correct mode of 
computation was by electronic digital means. 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L1-6 
Arvind 
Analytic Engine 
1833: Babbages paper was published 
	conceived during a hiatus in the development of the 
difference engine 
Inspiration: Jacquard Looms 
	looms were controlled by punched cards 
 The set of cards with fixed punched holes 
dictated the pattern of weave  program 
 The same set of cards coul d be used with different 
colored threads  numbers 
1871: Babbage dies 
	The machine remains unrealized. 
It is not clear if the analytic engine 
could be built even  today using only 
mechanical technology 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L1-19 
Arvind 
And then there was IBM 701 
IBM 701 -- 30 machines were sold in 1953-54 
IBM 650 -- a cheaper, drum based machine, 
more than 120 were sold in 1954 
and there were orders for 750 more! 
Users stopped building their own machines.
Why was IBM late getting into computer 
technology? 
IBM was making too much money!
Even without computers, IBM revenues 
were doubling every 4 to 5 years in 40s 
and 50s. 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L1-3 
Arvind 
Charles Babbage 
	Difference Engine 1823
	Analytic Engine 1833
	The forerunner of modern digital computer! 
Application 
	Mathematical Tables  Astronomy 
	Nautical Tables  Navy 
Background 
	Any continuous function can be approximated by a 
polynomial --- Weierstrass 
Technology
	mechanical - gears, Jacquards loom, simple 
calculators 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L1-4 
Arvind 
Difference Engine
A machine to compute mathematical tables
Weierstrass: 
	Any continuous function can be approximated by a 
polynomial 
	Any Polynomial can be computed from difference tables 
An example 
f(n) = n2+n+41
d1(n) = f(n) - f(n-1) = 2n
d2(n) = d1(n) - d1(n-1) = 2
f(n) = f(n-1) + d1(n) = f(n-1) + (d1(n-1) + 2) 
n 
d2(n) 
d1(n) 
f(n) 0 1 2 3  4 . . . 
222 
8 
53 61 6 2 4 
41 43 47 
all you need is an adder! 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L1-9 
Arvind 
Babbages Influence
	Babbages ideas had great influence later 
primarily because of 
	Luigi Menabrea, who published notes of Babbages 
lectures in Italy 
	Lady Lovelace, who translated Menabreas notes in 
English and thoroughly expanded them. 
... Analytic Engine weaves algebraic patterns .... 
	In the early twentieth century - the focus 
shifted to analog computers but 
	Harvard Mark I built in 1944 is very close in spirit to 
the Analytic Engine. 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L1-10 
Arvind 
Harvard Mark I 
	Built in 1944 in IBM Endicott laboratories 
	Howard Aiken  Professor of Physics at Harvard 
	Essentially mechanical but had some electro
magnetically controlled relays and gears 
	Weighed 5 tons and had 750,000 components 
	A synchronizing clock that beat every 0.015 
seconds 
Performance:
0.3 seconds for addition 
6 seconds for multiplication 
1 minute for a sine calculation 
Broke down once a week! 
September 7, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L1-24 
Arvind 
Perpetual tension
Language/ Compiler/ Architect/Hardware 
System software designer designer 
Need mechanisms  Decompose each 
to support important mechanism into essential 
abstractions micro-mechanisms and 
determine its feasibility 
and cost effectiveness 
Determine compilation  Propose mechanisms and 
strategy; new language features for performance 
abstractions 
Architects main concerns are performance (both 
absolute and MIPs/$), and power (both 
class of software systems. 
September 7, 2005 absolute and MIPs/watt) in supporting a broad</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L1-16 
Arvind 
The Spread of Ideas 
ENIAC &amp; EDVAC had immediate impact 
brilliant engineering: Eckert &amp; Mauchley 
lucid paper: Burks, Goldstein &amp; von Neumann 
IAS Princeton 46-52 Bigelow 
EDSAC Cambridge 46-50 Wilkes 
MANIAC Los Alamos 49-52 Metropolis 
JOHNIAC Rand 50-53 
ILLIAC Illinois 49-52 
Argonne 49-53 
SWAC UCLA-NBS 
UNIVAC - the first commercial computer, 1951 
Alan Turings direct influence on these developments 
is still being debated by historians. 
September 7, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Microprocessor Evolution: 4004 to Pentium 4 (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l15_micro_evlutn/</lecture_pdf_url>
      <lectureno>L15</lectureno>
      <slides>
        <slide>
          <slideno>12</slideno>
          <text>Emer Pentium 4 Block Diagram 6.823 L15- 13 
//Bus Unit System Bus 
MEMORY SUBSYSTEM Level 2 Cache Level 1 Data Cache 
INTEGER AND FP 
EXECUTION UNITS Execution Units 
FRONT END OUT-OF-ORDER ENGINE BTB Branch Prediction Branch History Update Fetch Decode Trace Cache 
Microcode 
ROM Out-of-Order 
Execution 
Logic Retirement 
November 2, 2005 Figure by MIT OCW.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L15- 14 
EmerPentium 4 Front End
L2 Cache 
x86 instructions, 
8 Bytes/cycle 
x86 Decoder Inst. Prefetch &amp; 
TLB Front End BTB 
(4K Entries) 
Fetch Buffer 
Trace Cache Fill Buffer 
Trace Cache (12K uops) Translation from x86 
instructions to internal uops only happens on trace cache 
miss, one x86 instruction per 
cycle. 
Translations are cached in 
trace cache. Single x86 instruction/cycle 
November 2, 2005 4 uops/cycle 
6 uops/line</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L15- 29 
EmerDeep Pipeline Design
Greater potential throughput but: 
	Clock uncertainty and latch delays eat into cycle time 
budget 
 doubling pipeline depth gives less than twice frequency improvement 
	Clock load and power increases 
	more latches running at higher frequencies 
	More complicated microarchitecture needed to cover long 
branch mispredict penalties and cache miss penalties 
	from Littles Law, need more instructions in flight to cover longer 
latencies  larger reorder buffers
	P-4 has three major clock domains 
	Double pumped ALU (3 GHz), small critical area at highest speed 
	Main CPU pipeline (1.5 GHz in 0.18m) 
	Trace cache (0.75 GHz), save power 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L15- 6 
EmerIntel 8086 
Class Register Purpose 
Data: 
Address: 
Segment: 
Control: AX,BX general purpose 
CX string and loop ops only 
DX mult/div and I/O only 
SP stack pointer 
BP base pointer (can also use BX) 
SI,DI index registers 
CS code segment 
SS stack segment 
DS data segment 
ES extra segment 
IP instruction pointer (low 16 bit of PC) 
FLAGS C, Z, N, B, P, V and 3 control bits 
 Typical format R &lt;= R op M[X], many addressing modes 
 Not a GPR organization! 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>November 2, 2005 6.823 L15- 19 
Emer Line Prediction 
(Alpha 21[234]64 ) 
 Line Predictor predicts line to fetch each cycle 
 21464 was to predict 2 lines per cycle 
 Icache fetches block, and predictors predict target 
 PC Calc checks accuracy of line prediction(s) Line 
Predictor Instr 
Cache 
Branch 
Predictor 
Stack 
Indirect 
Branch 
Predictor PC 
CalcReturn</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Emer The Eighties:	6.823 L15- 8 
Personal Computer Revolution 
Personal computer market emerges 
	Huge business and consumer market for spreadsheets, word 
processing and games
	Based on inexpensive 8-bit and 16-bit micros: Zilog Z80, Mostek
6502, Intel 8088/86, 
Minicomputers replaced by workstations
	Distributed network computing and high-performance graphics for 
scientific and engineering applications (Sun, Apollo, HP,) 
	Based on powerful 32-bit micr oprocessors with virtual memory, 
caches, pipelined execution, hardware floating-point
	Commercial RISC processors developed for workstation market 
Massively Parallel Processors (MPPs) appear 
	Use many cheap micros to approach supercomputer performance 
(Sequent, Intel, Parsytec) 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L15- 11 
EmerPentium 4 uOPs
	During L1 instruction cache refill, translates complex 
x86 instructions into RISC-l ike micro-operations (uops) 
	e.g., R  R op Mem translates into 
load T, Mem # Load from Mem into temp reg 
R  R op T # Operate using value in temp 
	Execute uops using speculative out-of-order superscalar 
engine with register renaming 
	uop translation introduced in Pentium Pro family 
architecture (P6 family) in 1995 
	also used on Pentium-II and Pentium-III processors, and new 
Pentium M (Centrino) processors 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L15- 2 
First MicroprocessorEmer 
Intel 4004, 1971 
Image removed due to copyright 
restrictions. 
To view image, visit 
http://news.com.com/Images+Moores+L 
aw+turns+40/2009-1041_3-5649019-
5.html 4 - b i t 
accumulator 
architecture 
8m pMOS 
 2,300 transistors 
 3 x 4 mm2 
7 5 0 k H z  c l o c k 
 8-16 cycles/inst. 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L15- 10 
Emer Intel Pentium 4 (2000) 
Image removed due to copyright restrictions. 
To view image, visit http://www-
vlsi.stanford.edu/group/chips_micropro_body.html 
This lecture contains figures and data taken from: The microarchitecture of the 
Pentium 4 processor, Intel Technology Journal, Q1, 2001 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L15- 22 
EmerP-4 Execution Ports 
(
) 
/ /
///
// /(
) ALU 
double 
speed
Add Sub Logic Store Data Branches FP SSE Move FP SSE Store FXCH FP SSE-Add FP SSE-Mul FP SSE-Div MMX All Loads LEA SW Prefetch Store Address Add Sub Shift Rotate Exec Port 0 
FP Move ALU 
double 
speedExec Port 1 
FP Execute Integer 
Operation Load Port 
Memory 
Load Store Port 
Memory 
Store 
Figure by MIT OCW. 
 Schedulers compete for access to execution ports 
 Loads and stores have dedicated ports 
 ALUs can execute two operations per cycle 
 Peak bandwidth of 6 uops per cycle 
 load, store, plus four double-pumped ALU operations 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>5
6
7
8
9
1021
3
4
15
16
17
18
19
201211
13
146.823 L15- 18 
Emer P-4 Trace Cache Fetch
Trace Cache 
(12K uops, 2K lines of 6 uops) 
Microcode 
ROM 
CPU cycles Trace BTB 
(512 entries) Trace IP (BTB) 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 Register File 2 
Execute 
Flags 
Drive Schedule 2 
Schedule 3 
16-entry 
subroutine return 
address stack 
November 2, 2005 6 uops every two 
uop buffer TC Next IP 
TC Fetch 
Rename 
Branch Check 3 uops/cycle</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L15- 3 
Emer 
Microprocessors in the Seventies
Initial target was embedded control 
	First micro, 4-bit 4004 from Intel, designed for a 
desktop printing calculator
Constrained by what could fit on single chip 
	Single accumulator architectures 
8-bit micros used in ho bbyist personal computers 
	Micral, Altair, TRS-80, Apple-II 
Little impact on conventional computer market 
until VISICALC spreadsheet for Apple-II (6502, 
1MHz) 
	First killer business application for personal
computers
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Emer MIPS R10000 (1995)	6.823 L15- 34 
	0.35m CMOS, 4 metal layers 
	Four instructions per cycle 
	Out-of-order execution 
	Register renaming 
	Speculative execution past 4 
branches 
	On-chip 32KB/32KB split I/D Image removed due to copyright cache, 2-way set-associative 
restrictions.	 Off-chip L2 cache To view the image, visit http://www-
vlsi.stanford.edu/group/chips_micropro_ Non-blocking caches 
body.html 
Compare with simple 5-stage 
pipeline (R5K series) 
	~1.6x performance SPECint95 
	~5x CPU logic area 
	~10x design effort 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L15- 23 
EmerP-4 Fast ALUs and Bypass Path
Register 
File and 
Bypass 
Network 
L1 Data 
Cache 
	Fast ALUs and bypass network runs  at twice global clock speed 
	All non-essential circuit paths handle d out of loop to reduce circuit 
loading (shifts, mults/divs, branches, flag/ops) 
	Other bypassing takes multiple clock cycles 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Fast 
Scheduler 
(x2)5
6
7
89
1021
3
4
1516
1718
19
201211
13
146.823 L15- 21 
Emer 
(BTB) 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 
Register File 2 
Execute 
Flags 
Drive Schedule 2 
Schedule 3 Allocated/Renamed uops 
Memory uop 
Queue 
Memory 
Scheduler Fast 
Scheduler 
(x2) General 
Scheduler Simple FP 
Scheduler 
November 2, 2005 P-4 mOp Queues and Schedulers 
TC Next IP 
TC Fetch 
Rename 
Branch Check 3 uops/cycle 
Arithmetic 
uop Queue 
Ready uops compete for dispatch ports 
(Fast schedulers can each dispatch 2 ALU 
operations per cycle)</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L15- 30 
EmerScaling of Wire Delay
	Over time, transistors are getting relatively faster than 
long wires 
	wire resistance growing dramatically with shrinking width and height 
	capacitance roughly fixed for constant length wire 
	RC delays of fixed length wire rising 
	Chips are getting bigger 
	P-4 &gt;2x size of P-III 
	Clock frequency rising faster than transistor speed 
	deeper pipelines, fewer logic gates per cycle 
	more advanced circuit designs (each gate goes faster) 
 Takes multiple cycles for signal to cross chip 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Emer IBM PC, 1981 6.823 L15- 7 
Hardware 
	Team from IBM building PC prototypes in 1979 
	Motorola 68000 chosen in itially, but 68000 was late 
	IBM builds stopgap prototypes using 8088 boards from 
Display Writer word processor
	8088 is 8-bit bus version of 8086 =&gt; allows cheaper system 
	Estimated sales of 250,000 
	100,000,000s sold 
Software 
	Microsoft negotiates to provid e OS for IBM.  Later buys and 
modifies QDOS from Seattle Computer Products.
Open System 
	Standard processor, Intel 8088 
S t a n d a r d  i n t e r f a c e s 
	Standard OS, MS-DOS 
	IBM permits cloning and third-party software 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>5
6
7
89
1021
3
4
1516
171819
201211
13
14P-4 Load Schedule Speculation 
TC Next IP 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 
Register File 2 
Load Execute 1 
Load Execute 2 
Drive Schedule 2 
Schedule 3 
November 2, 2005 TC Fetch 
Rename 
Branch Check 6.823 L15- 25 
Emer 
Long delay from 
schedulers to load 
hit/miss 
 P-4 guesses that load will hit in L1 and 
schedules dependent operations to use value 
 If load misses, only dependent operations are 
replayed</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L15- 9 
EmerThe Nineties 
Advanced superscalar microprocessors appear 
 first superscalar microprocessor is IBM POWER in 1990 
MPPs have limited success in supercomputing market 
 Highest-end mainframes and ve ctor supercomputers survive 
killer micro onslaught 
64-bit addressing becomes essential at high-end 
 In 2004, 4GB DRAM costs &lt;$1,000 
Parallel microprocessor-based SMPs take over low-end server 
and supercomputer market 
Workstation and PC markets merge 
 By late 90s (except for Appl e PowerPC-based systems) RISC 
vendors have tiny share of desktop market 
 CISC x86 ISA thrives! 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>5
6
7
89
1021
3
4
1516
1718
19
201211
13
14P-4 Branch Penalty 
TC Next IP 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 
Register File 2 
Execute 
Flags 
Drive Schedule 2 
Schedule 3 
November 2, 2005 TC Fetch 
Rename 
Branch Check 6.823 L15- 26 
Emer 
20 cycle branch 
mispredict penalty 
 P-4 uses new trade secret branch 
prediction algorithm
 Intel claims 1/3 fewer mispredicts than P6 
algorithm</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L15- 12 
EmerInstruction Set Translation: 
Convert a target ISA into a host machines ISA 
	Pentium Pro (P6 family) 
	translation in hardware after instruction fetch 
	also used in AMD x86 processors 
	Pentium-4 family 
	translation in hardware at level 1 instruction 
cache refill 
T r a n s m e t a C r u s o e
	translation in software using Code Morphing 
(see lecture 24) 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L15- 24 
EmerP-4 Staggered ALU Design
	Staggers 32-bit add and flag 
compare into three 1/2 cycle 
phases 
 low 16 bits 
h i g h  1 6  b i t s 
	flag checks 
	Bypass 16 bits around every  
cycle 
	back-to-back dependent 32-bit 
adds at 3GHz in 0.18mm 
(7.2GHz in 90nm) 
	L1 Data Cache access starts 
with bottom 16 bits as index, 
top 16 bits used as tag check 
later 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L15- 15 
EmerTrace Cache
Key Idea: Pack multiple non-contiguous basic 
blocks into one contiguous trace cache line 
BR BR BR 
BR BR BR 
	Single fetch brings in  multiple basic blocks 
	Trace cache indexed by start address and next n branch 
predictions 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Emer P-III vs. P-4 Pipelines 6.823 L15- 28 
1 2 3 4 5 6 7 8 
/9 R 
1 2 3 4 5 6 7 9 8 R Fetch Fetch Decode Decode Decode Rename ROB Rd Rdy Sch Dispatch 10 
Exec Basic Pentium III Processor Misprediction Pipeline 
TC Nxt IP 10 
Rename Drive Alloc Que Sch 11 
Sch 12 
Sch 13 
Disp 14 
Disp 15 
RF 16 
RF 17 
Ex 18 
Flgs 19 
Br Ck 20 
Drive TC Fetch Basic Pentium 4 Processor Misprediction Pipeline 
Figure by MIT OCW. 
 In same process technology, ~1.5x clock frequency 
 Performance Equation: 
Time = Instructions * Cycles * Time 
Program  Program  Instruction Cycle 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>5
6
7
89
1021
3
4
1516
1718
19
201211
13
146.823 L15- 20 P-III vs. P-4 Renaming Emer 
(BTB) 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 
Register File 2 
Execute 
Flags 
Drive Schedule 2 
Schedule 3 TC Next IP 
TC Fetch 
Rename 
Branch Check Figure by MIT OCW. 
P-4 physical register file separated from ROB status. 
ROB entries allocated sequentially as in P6 family. 
One of 128 physical register s allocated from free list. 
No data movement on retire, only Retirement RAT 
updated. 
November 2, 2005 
R ROB 
Data Status 
RRF Frontend RAT RF Data ROB 
Status 
Retirement RAT 
Pentium NetBurstTM III EAX RAT 
EBX 
ECX 
EDX 
ESL 
EDL 
ESP 
EBP EAX 
EBX 
ECX 
EDX 
ESL 
EDL 
ESP 
EBP 
EAX 
EBX 
ECX 
EDX 
ESL 
EDL 
ESP 
EBP</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L15- 4 
Emer 
DRAM in the Seventies
Dramatic progress in MOSFET memory 
technology 
1970, Intel introduces first DRAM (1Kbit 
1103) 
1979, Fujitsu introduces 64Kbit DRAM
=&gt; By mid-Seventies, obvious that PCs 
would soon have &gt; 64KBytes physical 
memory 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L15- 32 
Emer P-4 Microarchitecture
( ) 
( ) / 
(  ) 
/s/
 / 
( ) /
/
(
) 
/s /
November 2, 2005 Figure by MIT OCW. Front-End BTB 
4K Entries
Trace Cache BTB 
512 EntriesInstruction TLB
Prefetcher 
Trace Cache 12K opsop Queue Microcode 
ROM 
Quad 
Pumped 
3.2 GBSystem Bus 
256 bits 64 bits wide 
Bus 
Interface 
Unit Allocator Register Renamer 
Memory op Queue Integer Floating Point op Queue 
L1 Data Cache 8Kbyte 4-wayMemory Scheduler Fast 
AGU 
Load 
Address Slow General FP Scheduler Simple FP 
FP Register Bypass Instruction Decoder 
AGU 
Store 
Address 2x ALU 
Simple 
Instr. 2x ALU 
Simple 
Instr. Slow ALU 
Complex 
Instr. FP 
MMX 
SSE 
SSE2 FP 
Move L2 Cache 
256K byte 
8-way
48 GBInteger Register File Bypass Network</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L15- 5
EmerMicroprocessor Evolution 
Rapid progress in size and speed through 70s 
 Fueled by advances in MOSFET technology and expanding markets 
Intel i432 
 Most ambitious seventies micro; started in 1975 - released 1981 
 32-bit capability-based object-oriented architecture 
 Instructions variable number of bits long 
 Severe performance, complexity, and usability problems 
Motorola 68000 (1979, 8MHz, 68,000 transistors) 
 Heavily microcoded (and nanocoded) 
 32-bit general purpose register  architecture (24 address pins) 
 8 address registers, 8 data registers 
Intel 8086 (1978, 8MHz, 29,000 transistors) 
 Stopgap 16-bit processor,  architected in 10 weeks 
 Extended accumulator architecture, assembly-compatible with 8080 
 20-bit addressing through segmented addressing scheme 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>November 2, 2005 6.823 L15- 27 
Emer Tournament Branch Predictor 
(Alpha 21264) 
 Choice predictor learns whether best to use local or global 
branch history in predicting next branch 
 Global history is speculatively updated but restored on 
mispredict 
 Claim 90-100% success on range of applications Local 
history 
table 
(1,024x10b 
) 
PC Local 
prediction 
(1,024x3b) Global Prediction 
(4,096x2b) 
Choice Prediction 
(4,096x2b) 
Global History (12b) Prediction</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Microprocessor Evolution:
4004 to Pentium-4
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Based on the material prepared by
Krste Asanovic and Arvind
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L15- 33 
EmerMicroarchitecture Comparison
In-Order Out-of-Order 
Execution Execution
Decode 
Execute 
Commit Decode 
Execute 
Commit ROB Br. Pred. 
Resolve Br. Pred. 
Resolve 
In-Order 
Out-of-Order Fetch Fetch 
In-Order 
In-Order 
	Speculative fetch but not  Speculative execution, with
speculative execution -branches resolved after later 
branch resolves before instructions complete
later instructions complete
	Completed values held in rename 	Completed values held in registers in ROB or unified physical bypass network until register file until commit commit 
pipeline, and both can execute multiple instructions per cycle 
November 2, 2005  Both styles of machine can use same branch predictors in front-end fetch 
 Common to have 10-30 pipeline stages in either style of design</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L15- 17
EmerTrace Cache Advantages 
	Removes x86 decode from branch mispredict penalty 
	Parallel x86 decoder took 2.5 cycles in P6, would be 5 cycles in P-4 
design 
	Allows higher fetch bandwidth fetch for correctly predicted 
taken branches 
	P6 had one cycle bubble for correctly predicted taken branches 
	P-4 can fetch a branch and its target in same cycle 
	Saves energy 
	x86 decoder only powered up on trace cache refill 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>5
6
7
89
1021
3
4
1516
171819
201211
13
146.823 L15- 31 
Emer Visible Wire Delay in P-4 Design 
Pipeline stages dedicated to just 
driving signals across chip! TC Next IP 
Drive 
Alloc 
Queue 
Schedule 1 
Dispatch 1 
Dispatch 2 
Register File 1 
Register File 2 
Execute 
Flags 
Drive Schedule 2 
Schedule 3 TC Fetch 
Rename 
Branch Check 
November 2, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L15- 16 
EmerPentium 4 Trace Cache
	Holds decoded uops in predicted program flow order, 6 
uops per line 
Code in memory
cmp
br T1	Code packed in trace cache...
T1: sub (6 uops/line)
br T2
...
T2: mov
sub
br T3
...
T3: add
sub
cmp br T1 sub 
br T2 mov sub 
br T3 add sub 
mov br T4 T4:... 
mov Trace cache fetches one 6 uop line br T4 every 2 CPU clock cycles (runs at 1/2 ... 
T4:	main CPU rate) 
November 2, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Relaxed Memory Models (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l20_relaxedmm/</lecture_pdf_url>
      <lectureno>L20</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L20- 14 
ArvindWeaker Memory Models &amp;
Memory Fence Instructions
	Architectures with weaker memory models 
provide memory fence instructions to 
prevent the permitted reorderings of loads 
and stores 
Store(a1, v); The Load and Store can be 
Fencewr 
Load(a2); reordered if a1 =/= a2. 
Insertion of Fencewr will 
disallow this reordering 
Similarly: Fencerr; F e n c erw; F e n c eww; 
SUNs Sparc: MEMBAR; 
MEMBARRR; MEMBARRW; MEMBARWR; MEMBARWW 
PowerPC: Sync; EIEIO 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L20- 17 
Backlash in the architecture Arvind 
community 
	Abandon weaker memory models in 
favor of SC by employing aggressive 
speculative execution tricks.
	all modern microprocessors have some ability to 
execute instructions specul atively, i.e., ability to 
kill instructions if something goes wrong (e.g. 
branch prediction) 
	treat all loads and stores that are executed out of 
order as speculative and kill them if a signal is received from some other processor indicating that 
SC is about to be violated. 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L20- 7 
Arvind 
Example 3: Non-FIFO Store buffers
Process 1 Process 2 
Store(a,1); r1 := Load(flag); 
Store(flag,1); r2 := Load(a); 
Question: Is it possible that r1=1 but r2=0? 
 Sequential consistency: No 
 With non-FIFO store buffers: Yes 
Sparcs PSO memory model 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L20- 19 
Arvind 
Properly Synchronized Programs
	Very few programmers do programming that 
relies on SC; instead higher-level 
synchronization primitives are used 
	locks, semaphores, monitors, atomic transactions 
	A properly synchronized program is one 
where each shared writable variable is 
protected (say, by a lock) so that there is no 
race in updating the variable. 
	There is still race to get the lock 
	There is no way to check if a program is properly 
synchronized 
	For properly synchronized programs, 
instruction reordering does not matter as long as updated values are committed 
before leaving a locked region. 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L20- 8 
Arvind 
Example 4: Non-Blocking Caches
Process 1	 Process 2
Store(a,1); r1 := Load(flag);
Store(flag,1); r2 := Load(a);
Question: Is it possible that r1=1 but r2=0? 
	Sequential consistency: No 
	Assuming stores are ordered: Yes because 
Loads can be reordered 
Sparcs RMO, PowerPCs WO, Alpha 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L20- 15 
Arvind 
Enforcing SC using Fences 
Processor 1 Processor  2 
Store(a,10); L: r1 = Load(flag); 
Store(flag,1); Jz(r1,L); 
r2 = Load(a); 
Processor 1 Processor  2
Store(a,10);
Fenceww; 
Store(flag,1); 
L: r1 = Load(flag); 
Jz(r1,L); 
Fencerr; 
r2 = Load(a); 
Weak ordering 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L20- 12 
Arvind 
Example 8: Causality 
Process 1	 Process 2 Process 3
Store(flag1,1);	 r1 := Load(flag1); r2 := Load(flag2); 
Store(flag2,1); r3 := Load(flag1); 
Question: Is it possible that r1=1 and r2=1 
but r3=0 ? 
 Sequential consistency: No 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>2 
Beyond Sequential Consistency:
Relaxed Memory Models</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L20- 18 
Arvind 
Aggressive SC Implementations
Loads can go out of order 
Processor 1 Processor  2 
miss r1 = Load(flag ); Store( a,10); 
hit r2 = Load( a); 
kill Load( a) and the subsequent instructions if 
Store( a,10) happens before Load( flag) completes 
 Still not as efficient as weaker memory mode
 Scalable for Distributed Shared Memory systems? 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L20- 4 
Arvind 
Memory Model Issues
Architectural optimizations that are correct 
for uniprocessors, often violate sequential 
consistency and result in a new memory 
model for multiprocessors 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Beyond Sequential Consistency:
Relaxed Memory Models
Arvind
Computer Science and Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L20- 16 
Arvind 
Weaker (Relaxed) Memory Models 
Alpha, Sparc 
PowerPC, ... 
Write-
buffers Store is globally 
SMP, DSM performed 
TSO, PSO, 
RMO, ... 
RMO=WO? 
 Hard to understand and remember 
U n s t a b l e  - Modle de lanne
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L20- 20 
Arvind 
Release Consistency
 Only care about inter-processor memory ordering 
at thread synchronization points, not in between
	Can treat all synchronization instructions as the 
only ordering points 
 
Acquire(lock) // All following load s get most recent written values 
 Read and write shared data .. 
Release(lock) // All preceding writes are globally visible before 
// lock is freed.

November 21, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L20- 9 
Arvind 
Example 5: Register Renaming
Register
Process 1 Process 2 renaming 
willStore(flag1,r1);	 Store(flag2,r2); 
reliminate  
1 := Load(flag2); r2 := Load(flag1); this edge 
Initially both r1 and r2 contain 1. 
Question: Is it possible that r1=0 but r2=0? 
	Sequential consistency: No 
	Register renaming: Yes because it removes 
anti-dependencies 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>13 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L20- 5 
Arvind 
Example 1: Store Buffers 
Process 1	 Process 2 
rStore(flag1,1); Store(flag2,1); 
1 := Load(flag2); r2 := Load(flag1); 
Question: Is it possible that r1=0 and r2=0? 
	Sequential consistency: No 
	Suppose Loads can bypass stores in the 
store buffer: Yes ! 
Total Store Order (TSO): 
IBM 370, Sparcs TSO memory model 
Initially, all memory 
locations contain zeros 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L20- 6 
Arvind 
Example 2: Short-circuiting 
Process 1	 Process 2
Store(flag1,1);	 Store(flag2,1); 
r3 := Load(flag1);	 r4 := Load(flag2); 
r1 := Load(flag2);	 r2 := Load(flag1); 
Question: Do extra Loads have any effect? 
	Sequential consistency: No 
	Suppose Load-Store short-circuiting is 
permitted in the store buffer 
	No effect in Sparcs TSO model 
	A Load acts as a barrier on other loads in IBM 370 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L20- 10 
Arvind 
Example 6: Speculative Execution
Process 1	 Process 2 
Store(a,1); L: r1 := Load(flag); 
Store(flag,1); Jz(r1,L); 
r2 := Load(a); 
Question: Is it possible that r1=1 but r2=0? 
	Sequential consistency: No 
	With speculative loads: Yes even if the 
stores are ordered 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L20- 11 
Arvind 
Example 7: Store Atomicity 
Process 1 Process 2 Process 3 Process 4
rStore(a,1); Store(a,2); r1 := Load(a); r3 := Load(a); 
2 := Load(a); r4 := Load(a); 
Question: Is it possible that r1=1 and r2=2 
but r3=2 and r4=1 ? 
	Sequential consistency: No
	Even if Loads on a processor are ordered, 
the different ordering of stores can be 
observed if the Store operation is not 
atomic. 
November 21, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L20- 3 
Arvind 
Sequential Consistency 
Processor 1 Processor  2 
Store( a,10); 
rrL: 1 = Load( flag);
Store( flag,1); 
 Jz(r1,L);
2 = Load( a);
initially flag = 0 
 In-order instruction execution
 Atomic loads and stores 
SC is easy to understand but architects and compiler 
writers want to violate it for performance 
November 21, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Branch Prediction and Speculative Execution (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l13_brnchpred/</lecture_pdf_url>
      <lectureno>L13</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L13-5 
Arvind 
Decode &amp; Rename Stage
Opcode Rd Rsrc1 Rsrc2/Imm (Renaming is shown only 
for Rsrc2, similar for Rsrc1) 
R31 SignExt V Tag R31Committed R30 V Tag R30 Rename 
Architectural Table
Regfile R0
 V Tag R0 
1 X 
0 1 0 1 0 1 
1 
0 1 0 1 ImmSel 
t1 
t2 
tn Opcode U E P1 Tag1 Data1 P2 Tag2 Data2 Pd Rd Datad Cause 
ROB 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L13-31 
Arvind 
Mispredict Recovery 
In-order execution machines:
 Assume no instruction issued after branch can 
write-back before branch resolves 
 Kill all instructions in pipeline behind 
mispredicted branch
Out-of-order execution?
Multiple instructions following branch in program 
order can complete before branch resolves 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L13-26 
Arvind 
Consulting BTB Before Decoding
1028 132 Jump 100 
BPb target 
take 236 entry PC 
132 Add ..... 
 The match for PC=1028 fails and 1028+4 is fetched 
 eliminates false predictions after 
ALU instructions 
 BTB contains entries only for control transfer 
instructions 
 more room to store branch targets 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L13-11 
Arvind 
MIPS Branches and Jumps
Need to know (or guess) both target address and 
whether the branch/jump is taken or not 
Instruction Taken known? Target known? 
BEQZ/BNEZ After Reg. Fetch After Inst. Fetch 
J Always Taken After Inst. Fetch 
JR Always Taken After Reg. Fetch 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L13-14 
Arvind 
Branch Prediction
Motivation: branch penalties limit performance of 
deeply pipelined processors 
Modern branch predictors have high accuracy 
(&gt;95%) and can reduce branch penalties significantly 
Required hardware support:
Prediction structures: branch history tables, branch target 
buffers, etc. 
Mispredict recovery mechanisms: 
 In-order machines: kill instructions following 
branch in pipeline 
 Out-of-order machines: shadow registers and 
memory buffers for each speculated branch 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L13-17 
Arvind 
Branch Prediction Bits 
 Assume 2 BP bits per instruction 
 Change the prediction after two consecutive mistakes! 
take 
wrong 
taken t a k e n 
takentaken take take t a k e n 
right right 
taken 
t a k e n t a k e n 
take 
wrong 
BP state: 
(predict take/take) x (last prediction right/wrong) 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L13-9 
ArvindAverage Run-Length between
Branches 
Average dynamic instruction mix from SPEC92:
SPECint92 SPECfp92 
ALU 39 % 13 % 
FPU Add 20 % 
FPU Mult 13 % 
load 26 % 23 % 
store 9 % 9 % 
branch 16 % 8 % 
other 10 % 12 % 
SPECint92: compress, eqntott, espresso, gcc , li 
SPECfp92: doduc, ear, hydro2d, mdijdp2, su2cor 
What is the average run length between branches 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L13-13 
Arvind 
Outline 
 Control transfer penalty 
 Branch prediction schemes 
 Branch misprediction recovery schemes 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L13-18 
Arvind 
Branch History Table 
00Fetch PC 
+ I-Cache 
Opcode offset Instruction k 
BHT Index 2k-entry 
BHT, 
2 bits/entry 
Branch? Target PC Taken/Taken? 
4K-entry BHT, 2 bits/entry, ~80-90% correct predictions 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>6.823 L13-37 
Arvind 
Speculating Both Directions 
An alternative to branch prediction is to execute 
both directions of a branch speculatively 
 resource requirement is proportional to the 
number of concurrent speculative executions 
 only half the resources engage in useful work 
when both directions of a branch are executed 
speculatively 
 branch prediction takes less resources 
than speculative execution of both paths
With accurate branch pred iction, it is more cost 
effective to dedicate all resources to the predicted 
direction 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L13-8 
Arvind 
Branch Penalty
Next fetch 
started 
Modern processors may have &gt; 10 pipeline stages between next PC calculation 
and branch resolution ! 
I-cache 
Buffer 
Issue 
Buffer 
Func. 
Units 
Arch. 
State Execute Decode 
Result 
Buffer Commit PC 
Branch executed Fetch Fetch 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L13-7 
Arvind 
Commit Stage
	When instruction at ptr2 (commit point) has 
completed, write back result to architectural state
and check for exceptions 
	Check if rename table entry for architectural 
register written matches tag, if so, clear valid bit in
rename table 
October 26, 2005 
tn 
TagV 
TagV 
TagV Table 
R0 R30 R31 
R0 R30 R31Committed 
Regfile Exception? 
=?Clear rename valid? Opcode U E Tag1P1 Data1 Tag2P2 Data2 RdPd Datad Cause t1 
ptr2 
ROB 
Rename 
Architectural</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L13-16 
Arvind 
Dynamic Branch Prediction
learning based on past behavior 
Temporal correlation 
The way a branch resolves may be a good 
predictor of the way it will resolve at the next 
execution 
Spatial correlation
Several branches may resolve in a highly 
correlated manner (a preferred path of 
execution) 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.823 L13-35 
Arvind 
Branch Misprediction in Pipeline
Fetch Decode 
Execute Commit Reorder Buffer Kill 
Kill Kill Branch 
Resolution Inject correct PC 
Branch 
Prediction 
PC 
Complete 
 Can have multiple unresolved branches in ROB 
 Can resolve branches out-of-order by killing all the 
instructions in ROB that follow a mispredicted branch 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L13-19 
Arvind 
Two-Level Branch Predictor
Pentium Pro uses the result from the last two branches 
to select one of the four sets of BHT bits (~95% correct) 
October 26, 2005 
00 
kFetch PC 
results of each branch 2-bit global branch history shift register 
Shift in Taken/Taken 
Taken/Taken?</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L13-28 
Arvind 
Uses of Jump Register (JR) 
 Switch statements (jump to address of matching case) 
BTB works well if same case used repeatedly 
 Dynamic function call (jump to run-time function address) 
BTB works well if same function usually called, (e.g., in 
C++ programming, when objects have same type in virtual function call) 
 Subroutine returns (jump to return address) 
BTB works well if usually return to the same place Often one function called from many different call 
sites! 
How well does BTB work for each of these cases? 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>38 
Thank you !</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Branch Prediction 
and 
Speculative Execution
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L13-25 
Arvind 
Branch Target Buffer (BTB) 
I-Cache PC 2k-entry direct-mapped BTB 
k Valid 
valid Entry PC 
= 
match predicted 
target target PC (can also be associative) 
 Keep both the branch PC and target PC in the BTB 
 PC+4 is fetched if match fails 
O n l y taken branches and jumps held in BTB 
 Next PC determined before branch fetched and decoded 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L13-3 
Arvind 
Phases of Instruction Execution 
Fetch: Instruction bits retrieved 
from cache.I-cache 
Fetch 
Buffer 
Issue 
Buffer 
Func. 
Units 
State Execute: Instructions and operands sent to execution units . When execution completes, all results and 
exception flags are available. issue (aka dispatch) stage buffer 
Buffer Commit: Instruction irrevocably updates 
completion). PC 
Arch. Decode: Instructions placed in appropriate 
Result 
architectural state (aka graduation or 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L13-15 
Arvind 
Static Branch Prediction 
Overall probability a branch  is taken is ~60-70% but: 
backward 
JZ forward
90%
 50%
JZ 
ISA can attach additional semantics to branches about 
preferred direction, e.g., Motorola MC88110
bne0 (preferred taken) beq0 (not taken)
ISA can allow arbitrary choice of statically predicted direction (HP PA-RISC, Intel IA-64) 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L13-21 
Arvind 
Limitations of BHTs
Cannot redirect fetch stream until after branch instruction is 
fetched and decoded, and target address determined 
Correctly 
predicted 
taken branch 
penalty 
Jump Register 
penalty A 
P 
F 
B 
I 
J 
R 
E PC Generation/Mux 
Instruction Fetch Stage 1 
Instruction Fetch Stage 2 
Branch Address Calc/Begin Decode 
Complete Decode 
Steer Instructions to Functional units 
Register File Read 
Integer Execute 
Remainder of execute pipeline (+ another 6 stages) 
UltraSPARC-III fetch pipeline 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L13-22 
Arvind 
Branch Target Buffer 
IMEM 
PC Branch 
Target Buffer 
(2
k entries)
k BPb predicted 
target BP target 
BP bits are stored with the predicted target address. 
IF stage: If (BP=taken) then nPC=target else nPC=PC+4 
later: check prediction, if wrong then kill the instruction 
and update BTB &amp; BPb else update BPb 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823 L13-34 
Arvind 
Branch Misprediction Recovery 
pd dest data cause 
ptr2 
next to 
commit 
rollback 
next 
available 
ptr1 
next 
available Inst# use exec op p1 
BEQZ 
Speculative Instructions src1  p2  src2 
Reorder buffer 
On mispredict 
 Roll back next available pointer to just after branch 
 Reset use bits 
 Flush mis-speculated instructions from pipelines 
 Restart fetch on correct branch path 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L13-2 
Arvind 
Outline 
 Control transfer penalty 
 Branch prediction schemes 
 Branch misprediction recovery schemes 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L13-20 
ArvindExploiting Spatial Correlation
Yeh and Patt, 1992 
if (x[i] &lt; 7) then
y += 1;
if (x[i] &lt; 5) then
c -= 4; 
If first condition false, se cond condition also false
History bit: H records the direction of the last 
branch executed by the processor 
Two sets of BHT bits (BHT0 &amp; BHT1) per branch 
instruction 
H = 0 (not taken)  consult BHT0
H = 1 (taken)  consult BHT1
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L13-30 
Arvind 
Outline 
 Control transfer penalty 
 Branch prediction schemes 
 Branch misprediction recovery schemes 
Five-minute break to stretch your legs 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>ttt6.823 L13-36 
Arvind 
Recovering Renaming Table
vvv Register 
File 
Reorder 
buffer 
Load 
Unit FU FU FU Store 
Unit 
&lt; t, result &gt; t1 
t2 
. 
. 
tn Ins# use exec op p1 src1 p2 src2 pd dest data 
Commit Rename 
Table r1 t v 
r2 Rename Snapshots 
Take snapshot of register rename table at each predicted branch, recover earlier snapshot if branch mispredicted 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L13-4 
Arvind 
Fetch Stage 
PC 
Instruction Cache 
Hit? 
To Decode Stage Fetch Buffer Opcode Rd Rsrc1 Rsrc2/Imm 
Instructions 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L13-12 
Arvind 
Branch Penalties in Modern Pipelines
UltraSPARC-III instruction fetch pipeline stages
(in-order issue, 4-way superscalar, 750MHz, 2000)
P 
F 
B 
I 
J 
R 
E A PC Generation/Mux 
Instruction Fetch Stage 1 
Instruction Fetch Stage 2 
Branch Address Calc/Begin Decode 
Complete Decode 
Steer Instructions to Functional units 
Register File Read 
Integer Execute 
Remainder of execute pipeline 
(+ another 6 stages) 
Branch 
Target 
Address 
Known 
Branch Direction &amp; Jump 
Register 
Target Known 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L13-27 
Arvind 
Combining BTB and BHT
	BTB entries are considerably more expensive than BHT, 
but can redirect fetches at earlier stage in pipeline and 
can accelerate indirect branches (JR) 
	BHT can hold many more entries and is more accurate 
A 
P 
F 
B 
I 
J 
R 
E BTB 
BHT 
pipeline stage 
taken branch corrects when 
BTB misses a PC Generation/Mux 
Instruction Fetch Stage 1 
Instruction Fetch Stage 2 
BHT in later Branch Address Calc/Begin Decode 
Complete Decode 
Steer Instructions to Functional units 
predicted Register File Read 
Integer Execute 
BTB/BHT only updated after branch resolves in E stage 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L13-29 
Arvind 
Subroutine Return Stack
Small structure to accelerate JR for subroutine 
returns, typically much mo re accurate than BTBs. 
fa() { fb(); }
fb() { fc(); }
fc() { fd(); }
&amp;fb()&amp;fc() Push call address when 
&amp;fd() Pop return address 
when subroutinefunction call executed return decoded 
k entries (typically k=8-16) 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L13-33 
Arvind 
Extensions for Precise Exceptions 
Inst# use exec op p1 src1  p2  src2 pd dest data cause 
ptr2
next to
commit
ptr1 
next 
available 
Reorder buffer 
 add &lt;pd, dest, data, cause&gt; f ields in the instruction template 
 commit instructions to reg file and memory in program 
order  buffers can be maintained circularly 
 on exception, clear reorder buffer by resetting ptr1=ptr2 
(stores must wait for commit before updating memory) 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L13-32 
In-Order Commit for Precise Arvind 
Exceptions 
In-order Out-of-order In-order
Fetch Decode 
Execute Commit Reorder Buffer 
Kill Kill Kill 
Exception?Inject handler PC 
 Instructions fetched and decoded into instruction 
reorder buffer in-order 
 Execution is out-of-order (  out-of-order completion) 
 Commit (write-back to architectural state, i.e., regfile &amp; 
memory, is in-order 
October 26, 2005 
Temporary storage needed in ROB to hold results before 
commit</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L13-6 
Arvind 
Execute Stage
	Arbiter selects one ready instruction (P1=1 AND P2=1) to 
execute 
	Instruction reads operands from ROB, executes, and 
broadcasts tag and result to wa iting instructions in ROB.  
Also saves result and exception flags for commit. 
Func. Unit Opcode U E Tag1P1 Data1 Tag2P2 Data2 RdPd Datad Cause 
Opcode U E Tag1P1 Data1 Tag2P2 Data2 RdPd Datad Cause t1 
t2 
tn ROB 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L13-23 
Arvind 
Address Collisions
Assume a 
128-entry 
BTB 
target BPb 
Instruction 
What will be fetched after the instruction at 1028? Memory 
BTB prediction = 236 
Correct target = 1032 take 236 1028 132 Jump 100 
Add ..... 
 kill PC=236 and fetch PC=1032 
Is this a common occurrence? 
Can we avoid these bubbles? 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L13-10 
Arvind 
Reducing Control Transfer Penalties
Software solution 
 loop unrolling 
Increases the run length 
 instruction scheduling 
Compute the branch condition as early 
as possible (limited) 
Hardware  solution
 delay slots  
replaces pipeline bubbles with useful work 
(requires software cooperation) 
 branch prediction &amp; speculative execution 
of instructions beyond the branch 
October 26, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L13-24 
BTB should be for Control Transfer Arvind 
instructions only 
BTB contains useful information for branch and 
jump instructions only 
 it should not be updated for other 
instructions 
For all other instructions the next PC is (PC)+4 !
How to achieve this effect without decoding the instruction? 
October 26, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Advanced Superscalar Architectures (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l14_superscalar/</lecture_pdf_url>
      <lectureno>L14</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L14- 6 
Emer 
Physical Register Management
Rename Physical Regs Free List 
&lt;R6&gt;P5 
&lt;R7&gt;P6 
&lt;R3&gt;P7 P0 
Pn P1 
P2 
P3 
P4 
R5 
P5R6 
P6R7 R0 
P8R1 
R2 
P7R3 
R4 
ROB Table 
p 
p 
p P0 
P1 
P3 
P2 
P4 
&lt;R1&gt;P8 p ld r1, 0(r3) 
add r3, r1, #4 
sub r6, r7, r6 
add r3, r3, r6 
ld r6, 0(r1) 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd (LPRd requires 
third read port 
on Rename 
Table for each 
instruction) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L14- 23 
Emer 
Load Path 
Data Load Address 
Tags 
Store Commit Path Speculative 
Store Buffer 
Load Data Tag Data SV Tag Data SV Tag Data SV Tag Data SV Tag Data SV Tag Data SV L1 Data Cache 
	Hit in speculative store buffer has priority over hit in data cache 
	Hit to newer store has priority over hits to older stores in 
speculative store buffer
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L14- 18 
Emer 
Superscalar Register Renaming 
 During decode, instructions allocate d new physical destination register 
 Source operands renamed to physic al register with newest value 
 Execution unit only sees physical register numbers 
Update 
Rename Table Op Src1 Src2 Dest Op Src1 Src2 Dest 
Register 
Free List 
Op PSrc1 PSrc2 PDest Op PSrc1 PSrc2 PDest Mapping Inst 1 
Read Addresses 
Read Data Write
Ports Inst 2 
Does this work? 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L14- 4 
Emer 
Speculative &amp; Out-of-Order Execution
October 31, 2005 
Fetch Decode &amp; 
Rename Reorder Buffer PC Prediction Update predictors 
Commit Resolution 
Branch 
Unit ALU MEM Store 
Buffer D$ 
Execute In-Order In-Order Out-of-Order 
Physical Reg. File kill 
kill 
kill killBranch Branch</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>6.823 L14- 39 
Emer 
Branch Misprediction Recovery 
pd dest data cause 
ptr2 
next to 
commit 
rollback 
next 
available 
ptr1 
next 
available Inst# use exec op p1 
BEQZ 
Speculative Instructions src1  p2  src2 
Reorder buffer 
On mispredict 
 Roll back next available pointer to just after branch 
 Reset use bits 
 Flush mis-speculated instructions from pipelines 
 Restart fetch on correct branch path 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L14- 28 
EmerMemory Dependence Prediction
(Alpha 21264) 
st r1, (r2)
ld r3, (r4)
	Guess that r4 != r2 and execute load before store 
	If later find r4==r2, squash load and all following 
instructions, but mark load instruction as store-wait 
	Subsequent executions of the same load instruction 
will wait for all previous stores to complete 
	Periodically clear store-wait bits 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>35 
Extras</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L14- 19 
Emer 
Superscalar Register Renaming 
Rename Table Op Src1 Src2 Dest Op Src1 Src2 Dest 
Register 
Free List 
Op PSrc1 PSrc2 PDest Op PSrc1 PSrc2 PDest Update Inst 1 Inst 2 
Read Addresses 
Read Data Write
Ports=? =? 
Must check for 
between 
instructions 
issuing in same 
cycle. Can be 
with rename 
lookup. Mapping 
RAW hazards 
done in parallel 
MIPS R10K renames 4 serially -RAW-dependent insts/cycle) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L14- 22 
Emer 
Speculative Loads / Stores
Just like register updates, stores should not modify 
the memory until after the instruction is committed 
-&gt;store buffer entry must carry a speculation bit and 
the tag of the corresponding store instruction 
 If the instruction is committed, the speculation bit of 
the corresponding store buffer entry is cleared, and 
store is written to cache 
 If the instruction is killed, the corresponding store 
buffer entry is freed 
Loads work normally -- older store buffer entries 
needs to be searched before accessing the memory or the cache 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L14- 24 
EmerDatapath: Branch Prediction
and Speculative Execution
October 31, 2005 
Fetch Decode &amp; 
Rename Reorder Buffer PC Branch 
Prediction Update predictors 
Commit Branch 
Resolution 
Branch 
Unit ALU Reg. File 
MEM Store 
Buffer D$ 
Execute kill 
kill 
kill kill</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L14- 9 
Emer 
Physical Register Management
Rename Physical Regs Free List
Table P0
 P0 
R0 P1 P1
P3 ld r1, 0(r3) R1 P8P0 P2 
R2 P3 P2 add r3, r1, #4R3 P7P1 P4 P4 
R4 P5 &lt;R6&gt; p sub r6, r7, r6R5 P6 &lt;R7&gt; p
R6 P5P3 P7 &lt;R3&gt; p add r3, r3, r6 
R7P6 P8 &lt;R1&gt; p ld r6, 0(r1) 
Pn 
ROB 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd 
x ld p P7 r1 P8 P0 
x add P0 r3 P7 P1 
x sub p P6 p P5 r6 P5 P3 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L14- 13 
Emer 
Physical Register Management
Rename Physical Regs Free List 
ld r1, 0(r3) 
add r3, r1, #4 
sub r6, r7, r6 
add r3, r3, r6 
ld r6, 0(r1) 
op p1 PR1 p2 PR2 exuse Rd PRd LPRd ROB 
x sub p P6 p P5 r6 P3 x add P0 r3 P1 x add P0 r3 P1P0 
P1 
P3 
P2 
P4 
&lt;R6&gt;P5 
&lt;R7&gt;P6 
&lt;R3&gt;P7 P0 
Pn P1 
P2 
P3 
P4 
p 
p 
p
P8 
x x ld p P7 r1 P0 R5 
P5R6 
P6R7 R0 
P8R1 
R2 
P7R3 
R4 Table 
P0 
P8 
P7 P1 
P5 P3 
P1 P2 
x add P1 P3 r3 P2 
x ld P0 r6 P4 P3 P4 
p 
p p &lt;R1&gt; 
P8 
x 
p p &lt;R3&gt; 
P7 
Execute &amp; 
Commit 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L14- 11 
Emer 
Physical Register Management
R0 
R1 
R2 
R3 
R4 
R5 
R6 R7 P5 
P6 P8 
P7 P0 
P1 
P3 P2 
P4 Rename Physical Regs Free List 
Table P0 
P1 
P3 
P2 
P4 
&lt;R6&gt;P5 
&lt;R7&gt;P6 
&lt;R3&gt;P7 P0 
Pn P1 
P2 
P3 
P4 
p 
p 
p 
&lt;R1&gt;P8 p ld r1, 0(r3) 
add r3, r1, #4 
sub r6, r7, r6 
add r3, r3, r6 
ld r6, 0(r1) 
ROB 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd 
x ld p P7 r1 P8 P0 
x add P0 r3 P7 P1 
x sub p P6 p P5 r6 P5 P3 
x add P1 P3 r3 P1 P2 
x ld P0 r6 P3 P4 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L14- 5 
Emer 
Lifetime of Physical Registers 
 Physical regfile holds committed and speculative values 
 Physical registers decoupled from ROB entries (no data in ROB) 
ld r1, (r3) ld P1, (P x) 
add r3, r1, #4 add P2, P1, #4 
sub r6, r7, r9 sub P3, P y, Pz
add r3, r3, r6 Rename add P4, P2, P3
ld r6, (r1)
 ld P5, (P1)
add r6, r6, r3 add P6, P5, P4
st r6, (r1) st P6, (P1)
ld r6, (r11) ld P7, (P w)
When can we reuse a physical register? 
When next write of same architectural register commits 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L14- 30 
Memory Dependence Prediction Emer 
using Store Sets 
The processor approximates each loads 
store set by initially allowing nave 
speculation and recording memory-order 
violations.
 A load must wait for any stores in its 
store set that have not yet executed.
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L14- 31 
Emer 
The Store Set Map Table 
Index Index 
V 
V Program 
Order 
Store 
Set A Writer 
Reader Load Index Index 
Load 
Index Load . 
. 
. 
. 
. 
. 
. 
. 
. 
. . 
. Store Set Map Table 
Store Store 
- Store/Load Pair causing Memory Order Violation 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L14- 33 
Emer 
Store Set Map Table, cont. 
Index Index 
V 
V Program 
Order 
Store 
Set ALoad Index Index 
Load 
Index Load . 
. 
. 
. 
. 
. 
. 
. 
. 
. . 
. 
V V 
V Store 
Set B Store Set Map Table 
Store Store 
- Store/Load Pair causing Memory Order Violation 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L14- 10 
Emer 
Physical Register Management
Rename Physical Regs Free List
Table P0
 P0 
R0 P1 P1
P3 ld r1, 0(r3) R1 P8P0 P2 
R2 P3 
P4 P2 P2 add r3, r1, #4R3 P7P1 P4 
R4 P5 &lt;R6&gt; p sub r6, r7, r6R5 P6 &lt;R7&gt; p
R6 P5P3 P7 &lt;R3&gt; p add r3, r3, r6 
R7P6 P8 &lt;R1&gt; p ld r6, 0(r1) 
Pn 
ROB 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd 
x ld p P7 r1 P8 P0 
x add P0 r3 P7 P1 
x sub p P6 p P5 r6 P5 P3 
x add P1 P3 r3 P1 P2 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L14- 15 
Emer 
Issue Timing 
i1 Add R1,R1,#1 Issue1 Execute1 
i2 Sub R1,R1,#1 Issue2 Execute2 
How can we issue earlier? 
i1 Add R1,R1,#1 Issue1 Execute1 
i2 Sub R1,R1,#1 Issue2 Execute2 
What makes this schedule fail? 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Advanced Superscalar
Microprocessors
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L14- 21 
Emer 
Memory Dependencies
st r1, (r2)
ld r3, (r4)
When can we execute the load? 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L14- 27 
Emer 
Address Speculation
st r1, (r2)
ld r3, (r4)
	Guess that r4 != r2 
	Execute load before store address known 
	Need to hold all completed but uncommitted load/store 
addresses in program order 
	If subsequently find r4==r2, squash load and all following 
instructions 
=&gt; Large penalty for inaccurate address speculation 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>ttt6.823 L14- 41 
Emer 
Recovering Renaming Table
Reorder 
buffer 
Load 
Unit FU FU FU Store 
Unit 
&lt; t, result &gt; t1 
t2 
. 
. 
tn Ins# use exec op p1 src1 p2 src2 pd dest data 
Commit vvv Register 
File Table r1 t v 
r2 Snapshots Rename Rename 
Take snapshot of register rename table at each predicted 
branch, recover earlier snapshot if branch mispredicted 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>FU6.823 L14- 3 
Emer 
Unified Physical Register File
(MIPS R10K, Alpha 21264, Pentium 4) 
Rename 
Table r1 ti 
r2 tj 
FU Store 
UnitFULoad 
Unit FU t1 
t2 
. 
tn Reg 
File Snapshots for 
mispredict recovery 
(ROB not shown) &lt; t, result &gt; 
 One regfile for both committed and speculative values (no data in ROB) 
 During decode, instruction result allocated new physical register, source 
regs translated to physical regs through rename table 
 Instruction reads data from regfile at start of execute (not in decode) 
 Write-back updates reg. busy bits on instructions in ROB (assoc. search) 
 Snapshots of rename table taken at every branch to recover mispredicts 
 On exception, renaming undone in reverse order of issue (MIPS R10000) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Load
UnitFU FU FUStore
UnitCommittitj0
:t2
t10
1
16.823 L14- 2 
Emer 
O-o-O Execution with ROB
Rename
Rename 
Table
Table
Next to
Next to
commit
commit
Next
Next
available
available
Reorder
Reorder 
buffer
bufferRegister 
File 
&lt; t, result &gt; Ins# use exec op p1  src1 p2  src2  pd dest data R1 
R2 tag 
valid bit 
t1 
t2 
. 
. 
tn 0 X X add X  1  X 2  X R4 4 
8 X ld  X  256 R3 R1 1 
R2 2 
R3 3 
: 
: R3 
R4 Register
File
Load 
Unit FU FU FU Store 
Unit 
&lt; t, result &gt;Ins# use exec op p1  src1 p2  src2  pd dest dataIns# use exec op p1  src1 p2  src2  pd dest data
Commit R1 ti tj 0 
R2tag
valid bit
t1
t2
.
.
tn0 X X add X  1  X 2  X R4 4
8 X ld  X  256 R3R1 1
R2 2
R3 3
:
: :R3
R4t2 
t1 0 
1 
1 
Basic Operation: 
 Enter op and tag or data (if known) for each source 
 Replace tag with data as it becomes available 
 Issue instruction when a ll sources are available 
 Save dest data when operation finishes 
 Commit saved dest data when instruction commits 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L14- 26 
Emer 
Conservative O-o-O Load Execution
st r1, (r2)
ld r3, (r4)
	Split execution of store instruction into two phases: 
address calculation and data write 
	Can execute load before store, if addresses known and 
r4 != r2 
	Each load address compared with addresses of all 
previous uncommitted stores (can use partial
conservative check i.e., bottom 12 bits of address) 
	Dont execute load if any previous store address not 
known 
(MIPS R10K, 16 entry address queue) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L14- 12 
Emer 
Physical Register Management
Rename Physical Regs Free List 
ld r1, 0(r3) 
add r3, r1, #4 
sub r6, r7, r6 
add r3, r3, r6 
ld r6, 0(r1) 
op p1 PR1 p2 PR2 exuse Rd PRd LPRd ROB 
x ld p P7 r1 P0 
x add P0 r3 P1 
x sub p P6 p P5 r6 P3 x ld p P7 r1 P0P0 
P1 
P3 
P2 
P4 
&lt;R6&gt;P5 
&lt;R7&gt;P6 
&lt;R3&gt;P7 P0 
Pn P1 
P2 
P3 
P4 
p 
p 
p 
&lt;R1&gt;P8 p R5 
P5R6 
P6R7 R0 
P8R1 
R2 
P7R3 
R4 Table 
P0 
P8 
P7 P1 
P5 P3 
P1 P2 
x add P1 P3 r3 P2 
x ld P0 r6 P4 P3 P4 
p 
p p &lt;R1&gt; 
P8 
x Execute &amp; 
Commit 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>20 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L14- 17 
Emer 
Data-in-ROB vs. Single Register File 
Decode/ Read 
Reg 
File Read 
ROB 
Source FU 
Cache Write 
ROB 
Dest CommitRename Data-in-ROB style 
Single-register-file style 
Decode/ Issue 
Queue Read 
Reg 
File FU 
Cache Write 
Reg 
File CommitRename 
How does issue speculation differ? 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L14- 25 
Emer 
In-Order Memory Queue 
	Execute all loads and stores in program order 
=&gt; Load and store cannot leave ROB for 
execution until all previous loads and stores 
have completed execution 
	Can still execute loads and stores 
speculatively, and out-of-order with respect to other instructions 
	Stores held in store buffer until commit 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>6.823 L14- 36 
Emer 
Mispredict Recovery 
	In-order execution machines:
	Assume no instruction issued after branch can 
write-back before branch resolves
	Kill all instructions in pi peline behind mispredicted 
branch 
Out-of-order execution?
Multiple instructions following branch in program 
order can complete before branch resolves 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L14- 8 
Emer 
Physical Register Management
Rename Physical Regs Free List
Table P0
 P0 
R0 P1 P1
P3 ld r1, 0(r3) R1 P8P0 P2 
R2 P3 P2 add r3, r1, #4R3 P7P1 P4 P4 
R4 P5 &lt;R6&gt; p sub r6, r7, r6R5 P6 &lt;R7&gt; p
R6P5 P7&lt;R3&gt; p add r3, r3, r6 
R7P6 P8 &lt;R1&gt; p ld r6, 0(r1) 
Pn 
ROB 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd 
x ld p P7 r1 P8 P0 
x add P0 r3 P7 P1 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>6.823 L14- 40 
Emer 
Branch Misprediction in Pipeline
Fetch Decode 
Execute Commit Reorder Buffer Kill 
Kill Kill Branch 
Resolution Inject correct PC 
Branch 
Prediction 
PC 
Complete 
 Can have multiple unresolved branches in ROB 
 Can resolve branches out-of-order by killing all the 
instructions in ROB that follow a mispredicted branch 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>6.823 L14- 38 
Emer 
Extensions for Precise Exceptions 
Inst# use exec op p1 src1  p2  src2 pd dest data cause 
ptr2
next to
commit
ptr1 
next 
available 
Reorder buffer 
 add &lt;pd, dest, data, cause&gt; f ields in the instruction template 
 commit instructions to reg file and memory in program 
order  buffers can be maintained circularly 
 on exception, clear reorder buffer by resetting ptr1=ptr2 
(stores must wait for commit before updating memory) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L14- 16 
Emer 
Issue Queue with latency prediction
p1 lat1 src1 p2 lat2 src2 dest 
ptr2 
next to 
commit 
ptr1 
next 
available Inst# use exec op 
BEQZ 
Speculative Instructions 
Issue Queue (Reorder buffer) 
 Fixed latency: latency includ ed in queue entry (bypassed) 
 Predicted latency: latency incl uded in queue entry (speculated) 
 Variable latency: wait for completion signal (stall) 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L14- 14 
Reorder Buffer Holds Emer 
Active Instruction Window 
 
ld r1, (r3) 
add r3, r1, r2 
sub r6, r7, r9 
add r3, r3, r6 
ld r6, (r1) 
add r6, r6, r3 
ld r6, (r1) 
  
ld r1, (r3) 
add r3, r1, r2 
sub r6, r7, r9 
add r3, r3, r6 
ld r6, (r1) 
add r6, r6, r3 
ld r6, (r1) Commit 
Fetch Execute 
st r6, (r1) (Older instructions) 
(Newer instructions) st r6, (r1) 
 
Cycle t + 1 
Cycle t 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>34 
Thank you !</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L14- 29 
Store Sets Emer 
(Alpha 21464) 
Program 
Order PC 8 
PC 0 
PC 12 
PC 8 0 PC 
4 
8 
12 
Load28 
Load32 
Load36 
Load40 {Empty} Multiple Readers 
Store 
Store Store 
Store 
Multiple Writers 
- multiple code paths 
- multiple stack spills 
- multiple components 
of a single location 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>6.823 L14- 37 
Emer 
Precise Exceptions via In-Order Commit
In-order Out-of-order In-order 
Fetch Decode 
Execute Commit Reorder Buffer 
Kill Kill Kill 
Exception?Inject handler PC 
 Instructions fetched and decoded into instruction 
reorder buffer in-order 
 Execution is out-of-order (  out-of-order completion) 
 Commit (write-back to architectural state, i.e., regfile &amp; 
memory, is in-order 
October 31, 2005 
Temporary storage needed in ROB to hold results before commit</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L14- 7 
Emer 
Physical Register Management
Rename Physical Regs Free List
Table P0
 P0 
R0 P1 P1 ld r1, 0(r3) R1 P8P0 P2 P3 
R2 P3 P2 add r3, r1, #4R3P7 P4 P4 
R4 P5 &lt;R6&gt; p sub r6, r7, r6R5 P6 &lt;R7&gt; p
R6P5 P7&lt;R3&gt; p add r3, r3, r6 
R7P6 P8 &lt;R1&gt; p ld r6, 0(r1) 
Pn 
ROB 
use ex op p1 PR1 p2 PR2 Rd LPRd PRd 
x ld p P7 r1 P8 P0 
October 31, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L14- 32 
Emer 
Store Set Sharing for Multiple Readers 
Index Index 
V 
V Program 
Order 
Store 
Set ALoad Index Index 
Load 
Index Load . 
. 
. 
. 
. 
. 
. 
. 
. 
. . 
. 
V Store Set Map Table 
Store Store 
- Store/Load Pair causing Memory Order Violation 
October 31, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Multithreaded Processors (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l23_multithread/</lecture_pdf_url>
      <lectureno>L23</lectureno>
      <slides>
        <slide>
          <slideno>12</slideno>
          <text>Joel Emer 
December 5, 2005 
Coarse-Grain Multithreading 6.823, L23-13 
Tera MTA designed for supe rcomputing applications 
with large data sets and low locality 
 No data cache 
 Many parallel threads needed to hide large memory 
latency 
Other applications are more cache friendly
 Few pipeline bubbles wh en cache getting hits 
 Just add a few threads to hide occasional cache miss 
latencies 
 Swap threads on cache misses</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823, L23-25
SMT Fetch Policies (Locks) 
Spin looping thread consumes resources 
thread to sleep until a memory location
changes 
loop: 
ARM r1, 0(r2)
BEQ r1, got_it
QUIESCE
BR loop
got_it: Load and start 
watching 0(r2) 
Inhibit scheduling of 
observed on 0(r2) Joel Emer 
December 5, 2005 
P r o b l e m : 
 Solution: 
Provide quiescing operation that allows a
thread until activity</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823, L23-2
Pipeline Hazards 
LW r1, 0(r2) 
LW r5, 12(r1) 
ADDI r5, r5, #12 
SW 12(r1), r5 F D X M W t0 t1 t2 t3 t4 t5 t6 t7 t8 
F D X M W D D D 
F D X M W D D D F F F 
F D D D D F F F t9 t10 t12 t14 
What can be done to cope with this? Joel Emer 
December 5, 2005 
 Each instruction may depend on the next t11 t13 
 Even bypassing does not eliminate all delays</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823, L23-26
Adaptation to parallelism type 
For regions with high thread 
threads 
Issue width 
Time Issue width 
Time For regions with low thread level Joel Emer 
December 5, 2005 
level parallelism (TLP) entire 
machine width is shared by all parallelism (TLP) entire machine width is available for instruction 
level parallelism (ILP)</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823, L23-30
 Pentium-4 Hyperthreading 
Execution Pipeline Joel Emer 
December 5, 2005 
Figure removed due to copyright restrictions.
Refer to Figure 6 in Marr, D., et al. "Hyper-threading Technology Architecture and Microarchitecture."
Intel Technology Journal  6, no. 1 (2002): 10.
http://www.intel.com/technology/itj/2002/volume06issue01/vol6iss1_hyper_threading_technology.pdf_________________________________________________________________________________________</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Joel Emer 
December 5, 2005 
IBM Power RS64-IV (2000) 6.823, L23-15 
 Commercial coarse-grain multithreading CPU 
 Based on PowerPC with quad-issue in-order 
five-stage pipeline 
 Each physical CPU supports two virtual CPUs
 On L2 cache miss, pipeline is flushed and 
execution switches to second thread 
 short pipeline minimizes flush penalty (4 cycles), 
small compared to memory access latency 
 flush pipeline to simplify exception handling</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>GPR1GPR1GPR16.823, L23-5
Simple Multithreaded Pipeline 
ensure correct state bits read/written at each pipe 
stage +1 
2 Thread 
select PC 
1PC 
1PC 
1PC 
1 I$ IR GPR1 X 
Y 
2 D$ Joel Emer 
December 5, 2005 
Have to carry thread sele ct down pipeline to</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823, L23-32 
Fetch Decode &amp; 
Rename Reorder Buffer PC Commit 
Branch 
Unit ALU MEM Store 
Buffer D$ 
Execute In-Order In-Order Out-of-Order 
Physical Reg. File Speculative, Out-of-Order 
Superscalar Processor Joel Emer 
December 5, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-8 Denelcor HEP
(Burton Smith, 1982)
Image removed due to copyright restrictions. 
To view image, visit http://ftp.arl.mil/ftp/historic-
computers/png/hep2.png 
First commercial machine to use hardware threading in main CPU 
 120 threads per processor 
 10 MHz clock rate 
 Up to 8 processors 
 precursor to Tera MTA (Multithreaded Architecture)</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823, L23-16
Superscalar Machine Efficiency 
 Why horizontal waste? 
 Why vertical waste? Issue width 
Time Completely idle cycle 
(vertical waste ) Instruction issue 
Partially filled cycle, i.e., IPC &lt; 4 
(horizontal waste ) Joel Emer 
December 5, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823, L23-34
Granularity of Multithreading 
So far, assumed fine-grained multithreading 
 When does this make sense? 
Coarse-grained multithreading 
 When does this make sense? L1 
Data 
Cache L1 
Inst. 
Cache Unified 
L2 
Cache 
RF Memory Memory Memory Memory 
CPU Joel Emer 
December 5, 2005 
 CPU switches every cycle to a different thread 
 CPU switches every few cycles to a different thread</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823, L23-7
Thread Scheduling Policies 
(TI ASC PPUs, 1971) 
whichever thread is in that slot 
(HEP, 1982) 
scheme Joel Emer 
December 5, 2005 
 Fixed interleave (CDC 6600 PPUs, 1965) 
 each of N threads executes one instruction every N cycles 
 if thread not ready to go in its slot, insert pipeline bubble 
 Software-controlled interleave 
 OS allocates S pipeline slots amongst N threads 
 hardware performs fixed interl eave over S slots, executing 
 Hardware-controlled thread scheduling 
 hardware keeps track of which threads are ready to go  picks next thread to execut e based on hardware priority</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823, L23-28
 Pentium-4 Hyperthreading 
Front End Joel Emer 
December 5, 2005 
Refer to Figure 5a in Marr, D., et al. "Hyper-threading Technology Architecture and Microarchitecture."
Intel Technology Journal 6, no. 1 (2002): 8.
http://www.intel.com/technology/itj/2002/volume06issue01/vol6iss1_hyper_threading_technology.pdf_________________________________________________________________________________________Figure removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Joel Emer 
December 5, 2005 
Multithreading Design Choices 6.823, L23-12 
 Fine-grained multithreading 
 Context switch among threads every cycle 
 Coarse-grained multithreading 
 Context switch among threads every few cycles, e.g., on: 
 Function unit data hazard,
L 1  m i s s ,
L 2  m i s s 
 Why choose one style over another? 
 Choice depends on 
 Context-switch overhead 
 Number of threads supported (due to per-thread state) 
 Expected application-level parallelism</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Joel Emer 
December 5, 2005 
Pentium-4 Branch Predictor 6.823, L23-29 
 Separate return address stacks per thread 
Why? 
 Separate first-level global branch history table 
Why? 
 Shared second-level branch history table, 
tagged with logical processor IDs</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Joel Emer 
December 5, 2005 
Tera MTA (1990-97) 6.823, L23-9 
Image removed due to 
copyright restrictions. 
To view image, visit 
http://www.npaci.edu/online/v2.1/ 
mta.html 
 Up to 256 processors 
 Up to 128 active threads per processor
 Processors and memory modules populate a sparse 
3D torus interconnection fabric
 Flat, shared main memory 
 No data cache 
 Sustains one main memory access per cycle per processor 
 GaAs logic in prototype, 1KW/processor @ 260MHz
 CMOS version, MTA-2, 50W/processor</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Joel Emer 
December 5, 2005 
Why Does Icount Make Sense? 6.823, L23-24 
N 
T = -------
L 
Assuming latency (L) is unchanged with the addition of threading. 
For each thread i with original throughput Ti: 
N/4 
T/4 = -------i
L</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Joel Emer 
Pentium-4 Hyperthreading December 5, 2005 
6.823, L23-27 
(2002) 
 First commercial SMT design (2-way SMT) 
H y p e r t h r e a d i n g = =  S M T 
 Logical processors share nearly all resources of 
the physical processor 
 Caches, execution units, branch predictors 
 Die area overhead of hyperthreading  ~ 5%
 When one logical processo r is stalled, the other 
can make progress 
 No logical processor can use a ll entries in queues when two 
threads are active 
 Processor running only one active software 
thread runs at approximately same speed with 
or without hyperthreading</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Joel Emer 
6.823, L23-18Chip Multiprocessing December 5, 2005 
Issue width 
Time 
 What is the effect of splitt ing into multiple processors? 
 eliminates horizontal waste, 
 leaves some vertical waste, and 
 caps peak throughput of each thread.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823, L23-33
Joel Emer 
December 5, 2005 
Figure removed due to copyright considerations.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-4CDC 6600 Peripheral Processors
(Cray, 1964)
Image removed due to copyright restrictions. 
To view image, visit 
http://www.bambi.net/computer_museum/cdc6600_ 
and_console.jpg 
 First multithreaded hardware 
 10 virtual I/O processors 
 Fixed interleave on simple pipeline 
 Pipeline has 100ns cycle time 
 Each virtual processor executes one instruction every 1000ns 
 Accumulator-based instruction se t to reduce processor state</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Joel Emer 
December 5, 2005 
MTA Architecture	6.823, L23-10 
	Each processor supports 128 active hardware threads 
	1 x 128 = 128 stream status word (SSW) registers, 
	8 x 128 = 1024 branch-target registers, 
 32 x 128 = 4096 genera l-purpose registers 
	Three operations packed into 64 -bit instruction (short VLIW) 
	One memory operation, 
	One arithmetic operation, plus 
	One arithmetic or branch operation 
	Thread creation and termination instructions 
	Explicit 3-bit lookahead field in instruction gives number of 
subsequent instructions (0-7) that are independent of this 
one 
	c.f. instruction grouping in VLIW 
	allows fewer threads to fill machine pipeline 
	used for variable-sized branch delay slots</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-1 
Multithreading Architectures
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Based on the material prepared by 
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-21 Basic Out-of-order Pipeline
Fetch Decode Queue Reg Execute Dcache Reg Retire 
/Map Read /Store Write 
Buffer 
PC 
Icache 
Register 
Map 
Dcache
Regs Regs 
Thread-
blind 
[ EV8  Microprocessor Forum, Oct 1999]</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-31 
Extras</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823, L23-17
Vertical Multithreading 
 
 Issue width 
Time Second thread interleaved 
cycle-by-cycle Instruction issue 
Partially filled cycle, i.e., IPC &lt; 4 
(horizontal waste ) Joel Emer 
December 5, 2005 
What is the effect of cycle-by-cycle interleaving? 
removes vertical waste, but leaves some horizontal waste</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823, L23-19 
Ideal Superscalar Multithreading 
[Tullsen, Eggers, Levy, UW, 1995] 
slots with no restrictions Issue width 
Time Joel Emer 
December 5, 2005 
 Interleave multiple threads to multiple issue</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Joel Emer 
December 5, 2005 
Multithreading Costs 6.823, L23-6 
 Each thread requires its own user state 
P C
G P R s
 Also, needs its own system state
 virtual memory page table base register 
 exception handling registers 
 Other costs? 
 Appears to software (including OS) as 
multiple, albeit slower, CPUs</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823, L23-3
Multithreading 
How can we guarantee no dependencies 
between instructions in a pipeline? 
instructions from different program threads on 
same pipeline 
F D X M W t0 t1 t2 t3 t4 t5 t6 t7 t8 
T1: LW r1, 0(r2) 
T2: ADD r7, r1, r4 
T3: XORI r5, r4, #12 
T4: SW 0(r7), r5 
T1: LW r5, 12(r1) t9 
F D X M W 
F D X M W 
F D X M W 
F D X M W Prior instruction in 
completes write-
back before next 
instruction in 
same thread reads 
register file Joel Emer 
December 5, 2005 
-- One way is to interleave execution of 
Interleave 4 threads, T1-T4, on non-bypassed 5-stage pipe 
a thread always</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823, L23-11
MTA Pipeline 
A 
W C 
W M Inst Fetch 
W Issue Pool 
instruction from one 
active thread is launched into pipeline 
is 21 cycles long 
latency 
Effective single thread issue rate 
is 260/21 = 12.4 MIPS Joel Emer 
December 5, 2005 Memory Pool 
Retry Pool 
Interconnection Network Write Pool 
Memory pipeline  Every cycle, one 
 Instruction pipeline 
 Memory operations 
incur ~150 cycles of 
Assuming a single thread issues one 
instruction every 21 cycles, and clock 
rate is 260 MHz 
What is performance?</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-14 MIT Alewife (1990)
Image removed due to 
copyright restrictions. 
To view image, visit 
http://www.cag.lcs.mit.e 
du/alewife/pictures/jpg/1 
6-extender.jpg  Modified SPARC chips
 register windows hold different 
thread contexts 
 Up to four threads per node
 Thread switch on local cache 
miss</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-20 
O-o-O Simultaneous Multithreading
[Tullsen, Eggers, Emer, Levy, Stamm, Lo, DEC/UW, 1996] 
	Add multiple contexts and fetch engines and allow 
instructions fetched from different threads to issue 
simultaneously 
	Utilize wide out-of-order superscalar processor issue 
queue to find instructions to issue from multiple threads 
	OOO instruction window already has most of the circuitry 
required to schedule from multiple threads 
	Any single thread can utilize whole machine</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823, L23-23 
Why does this enhance throughput? Joel Emer 
December 5, 2005 
Icount Choosing Policy 
Fetch from thread with the least instructions in flight.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Joel Emer 
December 5, 2005 
6.823, L23-22 SMT Pipeline
Fetch Decode Queue Reg Execute Dcache Reg Retire 
/Map Read /Store Write 
Buffer 
Icache Dcache 
PC 
Register 
Map 
Regs Regs 
[ EV8  Microprocessor Forum, Oct 1999]</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Reliable Architectures (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l24_reliability/</lecture_pdf_url>
      <lectureno>L24</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-24 
Vulnerability of a structure 
AVF = fraction of cycles a bit contains ACE state
( 2 + 1 + 0 + 3 ) / 4( 2 + 1 + 0 + 3 ) / 4
= 44
Average number of ACE bits in a cycleAverage number of ACE bits in a cycle
= Total number of bits in the structureTotal number of bits in the structure</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-16 
Architectural Vulnerability Factor
Does a bit matter?
 Branch Predictor 
 Doesnt matter at all  (AVF = 0%) 
 Program Counter 
 Almost always matters (AVF ~ 100%)</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-17 Statistical Fault Injection (SFI) 
with RTL
LogicLogic1 
0 Simulate Strike on 
Latch 
0 
output 
Does Fault Propagate 
to Architectural State 
+ Naturally characterizes all logical structures</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-38
Anti- bit: coping with No-ops 
(assume parity-protected instruction queue) 
On issue, if the anti- bit is set, then do not set thebit is set, then do not set the bitbitIQFetch Decode Execute Commit 
Instruction 
Cache (IC) Data Cache RR 
inst inst 
(anti-))inst 
(anti-))inst inst inst 
anti-bitbit 
neutralizesneutralizes 
thethe bitbit</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-21 
Vulnerability of a structure 
AVF = fraction of cycles a bit contains ACE state
T = 2 
 ACE% = 1/4</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823, L24-31 
Idle Idle Fill Read Read Evict 
Write-through Data Cache 
ACE Lifetime Analysis (3) 
(e.g., write-through data cache) Joel Emer 
December 7, 2005 
 Data ACEness is a function of instruction ACEness 
 Second Read is by an unACE instruction 
 AVF = 1/5 = 20%</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823, L24-6
 Triple Modular Redundancy 
(Von Neumann, 1956) 
V does a majority vote on the results M 
M 
M V Result Joel Emer 
December 7, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>yes6.823, L24-11
Strike on a bit (e.g., in register file) 
Bit 
Bit has error 
protection? yes no 
detection &amp; 
correction no no error benign fault 
no error 
detection only 
outcome? 
True DUE False DUE noyes no outcome? 
benign fault 
no error SDC yes no Joel Emer 
December 7, 2005 
Read? 
affects program affects program 
SDC = Silent Data Corruption, DUE = Detected Unrecoverable Error</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-5 
Physical Solutions are hard 
 Shielding? 
 No practical absorbent (e.g., appr oximately &gt; 10 ft of concrete) 
 unlike Alpha particles 
 Technology solution: SOI? 
 Partially-depleted SOI of some  help, effect on logic unclear 
 Fully-depleted SOI may help, bu t is challenging to manufacture 
 Circuit level solution? 
 Radiation hardened circuits can provide 10x improvement with 
significant penalty in performance, area, cost 
 2-4x improvement may be possible with less penalty</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>06.823, L24-2 
Strike Changes State of a Single Bit 
1 Joel Emer 
December 7, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-1 
Reliable Architectures
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823, L24-29
ACE Lifetime Analysis (1) 
(e.g., write-through data cache) 
Idle Idle Valid Valid Valid Fill Read Read Evict Joel Emer 
December 7, 2005 
 Idle is unACE 
 Assuming all time intervals are equal 
 For 3/5 of the lifetime the bit is valid  Gives a measure of the structures utilization 
 Number of useful bits 
 Amount of time useful bits  are resident in structure 
 Valid for a particular trace</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-18 
Architecturally Correct Execution (ACE)
Program Input 
Program Outputs 
	ACE path requires only a subset of values to flow correctly 
through the programs data flow graph (and the machine) 
	Anything else (un-ACE path ) can be derated away</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>6.823, L24-39
 bit: avoiding False DUE on 
Dynamically Dead Instructions 
IQFetch Execute Commit 
Instruction 
Cache (IC) Data Cache RR write R1 write R1 write R1( ) write R1( ) write R1( ) write R1( ) 
read R1 read R1 read R1 read R1 ( ) 
 bit is set 
  bit can be used in caches &amp; main memory  Inst i: 
Inst i+n: Joel Emer 
December 7, 2005 
Decode 
 Declare the error on reading R1, if 
 If R1 isnt read (i.e., dynamically dead), then no False DUE</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-25Littles Law for ACEs
Nace = Tace  Lace 
NaceAVF = Ntotal</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>ACE
Inst6.823, L24-28
Mapping ACE &amp; un-ACE Instructions to 
the Instruction Queue 
Architectural un-ACE Micro-architectural un-ACE Wrong-
Path 
Inst 
Idle
NOP 
 Prefetch 
ACE 
Inst 
Ex-
ACE 
Inst Joel Emer 
December 7, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-27 
Dynamic Instruction Breakdown
20% 
1% 
26% ACE 
46% 
7% 
Average across Spec2K slices DYNAMICALLY 
DEAD 
PERFORMANCE 
INST 
NOP PREDICATED 
FALSE</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823, L24-14
# Vulnerable Bits Growing with Moores Law 
1 10 100 10000 
l
l12x GAP 
Typical DUE goal: 10-25 year MTBF Joel Emer 
December 7, 2005 
1000 
200 3 
200 4 
200 5 
200 6 
200 7 
200 8 
200 9 
201 0 
201 1 201 2 
Year 100% Vu nerable 
20% Vu nerable 
1000 year MTBF Goal 
Typical SDC goal: 1000 year MTBF</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823, L24-3 
Impact of Neutron Strike on a Si Device 
 drain to alter the state 
of the device + -++ +---
Transistor Device source drain Joel Emer 
December 7, 2005 
Secondary source of upsets: alpha particles from packaging Strikes release electron &amp; hole pairs that can be 
absorbed by source &amp; neutron strike</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-15 
Architectural Vulnerability Factor (AVF)
AVFbit = Probability Bit Matters 
# of Visible Errors 
=# of Bit Flips from Particle Strikes 
FITbit= intrinsic FITbit * AVFbit</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-37
The  (Possibly Incorrect) Bit 
(assume parity-protected instruction queue) 
At commit point, declare error only if not wrong-path 
instruction and bit is setbit is setIQFetch Decode Execute Commit 
Instruction 
Cache (IC) Data Cache RR 
inst inst 
POST ERROR 
IN BIT ON 
ISSUE inst ())inst ())inst ())inst ())</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-23 
Vulnerability of a structure 
AVF = fraction of cycles a bit contains ACE state
T = 4 
 ACE% = 3/4</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Joel Emer 
December 7, 2005 
% False DUE AVF Eliminated 6.823, L24-40 
(PI = ) 
PI bit till I/O
commit PI bit till register
12% commit
18%
PI bit till store 
commit
8%
PI bit till register
read
14%
CPU2000 
Asim anti-PI bit
Simpoint 48%
Itanium2-like
Practical to eliminate most of the False DUE AVF</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823, L24-8
 Pair and Spare Lockstep 
 
 M 
M C Mismatch? Primary 
M 
M C Mismatch? Backup Joel Emer 
December 7, 2005 
(e.g., Tandem, 1975) 
Primary creates periodic checkpoints 
Backup restarts from checkpoint on mismatch</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>nJoel Emer 
December 7, 2005 
6.823, L24-4
Cosmic Rays Come From Deep Space
p 
n p 
p n 
n p 
p n n 
Earths Surface 
 Neutron flux is higher in higher altitudes 
3x - 5x increase in Denver at 5,000 feet 
100x increase in airplanes at 30,000+ feet</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-20 
Vulnerability of a structure 
AVF = fraction of cycles a bit contains ACE state
T = 1 
 ACE% = 2/4</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823, L24-9
 Redundant Multithreading 
(e.g., Reinhardt, Mukherjee, 2000) 
 X W X X W X X W 
X W X X W X X W C Fault? Leading Thread 
Trailing Thread C Fault? C Fault? Joel Emer 
December 7, 2005 
Writes are checked</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-35Sources of False DUE in an 
Instruction Queue
 Instructions with uncommitted results
 e.g., wrong-path, predicated-false 
 solution:  (possibly incorrect) bit till commit 
 Instruction types neutral to errors 
 e.g., no-ops, prefetches, branch predict hints 
 solution: anti-  bit 
 Dynamically dead instructions 
 instructions whose results will not be used in future 
 solution:  bit beyond commit</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823, L24-7
 Dual Modular Redundancy 
(e.g., Binac, Stratus) 
 
 
restore state to other M 
M C Mismatch? Error? 
Error? Joel Emer 
December 7, 2005 
Processing stops on mismatch Error signal used to decide which processor be used to</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823, L24-10
Component Protection 
Error? ECC 
1 1 0 
Parity Parity 
1 1 0 
ECC 0 
1 1   
 Joel Emer 
December 7, 2005 
 Fujitsu SPARC in 130 nm technology (ISSCC 2003) 
 80% of 200k latches protected with parity 
v e r s u s  v e r y  f e w  l a t c h e s  p r o t e c ted in commodity microprocessors</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Joel Emer
December 7, 2005
6.823, L24-34
29% 
6% 
l 
16% 
11% Idl i
38% DUE AVF of Instruction Queue with Parity 
False DUE AVF 
33% Asim True DUE AVF 
Uncommitted 
Neutra Dynamically 
Dead e &amp; M sc 
CPU2000 
Simpoint 
Itanium2-like</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>6.823, L24-36
Coping with Wrong-Path Instructions 
(assume parity-protected instruction queue) 
DECLARE 
ERROR 
ON ISSUE IQFetch Execute Commit 
Instruction 
Cache (IC) Data Cache RR instX Joel Emer 
December 7, 2005 
 Problem: not enough information at issue Decode</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-19Example of un-ACE instruction: 
Dynamically Dead Instruction 
Dynamically 
Dead Instruction 
Most bits of an un-ACE instruction do not affect program output</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-22 
Vulnerability of a structure 
AVF = fraction of cycles a bit contains ACE state
T = 3 
 ACE% = 0/4</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Joel Emer 
6.823, L24-32Instruction Queue December 7, 2005 
15% ACE 
29% 31% 
10% 
3% 
8% 3% 
1% NOP IDLE 
Ex-ACE 
WRONG  PATH 
DYNAMICALLY 
DEAD PREDICATED 
FALSE 
PERFORMANCE 
INST 
ACE percentage = AVF = 29%</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Joel Emer 
December 7, 2005 
Metrics 6.823, L24-12 
I n t e r v a l - b a s e d 
 MTTF = Mean Time to Failure 
 MTTR = Mean Time to Repair 
 MTBF = Mean Time Between  Failures = MTTF + MTTR 
 Availability = MTTF / MTBF 
R a t e - b a s e d 
 FIT = Failure in Time = 1 failure in a billion hours
1  y e a r  M T T F  =  1 09 / (24 * 365) FIT = 114,155 FIT
 SER FIT = SDC FIT + DUE FIT 
Hypothetical Example 
Cache: 0 FIT
Image removed due to + IQ: 100K FIT
copyright restrictions. + FU: 58K FIT
Total of 158K FIT</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Joel Emer 
December 7, 2005 
6.823, L24-13Cosmic Ray Strikes: Evidence &amp; Reaction
 Publicly disclosed incidence 
 Error logs in large servers, E. Normand, Single Event Upset at 
Ground Level, IEEE Trans. on Nucl Sci, Vol. 43, No. 6, Dec 1996. 
 Sun Microsystems found cosmic ray strikes on L2 cache with 
defective error protection caused Suns flagship servers to crash, 
R. Baumann, IRPS Tutorial on SER, 2000. 
 Cypress Semiconductor reported in 2004 a single soft error 
brought a billion-dollar automoti ve factory to a halt once a 
month, Zielger &amp; Puchner, SER  History, Trends, and 
Challenges, Cypress, 2004.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823, L24-30 
Idle Idle Fill Read Read Evict 
Write-through Data Cache 
ACE Lifetime Analysis (2) 
(e.g., write-through data cache) Joel Emer 
December 7, 2005 
 Valid is not necessarily ACE 
 ACE % = AVF = 2/5 = 40% 
 Example Lifetime Components 
 ACE: fill-to-read, read-to-read 
 unACE: idle, read-to-evict, write-to-evict</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>yes6.823, L24-33
Strike on a bit (e.g., in register file) 
Bit 
Bit has error 
protection? yes no 
detection &amp; 
correction no no error benign fault 
no error 
detection only 
outcome? 
True DUE False DUE noyes no outcome? 
benign fault 
no error SDC yes no Joel Emer 
December 7, 2005 
Read? 
affects program affects program 
SDC = Silent Data Corruption, DUE = Detected Unrecoverable Error</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Joel Emer 
December 7, 2005 
Computing AVF 6.823, L24-26 
 Approach is conservative 
 Assume every bit is ACE unless proven otherwise 
 Data Analysis using a Performance Model 
 Prove that data held in a structure is un-ACE 
 Timing Analysis using a Performance Model 
 Tracks the time this data spent in the structure</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Complex Instruction Set Evolution in the Sixties: Stack and GPR Architectures (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l03_sixties/</lecture_pdf_url>
      <lectureno>L3</lectureno>
      <slides>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L3- 10 
Arvind 
Stack Size and Memory References
program 
push a 
push b 
push c 
* + push a push d 
push c 
* + push e 
-
/ 
September 14, 2005 a b c * + a d c * + e - / 
stack (size = 2) memory refs 
R0 a 
R0 R1 b 
R0 R1 R2 c, ss(a) 
R0 R1 sf(a) 
R0 
R0 R1 a 
R0 R1 R2 d, ss(a+b*c) 
R0 R1 R2 R3 c, ss(a) 
R0 R1 R2 sf(a) 
R0 R1 sf(a+b*c) 
R0 R1 R2 e,ss(a+b*c) 
R0 R1 sf(a+b*c) 
R0 
4 stores, 4 fetches (implicit)</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L3- 20 
IBM 360: A General-Purpose Arvind 
Register (GPR) Machine 
 Processor State 
 16 General-Purpose 32-bit Registers 
 may be used as index and base register 
 Register 0 has some special properties 
 4 Floating Point 64-bit Registers 
 A Program Status Word (PSW) 
 PC, Condition codes, Control flags 
 A 32-bit machine with 24-bit addresses
 No instruction contains a 24-bit address ! 
 Data Formats 
 8-bit bytes, 16-bit half-w ords, 32-bit words,     
64-bit double-words 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>bc6.823 L3- 7 
Arvind 
Evaluation of Expressions 
(a + b * c) / (a + d * c - e) 
a e 
d 
a * b * c / 
+ 
* + -
ac *b 
c 
Reverse Polish 
push a Push ca b c * + a d c * + e - / Evaluation Stack 
Push b multiply 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L3- 30 
Arvind 
CDC6600: Vector Addition
B0  -n 
loop:	 JZE  B0, exit 
A0  B0 + a0 load X0 
A1  B0 + b0 load X1 
X6  X0 + X1 
A6  B0 + c0 store X6 
B0  B0 + 1 
jump loop 
Ai = address register 
Bi = index register 
Xi = data register 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L3- 17 
Arvind 
Stack Machines (Mostly) Died by 1980 
1. Stack programs are not smaller if short 
(Register) addresses are permitted.
2. Modern compilers can manage fast register space 
better than the stack discipline. 
3. Lexical addressing is a useful abstract model for 
compilers but hardware support for it (i.e. 
display) is not necessary. 
GPRs and caches are better than stack and displays 
Early language-directed architectures often did not 
take into account the role of compilers! 
B5000, B6700, HP 3000, ICL 2900, Symbolics 3600 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L3- 25 
Arvind 
IBM 360: Character String Operations
length 8 4 12 8 4 12 
opcode B1 D1 B2 D2 
SS format: store to store instructions 
M[(B1) + D1]  M[(B1) + D1] op M[(B2) + D2] 
iterate length times 
Most operations on decimal and character strings 
use this format 
MVC move characters 
MP multiply two packed decimal strings 
CLC compare two character strings 
... 
Multiple memory operations per instruction 
September 14, 2005 
complicates exception &amp; interrupt handling</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L3- 32 
Arvind 
Full Employment for Architects 
	Good news: Ideal instruction set changes continually 
	Technology allows larger CPUs over time 
	Technology constraints change (e.g., now it is power) 
	Compiler technology improves (e.g., register allocation) 
	Programming styles change (assembly, HLL, object-oriented, ) 
	Applications change (e.g., multimedia, ....) 
	Bad news: Software compatibility imposes huge damping 
coefficient on instruction set innovation
	Software investment dwar fs hardware investment 
	Innovate at microarchitecture le vel, below the ISA level (this is 
what most computer architects do) 
	New instruction set can only be justified by new large market 
and technological advantage 
	Network processors 
 Multimedia processors
D S P  s
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L3- 23 
Arvind 
IBM S/390 z900 Microprocessor
 64-bit virtual addressing 
 first 64-bit S/390 design (o riginal S/360 was 24-bit, and 
S/370 was 31-bit extension) 
 1.1 GHz clock rate (announced ISSCC 2001) 
0 . 1 8 m CMOS, 7 layers copper wiring 
 770MHz systems shipped in 2000 
 Single-issue 7-stage CISC pipeline 
 Redundant datapaths 
 every instruction performed in two parallel datapaths and 
results compared 
 256KB L1 I-cache, 256KB L1 D-cache on-chip 
 20 CPUs + 32MB L2 cache per Multi-Chip Module 
 Water cooled to 10oC junction temp 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L3- 2 
Arvind 
The Sixties 
	Hardware costs started dropping 
- memories beyond 32K words seemed likely 
- separate I/O processors 
- large register files 
	Systems software development became 
essential 
-O p e r a t i n g  S y s t e m s
- I/O facilities
	Separation of Programming Model from 
implementation become essential 
- family of computers 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Instruction Set Evolution 
in the Sixties: 
GPR, Stack, and Load-Store
Architectures
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L3- 28 
Arvind 
CDC 6600: Datapath 
Operand Regs 
Address Regs Index Regs 
Inst. Stack IR 10 Functional 
Units 
Memory 
result 
addr result operand 
oprnd 
addr 8 x 18-bit  8 x 18-bit 8 x 60-bit 
8 x 60-bit Central 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L3- 14 
Arvind 
Procedure Calls
 Storage for procedure calls also follows 
a stack discipline 
 
frame 
 &lt; &gt; 
 
to stack frames 
Proc P 
Proc Q 
Proc R 
Q 
R 
Q P Q R Q R 
3 2 
ll = 1 
display dynamic 
links 
static
stack However, there is a need to access 
variables beyond the current stack 
lexical addressing  ll , d 
display registers to speed up accesses 
registers links 
September 14, 2005 
automatic loading of display registers?</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L3- 15 
Arvind 
Stack Machines: Essential features
	In addition to push, 
pop, + etc., the 
instruction set must 
provide the capability 
to 
 refer to any element in 
the data area 
 jump to any instruction in the code area 
 move any element in 
the stack frame to the 
top machinery to 
carry out 
+, -, etc. 
stack 
SP 
DP 
PC . 
. . a 
b 
c  
push a 
push b 
push c 
* 
+ 
push e data/ 
code 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L3- 22 
Arvind 
IBM 360: Original family
Model 30 . . . Model 70 
Storage 8K - 64 KB 256K - 512 KB 
Datapath 8-bit 64-bit 
Circuit Delay 30 nsec/level 5 nsec/level 
Local Store Main Store Transistor Registers 
Control Store Read only 1 sec Conventional circuits 
IBM 360 instruction set architecture completely hid 
the underlying technological differences between 
various models. 
With minor modifications it survives till today 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L3- 5 
Arvind The Burroughs B5000: 
An ALGOL Machine, Robert Barton, 1960
	Machine implementation can be completely 
hidden if the programmer is provided only a 
high-level language interface. 
	Stack machine organization because stacks are 
convenient for: 
1. expression evaluation; 
2. subroutine calls, recursion, nested interrupts; 
3. accessing variables in block-structured 
languages. 
	B6700, a later model, had many more innovative 
features 
 tagged data
v i r t u a l  m e m o r y
	multiple processors and memories 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19 
A five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L3- 18 
Arvind 
Stacks post-1980 
 Inmos Transputers (1985-2000) 
	Designed to support many parallel processes in Occam 
language 
	Fixed-height stack design simplified implementation 
	Stack trashed on context swap (fast context switches) 
	Inmos T800 was worlds fastest microprocessor in late 80s 
 Forth machines 
	Direct support for Forth execution in small embedded real-
time environments 
	Several manufacturers (Roc kwell, Patriot Scientific) 
 Java Virtual Machine 
	Designed for software emul ation not direct hardware 
execution 
	Sun PicoJava implementation + others 
 Intel x87 floating-point unit 
	Severely broken stack model for FP arithmetic 
	Deprecated in Pentium-4 repl aced with SSE2 FP registers 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L3- 26 
Arvind 
IBM 360: Branches &amp; Condition Codes
	Arithmetic and logic instructions set condition 
codes 
e q u a l  t o  z e r o 
 greater than zero 
o v e r f l o w
	carry... 
	I/O instructions also set condition codes
	channel busy 
	Conditional branch instructions are based on 
testing condition code registers (CCs) 
	RX and  RR formats 
 BC_	 branch conditionally 
 BAL_	 branch and link, i.e., R15  (PC)+1 
for subroutine calls 
	CCs must be part of the PSW 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L3- 21 
Arvind 
IBM 360: Precise Interrupts
	IBM 360 ISA (Instruction Set Architecture) 
preserves sequential execution model 
	Programmers view of machine was that 
each instruction either completed or 
signaled a fault before next instruction 
began execution 
	Exception/interrupt behavior constant 
across family of implementations 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L3- 13 
Arvind 
Register Usage in a GPR Machine 
(a + b * c) / (a + d * c - e) 
Load 
Load R0 R1 a c More control over register usage since registers can be named 
explicitly 
Reuse 
R2 Load 
Mul R2 
R2 b 
R1 Load Ri m 
Add R2 R0 Load Ri (Rj) 
Reuse Load R3 d Load Ri (Rj) (Rk) 
R3 Mul 
Add R3 
R3 R1 
R0  
Reuse Load R0 e -eliminates unnecessary 
R0 Sub R3 R0 Loads and Stores 
Div R2 R3 - fewer Registers 
but instructions may be longer! 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L3- 29 
ArvindCDC 6600: 
A Load/Store Architecture
 Separate instructions to ma nipulate three types of reg. 
8 60-bit data registers (X) 
8 18-bit address registers (A) 
8 18-bit index registers (B) 
	All arithmetic and logic instructions are reg-to-reg 
6 3  3 3 
opcode i j k 	 Ri  (Rj) op (Rk) 
 Only Load and Store instructions refer to memory! 
6 3 3 18 
opcode i j disp Ri  M[(Rj) + disp] 
Touching address registers 1 to 5 initiates a load 
6 to 7 initiates a store 
- very useful for vector operations 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L3- 3 
Arvind 
Issues for Architects in the Sixties
	Stable base for software development 
	Support for operating systems 
	processes, multiple users, I/O 
	Implementation of high-level languages 
	recursion, ... 
	Impact of large memories on instruction size 
	How to organize the processor state from the 
programming point of view 
	Architectures for which fast implementations  
could be developed 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L3- 27 
Arvind 
CDC 6600 Seymour Cray, 1964 
	A fast pipelined machine with 60-bit words 
	Ten functional units 
- Floating Point: adder, multiplier, divider 
- Integer: adder, multiplier 
... 
	Hardwired control (no microcoding) 
	Dynamic scheduling of instructions using a  
scoreboard 
	Ten Peripheral Processors for Input/Output 
- a fast time-shared 12-bit integer ALU 
 Very fast clock 
 Novel freon-based  technology for cooling 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L3- 31 
Arvind 
What makes a good instruction set?
One that provides a simple software interface yet 
allows simple, fast, efficient hardware 
implementations 
 but across 25+ year time frame 
Example of difficulties: 
	Current machines have register files with more storage 
than entire main memory of early machines! 
	On-chip test circuitry in current machines has hundreds of times more transistors than entire early computers! 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L3- 16 
Arvind Stack versus GPR Organization
Amdahl, Blaauw and Brooks, 1964 
1. The performance advantage of push down stack 
organization is derived from the presence of fast    
registers and not the way they are used. 
2.Surfacing of data in stack which are profitable is 
approximately 50% because of constants and 
common subexpressions. 
3. Advantage of instruction density because of implicit 
addresses is equaled if short addresses to specify registers are allowed. 
4. Management of finite depth stack causes complexity. 
5. Recursive subroutine advantage can be realized only 
with the help of an independent stack for addressing. 
6. Fitting variable length fields into fixed width word is 
awkward. 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L3- 24 
Arvind 
IBM 360: Some Addressing Modes
8 4 4 
RR opcode R1 R2 R1(R1) op (R2) 
8 4 4 4 1 2 
opcode R X B D RD 
R  (R) op M[(X) + (B) + D] 
a 24-bit address is formed by adding the 
12-bit displacement (D) to a base register (B) 
and an Index register (X), if desired 
The most common formats for arithmetic &amp; logic 
instructions, as well as Load and Store instructions 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L3- 12 
ArvindStack Size and Expression
Evaluation 
a b c * + a d c * + e - / 
a and c are 
loaded twice 

not the best use of registers! 
September 14, 2005 program 
push a push b 
push c 
* + push a push d 
push c 
* + push e 
-
/ stack (size = 2) 
R0 
R0 R1 
R0 R1 R2 
R0  R1 
R0 
R0 R1 
R0 R1 R2 
R0 R1 R2 R3 
R0 R1 R2 
R0  R1 
R0 R1 R2 
R0  R1 
R0</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L3- 9 
Arvind 
Hardware organization of the stack
 Stack is part of the processor state 
 stack must be bounded and small 
 number of Registers, 
not the size of main memory 
 Conceptually stack is unbounded
a part of the stack is included in the 
processor state; the rest is kept in the 
main memory 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L3- 4 
Three Different Directions in Arvind 
the Sixties 
	A machine with only a high-level language 
interface 
	Burroughs 5000, a stack machine 
	A family of computers based on a common 
ISA 
	IBM 360, a General Register Machine 
	A pipelined machine with a fast clock 
(Supercomputer) 
	CDC 6600, a Load/Store architecture 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L3- 11 
ArvindStack Operations and 
Implicit Memory References 
	Suppose the top 2 elements of the stack 
are kept in registers and the rest is kept in 
the memory. 
Each push operation  1 memory reference 
pop operation  1 memory reference 
No Good! 
	Better performance can be got if the top N 
elements are kept in registers and memory 
references are made only when register 
stack overflows or underflows. 
Issue - when to Load/Unload registers ? 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>ab * c6.823 L3- 8 
Arvind 
Evaluation of Expressions 
(a + b * c) / (a + d * c - e) 
a 
add
/ 
+ 
* + e -
ac 
d c *b 
Reverse Polish + 
Evaluation Stack a + b * c 
a b c * + a d c * + e - / 
September 14, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L3- 6 
Arvind 
A Stack Machine 
Processor A Stack machine has a stack as 
: stack Main 
Store 
a b 
a pop push c push b c b a 
 
  a part of the processor state 
typical operations: 
push, pop, +, *, ... 
Instructions like + implicitly 
specify the top 2 elements of 
the stack as operands. 
b 
a 
September 14, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Vector Computers (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l22_vector/</lecture_pdf_url>
      <lectureno>L22</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>6.823, L22-24
 Vector Stripmining 
Problem: Vector registers have finite length 
Solution: Break loops into pieces that fit in registers, Stripmining 
ANDI R1, N, 63 # N mod 64 
MTC1 VLR, R1 # Do remainder 
loop:
LV V1, RADSLL R2, R1, 3
DADDU RA, RA, R2 # Bump pointer
LV V2, RBDADDU RB, RB, R2
ADDV.D V3, V1, V2
SV V3, RC
DADDU RC, RC, R2
DSUBU N, N, R1 # Subtract elements
LI R1, 64
MTC1 VLR, R1 # Reset full lengthBGTZ N, loop # Any more to do? for (i=0; i&lt;N; i++)
C[i] = A[i]+B[i]; 
+ 
+ + AB C 
Remainder Joel Emer 
November 30, 2005 
# Multiply by 8    
64 elements</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>All operands must be read in and out of memory
ust check dependencies on memory addresses6.823, L22-22 
Vector Memory-Memory vs. Vector 
Register Machines 
 
greater main memory bandwidth, why? 
 
 
vector operations, why? 
 
 
 
 
vector machines since Cray-1 have had vector register 
architectures 
(we ignore vector memory-memory from now on) Joel Emer 
November 30, 2005 
Vector memory-memory architectures (VMMA) require 
VMMAs make if difficult to overlap execution of multiple 
 M 
VMMAs incur greater startup latency 
Scalar code was faster on CDC Star-100 for vectors &lt; 100 elements 
For Cray-1, vector/scalar breake ven point was around 2 elements 
Apart from CDC follow-ons (Cyber-205, ETA-10) all major</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Joel Emer 
Cray-1 (1976) November 30, 2005 
6.823, L22-6 
Core unit of the Cray 1 computer 
Image removed due to copyright restrictions. 
To view image, visit http://www.cray-
cyber.org/memory/scray.php.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-4Loop Unrolled Code Schedule
loop: 	 ld f1, 0(r1) 
ld f2, 8(r1) 
ld f3, 16(r1) 
ld f4, 24(r1) 
add r1, 32 
fadd f5, f0, f1 
fadd f6, f0, f2 
fadd f7, f0, f3 fadd f8, f0, f4 
sd f5, 0(r2) sd f6, 8(r2) 
sd f7, 16(r2) 
sd f8, 24(r2) 
add r2, 32 
bne r1, r3, loop loop: 
Schedule Int1 Int 2 M1 M2 FP+ FPx
ld f1 
ld f2 
ld f3 
add r1 ld f4 fadd f5 
fadd f6 
fadd f7 
fadd f8 
sd f5 
sd f6 
sd f7 
add r2 bne sd f8</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-27 
Vector Conditional Execution 
Problem: Want to vectorize loops with conditional code: 
for (i=0; i&lt;N; i++)
if (A[i]&gt;0) then
A[i] = B[i];
Solution: Add vector mask (or flag) registers 
 vector version of predicate registers, 1 bit per element 
and maskable vector instructions 
 vector operation becomes NOP at elements where mask bit is clear 
Code example: 
CVM # Turn on all elements 
LV vA, rA # Load entire A vector 
SGTVS.D vA, F0 # Set bits in mask register where A&gt;0 
LV vA, rB # Load B vector into A under mask 
SV vA, rA # Store A back to memory under mask</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-10 
Vector Instruction Set Advantages
C o m p a c t 
 one short instruction encodes N operations 
 Expressive, tells hardware  that these N operations:
 are independent 
 use the same functional unit 
 access disjoint registers 
 access registers in same pattern as previous instructions 
 access a contiguous block of memory
(unit-stride load/store)
 access memory in a known pattern 
(strided load/store) 
S c a l a b l e 
 can run same code on more parallel pipelines ( lanes)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-2 
Supercomputers 
Definition of a supercomputer: 
 Fastest machine in world at given task 
 A device to turn a compute-bound problem into an I/O 
bound problem 
 Any machine costing $30M+ 
 Any machine designed by Seymour Cray 
CDC6600 (Cray, 1964) regarded as first supercomputer</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823, L22-11
Vector Arithmetic Execution 
clock) to execute element 
operations 
pipeline because elements in 
vector are independent (=&gt; no hazards!) V 
1 V 
2 V 
3 
Six stage multiply pipeline Joel Emer 
November 30, 2005 
 Use deep pipeline (=&gt; fast 
 Simplifies control of deep 
V3 &lt;- v1 * v2</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-32 
Multimedia Extensions 
 Very short vectors added to existing ISAs for micros 
 Usually 64-bit registers split into 2x32b or 4x16b or 8x8b 
 Newer designs have 128-bit registers (Altivec, SSE2) 
 Limited instruction set: 
 no vector length control 
 no strided load/store or scatter/gather 
 unit-stride loads must be aligned to 64/128-bit boundary 
 Limited vector register length: 
 requires superscalar dispatch to keep multiply/add/load units busy 
 loop unrolling to hide latencies increases register pressure 
 Trend towards fuller vector support in microprocessors</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823, L22-14
Vector Unit Structure 
Lane Functional Unit 
Vector 
Registers 
Memory Subsystem Elements 
0, 4, 8,  Elements 
1, 5, 9,  Elements 
2, 6, 10,  Elements 
3, 7, 11,  Joel Emer 
November 30, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823, L22-7
Cray-1 (1976) 
Single Port 
Memory 
16 banks of 
64-bit words 
+ 
8-bit SECDED 
80MW/sec data load/store 
320MW/sec 
instruction 
buffer refill 
4 Instruction Buffers 64-bitx16 
LIP (A0) ( (Ah) + j k m ) 
64 
T Regs 
(A0) ( (Ah) + j k m ) 
64 
B Regs S0 
S1 S2 S3 S4 
S5 
S6 S7 
A0 A1 A2 A3 A4 
A5 
A6 A7 Si 
Tjk 
Ai 
Bjk FP Add 
FP Mul FP Recip 
Int Logic 
Pop Cnt Sj 
Si Sk 
Addr Add Addr Mul Aj 
Ai Ak 
memory bank cycle 50 ns processor cycle 12.5 ns (80MHz) V0 
V1 V2 V3 V4 
V5 
V6 V7 
Vk Vj Vi V. Mask 
V. Length 64 Element 
Vector Registers Joel Emer 
November 30, 2005 
NIP CIP Int Add 
Int Shift</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Joel Emer 
November 30, 2005 
A Modern Vector Super: NEC SX-6 (2003)6.823, L22-31 
 CMOS Technology 
 500 MHz CPU, fits on single chip 
 SDRAM main memory (up to 64GB) 
 Scalar unit 
 4-way superscalar with out-of-order and speculative Image removed due 
execution	to copyright 
restrictions.  64KB I-cache and 64KB data cache 
	Vector unit Image available in 
Kitagawa, K., S. 
 8 foreground VRegs + 64 background VRegs (256x64- Tagaya, Y. Hagihara, 
bit elements/VReg) and Y. Kanoh. "A 
 1 multiply unit, 1 divide unit, 1 add/shift unit, 1 logical hardware overview of 
unit, 1 mask unit SX-6 and SX-7 
8  l a n e s  ( 8  G F L O P S peak, 16 FLOPS/cycle)	 supercomputer." NEC 
 1 load &amp; store unit (32x8 byte accesses/cycle)	 Research &amp; 
 32 GB/s memory bandwidth per processor	Development Journal 
44, no. 1 (Jan 
S M P  s t r u c t u r e	 2003):2-7. 
 8 CPUs connected to memory through crossbar 
 256 GB/s shared memory ba ndwidth (4096 interleaved 
banks)</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823, L22-19
 Vector Startup 
Two components of vector startup penalty 
instruction can start down pipeline) 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W 
R X X X W Functional Unit Latency 
Dead Time Dead Time Joel Emer 
November 30, 2005 
 functional unit latency (time through pipeline) 
 dead time or recovery time (time before another vector 
First Vector Instruction 
Second Vector Instruction</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-20 
Dead Time and Short Vectors
No dead time 
4 cycles dead time T0, Eight lanes 
No dead time 
100% efficiency with 8 element 
vectors 
64 cycles active 
Cray C90, Two lanes
4 cycle dead time
Maximum efficiency 94% 
with 128 element vectors</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823, L22-18 
Vector Chaining Advantage 
 
result appears 
Load 
Mul 
Add Load 
Mul 
Add Time  
written before starting dependent instruction Joel Emer 
November 30, 2005 
With chaining, can start dependent instruction as soon as first Without chaining, must wait for last element of result to be</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-5 
Vector Supercomputers 
Epitomized by Cray-1, 1976: 
 Scalar Unit 
 Load/Store Architecture 
 Vector Extension 
 Vector Registers
V e c t o r  I n s t r u c t i o n s
 Implementation 
H a r d w i r e d  C o n t r o l 
 Highly Pipelined Functional Units 
 Interleaved Memory System
N o  D a t a  C a c h e s
N o  V i r t u a l  M e m o r y</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-28 
Masked Vector Instructions
Simple Implementation Density-Time Implementation 
 execute all N operations, turn off result  scan mask vector and only execute 
writeback according to mask elements with non-zero masks 
M[7]=1 A[7] B[7] M[7]=1
M[6]=0 A[6] B[6] M[6]=0
A[7] B[7] 
M[5]=1 A[5] B[5] M[5]=1 
M[4]=1 A[4] B[4] M[4]=1 
M[3]=0 A[3] B[3] M[3]=0 
M[2]=0 
M[1]=1
M[2]=0
M[0]=0
M[1]=1
C[4] C[5] 
C[1] 
Write data port 
M[0]=0 C[1]C[2]
C[0] 
Write Enable Write data port</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-1 
Vector Computers
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Based on the material prepared by 
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823, L22-8
 Vector Programming Model 
+ + + + + + 
[0] [VLR-1] Vector Arithmetic 
Instructions 
ADDV v3, v1, v2 v3 v2 v1 Scalar Registers 
r0 r15 Vector Registers 
v0 v15 
[0] [1] [2] [VLRMAX-1] 
VLR Vector Length Register 
v1 Vector Load and 
Store Instructions 
LV v1, r1, r2 
Base, r1 Stride, r2 Memory Joel Emer 
November 30, 2005 
[1] 
Vector Register</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-9 
Vector Code Example
# Scalar Code 
LI R4, 64 
loop:
L.D F0, 0(R1)
L.D F2, 0(R2)
ADD.D F4, F2, F0 
S.D F4, 0(R3)
DADDIU R1, 8 
DADDIU R2, 8 
DADDIU R3, 8 
DSUBIU R4, 1 BNEZ R4, loop # Vector Code 
LI VLR, 64 LV V1, R1 
LV V2, R2 
ADDV.D V3, V1, V2 
SV V3, R3 # C code 
for (i=0; i&lt;64; i++)
C[i] = A[i] + B[i];</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823, L22-12
Vector Instruction Execution 
ADDV C,A,B 
C[1] C[2] 
C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] unit 
C[4] C[8] 
C[0] A[12] A[16] A[20] A[24] 
C[5] C[9] 
C[1] A[13] A[17] A[21] A[25] 
C[6] C[10] 
C[2] A[14] B[14] A[18] B[18] A[22] B[22] A[26] B[26] 
C[7] C[11] 
C[3] A[15] B[15] A[19] B[19] A[23] B[23] A[27] B[27] Joel Emer 
November 30, 2005 
Execution using one 
pipelined functional 
B[12] B[16] B[20] B[24] 
B[13] B[17] B[21] B[25] Execution using 
four pipelined 
functional units</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823, L22-17
 Vector Chaining 
Memory V 
1 
Load 
Unit 
Mult. V 
2 V 
3 
Chain 
Add V 
4 V 
5 
Chain LV v1 
MULV v3,v1,v2 ADDV v5, v3, v4 Joel Emer 
November 30, 2005 
 Vector version of register bypassing 
 introduced with Cray-1</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-26 
Vector Scatter/Gather 
Scatter example : 
for (i=0; i&lt;N; i++)
A[B[i]]++;
Is following a correct translation?
LV vB, rB # Load indices in B vector 
LVI vA, rA, vB # Gather initial A values ADDV vA, vA, 1 # Increment SVI vA, rA, vB # Scatter incremented values</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823, L22-13
Vector Memory System 
0 1 2 3 4 5 6 7 8 9 A B C D E F + Base Stride 
Memory Banks Address 
Generator  Bank busy timeJoel Emer 
November 30, 2005 
Vector Registers Cray-1, 16 banks, 4 cycle bank busy time, 12 cycle latency 
: Cycles between accesses to same bank</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-23 Automatic Code Vectorization 
for (i=0; i &lt; N; i++)
C[i] = A[i] + B[i]; 
Scalar Sequential Code Vectorized Code 
load 
load 
add 
store 
load 
load 
add 
store Iter. 1 
Iter. 2 
reordering of operation sequencing 
 load 
load 
add 
store load 
load 
add 
store 
Iter 
. 1 Iter 
. 2 Time 
Vectorization is a massive compile-time 
requires extensive loop dependence analysis Vector Instruction</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Joel Emer 
November 30, 2005 
Vector Scatter/Gather 6.823, L22-25 
Want to vectorize loops with indirect accesses: 
for (i=0; i&lt;N; i++)
A[i] = B[i] + C[D[i]]
Indexed load instruction ( Gather )
LV vD, rD # Load indices in D vector 
LVI vC, rC, vD # Load indirect from rC base 
LV vB, rB # Load B vector 
ADDV.D vA, vB, vC # Do add SV vA, rA # Store result</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Joel Emer 
November 30, 2005 
6.823, L22-3 
Supercomputer Applications
Typical application areas 
 Military research (nuclear weapons, cryptography) 
 Scientific research 
 Weather forecasting 
 Oil exploration 
 Industrial design (car crash simulation) 
 Bioinformatics 
 Cryptography 
All involve huge computations on large data sets 
In 70s-80s, Supercomputer Vector Machine</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Joel Emer 
November 30, 2005 
Vector Reductions 6.823, L22-30 
Problem: Loop-carried dependence on reduction variables 
sum = 0;
for (i=0; i&lt;N; i++)
sum += A[i]; # Lo op-carried dependence on sum
Solution: Re-associate operations if possible, use binary tree to 
perform reduction
# Rearrange as:
sum[0:VL-1] = 0 # Vector of VL partial sums
for(i=0; i&lt;N; i+=VL)     # Stripmine VL-sized chunks
sum[0:VL-1] += A[i:i+VL-1]; # Vector sum
# Now have VL partial sums in one vector register
do {
VL = VL/2; # Halve vector length 
sum[0:VL-1] += sum[VL:2*VL-1] # Halve no. of partials 
} while (VL&gt;1)</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Joel Emer 
November 30, 2005 
Vector Memory-Memory versus Vector 6.823, L22-21 
Register Machines 
	Vector memory-memory instructions hold all vector 
operands in main memory
	The first vector machines, CDC Star-100 (73) and TI ASC 
(71), were memory-memory machines 
	Cray-1 (76) was first vector register machine 
Example Source Code 
for (i=0; i&lt;N; i++)
{ 
C[i] = A[i] + B[i];
D[i] = A[i] - B[i]; 
}
ADDV C, A, B SUBV D, A, B Vector Memory-Memory Code 
LV V1, A LV V2, B 
ADDV V3, V1, V2 
SV V3, C SUBV V4, V1, V2 
SV V4, D Vector Register Code</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>A[7]
A[1]A[4]A[5]
Joel Emer 
November 30, 2005 
6.823, L22-29 
Compress/Expand Operations
	Compress packs non-masked elements from one vector register 
contiguously at start of destination vector register 
	population count of mask vector gives packed vector length 
	Expand performs inverse operation 
M[3]=0 M[4]=1 M[5]=1 M[6]=0 
M[2]=0 
M[1]=1 
M[0]=0 M[7]=1 
A[3] A[4] A[5] A[6] A[7] 
A[0] A[1] A[2] M[3]=0 M[4]=1 M[5]=1 M[6]=0 
M[2]=0 
M[1]=1 
M[0]=0 M[7]=1 
B[3] A[4] A[5] B[6] A[7] 
B[0] A[1] B[2] A[7] 
A[1] A[4] A[5] 
Compress Expand 
Used for density-time conditionals  and also for general selection 
operations</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823, L22-16 
load Vector Instruction Parallelism 
Can overlap execution of multiple vector instructions 
 
load 
mul 
mul add 
add Load Unit Multiply Unit Add Unit 
time 
issue Joel Emer 
November 30, 2005 
example machine has 32 elements per vector register and 8 lanes 
Instruction 
Complete 24 operations/cycle while issuing 1 short instruction/cycle</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>_____________________________________________ 
Joel Emer 
6.823, L22-15T0 Vector Microprocessor (1995) November 30, 2005 
Vector register 
elements striped 
over lanes 
[24] [25][26][27][28] [29]
[16] [17][18][19][20] [21]
[8] [9] [10][11][12] [13]
[0] [1] [2] [3] [4] [5]
Lane 
[30][31] 
[22][23] 
[14][15] 
[6] [7] 
For more information, visit http://www.icsi.berkeley.edu/real/spert/t0-intro.html</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Influence of Technology and Software on Instruction Sets: Up to the dawn of IBM 360 (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l02_fifties/</lecture_pdf_url>
      <lectureno>L2</lectureno>
      <slides>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L2- 20 
Arvind 
Variety of Instruction Formats
 Two address formats: the destination is 
same as one of the operand sources
(Reg  Reg) to Reg	  (RI) + (RJ) RI 
(Reg  Mem) to Reg  (RI) + M[x] RI 
	x can be specified directly or via a register 
	effective address calculatio n for x could include    
indexing, indirection, ...
 Three address formats: One destination and 
up to two operand sources per instruction
(Reg x Reg) to Reg	 (RJ) + (RK) RI  
(Reg x Mem) to Reg (RJ) + M[x] RI 	 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>M+1M+26.823 L2- 17 
Indirect Addressing and Arvind 
Subroutine Calls 
Indirect addressing 
LOAD (F)
inc F 
STORE(F)
inc F
JUMP (F)M+3Subroutine 
LOAD (x) means AC  M[M[x]] F ... F+1 
Caller 
Events: 
S1 fetch M JSR  F Execute M arg arg Execute S1result Execute S2M+3 
Execute S3 S2 
store 
result 
Indirect addressing almost eliminates the 
need to write self-modifying code (location S3 
F still needs to be modified)
 Problems with recursive procedure calls 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>JUMP LOOP
DONE HLT6.823 L2- 9 
Arvind 
Self-Modifying Code
LOOP 
F1
F2
F3
modify the program 
for the next 
iteration 
DONE 
September 12, 2005 LOAD 
JGE 
ADD 
STORE 
LOAD 
ADD 
STORE N 
DONE 
ONE 
N 
A 
B 
C Ci  Ai + Bi, 1  i  n 
Each iteration involves 
total book-
LOAD ADR 
ADD 
STORE ADR 
LOAD ADR 
ADD 
STORE ADR 
LOAD ADR 
ADD 
STORE ADR F1 
ONE 
F1 
F2 
ONE 
F2 
F3 
ONE 
F3 
JUMP LOOP 
HLT instruction 
fetches 
operand 
fetches 
stores keeping 
17 14 
10 8 
5 4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Influence of Technology and
Software on Instruction Sets:
Up to the dawn of IBM 360
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L2- 8 
Programming: Arvind 
Single Accumulator Machine 
Ci  Ai + Bi, 1  i  n A 
LOOP LOAD N B 
JGE DONE 
ADD ONE 
STORE N C 
F1 LOAD A 
F2 ADD B 
F3 STORE C N 
JUMP LOOP ONE 
DONE HLT 
code 
How to modify the addresses A, B and C ? 
-n 
1 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L2- 5 
Arvind 
Computers in mid 50s 
H a r d w a r e  w a s  e x p e n s i v e 
	Stores were small (1000 words) 
	No resident system-software! 
	Memory access time was 10 to 50 times 
slower than the processor cycle 
 Instruction execution time was totally dominated by 
the memory reference time . 
T h e ability to design complex control 
circuits to execute an instruction was the 
central design concern as opposed to the 
speed of decoding or an ALU operation 
	Programmers view of the machine was 
inseparable from the actual hardware 
implementation 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L2- 26 
IBM 360: A General-Purpose Arvind 
Register (GPR) Machine 
 Processor State 
 16 General-Purpose 32-bit Registers 
 may be used as index and base register 
 Register 0 has some special properties 
 4 Floating Point 64-bit Registers 
 A Program Status Word (PSW) 
 PC, Condition codes, Control flags 
 A 32-bit machine with 24-bit addresses
 No instruction contains a 24-bit address ! 
 Data Formats 
 8-bit bytes, 16-bit half-w ords, 32-bit words,     
64-bit double-words 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L2- 14 
Arvind 
Indexing vs. Index Registers
Suppose instead of registers, memory locations 
are used to implement index registers. 
LOAD x, IX 
Arithmetic operations on index registers can be performed by bringing the contents to the 
accumulator 
Most bookkeeping instruct ions will be avoided but 
each instruction will implicitly cause several fetches and stores 
 complex control circuitry 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L2- 16 
Arvind 
Support for Subroutine Calls
Main
Program
call F 
a1 
a2 
call F 
b1 
b2 F: 
return Subroutine F 
A special subroutine jump instruction 
M:	 JSR F F  M + 1 and 
jump to F+1 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L2- 19 
Arvind 
Evolution of Addressing Modes
1. Single accumulator, absolute address
LOAD x 
2. Single accumulator, index registers
LOAD x, IX 
3. Indirection
LOAD (x) 
4. Multiple accumulators, index registers, indirection
LOAD R, IX, x 
or LOAD R, IX, (x) the meaning? 
R  M[M[x] + (IX)] 
or R  M[M[x + (IX)]] 
5. Indirect through registers
LOAD RI, (RJ) 
6. The works
LOAD RI, RJ, (RK) RJ = index, RK = base addr 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L2- 4 
Arvind 
But Software...
As people write programs and use computers, 
our understanding of programming and 
program behavior improves. 
This has profound though slower impact 
on computer architecture
Modern architects cannot avoid paying 
attention to software and compilation issues. 
Technology 
Software 
Computers 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L2- 7 
Arvind 
The Earliest Instruction Sets 
Single Accumulator - A carry-over from the calculators.
LOAD x 
STORE x 
ADD x 
SUB x 
MUL x 
DIV x 
SHIFT LEFT 
SHIFT RIGHT 
JUMP x 
JGE x 
LOAD ADR x 
STORE ADR x AC  M[x]
M[x]  (AC)
AC  (AC) + M[x]
Involved a quotient register
AC  2  (AC)
PC  x
if (AC)  0 then PC  x
AC  Extract addres s field(M[x])
Typically less than 2 dozen instructions! 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L2- 28 
Arvind 
IBM S/390 z900 Microprocessor
 64-bit virtual addressing 
 first 64-bit S/390 design (o riginal S/360 was 24-bit, and 
S/370 was 31-bit extension) 
 1.1 GHz clock rate (announced ISSCC 2001) 
0 . 1 8 m CMOS, 7 layers copper wiring 
 770MHz systems shipped in 2000 
 Single-issue 7-stage CISC pipeline 
 Redundant datapaths 
 every instruction performed in two parallel datapaths and 
results compared 
 256KB L1 I-cache, 256KB L1 D-cache on-chip 
 20 CPUs + 32MB L2 cache per Multi-Chip Module 
 Water cooled to 10oC junction temp 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L2- 15 
Arvind 
Operations on Index Registers
To increment index register by k 
AC  (IX) new instruction 
AC  (AC) + k 
IX  (AC) new instruction 
also the AC must be saved and restored. 
It may be better to increment IX directly 
INCi k, IX IX  (IX) + k 
More instructions to manipulate index register
STOREi x, IX M[x]  (IX) (extended to fit a word) 
... 
IX begins to look like an accumulator
 several index registers
several accumulators
 General Purpose Registers 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L2- 29 
Arvind 
What makes a good instruction set?
One that provides a simple software interface yet 
allows simple, fast, efficient hardware 
implementations 
 but across 25+ year time frame 
Example of difficulties: 
	Current machines have register files with more storage 
than entire main memory of early machines! 
	On-chip test circuitry in current machines has hundreds of times more transistors than entire early computers! 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L2- 30 
Arvind 
Full Employment for Architects 
	Good news: Ideal instruction set changes continually 
	Technology allows larger CPUs over time 
	Technology constraints change (e.g., now it is power) 
	Compiler technology improves (e.g., register allocation) 
	Programming styles change (assembly, HLL, object-oriented, ) 
	Applications change (e.g., multimedia, ....) 
	Bad news: Software compatibility imposes huge damping 
coefficient on instruction set innovation
	Software investment dwar fs hardware investment 
	Innovate at microarchitecture le vel, below the ISA level (this is 
what most computer architects do) 
	New instruction set can only be justified by new large market 
and technological advantage 
	Network processors 
 Multimedia processors
D S P  s
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L2- 11 
Arvind 
Processor State
The information held in the processor at the end of 
an instruction to provide the processing context for 
the next instruction. 
Program Counter, Accumulator, . . . 
Programmer visible state of the processor (and memory) 
plays a central role in computer organization for both 
hardware and software: 
	Software must make efficient use of it 
	If the processing of an instruction can be interrupted 
then the hardware must save and restore the state in 
a transparent manner 
Programmers machine model is a contract 
between the hardware and software 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L2- 13 
Arvind 
Using Index Registers 
Ci  Ai + Bi, 1  i  n 
LOADi -n, IX A 
LOOP JZi DONE, IX 
LOAD LASTA, IX 
ADD LASTB, IX 
STORE LASTC, IX 
JUMP LOOP LASTA
DONE HALT 
 Program does not modify itself 
 Efficiency has improved dramatically (ops / iter)
with index regs without index regs 
instruction fetch 5(2) 17 (14) 
operand fetch 2 10 (8) 
store 1 5 (4) 
 Costs: Instructions are 1 to 2 bits longer 
Index registers with ALU-like circuitry 
Complex control 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L2- 18 
Recursive Procedure Calls and Arvind 
Reentrant Codes 
Indirect Addressing through a register
LOAD R1, (R2) 
Load register R1 with the contents of the 
word whose address is contained in register R2 
PC 
SP registers Pure Code 
Data 
Stack memory 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L2- 25 
Arvind 
IBM 360 : Design Premises
Amdahl, Blaauw and Brooks, 1964 
	The design must lend itself to growth and 
successor machines 
	General method for connecting I/O devices 
	Total performance - answers per month rather 
than bits per microsecond  programming aids 
	Machine must be capable of supervising itself 
without manual intervention 
	Built-in hardware fault checking and locating aids 
to reduce down time 
	Simple to assemble systems with redundant I/O 
devices, memories etc. for fault tolerance 
	Some problems required floating point words 
larger than 36 bits 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L2- 2 
Arvind 
Importance of Technology
New technologies not only provide greater 
speed, size and reliability at lower cost, but 
more importantly these dictate the kinds of 
structures that can be considered and thus 
come to shape our whole view of what a computer is. 
Bell &amp; Newell 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L2- 6 
ArvindProgrammers view of the machine 
IBM 650 
A drum machine with 44 instructions
Instruction: 60 1234 1009 
	Load the contents of location 1234 into the 
distribution ; put it also into the upper accumulator ; 
set lower accumulator to zero; and then go to 
location 1009 for the next instruction. 
Good programmers optimized the placement of 
instructions on the drum to reduce latency! 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L2- 23 
Arvind 
Some Problems
	Should all addressing modes be provided for 
every operand? 
 regular vs. irregular instruction formats 
	Separate instructions to manipulate 
Accumulators, Index registers, Base registers 
 	large number of instructions 
	Instructions contained implicit memory 
references -- several contained more than one 
	very complex control 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L2- 10 
ArvindProcessor-Memory Bottleneck: 
Early Solutions 
	Fast local storage in the processor 
	8-16 registers as opposed to one accumulator 
	Indexing capability 
	to reduce book keeping instructions 
	Complex instructions 
 to reduce instruction fetches 
	Compact instructions 
	implicit address bits for operands, to reduce 
instruction fetches Memory 
Processor 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L2- 21 
Arvind 
More Instruction Formats 
	Zero address formats: operands on a stack 
add M[sp-1]  M[sp] + M[sp-1] 
load M[sp]  M[M[sp]] 
	Stack can be in registers or  in memory (usually top of 
stack cached in registers) 
	One address formats: Accumulator machines 
	Accumulator is always other implicit operand 
Many different formats are possible! 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L2- 22 
Arvind 
Data Formats and Memory Addresses
Data formats: 
Bytes, Half words, words and double words 
Some issues 
 Byte addressing 
B i g  E n d i a n 0 1 2 3 
vs. L i t t l e  E n d i a n 3 2 1 0 
 Word alignment 
Suppose the memory is organized in 32-bit words.
Can a word address begin only at 0, 4, 8, .... ? 
0 1 2 3 4 5 6 7 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L2- 3 
Technology is the dominant factor Arvind 
in computer design 
Technology 
Transistors 
VLSI (initially) 
Technology 
Core memories 
Magnetic tapes 
Disks 
Technology
ROMs, RAMs VLSI 
Packaging 
Low Power 
Computers 
Laser disk, CDs 
Computers 
Computers Integrated circuits 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L2- 27 
Arvind 
IBM 360: Implementation
Model 30 . . . Model 70 
Storage 8K - 64 KB 256K - 512 KB 
Datapath 8-bit 64-bit 
Circuit Delay 30 nsec/level 5 nsec/level 
Local Store Main Store Transistor Registers 
Control Store Read only 1 sec Conventional circuits 
IBM 360 instruction set architecture completely hid 
the underlying technological differences between 
various models. 
With minor modifications it survives till today 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L2- 24 
Arvind 
Compatibility Problem at IBM
By early 60s, IBM had 4 incompatible lines of 
computers! 
701  7094 
650  7074 
702  7080 
1401   7010 
Each system had its own
 Instruction set 
 I/O system and Secondary Storage: 
magnetic tapes, drums and disks 
 assemblers, compilers, libraries,... 
 market niche 
business, scientific, real time, ... 
 IBM 360 
September 12, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L2- 12 
Arvind Index Registers 
Tom Kilburn, Manchester University, mid 50s
One or more specialized registers to simplify 
address calculation 
Modify existing instructions 
LOAD x, IX AC  M[x + (IX)] 
ADD x, IX AC  (AC) + M[x + (IX)] 
... 
Add new instructions to manipulate index registers
JZi x, IX if (IX)=0 then PC  x 
else IX  (IX) + 1 
LOADi x, IX IX  M[x]  (truncated to fit IX) 
... 
Index registers have accumulator-like 
characteristics 
September 12, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Cache Coherence (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l17_cc/</lecture_pdf_url>
      <lectureno>L17</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L17- 6 
Arvind 
A System with Multiple Caches
L1 P 
L1 P 
L1 P 
L1 P 
L2 L2 L1 P 
L1 P 
M Interconnect 
	Modern systems often have hierarchical caches 
	Each cache has exactly one parent but can have zero 
or more children 
	Only a parent and its children can communicate 
directly 
	Inclusion property is maintained between a parent 
and its children, i.e.,
a  Li  a  Li+1
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L17- 8 
Arvind 
Warmup: Parallel I/O 
Either Cache or DMA can 
effect transfers DISK DMA Physical 
Memory 
Proc. 
R/W Data (D) Cache Address (A) 
A 
D 
R/W Page transfers 
occur while the 
Processor is running Memory
Bus 
be the Bus Master and 
DMA stands for Direct Memory Access 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L17- 3 
Arvind 
Write-back Caches &amp; SC
prog T1 
X= 1
Y=11
X= 1
Y=11
X= 1Y=11
X= 1Y=11
X= 1Y=11cache-1 memory cache-2 prog T2 
ST X, 1 LD Y, R1 
ST Y,11 ST Y, R1 T1 is executed LD X, R2 
ST X,R2 
 cache-1 writes back Y
 T2 executed 
 cache-1 writes back X
 cache-2 writes back
X &amp; Y
November 9, 2005 
X = 0 
Y =10 
X= 
Y= 
X = 0 Y =11 
X= 
Y= 
X = 0 Y =11 
X= 
Y= 
X = 1 
Y =11 
X= 
Y= 
X = 1 Y =11 
X= 0 
Y=11 Y = 
Y= 
X = 
X= 
Y = Y= 
X = 
X= 
Y = 11 
Y= 11 
X = 0 
X= 0 
Y = 11 Y= 11 
X = 0 
X= 0 
Y =11 Y=11 
X = 0 
X= 0 
n
tere
oh
nci</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L17- 2 
Arvind 
Memory Consistency in SMPs
cache-1 A 100 
CPU-Memory bus CPU-1 CPU-2 
cache-2 A 100 
memory A 100 
Suppose CPU-1 updates A to 200. 
write-back : memory and cache-2 have stale values 
write-through : cache-2 has a stale value 
Do these stale values matter? 
November 9, 2005 
What is the view of shar ed memory for programming?</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L17- 24 
Arvind 
Load-reserve &amp; Store-conditional
Special register(s) to hold reservation flag and 
address, and the outcome of store-conditional
Load-reserve(R, a): 
&lt;flag, adr&gt;  &lt;1, a&gt;; 
R  M[a]; Store-conditional(a, R): 
if &lt;flag, adr&gt; == &lt;1, a&gt; 
then cancel other procs 
reservation on a; 
M[a]  &lt;R&gt;; 
status  succeed; 
else status  fail; 
If the snooper sees a store transaction to the address in the reserve register, th e reserve bit is set to 0 
 Several processors may reserve  a simultaneously 
 These instructions are like ordinary loads and stores 
with respect to the bus traffic 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L17- 15 
Arvind 
Observation 
M 
S I P
1intents
towrite 
Other processor Other processor reads 
P1 writes back	
intents to write Write miss 
Other processor intents to write P1 reads
or writes
Read
miss
Read by any 
processor 
	If a line is in the M state then no other 
cache can have a copy of the line! 
	Memory stays coherent, multiple differing copies 
cannot exist 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L17- 16 
Arvind 
MESI: An Enhanced MSI protocol 
M: Modified Exclusive Each cache line has a tag 
E: Exclusive, unmodified
S: Shared 
I: InvalidAddress tag 
state 
bits 
P1 write	P1 read 
P1 write M	 Eor read 
Write miss 
Other processor reads Other processor
P1 writes back intent to write 
Read miss, 
shared 
S
Read by any	Other processor
intent to write
I P
1intenttowrite 
processor Cache state in 
processor P1 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L17- 5 
Arvind 
Maintaining Sequential Consistency
SC is sufficient for correct producer-consumer 
and mutual exclusion code (e.g., Dekker) 
Multiple copies of a location in various caches can cause SC to break down. 
Hardware support is required such that
 only one processor at a time has write 
permission for a location 
 no processor can load a stale copy of 
the location after a write 
 cache coherence protocols 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L17- 27 
Arvind 
next time
Designing a Cache Coherence
Protocol 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>17 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L17- 18 
Arvind 
Cache Coherence State Encoding 
tag 
= data block tag m offset V M 
Valid and dirty bits can be used 
to encode S, I, and (E, M) states indexblock Address 
V=0, D=x  Invalid Hit? word 
V=1, D=0  Shared (not dirty) 
V=1, D=1  Exclusive (dirty) 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L17- 20 
Arvind 
Intervention
cache-1 A CPU-1 CPU-2 
cache-2 
memory (stale data) A 2 0 0 
CPU-Memory bus 
1 0 0 
When a read-miss for A occurs in cache-2, 
a read request for A is placed on the bus 
 Cache-1 needs to supply &amp; change its state to shared 
 The memory may respond to the request also! 
Does memory know it has stale data? 
Cache-1 needs to intervene through memory 
controller to supply correct data to cache-2 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L17- 29 
Arvind 
2 Processor Example
Block b P1 write 
or read 
Write miss 
P2 intent to writeP1
Read
miss
M E 
S I P
1intentto
write 
P2 intent to write P1 
P2 reads, 
P1 writes back write 
Block b 
P2
Read
miss
P1 read 
M E 
S I Write miss P
2intentto
write 
P1 intent to write P2P2 
or read 
P1 reads, 
P2 writes back P2 read 
P1 intent to write write 
write 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L17- 10 
Arvind 
Snoopy Cache Goodman 1983
	Idea: Have cache watch (or snoop upon) 
DMA transfers, and then do the right 
thing 
	Snoopy cache tags are dual-ported
Proc. 
Cache Data 
(lines) Tags and A 
D R/W Used to drive Memory Bus 
A 
R/WState when Cache is Bus Master 
Snoopy read port 
attached to Memory 
Bus 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L17- 12 
Arvind 
Shared Memory Multiprocessor
Memory
Bus
M1 
M2 
M3 Snoopy 
Cache 
DMA Physical 
Memory 
Snoopy 
Cache 
Snoopy 
Cache DISKS 
Use snoopy mechanism to keep all 
processors view of memory coherent 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L17- 21 
Arvind 
False Sharing 
state data0 ... dataN blk addr  data1  
A cache block contains more than one word 
Cache-coherence is done at the block-level and 
not word-level 
Suppose M1 writes wordi and M2 writes wordk and 
both words have the same block address. 
What can happen? 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L17- 4 
Arvind 
Write-through Caches &amp; SC
X= 0
Y=10prog T1 ST X, 1 
ST Y,11 cache-1 memory cache-2 prog T2 
X = 0 Y = LD Y, R1 
Y =10 Y= ST Y, R1 
X= X = 0 LD X, R2 
Y= X= ST X,R2 
 T1 executed
 T2 executed
Y = 
Y= 
X = 0 
X= X = 1 
Y =11 
X= Y= X= 1 
Y=11 
Y = 11 
Y= 11 
X = 0 X= 0 X = 1 Y =11 
X= 0 
Y=11 X= 1 
Y=11 
Write-through caches dont preserve 
sequential consistency either 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L17- 11 
Arvind 
Snoopy Cache Actions 
Observed Bus 
Cycle Cache Action 
Address not cached 
Read Cycle Cached, unmodified 
Memory Disk Cached, modified 
Address not cached 
Write Cycle Cached, unmodified 
Disk Memory Cached, modified No action 
No action No action 
Cache intervenes 
Cache purges its copy 
??? Cache Stat e  
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>cache6.823 L17- 22 
Arvind 
Synchronization and Caches:
Performance Issues 
Processor 1 Processor 2 Processor 3
R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; 
CPU-Memory Bus mutex=1 cache cache L: swap(mutex, R); L: swap(mutex, R); L: swap(mutex, R); 
Cache-coherence protocols will cause mutex to ping-pong 
between P1s and P2s caches. 
Ping-ponging can be reduced by first reading the mutex 
location (non-atomically) and executing a swap only if it is 
found to be zero. 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L17- 7 
Arvind 
Cache Coherence Protocols for SC
write request: 
the address is invalidated (updated) in all other 
caches before (after) the write is performed 
read request:
if a dirty copy is found in some cache, a write-
back is performed before the memory is read 
We will focus on Invalidation protocols
as opposed to Update protocols
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Sequential Consistency
and
Cache Coherence Protocols
Arvind
Computer Science and Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L17- 9 
Arvind 
Problems with Parallel I/O 
Memory Disk: Physical memory may be DISK DMA Physical 
Memory 
Proc. 
Cache Memory
Bus of page 
DMA transfers Cached portions 
stale if Cache copy is dirty 
Disk Memory: Cache may have data 
corresponding to the memory 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L17- 23 
Performance Related to Bus Arvind 
occupancy 
In general, a read-modify-write instruction 
requires two memory (b us) operations without 
intervening memory operations by other 
processors
In a multiprocessor setting, bus needs to be 
locked for the entire duration of the atomic read 
and write operation
 expensive for simple buses 
 very expensive for split-transaction buses 
modern processors use 
load-reserve 
store-conditional 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>28 
Thank you !</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L17- 25 
Arvind 
Performance: 
Load-reserve &amp; Store-conditional
The total number of memory (bus) transactions 
is not necessarily reduced, but splitting an 
atomic instruction into load-reserve &amp; store-
conditional: 
 increases bus utilization (and reduces 
processor stall time), especially in split-transaction buses 
 reduces cache ping-pong effect because 
processors trying to acquire a semaphore do 
not have to perform a store each time 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L17- 26 
Arvind 
Out-of-Order Loads/Stores &amp; CC
snooper 
Wb-req, Inv-req, Inv-rep 
load/store
buffers
 pushout (Wb-rep) Memory 
CacheCPU 
(I/S/E) (S-rep, E-rep) 
(S-req, E-req) CPU/Memory Blocking caches 
One request at a time + CC  SC Interface 
Non-blocking caches 
Multiple requests (different addresses) concurrently + CC 
 Relaxed memory models 
CC ensures that all processors observe the same 
order of loads and stores to an address 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L17- 19 
Arvind 
2-Level Caches 
Snooper Snooper Snooper Snooper CPU 
L1 $ 
L2 $ CPU 
L1 $ 
L2 $ CPU 
L1 $ 
L2 $ CPU 
L1 $ 
L2 $ 
 Processors often have two-level caches
 Small L1 on chip, large L2 off chip 
 Inclusion property: entries in L1 must be in L2 
invalidation in L2  invalidation in L1 
 Snooping on L2 does not affect CPU-L1 bandwidth 
What problem could occur? 
November 9, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L17- 13 
Arvind 
Cache State Transition Diagram
The MSI protocol 
M: Modified Each cache line has a tag 
S: Shared
I: InvalidAddress tag 
state 
bits 
P1 reads 
or writes 
Other processor 
intents to write 
Read by any
processor
Cache state in 
processor P1 M 
S I P
1intents
towrite 
Other processor Other processor reads 
P1 writes back 
intents to write 
November 9, 2005 Write miss
Read
miss</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L17- 14 
Arvind 
2 Processor Example 
P1 readsP1 reads P1 or writes 
P1 writes 
Write miss P2 reads 
P2 writes P2 intent to writeP1 reads 
P1 writes Read 
missP2 writes 
P1 writes M 
S I P
1intentto
write 
P2 intent to write P2 reads, 
P1 writes back 
M 
S I Write miss 
Read 
miss 
P
2intentto
write 
P1 intent to write P1 reads, 
P2 writes back P2 reads 
or writes 
P1 intent to write P2 
November 9, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Virtual Memory: Part Deux (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l10_vrtl_mem/</lecture_pdf_url>
      <lectureno>L10</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L10-6 
Arvind 
Asynchronous Interrupts:
invoking the interrupt handler 
	An I/O device requests attention by 
asserting one of the prioritized interrupt 
request lines 
	When the processor decides to process the 
interrupt 
 It stops the current program at instruction Ii, 
completing all the instructions up to Ii-1 
(precise interrupt) 
 It saves the PC of instruction Ii in a special 
register (EPC) 
 It disables interrupts and transfers control to a 
designated interrupt handler running in the 
kernel mode 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L10-15 
Arvind 
Aliasing in Virtual-Address Caches
VA1 
VA2 Page Table 
Data Pages 
PA VA1 1st Copy of Data at PA 
VA2 2nd Copy of Data at PA 
Virtual cache can have two 
copies of same physical data.Two virtual pages share Writes to one copy not visibleone physical page to reads of other! 
General Solution: Disallow aliases to coexist in cache 
Software (i.e., OS) solution for direct-mapped cache 
VAs of shared pages must agr ee in cache index bits; this 
ensures all VAs accessing same PA will conflict in direct-
mapped cache (early SPARCs) 
Tag Data 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L10-24 
Arvind 
Page Fault Handler 
	When the referenced page is not in DRAM: 
	The missing page is located (or created) 
 It is brought in from disk, and page table is 
updated
Another job may be run on the CPU while the first 
job waits for the requested pa ge to be read from disk 
 If no free pages are left, a page is swapped out 
Pseudo-LRU replacement policy 
	Since it takes a long time to transfer a page 
(msecs), page faults are handled completely 
in software by the OS 
 Untranslated addressing mode is essential to allow 
kernel to access page tables 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L10-8 
Arvind 
Synchronous Interrupts
	A synchronous interrupt (exception) is caused 
by a particular instruction 
	In general, the instruction cannot be 
completed and needs to be restarted after the 
exception has been handled 
 requires undoing the effect of one or more  partially 
executed instructions 
	In case of a trap (system call), the instruction 
is considered to have been completed  
 a  special jump instruction involving a change to 
privileged kernel mode 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L10-16 
Arvind 
Concurrent Access to TLB &amp; Cache 
VPN L b 
TLB Direct-map Cache 
2L blocks 
2b-byte block 
PPN Page Offset 
= 
hit? Data Physical Tag Tag VA 
PA Virtual 
Index 
k 
Index L is available without consulting the TLB 
 cache and TLB accesses can begin simultaneously 
Tag comparison is made after both accesses are completed 
Cases: L + b = k L + b &lt; k L + b &gt; k 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L10-23 
Arvind 
Topics 
I n t e r r u p t s 
 Speeding up the common case: 
 TLB &amp; Cache organization 
 Speeding up page table walks 
M o d e r n  U s a g e 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L10-4 
Interrupts : Arvind 
altering the normal flow of control 
Ii-1 HI1 
interrupt 
program Ii HI2 handler 
HIn Ii+1 
An external or internal event that needs to be processed by 
another (system) program. The event is usually unexpected or 
rare from programs point of view. 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L10-13 
Arvind 
Address Translation in CPU Pipeline
PC Inst 
TLB Inst. 
Cache D Decode E M Data 
TLB Data 
Cache W + 
TLB miss? Page Fault? TLB miss? Page Fault? 
Protection violation? Protection violation? 
	Software handlers need a restartable exception on 
page fault or protection violation 
	Handling a TLB miss needs a hardware or software 
mechanism to refill TLB 
	Need mechanisms to cope with the additional latency 
of a TLB: 
 slow down the clock 
 pipeline the TLB and cache access 
 virtual address caches 
 parallel TLB/cache access 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>22 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>October 17, 20056.823 L10-25
Arvind
Hierarchical Page Table
Level 1 
Page Table
Level 2
Page Tables
Data Pagespage in primary memory 
page in secondary memoryRoot of the Current
Page Table
p1offset
p2Virtual Address
(Processor
Register)
PTE of a nonexistent pagep1 p2   offset0 1112 2122 31
10-bit
L1 index10-bit 
L2 indexA program that traverses the page table needs a no translationaddress
ing mode.</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L10-27 
Arvind 
Atlas Revisited 
	One PAR for each physical page 
PARs 
	PARs contain the VPNs of the 
pages resident in primary memory 
PPN 
	Advantage: The size is 
proportional to the size of the 
primary memory 
	What is the disadvantage ? VPN 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L10-2 
ArvindAddress Translation: 
putting it all together 
Virtual Address 
October 17, 2005 
TLB 
Lookup 
Page Table 
Walk 
Update TLBPage Fault 
(OS loads page) Protection 
Check 
Physical 
Address 
(to cache) miss hit 
page is 
 memory  memory denied permitted 
Protection 
Fault hardware 
hardware or software 
software 
SEGFAULT Restart instruction 
the</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L10-10 
Arvind 
Exception Handling 5-Stage Pipeline 
PC Inst. 
Mem D Decode E M Data 
Mem W + 
Illegal 
Opcode Overflow Data address Exceptions PC address Exception 
Asynchronous 
Interrupts Exc 
D 
PC 
D Exc 
E 
PC 
E Exc 
M 
PC 
M 
Cause EPC 
Kill D 
Stage Kill F 
Stage Kill E 
Stage Select 
PC Kill 
Writeback Commit 
Point 
Handler 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L10-3 
Arvind 
Topics 
I n t e r r u p t s 
 Speeding up the common case: 
 TLB &amp; Cache organization 
 Speeding up page table walks 
M o d e r n  U s a g e 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L10-14 
Arvind 
Virtual Address Caches 
CPU Physical 
CacheTLB Primary Memory VA PA 
Alternative: place the cache before the TLB 
CPU VA 
(StrongARM) Virtual Cache PATLB Primary Memory 
	one-step process in case of a hit (+) 
	cache needs to be flushed on a context switch unless 
address space identifiers (ASIDs) included in tags (-) 
	aliasing problems due to the sharing of pages (-) 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L10-18 
Arvind 
Concurrent Access to TLB &amp; Large L1
The problem with L1 &gt; Page size 
VPN a Page Offset b 
TLB 
PPN Page Offset b 
Tag VA 
PA Virtual Index 
L1 PA cache 
Direct-map 
= hit? PPNa Data 
PPNa Data VA1 
VA2 
Can VA1 and VA2 both map to PA ? 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Modern Virtual Memory Systems
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L10-32 
Arvind 
Virtual Memory Use Today - 1
	Desktops/servers have full demand-paged 
virtual memory 
	Portability between machines with different memory sizes 
	Protection between multiple  users or multiple tasks 
	Share small physical me mory among active tasks 
	Simplifies implementation of some OS features 
	Vector supercomputers have translation and 
protection but not demand-p aging           
(Crays: base&amp;bound, Japanese: pages) 
	Dont waste expensive CPU ti me thrashing to disk (make 
jobs fit in memory) 
	Mostly run in batch mode (run set of jobs that fits in 
memory) 
	Difficult to implement restartable vector instructions 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L10-29 
Arvind 
Global System Address Space
Global 
System 
Address 
Space Physical Memory User 
User map 
map map Level A 
Level B 
	Level A maps users address spaces into the 
global space providing privacy, protection, 
sharing etc. 
	Level B provides demand-paging for the large 
global system address space 
	Level A and Level B translations may be kept in 
separate TLBs 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L10-9 
Arvind 
Exception Handling 5-Stage Pipeline
PC Inst. 
Mem D Decode E M Data 
Mem W + 
Illegal 
Opcode Overflow Data address Exceptions PC address Exception 
Asynchronous Interrupts 
	How to handle multiple simultaneous 
exceptions in different pipeline stages? 
	How and where to handle external 
asynchronous interrupts? 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L10-28 
ArvindHashed Page Table: 
Approximating Associative Addressing 
hash Offset 
Base of Table + PA of PTE 
VPN PID PPN Page Table VPN d Virtual Address 
VPN PID DPN 
VPN PID PID 
	Hashed Page Table is typically 2 to 3 
times larger than the number of PPNs 
to reduce collision probability 
	It can also contain DPNs for some non
resident pages (not common) 
	If a translation cannot be resolved in 
this table then the software consults a 
data structure that has an entry for 
every existing page Primary
Memory
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L10-11 
Arvind 
Exception Handling 5-Stage Pipeline
	Hold exception flags in pipeline until 
commit point (M stage) 
 Exceptions in earlier pipe stages override 
later exceptions for a given instruction
	Inject external interrupts at commit 
point (override others) 
	If exception at commit: update Cause 
and EPC registers, kill all stages, inject 
handler PC into fetch stage 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L10-12 
Arvind 
Topics 
I n t e r r u p t s 
 Speeding up the common case: 
 TLB &amp; Cache organization 
 Speeding up page table walks 
M o d e r n  U s a g e 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L10-21 
ArvindVirtually-Addressed L1: 
Anti-Aliasing using L2 
VA 
PA 
used to avoid aliases in virtually-VPN Page Offset b 
TLB 
PPN Page Offset b 
Tag Virtual 
Physical L1 VA Cache 
PA VA1 Data VA1 Data 
VA2 Data 
Virtual 
Tag 
Physically-addressed L2 can also be Index &amp; Tag 
Index &amp; Tag 
addressed L1 L2 PA Cache 
L2 contains L1 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L10-26 
Arvind 
Swapping a Page of a Page Table
A PTE in primary memory contains 
primary or secondary memory addresses 
A PTE in secondary memory contains 
only secondary memory addresses 
 	a page of a PT can be swapped out only 
if none its PTEs point to pages in the 
primary memory 
Why?__________________________________ 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L10-30 
ArvindHashed Page Table Walk: 
PowerPC Two-level, Segmented Addressing
October 17, 2005 
Seg ID Page Offset 
0  51
Global Seg ID Page Offset 
0
PPN Offset 0 27hashP 
PA of Page Table + hashS 
+ 
40-bit PA per process PA 
PA 
[ IBM numbers bits                              35               63 
Hashed Segment Table 
80-bit System VA 
                                              51             67 79 
Hashed Page Table 
              39 PA of Seg Table 64-bit user VA 
system-wide 
with MSB=0 ]</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L10-31 
Arvind 
Power PC: Hashed Page Table
 
 Base of Table hash Offset + PA of Slot VPN PPN Page Table VPN d 80-bit VA 
VPN 
&lt;VPN,PPN&gt; that are searched sequentially 
function is used to look in another slot 
All these steps are done in hardware! Each hash table slot has 8 PTE's If the first hash slot fails, an alternate hash 
	Hashed Table is typically 2 to 3 times larger 
than the number of physical pages 
	The full backup Page Table is a software 
data structure Primary
Memory
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L10-20 
Arvind 
Anti-Aliasing Using L2: MIPS R10000
VA 
PA 
 

be detected in L2.
VPN a Page Offset b 
TLB 
PPN Page Offset b 
Tag Virtual Index L1 PA cache 
Direct-map 
= hit? PPNa Data 
PPNa Data VA1 
VA2 
PA a1 Data PPN into L2 tag 
and VA1 is already in L1, L2 (VA1  VA2) Suppose VA1 and VA2 both map to PA 
After VA2 is resolved to PA, a collision will 
	VA1 will be purged from L1 and L2, and Direct-Mapped L2 
VA2 will be loaded  no aliasing ! 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L10-33 
Arvind 
Virtual Memory Use Today - 2
	Most embedded processors and DSPs provide 
physical addressing only 
	Cant afford area/speed/power budget for virtual memory 
support 
	Often there is no secondary storage to swap to! 
	Programs custom written for particular memory 
configuration in product
	Difficult to implement restar table instructions for exposed 
architectures 
Given the software demands of modern embedded devices (e.g., 
cell phones, PDAs) all this ma y change in the near future! 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L10-7 
Arvind 
Interrupt Handler
	Saves EPC before enabling interrupts to 
allow nested interrupts  
	need an instruction to move EPC into GPRs 
	need a way to mask further interrupts at least until 
EPC can be saved 
	Needs to read a status register that 
indicates the cause of the interrupt 
	Uses a special indirect jump instruction 
RFE ( return-from-exception ) which 
	enables interrupts 
	restores the processor to the user mode 
	restores hardware status and control state 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L10-17 
Virtual-Index Physical-Tag Caches: Arvind 
Associative Organization 
Virtual2aVA VPN a L = k-b b Index 
Direct-map Direct-map TLB k 2L blocks 2L blocks 
Phy.
PA PPN Page Offset Tag 
= = 
Tag hit? 2a 
Data 
After the PPN is known, 2a physical tags are compared 
Is this scheme realistic? 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L10-5 
Arvind 
Causes of Interrupts 
Interrupt: an event that requests the attention of the processor 
	Asynchronous: an external event 
	input/output device service-request 
	timer expiration 
	power disruptions, hardware failure 
	Synchronous: an internal event (a.k.a 
exceptions) 
	undefined opcode, privileged instruction 
	arithmetic overflow, FPU exception 
	misaligned memory access 
 virtual memory exceptions: page faults, 
TLB misses, protection violations 
	traps: system calls, e.g., jumps into kernel 
October 17, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>34 
Thank you !</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L10-19 
Arvind 
A solution via Second Level Cache 
CPU 
L1 Data 
Cache L1 
Instruction 
Cache Unified L2 
Cache 
RF Memory Memory Memory Memory 
Usually a common L2 cache backs up both 
Instruction and Data L1 caches 
L2 is inclusive of both Instruction and Data caches 
October 17, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Multilevel Memories - Technology (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l07_caches/</lecture_pdf_url>
      <lectureno>L7</lectureno>
      <slides>
        <slide>
          <slideno>11</slideno>
          <text>Common Predictable Patterns
Two predictable properties of memory references:
 Temporal Locality : If a location is referenced it 
is likely to be referenced again in the near 
future. 
 Spatial Locality : If a location is referenced it is 
likely that locations near it will be referenced in 
the near future. 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>2-Way Set-Associative Cache
BlockTag Index Offset b 
t k 
V Tag Data Block 
t 
= V Tag Data Block 
= Data 
Word or Byte 
HIT 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L7- 4 
Joel Emer 
Semiconductor Memory, DRAM
	Semiconductor memory began to be 
competitive in early 1970s 
	Intel formed to exploit market for semiconductor 
memory 
	First commercial DRAM was Intel 1103
	1Kbit of storage on single chip 
	charge on a capacitor used to hold value 
	Semiconductor memory quickly replaced 
core in 1970s 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L7- 7 
Joel Emer 
Littles Law 
Throughput (T) = Number in Flight (N) / Latency (L)
Memory CPU Misses in 
flight table 
Example: 
--- Assume infinite bandwidth memory 
--- 100 cycles / memory reference 
--- 1 + 0.2 memory references / instruction 
 Table size = 1.2 * 100 = 120 entries 
120 independent memory operations in flight! 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Multilevel Memory
Strategy: Hide latency using small, fast 
memories called caches. 
Caches are a mechanism to hide memory 
latency based on the empirical observation 
that the patterns of memory references made by a processor are often highly 
predictable: 
PC 
 96 
loop: ADD r2, r1, r1 100 What is the pattern 
SUBI r3, r3, #1 104 of instruction 
BNEZ r3, loop 108 memory addresses? 
 112 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Inside a Cache
Address Address 
Processor CACHE Main
Memory 
Data
 Data 
copy of main
memory
location 100
copy of main memory location 101 
100 Data 
Byte Data Byte 
304 Data Byte 
6848 Line 
Address 
Tag 
Data Block 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.823 L7- 35 
Joel EmerDouble-Data Rate (DDR2) DRAM
October 3, 2005 
Figure removed for copyright reasons.
Source: Micron 256Mb DDR2 SDRAM datasheet - Bank Read Mode 
on pg. 44 of Micron Synchronous DRAM Specification.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L7- 14 
Joel Emer 
Memory Hierarchy 
Small, 
Fast 
Memory 
(RF, SRAM) CPU Big, Slow 
Memory 
(DRAM) A B 
holds frequently used data 
 size: Register &lt;&lt; SRAM &lt;&lt; DRAM    why? 
 latency: Register &lt;&lt; SRAM &lt;&lt; DRAM why? 
 bandwidth: on-chip &gt;&gt; off-chip why? 
On a data access: 
hit (data  fast memory)  low latency access 
miss (data  fast memory)  long latency access (DRAM) 
Fast mem. effective only if bandwidth requirement at B &lt;&lt; A 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L7- 8 
Joel Emer 
DRAM Architecture 
bit lines 
Col. word lines Col. 
2M 1 
N 
Row Address
Decoder 
N+M M Column Decoder &amp; 
Sense Amplifiers Row 1
Row 2N 
Memory cell 
(one bit) 
Data D 
 Bits stored in 2-dimensional arrays on chip 
 Modern chips have around 4 logical banks on each chip 
 each logical bank physically im plemented as many smaller arrays 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L7- 6 
Joel Emer 
Processor-DRAM Gap (latency) 
Proc 60%/year 
DRAM 
7%/year 
1 10 100 1000 
1980
1981
1983
19841985
1986
1987
1988
1989
1990
1991
1992
1993
1994
19951996
19971998
1999
2000 DRAM CPU 1982 Processor-Memory 
Performance Gap: (grows 50% / year)PerformanceMoores Law 
[From David Patterson, UC Berkeley ]Time 
Four-issue superscalar could execute 800 
instructions during cache miss!
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Direct Map Address Selection
higher-order vs. lower-order address bits 
TagV 
= Block 
OffsetIndex 
tk b 
t 
HIT 2k 
lines Tag 
Data Block 
Data Word or Byte 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Fully Associative Cache
October 3, 2005 
TagV 
= Block
Offset Tag 
t 
b HIT 
Data 
Word or Byte = 
= t Data Block</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L7- 30 
Write Performance Joel Emer 
Tag Data V 
= Block 
OffsetTag 
t k b 
t 
HIT 2k 
lines 
WE Index 
Data Word or Byte 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>33 
Backup</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823 L7- 34 
Joel EmerDRAM Packaging 
 
contains multiple chips with 
clock/control/address signals connected in parallel (sometimes need buffers to drive signals to all chips) 
 Address lines multiplexed 
row/column address Clock and control signals 
Data bus 
(4b,8b,16b,32b) DRAM 
chip 
~12 ~7 
DIMM (Dual Inline Memory Module) 
Data pins work together to return wide 
word (e.g., 64-bit data bus using 16x4-bit 
parts) 72-pin SO DIMM 168-pinn DIMM 
Images removed due to copyright 
restrictions. 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L7- 26 
Joel EmerReplacement Policy
In an associative cache, which block from a set 
should be evicted when the set becomes full? 
 Random 
 Least Recently Used (LRU) 
 LRU cache state must be updated on every access 
 true implementation only feasible for small sets (2-way) 
 pseudo-LRU binary tree often used for 4-8 way 
 First In, First Out (FIFO) a.k.a. Round-Robin 
 used in highly associative caches 
 Not Least Recently Used (NLRU) 
 FIFO with exception for most recently used block 
This is a second-order effect. Why? 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>18 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Multilevel Memories
Joel Emer
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Cache Algorithm (Read)
Look at Processor Address, search cache tags to find 
match. Then either 
Found in cache 
a.k.a. HIT Not in cache 
a.k.a. MISS 
Return copy of data from cache Read block of data from Main Memory 
Wait  
Return data to processor and update cache 
Q: Which line do we replace? 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Typical Memory Reference Patterns
Address 
Time Instruction 
fetches 
Stack 
accesses 
Data 
accesses n loop iterations linear sequence 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L7- 27 
Joel EmerBlock Size and Spatial Locality 
Block is unit of transfer between the cache and memory 
232-b bits b bits 
b = block size a.k.a line size (in bytes) Word3 Word0 Word1 Word2 
block address b Split CPU 
address Tag 4 word block, 
offsetb=2 
Larger block size has distinct hardware advantages 
l e s s  t a g  o v e r h e a d 
 exploit fast burst transfers from DRAM 
 exploit fast burst transfers over wide busses 
What are the disadvantages of increasing block size? 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L7- 16 
Joel Emer 
A Typical Memory Hierarchy c.2003
Split instruction &amp; data 
primary caches 
(on-chip SRAM)
Multiple interleaved 
memory banks 
(DRAM) 
L1 Data 
Cache L1 
Instruction 
Cache Unified L2 
Cache 
RF Memory Memory Memory Memory 
CPU 
Multiported Large unified secondary cache
register file (on-chip SRAM)
(part of CPU) 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L7- 29 
Joel EmerImproving Cache Performance
Average memory access time = 
Hit time + Miss rate x Miss penalty 
To improve performance: 
 reduce the miss rate (e.g., larger cache) 
 reduce the miss penalty (e.g., L2 cache) 
 reduce the hit time 
What is the simplest design strategy? 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L7- 5 
One Transistor Dynamic RAM Joel Emer 
TiN top electrode (VREF) 
1-T DRAM Cell Ta2O5 dielectric 
word 
Image removed 
due to copyright restrictions. access 
FET 
bitExplicit storage poly W bottom TiN/Ta2O5/W Capacitorcapacitor (FET word electrode 
gate, trench, line 
access fetstack) 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L7- 15 
Joel Emer 
Management of Memory Hierarchy
 Small/fast storage, e.g., registers 
 Address usually specified in instruction 
 Generally implemented directly as a register file 
 but hardware might do things behind softwares back, e.g., 
stack management, register renaming 
 Large/slower storage, e.g., memory 
 Address usually computed from values in register 
 Generally implemented as a cache hierarchy 
 hardware decides what is kept in fast memory 
 but software may provide hints, e.g., dont cache or 
prefetch 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L7- 9 
Joel Emer 
DRAM Operation 
Three steps in read/write access to a given bank 
 Row access (RAS) 
	decode row address, enable addressed row (often multiple Kb in row) 
	bitlines share charge with storage cell 
	small change in voltage detected by sense amplifiers which latch 
whole row of bits 
	sense amplifiers drive bitlines full rail to recharge storage cells 
 Column access (CAS) 
	decode column address to select small number of sense amplifier 
latches (4, 8, 16, or 32 bits depending on DRAM package) 
 on read, send latched bits out to chip pins 
	on write, change sense amplifier latches which then charge storage 
cells to required value 
	can perform multiple column acce sses on same row without another 
row access (burst mode) 
P r e c h a r g e 
	charges bit lines to known value, required before next row access 
Each step has a latency of around 20ns in modern DRAMs 
Various DRAM standards (DDR, RDRAM) ha ve different ways of encoding the 
signals for transmission to the DRAM, but all share the same core 
architecture 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>32 
Thank you !</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L7- 31 
Joel Emer 
Write Policy 
 Cache hit: 
w r i t e  t h r o u g h : write both cache &amp; memory 
 generally higher traffic but simplifies cache coherence 
w r i t e  b a c k : write cache only 
(memory is written only wh en the entry is evicted) 
 a dirty bit per block can further reduce the traffic 
 Cache miss: 
 no write allocate: only write to main memory 
 write allocate (aka fetch on write): fetch into cache 
 Common combinations: 
 write through and no write allocate 
 write back with write allocate 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L7- 21 
Joel EmerPlacement Policy 
0 1 2 3 4 5 6 7 8 9 1 1 1 1 1 1 1 1 1 1 
0 1 2 3 4 5 6 7 8 9 2 2 2 2 2 2 2 2 2 2 0 1 2 3 4 5 6 7 8 9 3 3 0 1 
Memory Block Number 
0 1 2 3 0 1 2 3 4 5 6 7 Set Number 
Cache 
Fully (2-way) Set  Direct 
Associative Associative Mapped 
block 12 anywhere anywhere in only into 
can be placed set 0 block 4 
(12 mod 4) (12 mod 8) 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Direct-Mapped Cache
TagV 
= Block 
OffsetTag 
t k b 
t 
HIT 2k 
lines Data Block Index 
Data Word or Byte 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Caches
Caches exploit both types of predictability:
 Exploit temporal locality by remembering 
the contents of recently accessed locations. 
 Exploit spatial locality by fetching blocks of 
data around recently accessed locations.
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L7- 2 
Joel Emer 
CPU-Memory Bottleneck
Memory CPU 
Performance of high-speed computers is usually 
limited by memory bandwidth &amp; latency 
 Latency (time for a single access)
Memory access time &gt;&gt; Processor cycle time
 Bandwidth (number of accesses per unit time) 
if fraction m of instructions access memory, 
1+m memory references / instruction 
 CPI = 1 requires 1+ m memory refs / cycle 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L7- 17 
Joel EmerWorkstation Memory System
(Apple PowerMac G5, 2003) 
Image removed due to copyright restrictions. 
To view image, visit 
http://www.apple.com/powermac/pciexpress.html 
 Dual 2GHz processors, each with 64KB I-
cache, 32KB D-cache, and 512KB L2 unified 
cache 
 AGP Graphics Card, 533MHz, 32-bit bus, 2.  1GB/s1GHz, 2x32-bit bus, 16GB/s 
 Up to 8GB DRAM, 400MHz, 128-bit bus, 
6.4GB/s  North Bridge Chip 
 PCI-X Expansion, 133MHz, 64-bit bus, 1 
GB/s 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L7- 3 
Joel Emer 
Core Memory
	Core memory was first large scale reliable main 
memory 
 invented by Forrester in late 40s at MIT for Whirlwind project 
	Bits stored as magnetization polarity on small ferrite 
cores threaded onto 2 dimensional grid of wires 
	Coincident current pulses on X and Y wires would write 
cell and also sense original state (destructive reads) 
	Robust, non-volatile storage 
	Used on space shuttle 
computers until recently Image removed due to 
	Cores threaded onto wires by copyright restrictions. 
hand (25 billion a year at 
peak production) 
	Core access time ~ 1 s 
DEC PDP-8/E Board,
4K words x 12 bits, (1968) 
October 3, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Average Cache Read Latency 
 is HIT RATIO: Fraction of references in cache
1 - is MISS RATIO: Remaining references
Average access time for serial search: 
Addr Addr 
Main tc+ (1 -) tmProcessor Memory
Data Data CACHE 
Average access time for parallel search: 
Addr 
Main  tc + (1 -) tm Processor Memory
Data Data CACHE 
tc is smallest for which type of cache? 
October 3, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Complex Pipelining (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l11_cmplx_pipes/</lecture_pdf_url>
      <lectureno>L11</lectureno>
      <slides>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L11-29
ArvindScoreboard Dynamics 
Functional Unit Status Registers Reserved 
Int(1) Add(1) Mult(3) Div(4) WB for Writes 
t0 I1 f6 f6 
t1 I2 f2 f6 f6, f2 
t2 f6 f2 f6, f2 I2 
t3 I3 f0 f6 f6, f0 
t4 f0 f6 f6, f0 I1 
t5 I4 f0 f8 f0, f8 
t6 f8 f0 f0, f8 I3 
t7 I5 f10 f8 f8, f10 
t8 f8 f10 f8, f10 I5 
t9 f8 f8 I4 
t10 I6 f6 f6 
t11 f6 f6 I6 
I1 DIVD f6, f6, f4 
I2 LD f2, 45(r3) 
I3 MULTD f0, f2, f4 
I4 DIVD f8, f6, f2 
I5 SUBD f10, f0, f6 
I6 ADDD f6, f8, f2 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L11-15 
Register vs. MemoryArvind 
Data Dependence 
Data hazards due to register operands can be
determined at the decode stage but
data hazards due to memory operands can be
determined only after computing the effective 
address
store M[(r1) + disp1]  (r2) 
load r3  M[(r4) + disp2]
Does (r1 + disp1) = (r4 + disp2) ? 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L11-7 
Arvind 
Complex Pipeline Structure 
IF ID WB ALU Mem 
Fadd 
Fmul 
Fdiv Issue 
GPRs 
FPRs 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L11-8 
Arvind 
Complex Pipeline Control Issues
 Structural conflicts at the write-back stage due to 
variable latencies of different function units 
 Structural conflicts at the execution stage if some 
FPU or memory unit is not pipelined and takes 
more than one cycle 
 Out-of-order write hazards due to variable 
latencies of different function units 
 How to handle exceptions? 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>October 19, 20056.823 L11-10
Arvind
Complex In-Order Pipeline
Commit 
PointPCInst. 
MemDDecode X1 X2Data 
Mem W + GPRs
X2 W Fadd X3X3
FPRs X1
X2 Fmul X3
X2 FDiv X3Unpipelined
divider Stall pipeline on long 
latency operations, e.g., 
divides, cache misses 
 Exceptions handled in 
program order at commit 
pointHow should we handle 
data hazards for very long latency operations?</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>October 19, 20056.823 L11-11
Arvind
Superscalar In-Order Pipeline
 Fetch two instructions per 
cycle; issue both 
simultaneously if one is 
integer/memory and other is floating-point
 Inexpensive way of 
increasing throughput, 
examples include Alpha 
21064 (1992) &amp; MIPS 
R5000 series (1996)
 Same idea can be extended 
to wider issue by 
duplicating functional units 
(e.g. 4-issue UltraSPARC) 
but register file ports and 
bypassing costs grow 
quicklyCommit 
Point2
PCInst. 
MemDDual
DecodeX1 X2Data 
Mem W + GPRs
X2 W Fadd X3X3
FPRs X1
X2 Fmul X3
X2 FDiv X3Unpipelined
divider</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L11-16 
Arvind 
Data Hazards: An Example
IIII1 DIVD f6, f6, f4 
2 LD 
3 MULTD f4 
4 DIVD 
If2, 
f2, 
f8, 
f10, f0, 45(r3) 
f0, 
f6, f2 
5 SUBD 
If6 
6 ADDD f6, f8, f2 
RAW Hazards 
WAR Hazards 
WAW Hazards 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L11-24 
When is it Safe to Issue an Arvind 
Instruction? 
Suppose a data structure keeps track of all the 
instructions in all the functional units 
The following checks need to be made before the 
Issue stage can dispat ch an instruction 
 Is the required function unit available? 
 Is the input data available?    RAW? 
 Is it safe to write the destination?   WAR? WAW? 
 Is there a structural conflict at the WB stage? 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L11-2 
Arvind 
Complex Pipelining: Motivation
Pipelining becomes complex when we want high 
performance in the presence of 
 Long latency or partially pipelined floating-point units 
 Multiple function and memory units 
 Memory systems with variable access time 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L11-13 
Arvind 
Types of Data Hazards 
Consider executing a sequence of 
rk  (ri) op (rj) 
type of instructions 
Data-dependence 
 
 (r1) op (r2) Read-after-Write r3 
(r3) op (r4) ( R A W )  h a z a r d r5 
Anti-dependence
r3  (r1) op (r2) 
r1  (r4) op (r5) Write-after-Read 
( W A R )  h a z a r d 
Output-dependence 
r3  (r1) op (r2) Write-after-Write 
r3  (r6) op (r7) (WAW) hazard 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L11-25 
Arvind 
A Data Structure for Correct Issues 
Keeps track of the status of Functional Units 
Name Busy Op Dest Src1 Src2 
Int 
Mem 
Add1 Add2 Add3 
Mult1 Mult2 
Div 
The instruction i at the Issue stage consults this table
FU available? check the busy column 
RAW? search the dest column for is sources 
WAR? search the source columns for is destination 
WAW? search the dest column for is destination 
An entry is added to the table if no hazard is detected; 
An entry is removed from the table after Write-Back 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L11-4 
Arvind 
Floating Point Unit
Much more hardware than an integer unit 
Single-cycle floating p oint unit is a bad idea - why? 
 it is common to have several floating point units 
 it is common to have different types of FPU's 
Fadd, Fmul, Fdiv, ... 
 an FPU may be pipelined, partially pipelined or not 
pipelined 
To operate several FPUs concurrently the register 
file needs to have more read and write ports 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L11-9 
Arvind 
Complex In-Order Pipeline
October 19, 2005 
 
operations have same 
latency to W stage 
 
oversubscribed (one inst. 
in &amp; one inst. out every 
cycle) Commit 
Point PC Inst. 
Mem D Decode X1 W + 
X2 W Fadd X3 FPRs X1 
X2 Fmul X3 
X2 FDiv X3 Unpipelined 
divider How to prevent increased 
slowing down single cycle Delay writeback so all 
Write ports never GPRs 
writeback latency from integer operations? 
Bypassing X2 Data 
X3 Mem</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>12 
Dependence Analysis</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L11-23 
Arvind 
Complex Pipeline
October 19, 2005 
IF ID WB ALU Mem 
Fadd 
Fmul 
Fdiv Issue 
GPRs 
FPRs 
Can we solve write hazards without 
equalizing all pipeline 
depths and without bypassing?</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L11-6 
Arvind 
Realistic Memory Systems 
Latency of access to the main memory is 
usually much greater than one cycle and often 
unpredictable 
Solving this problem is a central issue in computer 
architecture 
Common approaches to improving memory 
performance 
 separate instruction and data memory ports  no self-modifying code 
c a c h e s 
single cycle except in case of a miss  stall 
 interleaved memory 
multiple memory accesses  bank conflicts 
 split-phase memory operations  out-of-order responses 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L11-22 
Arvind 
IBM Memo on CDC6600 
Thomas Watson Jr., IBM CEO, August 1963: 
Last week, Control Data ... announced the 
6600 system. I understand that in the 
laboratory developing the system there are 
only 34 people including the janitor. Of 
these, 14 are engineers and 4 are 
programmers... Contrasting this modest 
effort with our vast development activities, 
I fail to understand wh y we have lost our 
industry leadership position by letting 
someone else offer the world's most 
powerful computer.
To which Cray replied: It seems like Mr. Watson 
has answered his own question. 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L11-26 
ArvindSimplifying the Data Structure 
Assuming In-order Issue 
Suppose the instruction is not dispatched by the 
Issue stage if a RAW hazard exists or the required 
FU is busy, and that operands are latched by 
functional unit on issue: 
Can the dispatched instruction cause a
WAR hazard ?
NO: Operands read at issue 
WAW hazard ? 
YES: Out-of-order completion 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L11-27 
Arvind 
Simplifying the Data Structure ...
No WAR hazard 
 no need to keep src1 and src2 
The Issue stage does not dispatch an instruction in 
case of a WAW hazard 
 a register name can occur at most once in the 
dest column 
WP[reg#] : a bit-vector to record the registers for which writes are pending 
These bits are set to true by the Issue stage and 
set to false by the WB stage 
 Each pipeline stage in the FU's must carry the 
dest field and a flag to indicate if it is valid 
the (we, ws) pair 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L11-28 
Arvind 
Scoreboard for In-order Issues
Busy[FU#] : a bit-vector to indicate FUs availability. 
(FU = Int, Add, Mult, Div) 
These bits are hardwired to FU's. 
WP[reg#] : a bit-vector to record the registers for which 
writes are pending. 
These bits are set to true by the Issue stage and set to 
false by the WB stage 
Issue checks the instruction (opcode dest src1 src2) 
against the scoreboard (Busy &amp; WP) to dispatch 
FU available? Busy[FU#] 
RAW? WP[src1] or WP[src2] 
WAR? cannot arise 
WAW? WP[dest] 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>30 
Thank you !</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>20 
Scoreboard:
A Hardware Data Structure to 
Detect Hazards Dynamically</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L11-5 
Arvind 
Function Unit Characteristics
fully
pipelined 1cyc
2 cyc 2 cycbusy1cyc
 1cyc accept 
partially
pipelined
accept busy 
Function units have internal pipeline registers
 	operands are latched when an instruction 
enters a function unit 
 	inputs to a function unit (e.g., register file) 
can change during a long latency operation 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L11-21 
Arvind 
CDC 6600 Seymour Cray , 1963 
	A fast pipelined machine with 60-bit words 
	128 Kword main memory capacity, 32 banks 
	Ten functional units (parallel, unpipelined) 
	Floating Point: adder, 2 multipliers, divider Image removed due to 
	Integer: adder, 2 incrementers, ... copyright restrictions. 	Hardwired control (no microcoding) 
	Dynamic scheduling of instructions using a 
scoreboard 
	Ten Peripheral Processors for Input/Output 
	a fast multi-thread ed 12-bit integer ALU 
 Very fast clock, 10 MHz (FP add in 4 clocks) 
Image removed due to  &gt;400,000 transistors,  750 sq. ft., 5 tons, 
copyright restrictions.	150 kW, novel freon-ba sed  technology for 
cooling 
	Fastest machine in world for 5 years (until 
7600) 
	over 100 sold ($7-10M each) 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L11-3 
Arvind 
Floating Point ISA
Interaction between the Floating point datapath 
and the Integer datapath is determined largely 
by the ISA 
MIPS ISA 
 separate register files for FP and Integer instructions 
the only interaction is via a set of move 
instructions (some ISAs dont even permit this) 
 separate load/store for FPRs and GPRs but both 
use GPRs for address calculation 
 separate conditions for branches 
FP branches are defined in terms of condition codes 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Complex Pipelining
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L11-17 
Arvind 
Instruction Scheduling
DIVD f6, f6, 
f2, 45(r3) 
f2, 
f8, 
f10, f0, 
f6, f0, 
f6, 
f8, f4 I1 
LD I2 
MULTD f4 I3 
DIVD f2 I4 
I5 SUBD f6 
ADDD f2 I6 
Valid orderings: 
in-order I1 I2 I3 I4 I5 I6 
out-of-order I2 I1 I3 I4 I5 I6 
out-of-order I1 I2 I3 I5 I4 I6 
I6 I2 
I4 I1 
I5 I3 
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L11-14 
Arvind 
Detecting Data Hazards
Range and Domain of instruction i
R(i) = Registers (or other storage) modified by 
instruction i 
D(i) = Registers (or other storage) read by 
instruction i 
Suppose instruction j follows instruction i in the 
program order. Executing instruction j before the 
effect of instruction i has taken place can cause a 
RAW hazard if R(i)  D(j)   
WAR hazard if D(i)  R(j)   
WAW hazard if R(i)  R(j)   
October 19, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L11-18 
Out-of-order CompletionArvind 
In-order Issue 
Latency 
I1 DIVD f6, f6, f4 4 
I2 LD f2, 45(r3) 1 
I3 MULTD f0, f2, f4 3 
I4 DIVD f8, f6, f2 4 
I5 SUBD f10, f0, f6 1 
I6 ADDD f6, f8, f2 1 
in-order comp 1 2 1 2 3 4 3 5 4 6 5 6 
out-of-order comp 1 2 2 3 1 4 3 5 5 4 6 6 
October 19, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Synchronization and Sequential Consistency (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l16_smps_sc/</lecture_pdf_url>
      <lectureno>L16</lectureno>
      <slides>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L16- 13 
Arvind 
Nonblocking Synchronization
Compare&amp;Swap(m,Rt,Rs):
if (Rt==M[m])
status is an 
then M[m]=Rs; implicit
Rs=Rt ; argument 
status  success; 
else	 status  fail; 
try: 	 Load(Rhead, head) 
spin:	 Load(Rtail, tail) 
if Rhead==Rtail goto spin 
Load(R, Rhead) 
Rnewhead = Rhead+1 
Compare&amp;Swap(head, Rhead, Rnewhead ) 
if (status==fail) goto try 
process(R) 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L16- 9 
Arvind 
Multiple Consumer Example
Producer posting Item x: 
Load(Rtail, tail) 
Store(Rtail, x) 
Rtail=Rtail+1 
Store(tail, Rtail) Consumer: 
Load(Rhead, head) 
spin: tail, tail) 
if Rhead==Rtail goto spin 
Load(R, Rhead) 
Rhead=Rhead+1 
Store(head, Rhead) 
process(R) 
What is wrong with this code? Critical section: 
Needs to be executed atomically 
by one consumer locks tail head 
Producer 
RConsumer 
1 R Rhead 
Rtail 
Consumer 
2 R Rhead 
Rtail 
November 7, 2005 Load(Rtail</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L16- 22 
Arvind 
Mutual Exclusion Using Load/Store 
A protocol based on two shared variables c1 and c2. 
Initially, both c1 and c2 are 0 (not busy)
Process 1	 Process 2 
... ...
c1=1; c2=1;
L: if c2=1 then go to L 
&lt; critical section&gt;
c1=0;
L: 	if c1=1 then go to L 
&lt; critical section&gt; 
c2=0; 
What is wrong?	 Deadlock! 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L16- 7 
Arvind 
Sequential Consistency
Sequential concurrent tasks: T1, T2 
Shared variables: X, Y (initially X = 0, Y = 10) 
T1: T2: 
Store(X, 1) (X = 1) Load(R1, Y) 
Store(Y, 11) (Y = 11) Store(Y, R1) (Y= Y) 
Load(R2, X) 
Store(X, R2) (X= X) 
what are the legitimate answers for X and Y ? 
(X,Y)  {(1,11), (0,10), (1,10), (0,11)} ? 
If y is 11 then x cannot be 0 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L16- 17 
Arvind 
Memory Fences
Instructions to sequentialize memory accesses
Processors with relaxed or weak memory models , i.e., 
permit Loads and Stores to different addresses to be 
reordered need to provide memory fence instructions 
to force the serialization of memory accesses 
Examples of processors with relaxed memory models: 
Sparc V8 (TSO,PSO): Membar 
Sparc V9 (RMO): 
Membar #LoadLoad, Membar #LoadStore 
Membar #StoreLoad, Membar #StoreStore 
PowerPC (WO): Sync, EIEIO 
Memory fences are expensive operations, however, one pays the cost of serialization only when it is required 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L16- 3 
Arvind 
Synchronization
The need for synchronization arises whenever 
there are parallel processes in a system 
(even in a uniprocessor system) 
Forks and Joins: In parallel programming 
a parallel process may want to wait until several events have occurred 
Producer-Consumer: A consumer process 
must wait until the producer process has produced data 
Exclusive use of a resource: Operating 
system has to ensure that only one process uses a resource at a given time 
producer 
consumer fork 
join P1 P2 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L16- 5 
Arvind 
A Producer-Consumer Example
continued 
RProducer posting Item x: Consumer: 
Load(Rtail, tail) Load(Rhead, head) 
1 Store(Rtail, x) spin: Load(Rtail, tail) 3 
tail=Rtail+1 if Rhead==Rtail goto spin 
2 Store(tail, Rtail) Load(R, Rhead) 4 
Rhead=Rhead+1
Can the tail pointer get updated Store(head, Rhead)
before the item x is stored? process(R)
Programmer assumes that if 3 happens after 2, then 4
happens after 1.
Problem sequences are: 
2, 3, 4, 1
4, 1, 2, 3
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Symmetric Multiprocessors: 
Synchronization 
and 
Sequential Consistency 
Arvind
Computer Science and Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L16- 25 
Arvind 
Analysis of Dekkers Algorithm
Scenario 2 Scenario 1
... Process 1 
c1=1;
turn = 1;
L: if c2=1 &amp; turn=1 
then go to L 
&lt; critical section&gt; 
c1=0; 
... Process 1 
c1=1;
turn = 1;
L: if c2=1 &amp; turn=1 
then go to L 
&lt; critical section&gt; 
c1=0; ... Process 2 
c2=1; 
turn = 2; 
L: if c1=1 &amp; turn=2 
then go to L 
&lt; critical section&gt; 
c2=0; 
... Process 2 
c2=1;
turn = 2;
L: if c1=1 &amp; turn=2 
then go to L 
&lt; critical section&gt; 
c2=0; 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L16- 18 
Arvind 
Using Memory Fences
Producer Consumer tail head 
R Rtail Rhead Rtail 
Producer posting Item x: Consumer:
Load(Rtail, tail) Load(Rhead, head)
Store(Rtail, x) spin: Load(Rtail, tail) 
MembarSS MemberLLif Rhead==Rtail goto spin 
RRtail=Rtail+1
Store(tail, Rtail) Load(R, Rhead)
head=Rhead+1
ensures that tail ptr ensures that R is Store(head, Rhead)
is not updated before not loaded before process(R)
x has been stored
x has been stored 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L16- 2 
Arvind 
Symmetric Multiprocessors
symmetric 
away from all processors 
(set up a DMA transfer) Memory 
I/O controller 
Graphics 
output CPU-Memory bus 
bridge Processor 
I/O controller I/O bus 
Networks Processor 
November 7, 2005  All memory is equally far 
 Any processor can do any I/O I/O controller</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L16- 20 
Fences in Data-Race Free Arvind 
Programs 
Process 1 Process 2 
... ... 
Acquire(mutex); Acquire(mutex); 
membar; membar; 
&lt; critical section&gt; &lt; critical section&gt; 
membar; membar; 
Release(mutex); Release(mutex); 
 Relaxed memory model allows reordering of instructions 
by the compiler or the processor as long as the reordering 
is not done across a fence 
 The processor also should not speculate or prefetch 
across fences 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L16- 14 
Arvind 
Load-reserve &amp; Store-conditional
Special register(s) to hold reservation flag and address, 
and the outcome of store-conditional 
Load-reserve(R, m): Store-conditional(m, R): 
&lt;flag, adr&gt;  &lt;1, m&gt;; if &lt;flag, adr&gt; == &lt;1, m&gt; 
R  M[m]; then cancel other procs 
reservation on m; 
M[m]  R; 
status  succeed; 
else status  fail; 
try: Load-reserve(Rhead, head) 
spin: Load (Rtail, tail) 
if Rhead==Rtail goto spin 
Load(R, Rhead) 
Rhead = Rhead + 1 
Store-conditional(head, Rhead) 
if (status==fail) goto try 
November 7, 2005 process(R)</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L16- 26 
Arvind 
N-process Mutual Exclusion
Lamports Bakery Algorithm 
Process i Initially num[j] = 0, for all j 
Entry Code 
choosing[i] = 1;
num[i] = max(num[0], , num[N-1]) + 1;
choosing[i] = 0;
for(j = 0; j &lt; N; j++) {
while( choosing[j] );
while( num[j] &amp;&amp;
( ( num[j] &lt; num[i] ) || 
( num[j] == num[i] &amp;&amp; j &lt; i ) ) ); 
} 
Exit Code 
num[i] = 0; 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L16- 8 
Arvind 
Sequential Consistency
Sequential consistency imposes more memory ordering 
constraints than those imposed by uniprocessor program dependencies ( 
) 
What are these in our example ? 
T1: T2: 
Store(X, 1) (X = 1) Load(R1, Y) 
Store(Y, 11) (Y = 11)
 Store(Y, R1) (Y= Y)
Load(R2, X) 
Store(X, R2) (X= X)
 additional SC requirements 
Does (can) a system with caches or out-of-order execution capability provide a sequentially consistent 
view of the memory ? 
more on this later 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L16- 19 
Data-Race Free Programs Arvind 
a.k.a. Properly Synchronized Programs 
Process 1 Process 2 
... ... 
Acquire(mutex); Acquire(mutex); 
&lt; critical section&gt; &lt; critical section&gt; 
Release(mutex); Release(mutex); 
Synchronization variables (e.g. mutex) are disjoint 
from data variables 
Accesses to writable shared data variables are 
protected in critical regions 
 no data races except for locks 
(Formal definition is elusive) 
In general, it cannot be proven if a program is data-race 
free. 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L16- 24 
Arvind 
A Protocol for Mutual Exclusion
T. Dekker, 1966
A protocol based on 3 shared variables c1, c2 and turn. 
Initially, both c1 and c2 are 0 (not busy) 
Process 1 Process 2 
... ... 
c1=1; c2=1; 
turn = 1; turn = 2; 
L: if c2=1 &amp; turn=1 L: if c1=1 &amp; turn=2 
then go to L then go to L 
&lt; critical section&gt; &lt; critical section&gt; 
c1=0; c2=0; 
 turn = i ensures that only process i can wait 
 variables c1 and c2 ensure mutual exclusion 
Solution for n processes was given by Dijkstra 
and is quite tricky! 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>21 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L16- 10 
Arvind 
Locks or Semaphores
E. W. Dijkstra, 1965
A semaphore is a non-negative integer, with the
following operations:
P(s): if s&gt;0 decrement s by 1 otherwise wait 
V(s): increment s by 1 and wake up one of 
the waiting processes 
Ps and Vs must be executed atomically, i.e., without 
 interruptions or 
 interleaved accesses to s by other processors 
Process i initial value of s determinesP(s) the maximum no. of processes&lt;critical section&gt; in the critical sectionV(s) 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L16- 11 
Arvind 
Implementation of Semaphores
Semaphores (mutual exclusion) can be implemented 
using ordinary Load and Store instructions in the Sequential Consistency memory model. However, 
protocols for mutual exclusion are difficult to design... 
Simpler solution: 
atomic read-modify-write instructions 
Examples: m is a memory location, R is a register 
Test&amp;Set(m, R): Fetch&amp;Add(m, RV, R): Swap(m,R): 
R  M[m]; R  M[m]; Rt  M[m]; 
if R==0 then M[m]  R + RV; M[m]  R; 
M[m]  1; R  Rt; 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L16- 4 
Arvind 
A Producer-Consumer Example
Producer Consumer tail head 
R Rtail Rhead Rtail 
Producer posting Item x: Consumer:
Load(Rtail, tail) Load(Rhead, head)
RStore(Rtail, x) spin: Load(Rtail, tail) 
tail=Rtail+1 if Rhead==Rtail goto spin 
RStore(tail, Rtail) Load(R, Rhead) 
head=Rhead+1 
Store(head, Rhead) 
process(R)
The program is written assuming 
instructions are executed in order. Problems?
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L16- 23 
Arvind 
Mutual Exclusion: second attempt 
To avoid deadlock, let a process give up the reservation 
(i.e. Process 1 sets c1 to 0) while waiting.
Process 1 Process 2 
... ... 
L: c1=1; L: c2=1; 
if c2=1 then if c1=1 then 
{ c1=0; go to L} { c2=0; go to L} 
&lt; critical section&gt; &lt; critical section&gt; 
c1=0 c2=0 
 Deadlock is not possible but with a low probability 
a livelock may occur. 
 An unlucky process may never get to enter the 
critical section  starvation 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L16- 6 
Arvind 
Sequential Consistency
A Memory Model 
M P P P P P P 
 A system is sequentially consistent if the result of 
any execution is the same as if the operations of all 
the processors were executed in some sequential 
order, and the operations of each individual processor 
appear in the order specified by the program 
Leslie Lamport 
Sequential Consistency = 
arbitrary order-preserving interleaving 
of memory references of sequential programs 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L16- 15 
Arvind 
Performance of Locks 
Blocking atomic read-modify-write instructions 
e.g., Test&amp;Set, Fetch&amp;Add, Swap 
vs 
Non-blocking atomic read-modify-write instructions 
e.g., Compare&amp;Swap, 
Load-reserve/Store-conditional 
vs 
Protocols based on ordinary Loads and Stores 
Performance depends on several interacting factors: 
degree of contention, 
caches, 
out-of-order execution of Loads and Stores 
later ... 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L16- 16 
Issues in Implementing Arvind 
Sequential Consistency 
M P P P P P P 
Implementation of SC is complicated by two issues 
 Our-of-order execution capability 
Load(a); Load(b) yes 
Load(a); Store(b) yes if a  b 
Store(a); Load(b) yes if a  b 
Store(a); Store(b) yes if a  b 
 Caches 
Caches can prevent the effect of a store from 
being seen by other processors 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>28 
Thank you !</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L16- 12 
Arvind 
Multiple Consumers Example
using the Test&amp;Set Instruction 
Critical 
Section P: Test&amp;Set(mutex,R ) 
if (Rtemp!=0) goto P 
Load(Rhead, head) 
spin: tail, tail) 
if Rhead==Rtail 
Load(R, Rhead) 
Rhead=Rhead+1 
Store(head, Rhead) temp
Load(R
goto spin 
V: 	 Store(mutex,0)
process(R)
Other atomic read-modify-write instructions (Swap, 
Fetch&amp;Add, etc.) can also implement Ps and Vs 
What if the process stops or is swapped out while 
in the critical section? 
November 7, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L16- 27 
Arvind 
next time
Effect of caches on 
Sequential Consistency 
November 7, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Cache (Memory) Performance Optimization (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l08_caches_2/</lecture_pdf_url>
      <lectureno>L8</lectureno>
      <slides>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L8- 30 
Joel Emer 
Software Prefetching Issues 
 Timing is the biggest issue, not predictability 
 If you prefetch very close to when the data is 
required, you might be too late 
 Prefetch too early, cause pollution 
 Estimate how long it will take for the data to come 
into L1, so we can set P appropriately 
 Why is this hard to do? 
for(i=0; i &lt; N; i++) {
prefetch( &amp;a[i +
prefetch( &amp;b[i + P
P] );
] );
} SUM = SUM + a[i] * b[i]; 
Must consider cost of prefetch instructions 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L8- 22 
Joel Emer 
Inclusion Policy 
	Inclusive multilevel cache: 
	Inner cache holds copies of data in outer cache 
	Extra-CPU access needs only check outer cache 
	Most common case 
	Exclusive multilevel caches: 
	Inner cache may hold data not in outer cache 
	Swap lines between inner/outer caches on miss 
	Used in Athlon with 64KB primary and 256KB 
secondary cache 
Why choose one type of the other? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L8- 33 
Joel Emer 
Loop Fusion
for(i=0; i &lt; N; i++)
for(j=0; j &lt; M; j++)
a[i][j] = b[i][j] * c[i][j]; 
for(i=0; i &lt; N; i++)
for(j=0; j &lt; M; j++)
d[i][j] = a[i][j] * c[i][j]; 
for(i=0; i &lt; M; i++)
for(j=0; j &lt; N; j++) {
a[i][j] = b[i][j] * c[i][j];
d[i][j] = a[i][j] * c[i][j];
} 
What type of locality does this improve? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L8- 6 
Joel Emer 
Write pipeline 
Instr RFMemory ALU Data 
Memory 
Data 
Memory 
I-Fetch Decode Address Tag Mem 
Reg Read Calc Read Data 
Write 
What hazard has been intr oduced in this pipeline? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L8- 10 
Joel Emer 
Improving Cache Performance
Average memory access time = 
Hit time + Miss rate x Miss penalty 
To improve performance: 
 reduce the miss rate (e.g., larger cache) 
 reduce the miss penalty (e.g., L2 cache) 
 reduce the hit time 
The simplest design strategy is to design the 
largest primary cache without slowing down the 
clock or adding pipeline stages 
(but design decisions are more complex with out-of-
order or highly pipelined CPUs) 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L8- 29 
Joel Emer 
Software Prefetching
for(i=0; i &lt; N; i++) {
prefetch( &amp;a[i + 1] );
prefetch( &amp;b[i + 1] );
SUM = SUM + a[i] * b[i];
}
	What property do we require of the cache 
for prefetching to work ? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Cache Optimizations
Joel Emer
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L8- 21 
Joel Emer 
Multilevel Caches 
 A memory cannot be large and fast
 Increasing sizes of cache at each level 
CPU L1 L2 DRAM 
Local miss rate = misses in cache / accesses to cache 
Global miss rate = misses in cache / CPU memory accesses 
Misses per instruction = misses in cache / number of instructions 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L8- 9 
Joel Emer 
Improving Cache Performance
Average memory access time = 
Hit time + Miss rate x Miss penalty 
To improve performance: 
 reduce the miss rate (e.g., larger cache) 
 reduce the miss penalty (e.g., L2 cache) 
 reduce the hit time 
What is the simplest design strategy? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L8- 15 
Joel Emer 
Set-Associative RAM-Tag Cache 
=? =? Status 
Index Offset Tag  
Tag  Data	 Tag  Status Data 
Not energy-efficient 
	A tag and data word 
is read from every 
way 
Two-phase approach 
	First read tags, then 
just read data from
selected way 
	More energy-
efficient 
	Doubles latency in 
L1 
	OK, for L2 and 
above, why? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>37 
Extras</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>wdataY6.823 L8- 2 
Joel EmerCPU-Cache Interaction 
(5-stage pipeline) 
0x4 
EAdd 
M 
A we 
ALU Y addr 
IR Decode,nop Primary B Register Data rdata 
Fetch Cache R addr inst PC D hit?wdata
PCen Primary
Instruction MD1 MD2
Cache
hit? 
Stall entire 
CPU on data 
cache miss 
To Memory Control 
Cache Refill Data from Lower Levels of 
Memory Hierarchy 
What about Instruction miss or writes to i-stream ? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823 L8- 34 
Joel Emer 
Blocking 
for(i=0; i &lt; N; i++)
for(j=0; j &lt; N; j++) {
r = 0;
for(k=0; k &lt; N; k++)
r = r + y[i][k] * z[k][j];
} x[i][j] = r; 
x y z j k j 
i i k 
Not touched Old access New access 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L8- 26 
Joel Emer 
Issues in Prefetching 
 Usefulness  should produce hits 
 Timeliness  not late and not too early 
 Cache and bandwidth pollution 
L1 Data L1 
Instruction 
Unified L2 
Cache 
RF CPU 
Prefetched data 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L8- 17 
Way Predicting Caches	Joel Emer 
(MIPS R10000 L2 cache ) 
 Use processor address to index into way prediction table 
 Look in predicted way at given index, then: 
MISS HIT 
Return copy Look in other way 
of data from 
cache 
MISSSLOW HIT 
(change entry in 
prediction table)	 Read block of data from 
next level of cache 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L8- 13 
Joel Emer 
Block Size and Spatial Locality 
Block is unit of transfer between the cache and memory 
232-b bits b bits 
b = block size a.k.a line size (in bytes) Word3 Word0 Word1 Word2 
block address b Split CPU 
address Tag 4 word block, 
offsetb=2 
Larger block size has distinct hardware advantages 
l e s s  t a g  o v e r h e a d 
 exploit fast burst transfers from DRAM 
 exploit fast burst transfers over wide busses 
What are the disadvantages of increasing block size? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L8- 24 
Joel Emer 
Reducing Read Miss Penalty
Data 
Cache Unified 
L2 
Cache 
RF CPU 
Write 
buffer 
Evicted dirty lines for writeback cache 
OR 
All writes in writethru cache 
	Write buffer may hold updated value of location 
needed by a read miss 
	Simple scheme: on a read miss, wait for the write 
buffer to go empty 
	Faster scheme: Check write buffer addresses 
against read miss addresses, if no match, allow 
read miss to go ahead of writes, else, return value 
in write bufferOctober 5, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L8- 31 
Joel Emer 
Compiler Optimizations
 Restructuring code affects the data block 
access sequence 
 Group data accesses together to improve spatial locality 
 Re-order data accesses to improve temporal locality 
 Prevent data from entering the cache 
 Useful for variables that will only be accessed once 
before being replaced 
 Needs mechanism for software to tell hardware not to 
cache data (instruction hints or page table bits) 
 Kill data that will never be used again
 Streaming data exploits spat ial locality but not temporal 
locality 
 Replace into dead cache locations 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L8- 3 
Joel Emer 
Write Performance 
Tag Data V 
= Block 
OffsetTag 
t k b 
t 
HIT 2k 
lines 
WE Index 
Data Word or Byte 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>36 
Thank you !</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L8- 4 
Joel Emer 
Reducing Write Hit Time
Problem: Writes take two cycles in memory 
stage, one cycle for tag check plus one cycle 
for data write if hit 
Solutions:
	Design data RAM that can perform read and write in one 
cycle, restore old value after tag miss 
	CAM-Tag caches: Word line only enabled if hit 
	Pipelined writes: Hold write data for store in single 
buffer ahead of cache, write cache data during next 
stores tag check 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>6.823 L8- 38 
Joel Emer 
Memory Hierarchy Example	AlphaStation 600/5 desktop workstation 
	Alpha 21164 @ 333 MHz 
	On-chip L1 and L2 caches 
	L1 instruction cache, 8KB direct-mapped, 32B lines, fetch 
four instructions/cycle (16B) 
	Instruction stream prefetches up to 4 cache lines ahead 
	L1 data cache, 8KB direct -mapped, 32B lines, write-
through, load two 8B words or  store one 8B word/cycle (2 
cycle latency) 
	up to 21 outstanding loads,  6x32B lines of outstanding 
writes 
	L2 unified cache, 96KB 3-way set-associative, 64B 
blocks/32B sub-blocks, write-back, 16B/cycle bandwidth 
(7 cycle latency) 
	Off-chip L3 unified cache, 8M B direct-mapped, 64B blocks, 
peak bandwidth is 16B every 7 cycles (15 cycle latency) 
	DRAM, peak bandwidth 16B every 10 cycles (60 cycle 
latency) 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L8- 7 
Joel Emer 
Write Policy 
 Cache hit: 
w r i t e  t h r o u g h : write both cache &amp; memory 
 generally higher traffic but simplifies cache coherence 
w r i t e  b a c k : write cache only 
(memory is written only wh en the entry is evicted) 
 a dirty bit per block can further reduce the traffic 
 Cache miss: 
 no write allocate: only write to main memory 
 write allocate (aka fetch on write): fetch into cache 
 Common combinations: 
 write through and no write allocate 
 write back with write allocate 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L8- 12 
Joel Emer 
Effect of Cache Parameters on Performance
 Larger cache size 
+ reduces capacity and conflict misses  
- hit time will increase
 Higher associativity
+ reduces conflict misses (up to around 4-8 way) 
- may increase access time 
 Larger block size 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L8- 25 
Joel Emer 
Prefetching
 Speculate on future instruction and 
data accesses and fetch them into 
cache(s) 
 Instruction accesses easier to predict 
than data accesses 
 Varieties of prefetching
 Hardware prefetching
 Software prefetching
M i x e d  s c h e m e s
 What types of misses does
prefetching affect? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.823 L8- 35 
Joel Emer 
Blocking
for(jj=0; jj &lt; N; jj=jj+B)
for(kk=0; kk &lt; N; kk=kk+B)
for(i=0; i &lt; N; i++)
for(j=jj; j &lt; min(jj+B,N); j++) {
r = 0;
for(k=kk; k &lt; min(kk+B,N); k++)
r = r + y[i][k] * z[k][j];
x[i][j] = x[i][j] + r; 
x j } y k z j 
i i k 
October 5, 2005 What type of locality does this improve?</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L8- 23 
Joel EmerItanium-2 On-Chip Caches
(Intel/HP, 2002) 
Image removed due to copyright restrictions. 
To view image, visit http://www-
vlsi.stanford.edu/group/chips_micropro_body 
.html 
October 5, 2005 Level 1, 16KB, 4-way s.a., 
64B line, quad-port (2 
load+2 store), single cycle 
latency 
Level 2, 256KB, 4-way s.a, 
128B line, quad-port (4 load or 4 store), five cycle 
latency 
Level 3, 3MB, 12-way s.a., 
128B line, single 32B port, 
twelve cycle latency</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L8- 11 
Joel Emer 
Causes for Cache Misses
 Compulsory: first-reference to a block a.k.a. cold 
start misses 
-misses that would occur even with infinite cache 
 Capacity: cache is too small to hold all data needed 
by the program 
- misses that would occur even under perfect 
placement &amp; replacement policy 
 Conflict: misses that occur because of collisions 
due to block-placement strategy 
- misses that would not occur with full associativity 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L8- 20 
Joel Emer 
Victim Caches (HP 7200)
L1 Data 
Cache Unified L2 
Cache 
RF CPU 
Victim 
FA Cache 
4 blocks Evicted data 
from L1 
Evicted data 
From VC Hit data from VC 
(miss in L1) where ? 
Victim cache is a small associative ba ck up cache, added to a direct 
mapped cache, which holds recently evicted lines 
 First look up in direct mapped cache 
 If miss, look in victim cache 
 If hit in victim cache, swap hit line with line now evicted from L1 
 If miss in victim cache, L1 victim -&gt; VC, VC victim-&gt;? 
Fast hit time of direct mapped but with reduced conflict misses 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>6.823 L8- 39 
Joel Emer 
Further Issues
There are several other factors that are intimately 
connected with cache design: 
 Virtual memory and associated address 
translation 
 Multiprocessor and associated memory
model issues - cache coherence
stay tuned 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Average Cache Read Latency 
 is HIT RATIO: Fraction of references in cache
1 - is MISS RATIO: Remaining references
Average access time for serial search: 
Addr Addr 
Main tc+ (1 -) tmProcessor Memory
Data Data CACHE 
Average access time for parallel search: 
Addr 
Main  tc + (1 -) tm Processor Memory
Data Data CACHE 
tc is smallest for which type of cache? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L8- 5 
Pipelining Cache Writes Joel Emer 
Address and Store Data From CPU 
Tag Index Store Data 
Delayed Write Addr. Delayed Write Data
Load/Store 
=? 
S 
Tags L Data 
1 0 =? 
Load Data to CPUHit? 
Data from a store hit written into data portion of cache 
during tag access of subsequent store 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L8- 18 
Joel EmerWay Predicting Instruction Cache
(Alpha 21264-like) 
PC addr inst 
Instruction 
Cache 0x4 
Add 
Sequential Way way Jump 
control 
Primary 
Branch Target Way Jump target 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L8- 32 
Joel Emer 
Loop Interchange
for(j=0; j &lt; N; j++) {
for(i=0; i &lt; M; i++) {
x[i][j] = 2 * x[i][j];
}
} 
for(i=0; i &lt; M; i++) {
for(j=0; j &lt; N; j++) {
} } x[i][j] = 2 * x[i][j];
What type of locality does this improve? 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L8- 28 
Joel Emer 
Hardware Data Prefetching 
 Prefetch-on-miss: 
P r e f e t c h b + 1 upon miss on b 
 One Block Lookahead (OBL) scheme 
 Initiate prefetch for block b + 1 when 
block b is accessed 
 Why is this different from doubling block
size? 
 Can extend to N block lookahead 
 Strided prefetch 
 If sequence of accesses to block b, b+N, 
b+2N , then prefetch b+3N etc. 
October 5, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L8- 14 
Joel Emer 
Block-level Optimizations 
 Tags are too large, i.e., too much overhead 
 Simple solution: Larger blocks, but miss penalty 
could be large. 
 Sub-block placement (aka sector cache)
 A valid bit added to units smaller than the full block, 
called sub-blocks 
 Only read a sub-block on a miss 
 If a tag matches, is the word in the cache? 
October 5, 2005 
100 
300 
204 1 1 1 1 
1 1  0 0 0 1 0 1</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Tag =? Data Block
Tag =? Data Block
Tag =? Data BlockTag =? Data Block
Tag =? Data Block
Tag =? Data Block6.823 L8- 16 
Joel Emer 
Highly-Associative CAM-Tag Caches
	For high associativity (e.g., 32-way), use content-addressable 
memory (CAM) for tags (Intel XScale) 
	Overhead: Tag+comparator bit 2-4x area of plain RAM-tag bit 
October 5, 2005 
tagt seti offsetb 
Tag =? Data Block 
Tag =? Data Block 
Tag =? Data Block Set 0 Set 1 Set i 
Hit? Data Only one set enabled 
Only hit data accessed  saves energy</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L8- 27 
Joel Emer 
Hardware Instruction Prefetching
 Instruction prefetch in Alpha AXP 21064 
 Fetch two blocks on a miss; the requested block and 
the next consecutive block 
 Requested block placed in cache, and next block in 
instruction stream buffer 
October 5, 2005 
L1 
Instruction Unified L2 
Cache 
RF CPU Stream 
Buffer 
(Prefetched 
instruction block 
4 blocks) Req 
block 
Req 
block</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Cache Coherence (Implementation) (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l18_ccprotocols/</lecture_pdf_url>
      <lectureno>L18</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L18- 6 
Arvind 
Cache State Implications
Sh  caches siblings and decedents can only 
have Sh copies 
Ex  each ancestor of the cache must be in Ex
 either all children can have Sh copies 
or one child can have an Ex copy
 Once a parent gives an Ex copy to a child, the 
parents data is considered stale 
 A processor cannot overwrite data in Sh state 
in L1 
 By definition all addresses in the home are in 
the Ex state 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>6.823 L18- 40 
Arvind 
Protocol Diagram 
... Cache 1 Cache 2 Cache N 
Main Memory Dir a: Ex {N} Pen: a 
ShReq 
a Ex: a 
Dir a: Ex {N}WBReq 
a 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>33 
Thank you !
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>6.823 L18- 37 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Sh: a 
Dir a: Sh {1,2} ShResp 
&lt;a,v&gt; Pen: a 
Sh: a Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>6.823 L18- 39 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Sh: a 
Dir a: Sh {1,2} Inv 
a Sh: a 
Inv 
a 
Dir a: Sh {} &lt;a,v&gt; 
Dir a: Ex {N} Pen: a 
Ex: a Cache 2 
ExResp 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>20 
Five-minute break to stretch your legs
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L18- 16 
Arvind 
H and L Priority Messages
	At the memory unprocessed requests cannot 
block the result messages. Hence all 
messages are classified as H or L priority. 
	all messages carrying results are classified as high 
priority
	Accomplished by having separate paths for H 
and L priority 
 In Theory: separate networks
I n  P r a c t i c e :
	Separate Queues H 
L 
 Shared buses for both networks 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L18- 30 
Arvind 
Processing Reply WbRep Messages 
(at home) 
WbRep 
Msg(id,Home,WbRep,a,v) == mmsg 
-- m.state(a) must be TW(id) or W(id)
  	 deq mmsg; 
m.setState(a, R(id))
m.setData(a,v)
FlushRep 
Msg(id,Home,FlushRep,a,v) == mmsg 
-- m.state(a) must be TW(id) or W(id)
  	 deq mmsg;
m.setState(a, R(Empty))
m.setData(a,v)
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L18- 2 
Arvind 
Systems view 
Blocking caches Cache Memory pushout load/store 
buffers 
CPU 
(ShReq, ExReq) (ShRep, ExRep) (WbReq, InvReq, InvRep) snooper 
(I/Sh/Ex) 
CPU/Memory (WbRep) 
In order, one request at a time + CC  SC Interface 
Non-blocking caches 
Multiple requests (different addresses) concurrently + CC 
 Relaxed memory models 
CC ensures that all processors observe the same 
order of loads and stores to an address 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L18- 8 
Arvind 
High-level Invariants in Protocol
Design 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L18- 11 
Arvind 
Caching Rules: Parent to Child 
Child c idc 
Parent m idp 
 Read caching rule 
R(dir) == m.state(a) &amp; idc  dir 
	m.setState(a, R(dir+ idc)) 
c.setState(a, Sh); c.setData(a, m.data(a)); 
 Write caching rule 
 == m.state(a) 
	m.setState(a, W(idc)) 
c.setState(a, Ex); c.setData(a, m.data(a)); 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L18- 22 
Arvind 
Store Rules (at cache)
 Store-hit rule 
Store(a,v)==inst 
&amp; cache.state(a) is Ex
	 p2m.deq; 
m2p.enq(Ack)
cache.setData(a, v) 
 Store-miss rules
Store(a,v)==inst 
&amp; cache.state(a) is Nothing
	 c2m.enq(Msg(id, Home, ExReq, a); 
cache.setState(a,Pending)	 Already covered
by the
Invalidate 
&amp; cache.state(a) is Sh voluntary ruleStore(a,v)==inst 
	 c2m.enq(Msg(id, Home, InvRep, a); 
cache.setState(a,Nothing)
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L18- 28 
Arvind 
Processing FlushReq Message (at cache)
FlushReq 
Msg(Home,id,FlushReq,a) == msg
&amp; cache.state(a) is Ex
  	 m2c.deq 
cache.invalidate(a) 
c2m.enq (Msg(id, Home, FlushRep, a, cache.data(v))) 
Msg(Home,id,FlushReq,a) == msg
&amp; cache.state(a) is Sh
 	 m2c.deq
cache.invalidate(a)
c2m.enq (Msg(id, Home, InvRep, a))
Msg(Home,id,FlushReq,a) == msg
&amp; cache.state(a) is Nothing or Pending
	 m2c.deq 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L18- 29 
Arvind 
Processing Reply InvRep Messages 
(at home) 
InvRep 
Msg(id,Home,InvRep,a) == mmsg 
&amp; m.state(a) is TR(dir)
  	 deq mmsg; 
m.setState(a, TR(dir-{id}))
Msg(id,Home,InvRep,a) == mmsg 
&amp; m.state(a) is R(dir) 

 	 deq mmsg; 
m.setState(a, R(dir-{id}))
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>6.823 L18- 36 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Dir a: Sh {1} Sh: a 
Dir a: Sh {1,2} ShReq 
a Pen: a Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823 L18- 34 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Dir a: Sh {} Pen: a 
Dir a: Sh {1} ShReq 
a Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L18- 21 
Arvind 
Load Rules (at cache)
 Load-hit rule
Load(a)==inst 
&amp; cache.state(a) is Sh or Ex 
	 p2m.deq
m2p.enq(cache.data(a))
 Load-miss rule 
 Load(a)==inst 
&amp; 	cache.state(a) is Nothing 
c2m.enq(Msg(id, Home, ShReq, a) 
cache.setState(a,Pending) 
This is blocking cache because the Load miss
rule does not remove the request from the
input queue (p2m) ... more later 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>6.823 L18- 38 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Dir a: Sh {1,2} ExReq 
a Sh: a Sh: a Pen: a 
InvReq 
a InvReq 
a Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L18- 31 
Arvind 
Non-Blocking Caches
	Non-blocking caches are 
needed to tolerate large 
memory latencies 
	To get non-blocking property 
we implement p2m with 2 
FIFOs (deferQ, incomingQ) 
	Requests moved to deferQ 
when: 
	address not there 
	needed for consistency 
new reqs 
Handle 
Req. deferQincomingQ 
deq enq 
p2m 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>6.823 L18- 41 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory Dir a: Ex {N} Pen: a 
ShResp 
&lt;a,v'&gt; Ex: a 
Dir a: Sh {1,N} WBResp 
&lt;a,v'&gt; Sh: a Sh: a Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L18- 23 
Arvind 
Processing ShReq Messages (at Home)
Uncached or Outstanding Shared Copies 
Msg(id,Home,ShReq,a) ==mmsg 
&amp; m.state(a) is R(dir) &amp;  id  dir
  	 in.deq; 
m.setState(a, R(dir+{id})); 
out.enq(Msg(Home,id,ShRep, a,m.data(a))) 
Outstanding Exclusive Copy
Msg(id,Home,ShReq,a) ==mmsg 
&amp; m.state(a) is W(id) &amp; (id is not id)
  	 m.setState(a, TW(id));
out.enq(Msg(Home,id,WbReq, a))
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L18- 14 
Arvind 
Protocol Design
*Note*
We will not be able to finish this part today.
(The rest of the material will be covered during the next lecture.)
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L18- 17 
A Protocol for a system with two Arvind 
memory levels (L1 + M) 
Cache states: Sh, Ex, Pending, Nothing
Memory states: R(dir),  W(id), TR (dir), TW(id)
If dir is empty then R(dir) and TR(dir) 
represent the same state 
Messages: 
Cache to Memory requests: ShReq, ExReq 
Memory to Cache requests: WbReq, InvReq, FlushReq 
Cache to Memory responses: WbRep(v), InvRep, FlushRep(v) 
Memory to Cache responses: ShRep(v), ExRep(v) 
Operations on cache: 
cache.state(a)  returns state s 
cache.data(a)  - returns data v 
cache.setState(a,s), cache.setD ata(a,v), cache.invalidate(a) 
inst = first(p2m); msg= first(m2c); mmsg = first(in) 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L18- 5 
Arvind 
State Encoding 
a P 
L1 P 
L1 P 
L1 P 
a L2 L1 P 
a P 
Interconnect 2 3 4 5 6 7 8 9 
(Sh, R(6)) (Sh, ) (Sh, ) 
a1 (Ex, R(2,4)) 
Each address in a cache keeps two types of state 
info 
 sibling info: do my siblings have a copy of address a 
- Ex (means no),  Sh (means may be) 
 children info: has this address been passed on to 
any of my children 
- W(id) means child id has a writable version 
- R(dir) means only children named in the directory 
dir have copies 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L18- 18 
Arvind 
Voluntary rules: Cache must be able to 
evict values to create space 
Invalidate rule 
cache.state(a) is ShIt would be good to have 
silent drops but difficult in 
a directory-based protocol  	 cache.invalidate(a) 
c2m.enq (Msg(id, Home, InvRep, a)) 
Flush rule 
cache.state(a) is Ex
  	 cache.invalidate(a) 
c2m.enq (Msg(id, Home, FlushRep, a, cache.data(v)) 
Writeback rule 
cache.state(a) is Ex
  	 cache.setState(a, Sh) 
c2m.enq (Msg(id, Home, WbRep, a, cache.data(v))) 
This rule may be applied if the cache/processor knows it 
is the last store operation to the location. 
Such voluntary rules can be used to construct
adaptive protocols. 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L18- 9 
Arvind 
Guarded Atomic Actions
	Rules specified using guarded atomic 
actions: 
&lt;guard predicate&gt; 
	{set of state updates that must occur 
atomically with respect to other rules} 
	E.g.: 
m.state(a) == R(dir) &amp; idc  dir 
	m.setState(a, R(dir+ idc)), 
c.setState(a, Sh); c.setData(a, m.data(a)); 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L18- 26 
Arvind 
Processing InvReq Message (at cache)
InvReq 
Msg(Home,id,InvReq,a) == msg 
&amp; cache.state(a) is Sh
  m2c.deq 
cache.invalidate(a) 
c2m.enq (Msg(id, Home, InvRep, a)) 
Msg(Home,id,InvReq,a) == msg 
&amp; cache.state(a) is Nothing or Pending
  m2c.deq 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L18- 12 
Arvind 
De-caching Rules: Child to Parent 
Child c idc 
Parent m idp 
 Writeback rule 
W(idc) == m.state(a) &amp; Ex == c.state(a) 
	m.setState(a, R({idc})) 
msetData(a, c.data(a)); 
c.setState(a, Sh); 
 Invalidate rule 
R(dir) == m.state(a) &amp; idc  dir &amp; Sh == c.state(a) 
	m.setState(a, R(dir - idc)) 
c.invalidate(a); 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Cache Coherence Protocols
for
Sequential Consistency 
Arvind
Computer Science and Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L18- 7 
Arvind 
Cache State Transitions 
Sh Ex Inv 
store 
load 
write-back invalidate flush 
store optimizations 
This state diagram is helpful as long as one 
remembers that each transition involves 
cooperation of other caches and the main 
memory. 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L18- 32 
Arvind 
Conclusion
	This protocol with its voluntary rules 
captures many other protocols that are 
used in practice. 
	we will discuss a bus-based version of this protocol 
in the next lecture 
	We need policies and mechanisms to 
invoke voluntary rules to build truly 
adaptive protocols. 
	search for such policies and mechanisms in an 
active area of research 
	Quantitative evaluation of protocols or 
protocol features is extremely difficult. 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L18- 3 
Arvind 
A System with Multiple Caches 
L1 P 
L1 P 
L1 P 
L1 P 
L2 L2 L1 P 
L1 P 
Interconnect 
M aka Home 
Assumptions: Caches are organized in a hierarchical manner 
 Each cache has exactly one parent but can have 
zero or more children
 Only a parent and its children can communicate directly 
 Inclusion property is maintained between a parent 
and its children, i.e.,
a  Li  a  Li+1
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L18- 4 
Arvind 
Maintaining Cache Coherence
Hardware support is required such that 
 only one processor at a time has write 
permission for a location 
 no processor can load a stale copy of 
the location after a write 
 
write request:
The address is invalidated in all other caches before 
the write is performed 
read request:
If a dirty copy is found in some cache, a write-back 
is performed before the memory is read 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L18- 24 
Arvind 
Processing ExReq Messages (at home)
Uncached or cached only at the requester cache 
Msg(id,Home,ExReq,a) ==mmsg 
&amp; m.state(a) is R(dir) &amp; (dir is empty or has only id)
  	in.deq 
m.setState(a, W(id)) 
out.enq(Msg(Home,id,ExRep, a, m.data(a)) 
Outstanding Shared Copies
Msg(id,Home,ExReq,a) ==mmsg 
&amp; m.state(a) is R(dir) &amp; !(dir is empty or has only id)
  	m.setState(a, TR(dir-{id})) 
out.enq(multicast(Home,dir-{id},InvReq, a) 
Outstanding Exclusive Copy
Msg(id,Home,ExReq,a) ==mmsg 
&amp; m.state(a) is W(id)  &amp; (id is not id)
  	m.setState(a, TW(id)) 
out.enq(Msg(Home,id,FlushReq, a) 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L18- 27 
Arvind 
Processing WbReq Message (at cache)
WbReq 
Msg(Home,id,WbReq,a) == msg 
&amp; cache.state(a) is Ex
  m2c.deq 
cache.setState(a, Sh) 
c2m.enq (Msg(id, Home, WbRep, a, cache.data(v))) 
Msg(Home,id,WbReq,a) == msg 
&amp; cache.state(a) is Sh or Nothing or Pending
  m2c.deq 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L18- 25 
Arvind 
Processing Reply Messages (at cache)
ShRep 
Msg(Home, id, ShRep, a, v) == msg 
-- cache.state(a) must be Pending or Nothing 
	 m2c.deq
cache.setState(a, Sh)
cache.setData(a, v)
ExRep 
Msg(Home, id, ExRep, a, v) == msg 
-- cache.state(a) must be Pending or Nothing 
	 m2c.deq
cache.setState(a, Ex)
cache.setData(a, v)
-- In general only a part of v will be 
overwritten by the Store instruction. 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L18- 13 
Arvind 
Making the Rules Local &amp; Reactive
Child c idc 
Parent m idp 
	Some rules require observing and changing the state 
of multiple caches simultaneously (atomically). 
	very difficult to implement, espe cially if caches are separated 
by a network 
	Each rule must be triggered by some action 
	Split rules are into multiple rules  request for an 
action followed by an action and an ack. 
	ultimately all actions are triggered by some processor 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L18- 19 
Arvind 
Voluntary rules: Memory should be able 
to send more values than requested 
Cache Rule 
m.state(a) is R(dir) &amp;  id  dir
  	 m.setState(a, R(id+dir)) 
out.enq(Msg(Home,id,ShRep, a,m.data(a))) 
It is a rule like this that allows us to fetch locations a+1, 
a+2, ... when a processor requests address a. 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.823 L18- 35 
Arvind 
Protocol Diagram 
... Cache 1 Cache N 
Main Memory ShResp Pen: a 
Dir a: Sh {1} Sh: a 
&lt;a,v&gt; Cache 2 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L18- 10 
Arvind 
Data Propagation Between Caches
Parent c 
m Child 
Parent c 
m Child 
Caching rules De-caching rules 
 Read caching rule  Write-back rule 
 Write caching rule  Invalidate rule 
November 14, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L18- 15 
Arvind 
Protocol Processors an abstract view
m interconnect 
PP PP P 
c2m 
m2c L1 p2m 
in out PP P 
c2m m2c 
L1 p2m m2p m2p 
	Each cache has 2 pairs of queues 
 one pair (c2m, m2c) to co mmunicate with the memory 
 one pair (p2m, m2p) to comm unicate with the processor 
	Messages format: 
msg(idsrc,iddest,cmd,priority,a,v) 
	FIFO messages passing between each (src,dest) pair 
except a Low priority (L) msg cannot block a high 
priority (H) msg 
November 14, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Virtual Machines (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l25_vms/</lecture_pdf_url>
      <lectureno>L25</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Joel Emer 
December 12, 2005 
IBM System/38 and AS/400 6.823, L25-17 
	System/38 announced 1978, AS/400 is follow-on line 
	High-level instruction set interface designed for binary 
translation 
	Memory-memory style instruction set, never directly 
executed by hardware 
Replaced by modified 
Used 48-bit CISC PowerPC cores in newer 
engine in earlier AS/400 machines 
machines User Applications 
Languages, 
Database, 
Utilities Control 
Program 
Facility 
High-Level Architecture 
Interface 
Vertical Microcode 
Horizontal Microcode 
Hardware Machine</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823, L25-20 
Chaining 
Runtime -
Execution Code Cache Code Cache Tags Pre Chained 
add %r5, %r6, %r7 
#of next block 
j dispatch loop 
Chained 
add %r5, %r6, %r7 
j physical location of translated 
code for next_block Joel Emer 
December 12, 2005 
li %next_addr_reg, next_addr #load address</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-8 Supporting Non-Native ISAs 
Run programs for one ISA on hardware with different ISA 
E m u l a t i o n (OS software interprets in structions at run-time) 
 E.g., OS for PowerPC Macs had emulator for 68000 code 
 Binary Translation (convert at install and/or load time) 
 IBM AS/400 to modified PowerPC cores 
 DEC tools for VAX-&gt;Alpha and MIPS-&gt;Alpha 
 Dynamic Translation (non-native ISA to native ISA at run time) 
 Suns HotSpot Java JIT (just-in-time) compiler 
 Transmeta Crusoe, x86-&gt;VLIW code morphing 
 Run-time Hardware Emulation 
 IBM 360 had IBM 1401 emulator in microcode 
 Intel Itanium converts x86 to native VLIW (two software-visible ISAs) 
 ARM cores support 32-bit ARM, 16-bit Thumb, and JVM (three software-
visible ISAs!)</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-3 ISA + Environment =
Virtual Machine
ISA alone not sufficient to write useful programs, need I/O 
	Direct access to memory mapped I/O via load/store instructions 
problematic 
	time-shared systems 
	portability 
	Operating system responsible for I/O 
	sharing devices and managing security 
	hiding different types of hardwa re (e.g., EIDE vs. SCSI disks) 
	ISA communicates with operating system through some 
standard mechanism, i.e., syscall instructions
	example convention to open file: 
addi r1, r0, 27 # 27 is code for file open 
addu r2, r0, rfname # r2 p oints to filename string 
syscall # cause trap into OS 
# On return from syscall, r1 holds file descriptor</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Joel Emer 
Transmeta Crusoe December 12, 2005 
6.823, L25-21 
(2000) 
 Converts x86 ISA into internal native VLIW 
format using software at run-time  Code 
Morphing 
 Optimizes across x86 instruction boundaries to 
improve performance 
 Translations cached to avoid translator 
overhead on repeated execution 
 Completely invisible to operating system  
looks like x86 hardware processor 
[ Following slides contain examples taken from 
The Technology Behind Crusoe Processors,
Transmeta Corporation, 2000 ]</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Joel Emer 
6.823, L25-25Compiler Optimizations December 12, 2005 
RISC ops: 
ld %r30, [%esp] # load from stack into temp 
add.c %eax, %eax, %r30 # add to %eax, set cond.codes 
ld %r31, [%esp] 
add.c %ebx, %ebx, %r31 
ld %esi, [%ebp] 
sub.c %ecx, %ecx, 5 
Optimize: 
ld %r30, [%esp] # load from stack only once 
add %eax, %eax, %r30 
add %ebx, %ebx, %r30 # reuse data loaded earlier 
ld %esi, [%ebp] 
sub.c %ecx, %ecx, 5 # only this cond. code needed</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-6 Supporting Multiple OSs on 
Same Hardware
	Can virtualize the environment that an operating system 
sees, an OS-level VM
	Hypervisor layer implements sharing of real hardware 
resources by multiple OS VMs that each think they have a 
complete copy of the machine 
 Popular in early days to allow mainframe to be shared by multiple 
groups developing OS code (VM/360) 
 Used in modern mainframes to allow multiple versions of OS to be 
running simultaneously  OS upgrades with no downtime! 
 Example for PCs: VMware allows Windows OS to run on top of Linux (or 
vice-versa) 
	Requires trap on access to privileged hardware state 
	easier if OS interface to hardware well defined</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Joel Emer 
6.823, L25-16Binary Translation, Take 2 December 12, 2005 
Executable 
on Disk 
Guest 
ISA 
Code Guest 
ISA on Disk 
Native 
ISA Code PC 
Table Guest ISA 
Code Guest ISA 
Native Translate to data in native 
Data Executable 
Mapping Data 
Emulator native ISA code Keep copy of 
code and 
data segment Mapping table used for 
indirect jumps and to 
jump from emulator 
back into native 
translations 
Translation has to check 
then jump to emulator for modified code pages 
Emulator used for run
time modified code, 
checks for jumps back 
into native code using 
PC mapping table</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-4Application Binary Interface 
(ABI)
	Programs are usually distributed in a binary format that 
encodes the program text (instructions) and initial values of 
some data segments (ABI) 
	Virtual machine specifications include 
	which instructions are available (the ISA) 
	what system calls are possible (I/O, or the environment ) 
	what state is available at process creation 
	Operating system implements the virtual machine 
 at process startup, OS reads the binary program, creates an 
environment for it, then begins to execute the code, handling traps for 
I/O calls, emulation, etc.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-28Exceptions 
Original x86 code: 
addl %eax, (%esp) # load data from stack, add to eax 
addl %ebx, (%esp) # load data from stack, add to ebx 
movl %esi, (%ebp) # load esi from memory 
subl %ecx, 5  # sub 5 from ecx 
Scheduled VLIW code: 
ld %r30, [%esp]; sub.c %ecx, %ecx, 5 
ld %esi, [%ebp]; add %eax, %eax, %r30; add %ebx, %ebx, %r30 
 x86 instructions executed out-of-order with respect to 
original program flow
 Need to restore state for precise traps</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-19
Dynamic Translation Example 
Data RAM 
Disk x86 
Binary 
Runtime -- Execution x86
Binary 
Code Cache Code Cache 
Tags 
Translator x86 Parser &amp; 
High Level 
Translator 
High Level 
Optimization 
Low Level 
Code Generation 
Low Level 
Optimization and 
Scheduling</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Joel Emer 
6.823, L25-11Binary Translation	December 12, 2005 
	Each guest ISA instruction translates into some set of host (or 
native ) ISA instructions 
	Instead of dynamically fetching  and decoding instructions at 
run-time, translate entire binary program and save result as 
new native ISA executable 
	Removes interpretive fetch-decode overhead 
	Can optimize translated code to improve performance 
	register allocation for values flowing between guest ISA instructions 
	native instruction scheduling to improve performance 
	remove unreachable code 
	inline assembly procedures 
	remove dead code e.g., unneeded ISA side effects</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Joel Emer 
December 12, 2005 
Transmeta VLIW Engine 6.823, L25-22 
	Two VLIW formats, 64-bit and 128-bit, contains 2 or 4 
RISC-like operations 
	VLIW engine optimized for x86 code emulation 
	evaluates condition codes the same way as x86 
	has 80-bit floating-point unit 
	partial register writes (update 8 bits in 32 bit register) 
	Support for fast instruction writes 
	run-time code generation important 
	Initially, two different VLIW implementations, low-end 
TM3120, high-end TM5400 
	native ISA differences invisible to user, hidden by translation system 
	new eight-issue VLIW core released (Efficeon/TM8000 series)</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-29 
Shadow Registers and Store Buffer
	All registers have working copy and shadow copy 
	Stores held in software controlled store buffer, loads can 
snoop 
	At end of translation block, commit changes by copying 
values from working regs to shadow regs, and by 
releasing stores in store buffer 
	On exception, re-execute x86 code using interpreter</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Joel Emer 
December 12, 2005 
Software Applications 6.823, L25-2 
How is a software application encoded? 
 What are you getting when yo u buy a software application? 
 What machines will it work on? 
 Who do you blame if it doesnt work, 
 i.e., what contract(s) were violated?</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823, L25-23
Crusoe System 
VLIW Processor 
Crusoe CPU 
x86 DRAM Code Morph DRAM Flash Code Morph
Compiler Code
(VLIW) 
Workspace Portion of system DRAM is
used by Code Morph
software and is invisible to 
x86 machine Crusoe 
Boot 
Flash 
ROM 
Compressed
compiler held in
boot ROM 
System DRAM Joel Emer 
December 12, 2005 
Inst. Cache 
Data Cache 
x86 BIOS Translation 
Cache (VLIW)</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-26Scheduling 
Optimized RISC ops: 
ld %r30, [%esp] # load from stack only once 
add %eax, %eax, %r30 
add %ebx, %ebx, %r30 # reuse data loaded earlier 
ld %esi, [%ebp] 
sub.c %ecx, %ecx, 5 # only this cond. code needed 
Schedule into VLIW code: 
ld %r30, [%esp]; sub.c %ecx, %ecx, 5 
ld %esi, [%ebp]; add %eax, %eax, %r30; add %ebx, %ebx, %r30</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Joel Emer 
Emulation December 12, 2005 
6.823, L25-10 
 Easy to code, small code footprint 
 Slow, approximately 100x slower than native 
execution for RISC ISA hosted on RISC ISA
 Problem is time taken to decode instructions
 fetch instruction from memory 
 switch tables to decode opcodes 
 extract register specifiers using bit shifts 
 access register file data structure 
 execute operation 
 return to main fetch loop</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Joel Emer 
December 12, 2005 
OS Can Support Multiple VMs 6.823, L25-5 
	Virtual machine features change over time with new 
versions of operating system 
	new ISA instructions added 
	new types of I/O are added (e.g., asynchronous file I/O) 
	Common to provide backwards compatibility so old 
binaries run on new OS 
 SunOS 5 (System V Release 4 Unix, Solaris) can run binaries 
compiled for SunOS4 (BSD-style Unix)
	Windows 98 runs MS-DOS programs 
	Solaris 10 runs Linux binaries 
	If ABI needs instructions not supported by native 
hardware, OS can provide in software</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Joel Emer 
December 12, 2005 
Binary Translation Problems 6.823, L25-15 
 Self-modifying code! 
 sw r1, (r2) # r2 points into code space 
 Rare in most code, but has to be handled if 
allowed by guest ISA 
 Usually handled by in cluding interpreter and 
marking modified code pages as interpret only 
 Have to invalidate all native branches into 
modified code pages</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Joel Emer 
Emulation	December 12, 2005 
6.823, L25-9 
	Software instruction set interpreter fetches and decodes 
one instruction at a time in emulated VM 
Memory image of 
guest VM lives in 
host emulator data 
memory 
Guest 
ISA 
Code Guest 
ISA on Disk 
Guest 
ISA 
Code Guest 
ISA Guest 
Stack 
Load into 
memory 
Emulator Data 
Emulator Code Emulator Stack 
Data	Executable 
Data emulator fetch-decode loop 
while(!stop) 
{ 
inst = Code[PC]; 
PC += 4; 
execute(inst); 
}</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-1 
Virtual Machines and Dynamic 
Translation:
Implementing ISAs in Software
Joel Emer 
Computer Science and Artificial Intelligence Laboratory 
Massachusetts Institute of Technology 
Based on the material prepared by 
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823, L25-13
Binary Translation Problems 
Branch and Jump targets 
j L1 
... 
j 
translation 
lw 
translation 
jr 
translation block jumps to native 
translation of lw 
Where should the jump register go? Joel Emer 
December 12, 2005 
 guest code: 
L1: lw r1, (r4) 
jr (r1) 
n a t i v e  c o d e 
native jump at end of</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Joel Emer 
6.823, L25-12Binary Translation, Take 1 December 12, 2005 
Guest 
ISA 
Code Guest 
ISA on Disk 
Native 
ISA 
Code Guest 
ISA on Disk 
Native 
Translate to Data 
unchanged 
might need extra dataData Executable 
Data Executable 
Data 
native ISA code Native translation 
workspace</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Joel Emer 
December 12, 2005 
6.823, L25-7 
ISA Implementations Partly in Software
Often good idea to implement part of ISA in software: 
	Expensive but rarely used inst ructions can cause trap to OS 
emulation routine: 
 e.g., decimal arithmetic in Vax implementation of VAX ISA 
	Infrequent but difficult operand values can cause trap 
 e.g., IEEE floating-point denorm als cause traps in almost all 
floating-point unit implementations 
	Old machine can trap unused opcodes, allows binaries for new 
ISA to run on old hardware 
 e.g., Sun SPARC v8 added integer multiply instructions, older v7 
CPUs trap and emulate</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Joel Emer 
December 12, 2005 
Dynamic Translation 6.823, L25-18 
 Translate code sequences as needed at run
time, but cache results 
 Can optimize code sequences based on
dynamic information (e.g., branch targets
encountered) 
 Tradeoff between optimizer run-time and time 
saved by optimizations in translated code 
 Technique used in Java JIT (Just-In-Time) 
compilers 
 Also, Transmeta Crusoe for x86 emulation</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Joel Emer 
6.823, L25-24Transmeta Translation December 12, 2005 
x86 code: 
addl %eax, (%esp) # load data from stack, add to eax 
addl %ebx, (%esp) # load data from stack, add to ebx 
movl %esi, (%ebp) # load esi from memory 
subl %ecx, 5  # sub 5 from ecx 
first step, translate into RISC ops: 
ld %r30, [%esp] # load from stack into temp 
add.c %eax, %eax, %r30 # add to %eax, set cond.codes 
ld %r31, [%esp] add.c %ebx, %ebx, %r31 
ld %esi, [%ebp] 
sub.c %ecx, %ecx, 5</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Joel Emer 
December 12, 2005 
Handling Self-Modifying Code 6.823, L25-30 
 When a translation is made, mark the 
associated x86 code page as being translated 
in page table 
 Store to translated code page causes trap, and 
associated translations are invalidated</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Joel Emer 
December 12, 2005 
Translation Overhead 6.823, L25-27 
 Highly optimizing compiler takes considerable 
time to run, adds run-time overhead 
 Only worth doing for frequently executed code
 Translation adds instrumentation into 
translations that counts how often code 
executed, and which way branches usually go 
 As count for a block increases, higher 
optimization levels are invoked on that code</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Joel Emer 
PC Mapping Table	December 12, 2005 
6.823, L25-14 
	Table gives translated PC for each guest PC 
	Indirect jumps translated into code that looks in table to 
find where to jump to 
 can optimize well-behaved guest code for subroutine call/return by 
using native PC in return links 
	If can branch to any guest PC , then need one table entry 
for every instruction in hosted program  big table 
	If can branch to any PC, then either 
	limit inter-instruction optimizations 
 large code explosion to hold optimizations for each possible entry 
into sequential code sequence 
	Only minority of guest instructions are indirect jump 
targets, want to find these 
	design a highly structured VM design 
	use run-time feedback of target locations</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>VLIW/EPIC: Statically Scheduled ILP (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l21_vliw/</lecture_pdf_url>
      <lectureno>L21</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.823, L21-14
Software Pipelining 
vs. Loop Unrolling 
time performance 
time performance Loop Unrolled 
Software Pipelined Startup overhead Wind-down overhead 
Loop Iteration 
Loop Iteration 
costs only once per loop, not once per iteration Joel Emer 
November 28, 2005 
Software pipelining pa ys startup/wind-down</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823, L21-5
Out-of-Order Control Complexity: 
MIPS R10000 
Control 
Logic 
[ SGI/MIPS 
Technologies Joel Emer 
November 28, 2005 
Inc., 1995 ]</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823, L21-10
Loop Execution 
for (i=0; i&lt;N; i++) 
B[i] = A[i] + C; Int1 M1 M2 FP+ 
loop: 
How many FP ops/cycle? ld add r1 
fadd 
sd loop: ld f1, 0(r1) 
add r1, 8 
add r2, 8 Schedule Joel Emer 
November 28, 2005 
Int 2 FPx 
add r2 bne 
1 fadd / 8 cycles = 0.125 fadd f2, f0, f1 
sd f2, 0(r2) 
bne r1, r3, loop Compile</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823, L21-27
IA-64 Predicated Execution 
 
 
Inst 1 
Inst 2 
Inst 3 Inst 4 
Inst 5 Inst 6 
Inst 7 Inst 8 b0
: 
b1: 
b2: 
b3: if 
else 
then 
Four basic blocks Inst 1 
Inst 2 
(
|| (p2) Inst 5 || (p2) Inst 6 
Inst 7 Inst 8 Predication 
One basic block Joel Emer 
November 28, 2005 
Problem: Mispredicted branches limit ILP 
Solution: Eliminate hard to predict branches with predicated execution 
Almost all IA-64 instructions can be ex ecuted conditionally under predicate 
Instruction becomes NOP if predicate register false 
br a==b, b2 
br b3 p1,p2 &lt;- cmp a==b) 
(p1) Inst 3     
(p1) Inst 4     
Mahlke et al, ISCA95: On average 
&gt;50% branches removed</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-22Cydra-5:
Memory Latency Register (MLR)
Problem: Loads have variable latency
Solution: Let software choose desired memory latency
	Compiler schedules code f or maximum load-use distance 
	Software sets MLR to latency that matches code schedule 
	Hardware ensures that loads take exactly MLR cycles to 
return values into processor pipeline
 Hardware buffers loads that return early 
 Hardware stalls processor if loads return late</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-9 
Early VLIW Machines 
 FPS AP120B (1976) 
 scientific attached array processor 
 first commercial wide instruction machine 
 hand-coded vector math librari es using software pipelining 
and loop unrolling 
 Multiflow Trace (1987)
 commercialization of ideas from Fishers Yale group including 
trace scheduling 
 available in configurations with 7, 14, or 28
operations/instruction
 28 operations packed into a 1024-bit instruction word 
 Cydrome Cydra-5 (1987) 
 7 operations encoded in 256-bit instruction word 
 rotating register file</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-18 
VLIW Instruction Encoding 
Group 1 Group 2 Group 3 
 Schemes to reduce effect of unused fields
 Compressed format in memory, expand on I-cache refill 
 used in Multiflow Trace 
 introduces instruction addressing challenge 
 Mark parallel groups
 used in TMS320C6x DSPs, Intel IA-64
 Provide a single-op VLIW instruction
 Cydra-5 UniOp instructions</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-26 
IA-64 Registers
 128 General Purpose 64-bit Integer Registers
 128 General Purpose 64/80-bit Floating Point 
Registers 
 64 1-bit Predicate Registers
 GPRs rotate to reduce code size for software 
pipelined loops</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823, L21-19
Rotating Register Files 
Lots of duplicated code in prolog, epilog 
Solution: Allocate new set of registers for each loop iteration ld r1, () 
add r2, r1, #1 ld r1, () 
add r2, r1, #1 ld r1, () 
add r2, r1, #1 
ld r1, () 
add r2, r1, #1 ld r1, () 
add r2, r1, #1 ld r1, () 
add r2, r1, #1 Prolog 
Epilog Loop Joel Emer 
November 28, 2005 
Problems: Scheduled loops re quire lots of registers, 
st r2, () 
st r2, () 
st r2, () 
st r2, () 
st r2, () 
st r2, ()</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-24 
Intel EPIC IA-64
 EPIC is the style of architecture (cf. CISC, RISC)
 Explicitly Parallel Instruction Computing 
 IA-64 is Intels chosen ISA (cf. x86, MIPS) 
 IA-64 = Intel Architecture 64-bit 
 An object-code compatible VLIW 
 Itanium (aka Merced) is first implementation 
(cf. 8086) 
 First customer shipment expected 1997 (actually 2001) 
 McKinley, second implementation shipped in 2002</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823, L21-6 
Check instruction 
dependencies Superscalar processor Sequential ISA Bottleneck 
for (i=0, i&lt; Sequential 
source code Superscalar compiler 
operations Schedule 
operations Sequential 
machine code 
Schedule 
execution Joel Emer 
November 28, 2005 
a = foo(b); 
Find independent</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823, L21-20
 Rotating Register File 
P0 P1 P2 P3 P4 P5 P6 P7 
RRB=3 
+ R1 
current register set. 
Usually, split 
into rotating and non-rotating registers. 
bloop ld r1, () 
add r3, r2, #1 ld r1, () 
add r3, r2, #1 ld r1, () 
add r2, r1, #1 Prolog 
Epilog Loop Loop closing 
branch 
decrements RRB Joel Emer 
November 28, 2005 
Rotating Register Base (RRB) register points to base of 
Value added on to logical register 
specifier to give physical register number.  
dec RRB 
dec RRB 
dec RRB dec RRB 
st r4, () 
st r4, () 
st r4, ()</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823, L21-15 
What if there are no loops? 
 
in control-flow intensive 
irregular code 
 
individual basic blocksBasic block Joel Emer 
November 28, 2005 
Branches limit basic block size 
Difficult to find ILP in</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-7 
VLIW: Very Long Instruction Word
Two Integer Units, 
Single Cycle Latency 
Two Load/Store Units, FP Op 1 FP Op 2 Int Op 2 Mem Op 1 Mem Op 2 Int Op 1 
Three Cycle Latency Two Floating-Point Units, 
Four Cycle Latency 
 Multiple operations packed into one instruction 
 Each operation slot is for a fixed function 
 Constant operation latencies are specified 
 Architecture requires guarantee of: 
 Parallelism within an instructio n =&gt; no x-operation RAW check 
 No data use before data ready =&gt; no data interlocks</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-13Software Pipelining 
Unroll 4 ways first	Int1 Int 2 M1 M2 FP+ FPx 
loop: 	 ld f1, 0(r1)
ld f2, 8(r1)
ld f3, 16(r1)
ld f4, 24(r1)
add r1, 32
fadd f5, f0, f1
fadd f6, f0, f2 
fadd f7, f0, f3 
fadd f8, f0, f4
sd f5, 0(r2)
sd f6, 8(r2)
sd f7, 16(r2)
add r2, 32
sd f8, -8(r2)
bne r1, r3, loop
How many FLOPS/cycle? loop:iterate prolog 
epilog ld f1 
ld f2 
ld f3 
add r1 ld f4 
ld f1 fadd f5 
ld f2 fadd f6 
ld f3 fadd f7 
add r1 ld f4 fadd f8 
ld f1 sd f5 fadd f5 
ld f2 sd f6 fadd f6 
add r2 ld f3 sd f7 fadd f7 
add r1 bne ld f4 sd f8 fadd f8 
sd f5 fadd f5 
sd f6 fadd f6 
add r2 sd f7 fadd f7 
bne sd f8 fadd f8 
sd f5 
4 fadds / 4 cycles = 1</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-1 
VLIW/EPIC: Statically Scheduled ILP
Joel Emer
Computer Science &amp; Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-25 
IA-64 Instruction Format
Instruction 2 Instruction 1 Instruction 0 Template 
128-bit instruction bundle 
 Template bits describe grouping of these 
instructions with others in adjacent bundles 
 Each group contains instructions that can 
execute in parallel 
bundle j bundle j-1 bundle j+1 bundle j+2 
group i-1 group i group i+1 group i+2</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823, L21-21
 Rotating Register File 
(Previous Loop Example) 
bloop ld f1, () Three cycle load latency 
encoded as difference of 3 
in register specifier encoded as difference of 4 
in register specifier Joel Emer 
November 28, 2005 
sd f9, () fadd f5, f4, ... number (f4 - f1 = 3) Four cycle fadd latency 
number (f9  f5 = 4) 
ld P9, () fadd P13, P12, sd P17, () bloop 
ld P8, () fadd P12, P11, sd P16, () bloop 
ld P7, () fadd P11, P10, sd P15, () bloop 
ld P6, () fadd P10, P9, sd P14, () bloop 
ld P5, () fadd P9, P8, sd P13, () bloop 
ld P4, () fadd P8, P7, sd P12, () bloop 
ld P3, () fadd P7, P6, sd P11, () bloop 
ld P2, () fadd P6, P5, sd P10, () bloop RRB=8 
RRB=7 
RRB=6 
RRB=5 RRB=4 
RRB=3 RRB=2 
RRB=1</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823, L21-11
 Loop Unrolling 
for (i=0; i&lt;N; i++) 
B[i] = A[i] + C; 
for (i=0; i&lt;N; i+=4) 
{ 
B[i] = A[i] + C; B[i+1] = A[i+1] + C; B[i+2] = A[i+2] + C; 
B[i+3] = A[i+3] + C; 
} Unroll inner loop to perform 4 
iterations at once 
of unrolling factor with final cleanup loop Joel Emer 
November 28, 2005 
Need to handle values of  N that are not multiples</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823, L21-30
 IA-64 Speculative Execution 
Problem: Branches restrict compiler code motion 
Inst 1 
Inst 2 
Use r1 
Inst 3 
exception Inst 1 Inst 2 
Use r1 Inst 3 never causes 
original home block 
exception detected Solution: Speculative operations that dont cause exceptions Joel Emer 
November 28, 2005 
br a==b, b2 
Load r1 
Cant move load above branch 
because might cause spurious Load.s r1 
br a==b, b2 
Chk.s r1 Speculative load 
exception, but sets 
poison bit on 
destination register 
Check for exception in 
jumps to fixup code if 
Particularly useful for scheduling long latency loads early</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823, L21-16 
Trace Scheduling [ Fisher,Ellis] 
 trace, 
that represents most frequent branch 
path 
 
heuristics to find common branch paths 
 
 
jumping out of trace Joel Emer 
November 28, 2005 
Pick string of basic blocks, a 
Use profiling feedback or compiler 
Schedule whole trace at once 
Add fixup code to cope with branches</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-34 
Thank you !</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>wdata6.823, L21-29 
Fully Bypassed Datapath 
ASrc IR IR IR 
PC A 
B Y 
R 
MD1 MD2 addr 
inst 
Inst 
Memory 0x4 
Add 
IR 
Imm 
Ext rd1 
GPRs rs1 
rs2 
ws 
wd rd2 we 
addr 
wdata Memory we 31 nop stall 
D E M W PC for JAL, ... 
BSrc 
Where does predication fit in? Joel Emer 
November 28, 2005 
ALU 
rdata 
Data</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-33 
Limits of Static Scheduling
 Unpredictable branches 
 Variable memory latency 
(unpredictable cache misses) 
 Code size explosion 
C o m p i l e r  c o m p l e x i t y 
Question: 
How applicable are the VLIW-inspired 
techniques to traditional RISC/CISC 
processor architectures?</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823, L21-32
Clustered VLIW 
 
local register files and local 
functional units 
 
interconnect between clusters 
 
minimize communication 
overhead 
 
processors, .e.g., Alpha 21264 
 
embedded processors, examples include TI C6x series DSPs, and 
HP Lx processor Cluster 
Interconnect 
Local Local 
Memory Interconnect 
Cache/Memory Banks Cluster Joel Emer 
November 28, 2005 
Divide machine into clusters of 
Lower bandwidth/higher latency 
Software responsible for 
mapping computations to 
Exists in some superscalar 
Common in commercial Regfile Regfile</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-23 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-2 
Littles Law
Parallelism = Throughput * Latency
or
L T N 
= 
One Operation Throughput per Cycle 
Latency in Cycles</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-4 
Superscalar Control Logic Scaling
Issue Width W 
Issue Group 
Previously 
Issued 
Instructions Lifetime L
	Each issued instructions must make interlock checks against 
W*L instructions, i.e., growth in interlocks  W*(W*L) 
	For in-order machines, L is related to pipeline latencies 
	For out-of-order machines, L also includes time spent in 
instruction buffers (instruction window or ROB) 
	As W increases, larger instruction window is needed to find 
enough parallelism to keep  machine busy =&gt; greater L 
=&gt; Out-of-order control logic grows faster than W2 (~W3)</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-8 
VLIW Compiler Responsibilities 
The compiler: 
 Schedules to maximize parallel execution 
 Guarantees intra-instruction parallelism 
S c h e d u l e s  t o  a v o i d  d a t a  hazards (no interlocks) 
 Typically separates operations with explicit NOPs</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-3 
Example Pipelined ILP Machine 
Max Throughput, Six Instructions per Cycle 
One Pipeline Stage 
Latency 
in 
Cycles 
Two Floating-Point Units, 
Four Cycle Latency Two Integer Units, 
Single Cycle Latency 
Two Load/Store Units, 
Three Cycle Latency 
 How much instruction-level parallelism (ILP) 
required to keep machine pipelines busy?
T = 6L = (2x1 + 2x3 + 2x4) 2 2 = 2 N = 6  2 = 6 1 6 3 3</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-17 
Problems with Classic VLIW 
 Object-code compatibility 
 have to recompile all code fo r every machine, even for two 
machines in same generation 
 Object code size 
 instruction padding wastes  instruction memory/cache 
 loop unrolling/software pi pelining replicates code 
 Scheduling variable latency memory operations
 caches and/or memory bank conflicts impose statically 
unpredictable variability 
 Knowing branch probabilities 
 Profiling requires an significa nt extra step in build process 
 Scheduling for statically unpredictable branches
 optimal schedule vari es with branch path</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Joel Emer 
November 28, 2005 Scheduling Loop Unrolled Code 6.823, L21-12 
Unroll 4 ways 
loop: 	 ld f1, 0(r1) Int1 Int 2 M1 M2 FP+ FPx
ld f2, 8(r1)
ld f3, 16(r1) loop:
ld f4, 24(r1)
add r1, 32
fadd f5, f0, f1
fadd f6, f0, f2 
 Schedule 
fadd f7, f0, f3 
fadd f8, f0, f4
sd f5, 0(r2)
sd f6, 8(r2)
sd f7, 16(r2)
sd f8, 24(r2)
add r2, 32
bne r1, r3, loop
ld f1 
ld f2 
ld f3 
add r1 ld f4 fadd f5 
fadd f6 
fadd f7 
fadd f8 
sd f5 
sd f6 
sd f7 
add r2 bne sd f8 
How many FLOPS/cycle? 
4 fadds / 11 cycles = 0.36</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-28 
Predicate Software Pipeline Stages
Single VLIW Instruction 
(p1) ld r1 (p2) add r3 (p3) st r4 (p1) bloop 
Dynamic Execution 
(p1) ld r1 (p1) bloop 
(p1) ld r1 (p2) add r3 (p1) bloop 
(p1) ld r1 (p2) add r3 (p3) st r4 (p1) bloop 
(p1) ld r1 (p2) add r3 (p3) st r4 (p1) bloop 
(p1) ld r1 (p2) add r3 (p3) st r4 (p1) bloop 
(p2) add r3 (p3) st r4 (p1) bloop 
(p3) st r4 (p1) bloop 
Software pipeline stages turned on by rotating predicate 
registers  Much denser encoding of loops</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Joel Emer 
November 28, 2005 
6.823, L21-31 IA-64 Data Speculation
Problem: Possible memory hazards limit code scheduling
Solution: Hardware to check pointer hazards
Inst 1 
Inst 2 
Store 
Use r1 
Inst 3 Load r1 Data speculative load 
adds address to 
address check table 
Inst 1 
Inst 2 
Store 
Use r1 Inst 3 Load.a r1 
Load.c Store invalidates any 
matching loads in 
address check table 
Cant move load above store Check if load invalid (or 
because store might be to same missing), jump to fixup 
address code if so 
Requires associative hardware in address check table</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Out of Order Execution and Register Renaming (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l12_ooo_pipes/</lecture_pdf_url>
      <lectureno>L12</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L12-14 
ArvindSimplifying
Allocation/Deallocation 
.
.
.
Ins# use exec op p1 src1 p2  src2 
t1 ptr2 t2 
next to 
deallocate
prt1
next
available
Reorder buffer 
Instruction buffer is managed circularly 
exec bit is set when instruction begins execution 
When an instruction completes its use bit is marked free 
p t r2 is incremented only if the use bit is marked free 
October 24, 2005 nt</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L12-4 
Arvind 
In-Order Issue Limitations: an example
latency
1 LD F2, 34(R2) 1
2 LD F4, 45(R3) long
3 MULTD F6, F4, F2 3
4 SUBD F8, F2, F2 1
5 DIVD F4, F2, F8 4
6 ADDD F10, F6, F4 1
1 2 
3 4 
5 
6 
In-order: ) .  .  .  .  .  .  2 3 4 4 3 5 . 1 (2,1 . . 5 6 6 
In-order restriction prevents instruction 4 
from being dispatched 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L12-12 
Arvind 
Data-Driven Execution
Renaming 
table &amp; reg file 
Reorder buffer 
Load 
Unit FU FU Store 
Unit 
&lt; t, result &gt; Ins# use exec op p1 src1 src2 t1 
t2 
. 
. 
tn 
Replacing the 
tag by its value is an expensive 
operation p2
 Instruction template (i.e., tag t) is allocated by the 
Decode stage, which also stores the tag in the reg file 
 When an instruction completes, its tag is deallocated 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L12-11 
Arvind 
Renaming &amp; Out-of-order Issue
An example 
Renaming table Reorder buffer
Ins# use exec op p1  src1 p2 src2 
t1 
t2 
. 
. 
. 
data / ti p data 
F1 F2 
F3 
F4 
F5 
F6 
F7 
F8 
1 LD F2, 34(R2)
2 LD F4, 45(R3)  When are names in sources 
3 MULTD F6, F4, F2 replaced by data?
4 SUBD F8, F2, F2 Whenever an FU produces data
5 DIVD F4, F2, F8  When can a name be reused?
6 ADDD F10, F6, F4
October 24, 2005 
Whenever an instruction completes</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L12-22 
In-Order Commit for Precise Arvind 
Exceptions 
In-order Out-of-order In-order
Fetch Decode 
Execute Commit Reorder Buffer 
Kill Kill Kill 
Exception?Inject handler PC 
 Instructions fetched and decoded into instruction 
reorder buffer in-order 
 Execution is out-of-order (  out-of-order completion) 
 Commit (write-back to architectural state, i.e., regfile &amp; 
memory, is in-order 
October 24, 2005 
Temporary storage needed to hold results before commit 
(shadow registers and store buffers)</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L12-7 
ArvindHow many Instructions can
be in the pipeline 
Which features of an ISA limit the number of 
instructions in the pipeline? 
Number of Registers
Which features of a program limit the number of instructions in the pipeline? 
Control transfers
Out-of-order dispatch by itself does not provide any significant performance improvement ! 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L12-23 
Arvind 
Extensions for Precise Exceptions 
Inst# use exec op p1 src1  p2  src2 pd dest data cause 
ptr2
next to
commit
ptr1 
next 
available 
Reorder buffer 
 add &lt;pd, dest, data, cause&gt; f ields in the instruction template 
 commit instructions to reg file and memory in program 
order  buffers can be maintained circularly 
 on exception, clear reorder buffer by resetting ptr1=ptr2 
(stores must wait for commit before updating memory) 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Complex Pipelining:
Out-of-Order Execution &amp; Register 
Renaming
Arvind
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Krste Asanovic and Arvind</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823  L12-9
ArvindInstruction-Level Parallelism with 
Renaming 
latency
1 LD F2, 34(R2) 1
2 LD F4, 45(R3) long
3 MULTD F6, F4, F2 3
4 SUBD F8, F2, F2 1
5 DIVD F4, F 2 , F 8 4
6 ADDD F10, F6, F4 1
1 2 
3 4 
5 
6 X 
In-order: 1 (2,1 ) .  .  .  .  .  . 23 4 4 35 . . . 5 6 6
Out-of-order: 1 (2,1 ) 4 4 5  .  .  .  2 (3,5) 3 6 6
Any antidependence can be eliminated by renaming.
(renaming  additional storage) 
Can it be done in hardware? yes!
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L12-3 
Arvind 
Scoreboard for In-order Issues
Busy[FU#] : a bit-vector to indicate FUs availability. 
(FU = Int, Add, Mult, Div) 
These bits are hardwired to FU's. 
WP[reg#] : a bit-vector to record the registers for which 
writes are pending. 
These bits are set to true by the Issue stage and set to 
false by the WB stage 
Issue checks the instruction (opcode dest src1 src2) 
against the scoreboard (Busy &amp; WP) to dispatch 
FU available? Busy[FU#] 
RAW? WP[src1] or WP[src2] 
WAR? cannot arise 
WAW? WP[dest] 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>17 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L12-15 
Arvind 
IBM 360/91 Floating Point Unit
R. M. Tomasulo, 1967
October 24, 2005 
Mult p data 1 
2 p data 1 
2 
3 
4 5 
6 data load 
buffers 
(from 
memory) 1 
2 
3 
4 
Adder p data 1 2 
3 Floating 
Point 
Reg 
store buffers 
(to memory) ... 
Common bus ensures that data is made 
available immediately to all the instructions waiting for it distribute instruction templates by functional 
units 
&lt; t, result &gt; 
p data pd a t a pd a t a instructions</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L12-18 
Arvind 
Precise Interrupts
It must appear as if an interrupt is taken between 
two instructions (say Ii and Ii+1) 
 the effect of all instructions up to and including Ii is 
totally complete 
 no effect of any instruction after Ii has taken place 
The interrupt handler either aborts the program or restarts it at I
i+1 . 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L12-5 
Arvind 
Out-of-Order Issue
IF ID WB ALU 
Fadd Issue Mem 
Fmul 
	Issue stage buffer holds multiple instructions waiting 
to issue. 
	Decode adds next instruction to buffer if there is  
space and the instruction does not cause a WAR or 
WAW hazard. 
	Any instruction in buffer whose RAW hazards are  
satisfied can be issued (for now at most one dispatch 
per cycle). On a write back (WB), new instructions 
may get enabled. 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L12-27 
ArvindAverage Run-Length between
Branches 
Average dynamic instruction mix from SPEC92:
SPECint92 SPECfp92 
ALU 39 % 13 % 
FPU Add 20 % 
FPU Mult 13 % 
load 26 % 23 % 
store 9 % 9 % 
branch 16 % 8 % 
other 10 % 12 % 
SPECint92: compress, eqntott, espresso, gcc , li 
SPECfp92: doduc, ear, hydro2d, mdijdp2, su2cor 
What is the average run length between branches 
next lecture: Branch prediction &amp; Speculative excecution 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L12-13 
Arvind 
Dataflow execution
.
.
.
Ins# use exec op p1 src1 p2  src2 
t1 ptr2 t2 
next to 
deallocate
prt1
next
available
Reorder buffer 
Instruction slot is candidate for execution when: 
It holds a valid instruction (use bit is set) 
It has not already started execution (exec bit is clear) 
Both operands are availble (p1 and p2 are set) nt
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823  L12-6
Arvind 
In-Order Issue Limitations: an example
latency
1 LD F2, 34(R2) 1
2 LD F4, 45(R3) long
3 MULTD F6, F4, F2 3
4 SUBD F8, F2, F2 1
5 DIVD F4, F2, F8 4
6 ADDD F10, F6, F4 1
1 2 
3 4 
5 
6 
In-order: 1 (2,1 ) .  .  .  .  .  . 23 4 4 35 . . . 5 6 6
Out-of-order: 1 (2,1 ) 4 4 .  .  .  .  2 3  .  .  3 5 . . . 5 6 6
Out-of-order execution did not al low any significant improvement! 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L12-26 
Arvind 
Branch Penalty
Next fetch 
started 
How many instructions need to be killed on a 
misprediction? 
Modern processors may 
have &gt; 10 pipeline stages 
between next pc calculation 
and branch resolution ! 
I-cache 
Buffer 
Issue 
Buffer 
Func. 
Units 
Arch. 
State Execute Decode 
Result 
Buffer Commit PC 
Branch executed Fetch Fetch 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L12-2 
Arvind 
In-Order Issue Pipeline 
IF ID WB ALU Mem 
Fadd 
Fmul 
Fdiv Issue 
GPRs 
FPRs 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L12-10 
Arvind 
Register Renaming
IF ID WB ALU 
Fadd Issue Mem 
Fmul 
	Decode does register renami ng and adds instructions 
to the issue stage reorder buffer (ROB) 
 renaming makes WAR or WAW hazards 
impossible 
	Any instruction in ROB whose RAW hazards have  
been satisfied can be dispatched. 
 Out-of-order or dataflow execution 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L12-24 
Arvind 
Rollback and Renaming
Reorder 
buffer Register File 
(now holds only 
committed state) 
Load 
Unit FU FU FU Store 
Unit 
&lt; t, result &gt; t1 
t2 
. 
. 
tn Ins# use exec op p1 src1 p2 src2 pd dest data 
Commit 
Register file does not contain renaming tags any more. 
How does the decode stage find the tag of a source register? 
Search the dest field in the reorder buffer 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L12-8 
ArvindOvercoming the Lack of
Register Names 
Floating Point pipelines of ten cannot be kept filled 
with small number of registers. 
IBM 360 had only 4 Floating Point Registers 
Can a microarchitecture use more registers than 
specified by the ISA without loss of ISA 
compatibility ? 
Robert Tomasulo of IBM suggested an ingenious 
solution in 1967 based on on-the-fly register renaming 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L12-19 
Arvind Effect on Interrupts
Out-of-order Completion 
IIIIII1 DIVD
2 LD
3 MULTD
4 DIVD
5 SUBD
6 ADDD
f6, f6, f4 
f2, 45(r3) 
f0, f2, f4 
f8, f6, f2 
f10, f0, f6 
f6, f8, f2 
out-of-order comp 1 2 2 3 	14 3 5 5 46 6 
restore f2 restore f10 
Consider interrupts 
Precise interrupts are difficult to implement at high speed 
- want to start execution of later instructions before 
exception checks finished on earlier instructions 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L12-20 
ArvindException Handling
Commit(In-Order Five-Stage Pipeline) 
Asynchronous 
Interrupts Exc 
D 
PC 
D PC Inst. 
Mem D Decode E M Data 
Mem W + 
Exc 
E 
PC 
E Exc 
M 
PC 
M Cause 
EPC 
Kill D Kill F Kill E Illegal 
Opcode Overflow Data Addr 
Except 
PC Address Kill 
WritebackSelect 
PC Point 
Stage Stage Stage Exceptions Handler 
 Hold exception flags in pipeli ne until commit point (M stage) 
 Exceptions in earlier pipe st ages override later exceptions 
 Inject external interrupts at  commit point (override others) 
 If exception at commit: update Cause and EPC registers, kill 
all stages, inject handler PC into fetch stage 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L12-21 
Arvind 
Phases of Instruction Execution 
Fetch: Instruction bits retrieved 
from cache.I-cache 
Fetch 
Buffer 
Issue 
Buffer 
Func. 
Units 
State Execute: Instructions and operands sent to execution units . When execution completes, all results and 
exception flags are available. issue (aka dispatch) stage buffer 
Buffer Commit: Instruction irrevocably updates 
completion). PC 
Arch. Decode: Instructions placed in appropriate 
Result 
architectural state (aka graduation or 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>28 
Thank you !</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L12-16 
Arvind 
Effectiveness?
Renaming and Out-of-order execution was first 
implemented in 1969 in IBM 360/91 but did not 
show up in the subsequent models until mid-
Nineties. 
Why ? 
Reasons 
1. Exceptions not precise!
2. Effective on a very small class of programs
One more problem needed to be solved 
Control transfers 
October 24, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L12-25 
Arvind 
Renaming Table
Rename 
Table 
Reorder buffer Register 
File 
Load 
Unit FU FU FU Store 
Unit 
&lt; t, result &gt; t1 
t2 
. 
. 
tn Ins# use exec op p1 src1 p2 src2 pd dest data 
Commit r1 t v 
r2 tag 
valid bit 
Renaming table is a cache to speed up register name look up. 
It needs to be cleared after each exception taken. 
When else are valid bits cleared? Control transfers
October 24, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Pipeline Hazards (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l06_pipeline/</lecture_pdf_url>
      <lectureno>L6</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>September 28, 20056.823 L6- 11
Arvind
Feedback to Resolve Hazards
 Detect a hazard and provide feedback to previous 
stages to stall or kill instructionsFB1
stage
1stage
2stage
3stage
4FB2 FB3 FB4
 Controlling a pipeline in this manner works provided 
the instruction at stage i+1 can complete without 
any interference from instructions in stages 1 to i
(otherwise deadlocks may occur)</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>September 28, 20056.823 L6- 32
Arvind
Bypassing
Each stall or kill introduces a bubble in the pipeline
   CPI  &gt;  1time t0 t1 t2 t3 t4 t5 t6 t7 . . . .
(I1)r 1   r0 + 10 IF1ID1EX1MA1WB1
(I2) r4  r1 + 17 IF2ID2ID2ID2ID2EX2MA2WB2
(I3)I F3IF3IF3IF3ID3EX3MA3
(I4) stalled stages IF4ID4EX4
(I5)I F5ID5
time t0 t1 t2 t3 t4 t5 t6 t7 . . . .
(I1) r1  r0 + 10 IF1ID1EX1MA1WB1
(I2) r4 r1 + 17 IF2ID2EX2MA2WB2
(I3)I F3ID3EX3MA3WB3
(I4)I F4ID4EX4MA4WB4
(I5)I F5ID5EX5MA5WB5A new datapath, i.e., a bypass , can get the data from 
the output of the ALU to its input</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1
Arvind
Computer Science and Artificial Intelligence Laboratory
M.I.T.Pipeline Hazards
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>September 28, 20056.823 L6- 18
Arvind
Hazards due to Loads &amp; Stores
...
M[(r1)+7] (r2) 
r4 M[(r3)+5]
...IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
MemorywenopStall Condition
Is there any possible data hazard
in this instruction sequence?What if
(r1)+7 = (r3)+5 ?</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>September 28, 20056.823 L6- 6
Arvind
IRs and Control points
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe
Are control points connected properly?
- ALU instructions 
- Load/Store instructions-W r i t e  b a c k</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>20
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>September 28, 20056.823 L6- 21
Arvind
Complications due to Jumps
I1 096 ADD 
I2 100 J       200
I3 104 ADD
I4 304 ADDkillA jump instruction kills (not stalls)
the following instructionstall
How?I2I1
104IR IR
PCaddr
inst
Inst
Memory0x4
Addnop
IRE MAdd
Jump?PCSrc (pc+4 / jabs / rind/ br)
Note fetching the 
next instruction 
before decode is 
speculation kill</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>September 28, 20056.823 L6- 24
Arvind
Pipelining Conditional Branches
I1 096 ADD 
I2 100 BEQZ r1 200
I3 104 ADD
I4 304 ADDBEQZ?
I2I1
104stall
IR IR
PCaddr
inst
Inst
Memory0x4
Addnop
IRE MAddPCSrc (pc+4 / jabs / rind / br)
nopIRSrcD
Branch condition is not known until 
the execute stage 
what action should be taken in thedecode stage ?A
Y ALUzero?</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>September 28, 20056.823 L6- 8
Arvind
How Instructions can Interact 
with each other in a pipeline
 An instruction in the pipeline may need a 
resource being used by another instruction 
in the pipeline
structural  hazard
 An instruction may produce data that is 
needed by a later instruction
data hazard
 In the extreme case, an instruction may 
determine the next instruction to be 
executed
control hazard  (branches, interrupts,...)</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>September 28, 20056.823 L6- 15
Arvind
CdestInterlocks Control Logic
ignoring jumps &amp; branches
Should we always stall if the rs field matches some rd?IR IR IR
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe31nopstall
Cstallws
rs
rt?we
re1 re2
Crews we ws
CdestCdestwe
not every instruction writes a register  we 
not every instruction reads a register   re</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>September 28, 20056.823 L6- 16
Arvind
Source &amp; Destination Registers
source(s) destination
ALU rd  (rs) func (rt) rs, rt rd
ALUi rt  (rs) op imm rs rt
LW rt  M [(rs) + imm] rs rt
SW M [(rs) + imm]  (rt) rs, rt
BZ cond (rs)
true: PC  (PC) + imm rs
false: PC  (PC) + 4 rs
JP C   (PC) + imm
JAL r31  (PC), PC  (PC) + imm 31  
JR PC  (rs) rs
JALR r31  (PC), PC  (rs) rs 31R-type: op rs rt rd              func
I-type: op rs rt immediate16
J-type: op immediate26</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>September 28, 20056.823 L6- 37
Arvind
Why an Instruction may not be 
dispatched every cycle (CPI&gt;1)
 Full bypassing may be too expensive to 
implement
 typically all frequently  used paths are provided
 some infrequently used bypass paths may increase 
cycle time and counteract th e benefit of reducing CPI
Loads have two cycle latency
 Instruction after load cannot use load result
 MIPS-I ISA defined load delay slots , a software-visible 
pipeline hazard (compiler schedules independent instruction or inserts NOP to avoid hazard).  Removed 
in MIPS-II.
Conditional branches may cause bubbles
 kill following instruction(s) if no delay slots
Machines with software-visible de lay slots may execute significant 
number of NOP instructions inserted by the compiler.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>September 28, 20056.823 L6- 26
Arvind
Pipelining Conditional Branches
I1 096 ADD 
I2 100 BEQZ r1 200
I3 104 ADD
I4 304 ADDstall
IR IR
PCaddr
inst
Inst
Memory0x4
Addnop
IRE MPCSrc (pc+4/jabs/rind/br)
nop A
Y ALUzero?I2 I1
108
I3BEQZ?
Jump?
IRSrcDIRSrcE
If the branch is taken
- kill the two following instructions
- the instruction at the decode stage 
is not valid
stall signal is not validAdd
PC</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>September 28, 20056.823 L6- 34
Arvind
The Bypass Signal
Deriving it from the Stall Signal
ASrc = (rsD=wsE).weE.re1Dwe = Case opcode
ALU, ALUi, LW (ws 0) 
JAL, JALR   on
...  off
No because only ALU and ALUi instructions can benefit 
from this bypassIs this correct?
Split weEinto two components: we-bypass, we-stallstall = ( ((rsD=wsE).weE+ (rsD=wsM).weM+ (rsD=wsW).weW).re1D 
+((rtD=wsE).weE+ (rtD=wsM).weM+ (rtD=wsW).weW).re2D )
ws = Case opcode
ALU  rd
ALUi, LW  rt
JAL, JALR  R31</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>September 28, 20056.823 L6- 5
Arvind
Pipelined Execution: 
ALU Instructions
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe
Not quite correct!
We need an Instruction Reg (IR) for each stage</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>September 28, 20056.823 L6- 14
Arvind
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
MemorywenopInterlock Control Logic
Compare the source registers of the instruction in the decode 
stage with the destination register of the uncommitted 
instructions.stall
Cstallws
rs
rt?</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>September 28, 20056.823 L6- 25
Arvind
Pipelining Conditional Branches
I1 096 ADD 
I2 100 BEQZ r1 200
I3 104 ADD
I4 304 ADDstall
IR IR
PCaddr
inst
Inst
Memory0x4
Addnop
IRE MAddPCSrc (pc+4 / jabs / rind / br)
nopIRSrcD
A
Y ALUzero?
If the branch is taken
- kill the two following instructions
- the instruction at the decode stage 
is not valid
stall signal is not validI2 I1
108
I3BEQZ??</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>September 28, 20056.823 L6- 33
Arvind
Adding a Bypass
ASrc
...
(I1)r 1   r0 + 10
(I2)r 4   r1 + 17r4 r1... r1 ... 
IR IR IR
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe31nopstall
DEM W
When does this bypass help?
r1  M[r0 + 10]
r4  r1 + 17JAL  500
r4  r31 + 17
yes no no</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>September 28, 20056.823 L6- 17
Arvind
Deriving the Stall Signal
Cdest
ws = Case opcode
ALU  rd
ALUi, LW  rt
JAL, JALR  R31
we = Case opcode
ALU, ALUi, LW (ws 0) 
JAL, JALR  on
...  offCre
re1 = Case opcode
ALU, ALUi, 
 on
 off
re2 = Case opcode
 on
 offLW, SW, BZ, 
JR, JALR
J, JAL
ALU, SW
...
Cstall
stall = ((rsD=wsE).weE+ 
(rsD=wsM).weM+ 
(rsD=wsW).weW) . re1D +
((rtD=wsE).weE+ 
(rtD=wsM).weM+ 
(rtD=wsW).weW) . re2D
This is not the full story !</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>September 28, 20056.823 L6- 35
Arvind
Bypass and Stall Signals
we-bypassE= Case opcodeE
ALU, ALUi (ws 0) 
...  off
ASrc = (rsD=wsE).we-bypassE. re1DSplit weEinto two components: we-bypass, we-stall
stall     = ((rsD=wsE).we-stallE+ 
(rsD=wsM).weM+ (rsD=wsW).weW). re1D
+((rtD= wsE).weE+ (rtD= wsM).weM+ (rtD= wsW).weW). re2Dwe-stallE= Case opcodeE
LW (ws 0) 
JAL, JALR  on
...  off</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>September 28, 20056.823 L6- 9
Arvind
Data Hazards
...
r1  r0 + 10
r4  r1 + 17
...r1 is stale. Oops!r1   r4  r1 
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>September 28, 20056.823 L6- 29
Arvind
timet0 t1 t2 t3 t4 t5 t6 t7 . . . .
IFI
1I2I3I4I5
ID I1I2I3 nopI5
EX I1I2nop nop I5
MA      I1I2nop nop I5
WB     I1I2nop nop I5Branch Pipeline Diagrams
(resolved in execute stage)
time
t0 t1 t2 t3 t4 t5 t6 t7 . . . .
(I1) 096: ADD IF1ID1EX1MA1WB1
(I2) 100: BEQZ 200 IF2ID2EX2MA2WB2
(I3) 104: ADD IF3ID3nop nop nop
(I4)108: IF4nop nop nop nop
(I5)304: ADD IF5ID5EX5MA5WB5
Resource 
Usage
nop pipeline bubble</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>September 28, 20056.823 L6- 7
Arvind
Pipelined MIPS Datapath
without jumps
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
MemoryweOpSel
ExtSel BSrcWBSrcMemWriteRegDst
RegWriteFD E M W</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>September 28, 20056.823 L6- 3
Arvind
5-Stage Pipelined Execution
time t0 t1 t2 t3 t4 t5 t6 t7 . . . .
instruction1 IF1ID1EX1MA1WB1
instruction2 IF2ID2EX2MA2WB2
instruction3 IF3ID3EX3MA3WB3
instruction4 IF4ID4EX4MA4WB4
instruction5 IF5ID5EX5MA5WB5Write
-Back 
(WB)I-Fetch 
(IF)Execute 
(EX)Decode, Reg. Fetch 
(ID)Memory 
(MA)addr
wdatardata
Data
Memorywe
ALU
Imm
Ext0x4
Add
addr
rdata
Inst.
Memoryrd1
GPRsrs1
rs2
ws
wdrd2we
IRPC</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>September 28, 20056.823 L6- 4
Arvind
5-Stage Pipelined Execution
Resource Usage Diagram
time t0 t1 t2 t3 t4 t5 t6 t7 . . . .
IF I1I2I3I4I5
ID I1I2I3I4I5
EX I1I2I3I4I5
MA I1I2I3I4I5
WB I1I2I3I4I5ResourcesWrite
-Back 
(WB)I-Fetch 
(IF)Execute 
(EX)Decode, Reg. Fetch 
(ID)Memory 
(MA)addr
wdatardata
Data
Memorywe
ALU
Imm
Ext0x4
Add
addr
rdata
Inst.
Memoryrd1
GPRsrs1
rs2
ws
wdrd2we
IRPC</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>September 28, 20056.823 L6- 19
Arvind
Load &amp; Store Hazards
However, the hazard is avoided because our 
memory system completes writes in one cycle !
Load/Store hazards, even when they do exist, are 
often resolved in the memory system itself.
More on this later in the course....
M[(r1)+7] (r2) 
r4 M[(r3)+5]
...(r1)+7 = (r3)+5  data hazard</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>September 28, 20056.823 L6- 2
Arvind
Technology Assumptions
It makes the following timing assumption validA small amount of very fast memory (caches)
backed up by a large, slower memory 
 Fast ALU (at least for integers)  Multiported Register files (slower!)
tIM tRF tALU tDMtRW
A 5-stage pipelined Harvard architecture will be 
the focus of our detailed design</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>38
Thank you !</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>September 28, 20056.823 L6- 22
Arvind
Pipelining Jumps
I1 096 ADD 
I2 100 J       200
I3 104 ADD
I4 304 ADDkillI2I1
104stall
IR IR
PCaddr
inst
Inst
Memory0x4
Addnop
IRE MAdd
Jump?PCSrc (pc+4 / jabs / rind/ br)
IRSrcD= Case opcodeD
J, JAL  nop
...  IMTo kill a fetched 
instruction -- Insert 
a mux before IR
Any 
interaction 
between 
stall and 
jump?nopIRSrcDI2 I1
304
nop</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>September 28, 20056.823 L6- 13
Arvind
stalled stages
timet0 t1 t2 t3 t4 t5 t6 t7 . . . .
IFI
1I2I3I3I3I3I4I5
ID I1I2I2I2I2I3I4I5
EX I1nop nop nop I2I3I4I5
MA      I1nop nop nop I2I3I4I5
WB     I1nop nop nop I2I3I4I5Stalled Stages and Pipeline Bubbles
time
t0 t1 t2 t3 t4 t5 t6 t7 . . . .
(I1) r1  (r0) + 10 IF1ID1EX1MA1WB1
(I2) r4  (r1) + 17 IF2ID2ID2ID2ID2EX2MA2WB2
(I3)I F3IF3IF3IF3ID3EX3MA3WB3
(I4)I F4ID4EX4MA4WB4
(I5)I F5ID5EX5MA5WB5
Resource 
Usage
nop pipeline bubble</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>September 28, 20056.823 L6- 36
Arvind
Fully Bypassed Datapath
ASrcIR IR IR
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IRALU
Imm
Extrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
Memorywe31nopstall
DEM WPC for JAL, ...
BSrc
Is there still
a need for thestall signal ?
stall =   (rsD=wsE). (opcodeE=LWE).(wsE0).re1D
+ (rtD=wsE). (opcodeE=LWE).(wsE0).re2D</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>September 28, 20056.823 L6- 10
Arvind
Resolving Data Hazards
Freeze earlier pipeline stages until the data 
becomes available interlocks
If data is available somewhere in the datapath
provide a bypass to get it to the right stage
Speculate about the hazard resolution and kill
the instruction later if the speculation is wrong.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>September 28, 20056.823 L6- 12
Arvind
IR IR IR
31
PCA
BY
R
MD1 MD2addr
inst
Inst
Memory0x4
Add
IR
Imm
ExtALUrd1
GPRsrs1
rs2
ws
wdrd2we
wdataaddr
wdatardata
Data 
MemorywenopInterlocks to resolve Data 
Hazards
...
r1  r0 + 10
r4  r1 + 17
...Stall Condition</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>September 28, 20056.823 L6- 28
Arvind
Control Equations for PC and IR 
Muxes
PCSrc = Case opcodeE
BEQZ.z, BNEZ.!z  br
...  
Case opcodeD
J, JAL  jabs
JR, JALR  rind
...  pc+4
IRSrcD= Case opcodeE
BEQZ.z, BNEZ.!z  nop
...  
Case opcodeD
J, JAL, JR, JALR  nop
...  IMGive priority 
to the older instruction,i.e., execute stage instructionover decode
stage instruction
IRSrcE= Case opcodeE
BEQZ.z, BNEZ.!z  nop
...  stall.nop + !stall.IRD</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>September 28, 20056.823 L6- 30
Arvind
 One pipeline bubble can be removed if an extra 
comparator is used in the Decode stage
PCaddr
inst
Inst
Memory0x4
Add
IRIRnopEAddPCSrc (pc+4 / jabs / rind/ br)
rd1
GPRsrs1
rs2
ws
wdrd2we
nopZero detect on 
register file output
Pipeline diagram now same as for jumpsDReducing Branch Penalty
(resolve in decode stage)</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>September 28, 20056.823 L6- 27
Arvind
New Stall Signal
stall = (   ((rsD=wsE).weE+ (rsD=wsM).weM+ (rsD=wsW).weW).re1D 
+ ((rtD=wsE).weE+ (rtD=wsM).weM+ (rtD=wsW).weW).re2D
) . !((opcodeE=BEQZ).z + (opcodeE=BNEZ).!z)
Dont stall if the branch is taken. Why?
Instruction at the decode stage is invalid</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>September 28, 20056.823 L6- 31
Arvind
Branch Delay Slots
(expose control hazard to software)
 Change the ISA semantics so that the instruction that 
follows a jump or branch is always executed
 gives compiler the flexibility to put in a useful instruction where 
normally a pipeline bubble would have resulted.
I1 096 ADD 
I2 100 BEQZ r1 200
I3 104 ADD
I4 304 ADDDelay slot instruction 
executed regardless of 
branch outcome
 Other techniques include branch prediction, 
which can dramatically reduce the branch 
penalty... to come later</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>September 28, 20056.823 L6- 23
Arvind
timet0 t1 t2 t3 t4 t5 t6 t7 . . . .
IFI
1I2I3I4I5
ID I1I2nopI4I5
EX I1I2nopI4I5
MA      I1I2nopI4I5
WB     I1I2nopI4I5Jump Pipeline Diagrams
time
t0 t1 t2 t3 t4 t5 t6 t7 . . . .
(I1) 096: ADD IF1ID1EX1MA1WB1
(I2) 100: J 200 IF2ID2EX2MA2WB2
(I3) 104: ADD IF3nop nop nop nop
(I4)304: ADD IF4ID4EX4MA4WB4
Resource 
Usage
nop pipeline bubble</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Snoopy Protocols (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l19_snoopy_prot/</lecture_pdf_url>
      <lectureno>L19</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L19- 18 
Arvind 
Performance: 
Load-reserve &amp; Store-conditional
The total number of memory (bus) transactions 
is not necessarily reduced, but splitting an 
atomic instruction into load-reserve &amp; store-
conditional: 
 increases bus utilization (and reduces 
processor stall time), especially in split-transaction buses 
 reduces cache ping-pong effect because 
processors trying to acquire a semaphore do 
not have to perform a store each time 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>2 
Bus-Based Protocols:
One derived from the directory based protocol</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L19- 5 
Arvind 
Snooping on the Bus
CPU CPU 
Cache Cache 
addr 
data addr-resp Mem 
Controller M Snooper Snooper 
s-resp s-resp 
	All snoopers listen to the bus requests (ShReq, ExReq, 
WbRes) of each processor 
	A snooper interprets a ShReq as WbReq and ExReq as 
an InvReq or FlushReq (and ignores WbRes) 
	Snoopers response: 
	ok means the processor is in the right state (either it does 
not have the requested data or has it in read only state). 
	retry means the processor state is not yet correct for the 
operation being requested. 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L19- 6 
Arvind 
Typical Processor-Memory Interface
Cache Memory load/store 
buffers 
CPU 
(ShReq, ExReq, WbRes) requested data snooper 
(I/Sh/Ex) (ShReq, ExReq) 
pushout data 
	Distinct address cycle followed by zero or more data 
cycles 
	In effect more than one request per processor can 
be on the bus at the same time  bus tags 
	Snooper must respond immediately either with an 
ok or retry 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L19- 4 
Arvind 
Bus: A Broadcast Medium 
CPU CPU 
Cache Cache 
addr 
data addr-resp Mem 
Controller M Snooper Snooper 
s-resp s-resp 
	Address cycle: two consecutive phases 
	request phase: a processor is selected to issue a 
request which is assigned a bus tag (i.e. the processor 
becomes the bus master 
	response phase: summary of responses from all the 
snoopers is returned to the requesting processor 
	Data cycle (if necessary): 
	The data with its bus tag appear on the data bus 
	The bus tag is retired when the transaction terminates 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Snoopy Protocol
Arvind
Computer Science and Artificial Intelligence Lab
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic
* Note: This lecture note is shorter than usual 
in order to finish the material in the previous lecture.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L19- 11 
Effect of MCs Response on Arvind 
the Bus Master 
Address Bus transaction &lt;tag, a&gt; 
Unanimous-ok 
&lt;a, type&gt;==c2m.first 
Set up for the
data cycleobt.enq (tag, type, a)  	c2m.deq  
Retry 
&lt;a, type&gt;==c2m.first 
 	c2m.deq  
c2m.enq (type, a) 
Data Bus transaction &lt;tag, v&gt;
&lt;tag, type, a &gt;==obt.first 
 	cache.setState(a,type); 
cache.setData(a,v); 
obt.deq randomization 
for retry 
November 16, 2005 
type :: Sh | Ex</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L19- 9 
Arvind 
Snoopers Response: ExReq
P P P P 
&lt; a, ExReq&gt; 
&lt;cache, c2m, obt&gt; 
ExReq when input to a snooper acts like either a InvReq 
or FluShReq 
if a  cache &amp; &lt;Wb, a, - &gt;  c2m 
 	ok 
if cache.state(a)==Sh &amp; &lt;Wb, a, - &gt;  c2m 
 ok ; cache.invalidate(a) 
if cache.state(a)==Ex 
 	retry; cache.invalidate(a); c2m.enq (Wb, a, v) 
if &lt;Wb, a, - &gt;  c2m 
 	retry 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L19- 14 
Arvind 
False Sharing 
state data0 ... dataN blk addr  data1  
A cache block contains more than one word 
Cache-coherence is done at the block-level and 
not word-level 
Suppose M1 writes wordi and M2 writes wordk and 
both words have the same block address. 
What can happen? The block will ping-pong between 
caches unnecessarily 
Solutions: 1. Compiler can pack data differently 
2. A dirty bit per word as opposed to per block
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L19- 10 
Arvind 
Memory Controller Response
CPU CPU 
Cache Cache 
addr 
data addr-resp Mem 
Controller M 
Addr-Request Addr-Response 
&lt;tag,ShReq,a&gt; retry 
u-ok &lt;tag,M[a]&gt; 
&lt;tag,ExReq,a&gt; retry 
u-ok &lt;tag,M[a]&gt; 
&lt;tag,Wb,a&gt; u-ok &lt;tag,Wb,a,data&gt; 
data to be written 
in the memoryNovember 16, 2005 Snooper Snooper 
s-resp s-resp 
Data</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L19- 3 
Arvind 
Bus based SMPs
P P P 
c P 
a &lt; c, Ex&gt; &lt; a, Sh&gt; a1 2 3 4 
a, b, c &lt; a, R(1, 2) &gt; 
&lt; b, R( ) &gt; 
&lt; c, W(4) &gt; 
 In a bus based system, it may be more efficient 
to broadcast the request directly to all caches 
and then collect their responses 
 eliminates the need for home directory 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L19- 17 
Arvind 
Load-reserve &amp; Store-conditional
Special register(s) to hold reservation flag and 
address, and the outcome of store-conditional
Load-reserve(R, a):
&lt;flag, adr&gt;  &lt;1, a&gt;; 
Store-conditional(a, R): 
if &lt;flag, adr&gt; == &lt;1, a&gt; 
then cancel other procs 
reservation on a;
R  M[a]; M[a]  &lt;R&gt;; 
status  succeed; 
else status  fail; 
If the snooper sees a store transaction to the address in the reserve register, th e reserve bit is set to 0 
 Several processors may reserve  a simultaneously 
 These instructions are like ordinary loads and stores 
with respect to the bus traffic 
set to 1. 
November 16, 2005 
 A store (-conditional) is performed only if the reserve bit is</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>cache6.823 L19- 15 
Arvind 
Synchronization and Caches:
Performance Issues 
Processor 1 Processor 2 Processor 3
R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; R  1 
if &lt;R&gt; then goto L; 
&lt;critical section&gt; 
M[mutex]  0; 
CPU-Memory Bus mutex=1 cache cache L: swap(mutex, R); L: swap(mutex, R); L: swap(mutex, R); 
Cache-coherence protocols will cause mutex to ping-pong 
between P1s and P2s caches. 
Ping-ponging can be reduced by first reading the mutex 
location (non-atomically) and executing a swap only if it is 
found to be zero. 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>12 
Bus Occupancy Issues
and 
Synchronization Primitives</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L19- 13 
Arvind 
Intervention: an important optimization
cache-1 A CPU-1 CPU-2 
cache-2 
memory (stale data) A 2 0 0 
CPU-Memory bus 
1 0 0 
On a cache miss, if the data is present in any 
other cache it is faster to supply the data to the 
requester cache from the cache that has it. 
This is done in cooperation with the memory 
controller and by declarin g one of the caches to 
be the owner of the address. 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L19- 7 
Arvind 
Snoopers Input &amp; Output
L1 &amp; Snooper State 
&lt;cache, c2m, obt&gt; 
Outstanding 
bus transactions: 
&lt;ShReq, a&gt; a set of &lt;btag, a&gt; 
&lt;ExReq, a&gt; Needed to capture the 
&lt;WbRes, a, v&gt; data during a data cycle 
	When L1 gets control of the bus, one message from 
c2m is assigned the tag and put on the bus 
	&lt;btag, WbRes, a, v&gt; transactions only affect M 
	&lt;btag, ShReq, a&gt; and &lt;btag, ExReq, a&gt; 
transactions are input to all other Snoopers 
	Each Snooper responds ok or retry 
	MC summarizes s-resps into unanimous-ok or retry 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L19- 16 
Performance Related to Bus Arvind 
occupancy 
In general, a read-modify-write instruction 
requires two memory (b us) operations without 
intervening memory operations by other 
processors
In a multiprocessor setting, bus needs to be 
locked for the entire duration of the atomic read 
and write operation
 expensive for simple buses 
 very expensive for split-transaction buses 
modern processors use 
load-reserve 
store-conditional 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L19- 8 
Arvind 
Snoopers Response: ShReq 
P P P P 
&lt; a, ShReq&gt; 
&lt;cache, c2m, obt&gt; 
ShReq when input to a snooper acts like a WbReq 
if a  cache &amp; &lt;Wb, a, - &gt;  c2m 
 ok 
if cache.state(a)==Sh &amp; &lt;Wb, a, - &gt;  c2m 
 ok 
if cache.state(a)==Ex 
 	retry; cache.setState(a, Sh); c2m.enq (Wb, a, v) 
if &lt;Wb, a, - &gt;  c2m 
 	retry 
November 16, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19 
Next Lecture
Beyond Sequential Consistency:
Relaxed Memory Models</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Virtual Memory Basics (J) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l09_add_trans/</lecture_pdf_url>
      <lectureno>L9</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L9-21 
Emer 
Linear Page Table
 
contains: 
 
exists 
 PPN (physical page 
resident page

a page on the disk

and usage 
 
Base Register 
process changes
VPN Offset 
Virtual address VPN Data Pages 
Offset 
PPN PPN DPN 
PPN PPN PPN Page Table 
DPN PPN 
DPN DPN DPN 
PPN number) for a memory-
whenever active user 
PT Base Register Data word Page Table Entry (PTE) 
A bit to indicate if a page 
DPN (disk page number) for 
Status bits for protection 
OS sets the Page Table 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L9-9 
Emer 
Paged Memory Systems
 Processor generated address can be 
interpreted as a pair &lt;page number, offset&gt; 
page number offset 
 A page table contains the physical address 
0 
1 2 
3 0 
1 
2 
3 
Address Space 
of User-1 Page Table 
of User-1 1 
0 
2 3 of the base of each page 
Page tables make it possible to store the 
pages of a program non-contiguously. 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L9-20 
Emer Modern Virtual Memory Systems
Illusion of a large, private, uniform store 
Protection &amp; Privacy 
several users, each with their private 
address space and one or more shared address spaces 
page table  name space OS 
useri 
Demand Paging 
Primary
Memory StoreSwapping 
Provides the ability to run programs larger than the primary memory 
Hides differences in machine 
configurations
The price is address translation on 
each memory reference VA
 PA 
October 12, 2005 
mapping 
TLB</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L9-14 
Emer 
Manual Overlays 
 Assume an instruction can address all 
the storage on the drum 
	Method 1: programmer keeps track of 
addresses in the main memory and 
initiates an I/O transfer when required 
 Method 2: automatic initiation of I/O 
transfers by software address 
translation Ferranti Mercury 
Brookers interpretive coding, 1960 1956 
Problems? 40k bits 
main 
640k bits 
drum 
Central Store 
Method1: Difficult, error prone Method2: Inefficient 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>33 
Thank you !</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.823 L9-8 
Emer 
Memory Fragmentation
OS 
Space 
16K 
24K 
24K 
32K 
24K user 1 
user 2 
user 3 OS 
Space 
16K 24K 
16K 
32K 
24K user 1 
user 2 
user 3 
user 5 user 4 
8K Users 2 &amp; 5 
leave OS 
Space 
16K 
24K 
16K 
32K 
24K user 1 
user 4 
8K 
user 3 freeUsers 4 &amp; 5 
arrive 
As users come and go, the storage is fragmented. 
Therefore, at some stage programs have to be moved 
around to compact the storage. 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L9-3 
Emer 
Names for Memory Locations
address virtual 
address machine 
language 
address Address 
MappingISA Physical 
Memory 
(DRAM)physical 
 Machine language address 
 as specified in machine code 
 Virtual address 
 ISA specifies translation of machine code address 
into virtual address of program variable (sometime called effective address) 
P h y s i c a l  a d d r e s s
 operating system specifies mapping of virtual 
address into name for a physical memory location 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L9-22 
Emer 
Size of Linear Page Table 
With 32-bit addresses, 4-KB pages &amp; 4-byte PTEs:
 220 PTEs, i.e, 4 MB page table per user 
	4 GB of swap needed to back up full virtual address 
space 
Larger pages?
	Internal fragmentation (Not all memory in a page is 
used) 
	Larger page fault penalty (more time to read from disk) 
What about 64-bit virtual address space??? 
	Even 1MB pages would require 244 8-byte PTEs (35 TB!) 
What is the saving grace ? 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L9-28 
Emer 
Variable Size Page TLB 
Some systems support multiple page sizes. 
VPN offset 
PPN virtual address 
hit? V R W D Tag PPN L 
physical address offset 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L9-7 
Emer 
Separate Areas for Program and Data 
What is an advantage of this separation? Load X 
Program 
Address Space 
Main Memory data Data Bound 
Effective Addr 
Data Base  
+ Bounds 
Violation? 
Program Bound 
Program 
Counter 
Program Base  
+ Bounds 
Violation? 
program segment Register 
Register 
Register 
Register 
Register segment 
(Scheme still used today on Cray vector supercomputers) 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L9-10 
Emer 
Private Address Space per User 
VA1 User 1 
Page Table 
VA1 User 2 
Page Table 
VA1User 3 
Page Table 
Physical
Memory 
free OS 
pages 
 Each user has a page table 
 Page table contains an entry for each user page 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.823 L9-26 
Emer 
TLB Designs 
	Typically 32-128 entries, usually fully associative 
	Each entry maps a large page , hence less spatial locality 
across pages  more likely that two entries conflict 
	Sometimes larger TLBs (256-512 entries) are 4-8 way set-
associative 
	Random or FIFO replacement policy
	No process information in TLB ? 
	TLB Reach: Size of largest virtual address space 
that can be simultaneously mapped by TLB 
Example: 64 TLB entries, 4KB pages, one page per entry 
TLB Reach = _______ ______________________ ? 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L9-23 
Emer 
Hierarchical Page Table
Virtual Address 
31 21 22 11 12 0 
p1 p2 offset 
10-bit 10-bit 
L1 index L2 index offset 
Root of the Current 
Page Table p2 
p1 
(Processor Level 1 
Register) Page Table 
Level 2 
page in primary memory Page Tables 
page in secondary memory 
PTE of a nonexistent page Data Pages 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L9-29 
Emer 
Handling A TLB Miss
Software (MIPS, Alpha) 
TLB miss causes an exception and the operating system 
walks the page tables and reloads TLB. A privileged 
untranslated addressing mode used for walk 
Hardware (SPARC v8, x86, PowerPC) 
A memory management unit (MMU) walks the page tables and reloads the TLB 
If a missing (data or PT) page is encountered during the TLB reloading, MMU gives up and signals a Page-Fault exception for the original instruction 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>06.823 L9-30 
Hierarchical Page Table Walk: Emer 
SPARC v8 
31 11 Virtual Address Index 1 Index 3 Offset 
31 23 17 11 0 
Context 
Table 
Context root ptr 
PTP 
PTP 
PTE Context Table 
L1 Table 
L2 Table 
L3 Table 
Physical Address PPN Offset Index 2  
Register 
Register 
MMU does this table walk in hardware on a TLB miss 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L9-24 
Emer 
Address Translation &amp; Protection
Physical Address Virtual Address 
Address 
Translation Virtual Page No. (VPN) offset 
Physical Page No. (PPN) offset Protection 
Check 
Exception? Kernel/User Mode 
Read/Write 
 Every instruction and data access needs address 
translation and protection checks
A good VM design needs to be fast (~ one cycle) and 
space efficient 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Memory Management:
From Absolute Addresses
to Demand Paging
Joel Emer
Computer Science and Artificial Intelligence Laboratory 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L9-6 
Emer 
Simple Base and Bound Translation
Load X 
Program 
Address 
Space Bound 
Register  Bounds 
Violation? 
Main Memorycurrent 
Base 
Register + Physical Address
Effective 
Address 
Base Physical Address Segment Length 
segment 
Base and bounds registers are visible/accessible only 
when processor is running in the supervisor mode 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L9-27 
Emer 
Variable Sized Page Support
October 12, 2005 
Level 1 
Page Table 
Level 2 
Page Tables 
Data Pages page in primary memory 
large page in primary memory 
page in secondary memory Page Table 
p1 offset 
p2 Virtual Address 
(Processor 
Register) p1 p2 offset 0 31 
10-bit 
L1 index 10-bit 
L2 index 
PTE of a nonexistent page Root of the Current 11 12 21 22</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L9-12 
Emer 
Page Tables in Physical Memory
October 12, 2005 
VA1 
User 1 PT User 1 
PT User 2 
VA1 
User 2</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L9-32 
EmerAddress Translation: 
putting it all together 
Virtual Address 
October 12, 2005 
TLB 
Lookup 
Page Table 
Walk 
Update TLBPage Fault 
(OS loads page) Protection 
Check 
Physical 
Address 
(to cache) miss hit 
page is 
 memory  memory denied permitted 
Protection 
Fault hardware 
hardware or software 
software 
SEGFAULT Where? the</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L9-18 
Emer 
Caching vs. Demand Paging 
CPU cache primary 
memory secondary 
memory 
primary 
memory CPU 
Caching Demand paging 
cache entry page-frame 
cache block (~32 bytes)       page (~4K bytes) 
cache miss (1% to 20%)       page miss (&lt;0.001%) 
cache hit (~1 cycle) page hit (~100 cycles) 
cache miss (~100 cycles)     page miss(~5M cycles) 
a miss is handled a miss is handled 
in hardware mostly in software 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L9-13 
Emer 
A Problem in Early Sixties
	There were many applications whose data 
could not fit in the main memory, e.g., payroll 
	Paged memory system reduced fragmentation but still 
required the whole program to be resident in the main 
memory 
	Programmers moved the data back and forth 
from the secondary store by overlaying it 
repeatedly on the primary store 
tricky programming! 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L9-17 
Emer 
Atlas Demand Paging Scheme
	On a page fault: 
	Input transfer into a free page is initiated 
	The Page Address Register (PAR) is updated 
	If no free page is left, a page is selected to be 
replaced (based on usage) 
	The replaced page is written on the drum 
 to minimize drum latenc y effect, the first empty 
page on the drum was selected 
	The page table is updated to point to the new 
location of the page on the drum 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L9-4 
Emer 
Absolute Addresses 
EDSAC, early 50s 
virtual address = physical memory address 
	Only one program ran at a time, with 
unrestricted access to entire machine (RAM + 
I/O devices) 
	Addresses in a program depended upon where 
the program was to be loaded in memory 
 But it was more convenient for programmers 
to write location-independent subroutines
How could location independence be achieved? 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.823 L9-25 
Emer 
Translation Lookaside Buffers
Address translation is very expensive!
In a two-level page table, each reference 
becomes several memory accesses
Solution: Cache translations in TLB
TLB hit  Single Cycle Translation
TLB miss  Page Table Walk to refill 
October 12, 2005 
VPN offset 
V R W D tag PPN 
PPN virtual address 
hit? (VPN = virtual page number) 
(PPN = physical page number) 
physical address offset</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L9-31 
Emer 
Translation for Page Tables 
 Can references to page tables TLB miss 
October 12, 2005 
 
(in virtual space) 
Data Pages User PTE Base 
System PTE Base Can this go on forever? 
User Page Table 
System Page Table 
(in physical space)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L9-2 
Emer 
Memory Management
T h e  F i f t i e s 
- Absolute Addresses
- Dynamic address translation
T h e  S i x t i e s
- Paged memory systems and TLBs - Atlas Demand paging 
 Modern Virtual Memory Systems 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L9-15 
Emer 
Demand Paging in Atlas (1962)
A page from secondary 
storage is brought into the 
primary storage whenever 
it is (implicitly) demanded by the processor. 
Tom Kilburn 
Primary memory as a cache 
for secondary memory 
User sees 32 x 6 x 512 words of storage Secondary 
(Drum) 
32x6 pages Primary 
32 Pages 
512 words/page 
Central 
Memory 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L9-5 
Emer 
Dynamic Address Translation
Motivation 
In the early machines, I/O operations were slow 
and each word transferred involved the CPU 
Higher throughput if CPU and I/O of 2 or more programs were overlapped.  How? 
 multiprogramming 
Location independent programs 
Programming and storage management ease 
 need for a base register 
Protection 
Independent programs should not affect
each other inadvertently
 need for a bound register 
prog1 
prog2 
Physical Memory 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L9-11 
Emer 
Where Should Page Tables Reside?
	Space required by the page tables (PT) is 
proportional to the address space, number 
of users, ... 
 	Space requirement is large 
 	Too expensive to keep in registers 
	Idea: Keep PT of the current user in special 
registers 
	may not be feasible for large page tables 
	Increases the cost of context swap 
	Idea: Keep PTs in the main memory
	needs one reference to retrie ve the page base address 
and another to access the data word 
	doubles the number of memory references! 
October 12, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L9-16 
Emer 
Hardware Organization of Atlas 
Initial 
Address 
Decode 16 ROM pages 
sec 
2 subsidiary pages 
1.4 sec 
Main 
32 pages 
1.4 sec Drum (4) 8 Tape decks 
88 sec/word 48-bit words 
512-word pages 
1 Page Address 
Register (PAR) Effective 
Address (not swapped) 
(not swapped) 
0 
31 PARs 0.4 ~1 
192 pages system code 
system data 
per page frame &lt;effective PN , status&gt; 
Compare the effective page address against all 32 PARs 
match  normal access 
no match  page fault 
save the state of the partially executed 
instruction 
October 12, 2005</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Microprogramming (A) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-823-computer-system-architecture-fall-2005/resources/l04_microprog/</lecture_pdf_url>
      <lectureno>L4</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>6.823 L4- 11 
Arvind 
Microprogram Fragments
instr fetch: 	 MA  PC 
A  PC can be 
treated as IR  Memory 
a macro PC  A + 4 
dispatch on OPcode 
ALU: 	 A  Reg[rs]
B  Reg[rt] 
Reg[rd]  func(A,B) 
do instruction fetch 
ALUi: 	 A  Reg[rs]
B  Imm sign extension ...
Reg[rt]  Opcode(A,B) 
do instruction fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.823 L4- 20 
Arvind 
Jump Logic
PCSrc = Case JumpTypes 
next PC+1 
spin  if (busy) then PC else  PC+1 
fetch  absolute 
dispatch  op-group 
feqz  if (zero) then absolute else  PC+1 
fnez  if (zero) then PC+1 else absolute 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.823 L4- 30 
Arvind 
Nanocoding
Exploits recurring 
control signal patterns 
in code, e.g., 
ALU0 A  Reg[rs] 
...
ALUi0 A  Reg[rs]
...

nanoaddress code 
next-state 
address PC (state) 
data code ROM 
nanoinstruction ROM 
	MC68000 had 17-bit code containing either 10-bit jump or 9
bit nanoinstruction pointer 
 Nanoinstructions were 68 bits wide, decoded to give 196 
control signals 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>6.823 L4- 37 
Arvind 
Modern Usage 
 Microprogramming is far from extinct 
 Played a crucial role in micros of the Eighties
Motorola 68K series
Intel 386 and 486
 Microcode pays an assisting role in most modern
CISC micros (AMD Athlon, Intel Pentium-4 ...)
 Most instructions are executed directly, i.e., with hard-wired 
control 
 Infrequently-used and/or complic ated instructions invoke the 
microcode engine 
 Patchable microcode common for post-fabrication 
bug fixes, e.g. Intel Pentiums load code patches 
at bootup 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.823 L4- 10 
Arvind 
Instruction Execution 
Execution of a MIPS instruction involves
1. instruction fetch
2. decode and register fetch
3. ALU operation
4. memory operation (optional)
5. write back to register file (optional)
+ the computation of the 
next instruction address 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.823 L4- 27 
ArvindMem-Mem ALU Instructions: 
MIPS-Controller-2 
Mem-Mem ALU op M[(rd)]  M[(rs)] op M[(rt)] 
ALUMM0 MA  Reg[rs] next 
ALUMM1 A  Memory spin 
ALUMM2 MA  Reg[rt] next 
ALUMM3 B  Memory spin 
ALUMM4 MA Reg[rd] next 
ALUMM5 Memory  func(A,B) spin
ALUMM6 fetch
Complex instructions usually do not require datapath 
modifications in a microprogrammed implementation 
-- only extra space for the control program 
Implementing these instructions using a hardwired controller is difficult without datapath modifications 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>25 
Five-minute break to stretch your legs</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>2 6.823 L4- 26 
ArvindImplementing Complex
Instructions
Opcode zero? Busy? 
ldIR OpSel ldA ldB 32(PC) ldMA31(Link)rd rt2 rs 
RegSel MA 
3rd rt A B addr addr IR rs 
32 GPRs 
ExtSel + PC ... Memory MemWrt Imm ALU RegWrt
Ext control ALU 32-bit Reg enReg 
data data enMem enImm enALU 
Bus 32 
rd  M[(rs)] op (rt)
M[(rd)]  (rs) op (rt)
M[(rd)]  M[(rs)] op M[(rt)]
Reg-Memory-src ALU op 
Reg-Memory-dst ALU op Mem-Mem ALU op 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.823 L4- 3 
Arvind 
Microarchitecture: Implementation of an ISA
Controller 
Data 
path control 
points status 
lines 
Structure: How components are connected. 
Static 
Behavior: How data moves between components 
Dynamic 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.823 L4- 32 
Arvind 
Microprogramming in IBM 360 
M30 M40 M50 M65 
Datapath 
width (bits) 8 16 32 64 
inst width 
(bits) 50 52 85 87 
code size 
(K minsts) 4 4 2.75 2.75 
store 
technology CCROS TCROS BCROS BCROS 
store cycle 
(ns) 750 625 500 200 
memory cycle (ns) 1500 2500 2000 750 
Rental fee 
($K/month) 4 7 15 35 
Only the fastest models (75 and 95) were hardwired 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.823 L4- 22 
Arvind 
Load &amp; Store: MIPS-Controller-2 
State Control points next-state 
LW0 A  Reg[rs] next 
LW1 B  sExt16(Imm) next 
LW2 MA  A+B next 
LW3 Reg[rt]  Memory spin 
LW4 fetch 
SW0 A  Reg[rs] next 
SW1 B  sExt16(Imm) next 
SW2 MA  A+B next 
SW3 Memory  Reg[rt] spin 
SW4 fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.823 L4- 6 
Arvind 
The MIPS32 ISA
 Processor State
32 32-bit GPRs, R0 always contains a 0 
16 double-precision/32 single-precision FPRs 
FP status register, used for FP compares &amp; exceptions 
PC, the program counter 
some other special registers See H&amp;P p129
137 &amp; Appendix 
 Data types	 C (online) for full 
8-bit byte, 16-bit half word description 
32-bit word for integers 
32-bit word for single precision floating point
64-bit word for double precision floating point
 Load/Store style instruction set
data addressing modes- immediate &amp; indexed branch addressing modes- PC relative &amp; register indirect Byte addressable memory- big-endian mode 
All instructions are 32 bits 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.823 L4- 33 
Arvind 
Microcode Emulation
	IBM initially miscalculated the importance of 
software compatibility with earlier models 
when introducing the 360 series 
	Honeywell stole some IBM 1401 customers by 
offering translation software (Liberator) for 
Honeywell H200 series machine 
	IBM retaliated with optional additional 
microcode for 360 series that could emulate 
IBM 1401 ISA, later extended for IBM 7000 
series 
	one popular program on 1401 was a 650 simulator, so 
some customers ran many 650 programs on emulated 
1401s 
 (650 simulated on 1401 emulated on 360) 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>6.823 L4- 36 
Arvind 
Microprogramming: late seventies
	With the advent of VLSI technology 
assumptions about ROM &amp; RAM speed 
became invalid 
 Micromachines became more complicated
	Micromachines were pipelined to overcome slower 
ROM 
	Complex instruction sets led to the need for 
subroutine and call stacks in code 
	Need for fixing bugs in control programs was in 
conflict with read-only nature of ROM 
	WCS (B1700, QMachine, Intel432, ) 
	Introduction of caches and buffers, especially 
for instructions, made multiple-cycle execution of reg-reg instructions unattractive 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.823 L4- 9 
Arvind 
Memory Module
Enable Write(1)/Read(0)RAM 
din we addr busy 
bus dout 
Assumption: Memory operates asynchronously 
and is slow as compared to Reg-to-Reg transfers 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>38 
Thank you !</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>6.823 L4- 31 
Arvind 
Some more history  
 IBM 360 
 Microcoding through the seventies 
 Microcoding now 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.823 L4- 2 
Arvind 
ISA to Microarchitecture Mapping
	An ISA often designed for a particular 
microarchitectural style, e.g.,
C I S C  microcoded
R I S C  hardwired, pipelined
V L I W  fixed latency in-order pipelines
J V M  software interpretation
	But an ISA can be implemented in any 
microarchitectural style
 Pentium-4: hardwired pipelined CISC (x86) machine (with 
some microcode support) 
 This lecture: a microcod ed RISC (MIPS) machine 
 Intel will probably eventually  have a dynamically scheduled 
out-of-order VLIW (IA-64) processor 
 PicoJava: A hardware JVM processor 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Microinstruction: register to register transfer (17 control signals) Bus A B OpSel ldA ldB 
ALU 
enALU ALU 
control 2 
rs rt rd 
ExtSel IR ldIR 
Imm 
Ext 
enImm 2 6.823 L4- 8 
Arvind 
A Bus-based Datapath for MIPS
MA
addr
dataMemoryOpcode zero? Busy?
ldMA
MemWrt 
enMem 
32 RegWrt 
enReg addr 
data rs rt rd 32(PC)
31(Link) 
RegSel 
32 GPRs 
32-bit Reg 3 
+ PC ... 
MA  PC means RegSel = PC;  enReg=yes; ldMA= yes 
B  Reg[rt] means RegSel = rt;  enReg=yes; ldB = yes 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.823 L4- 13 
Arvind 
MIPS Microcontroller: first attempt 
next 
state PC (state) Opcode 
zero? 
Busy (memory) 
s 
s 6 
Program ROM addr 
data latching the inputs 
may cause a one-cycle delay 
= 2(opcode+status+s) words How big 
is s? 
ROM size ? 
Word size ? 
= control+s bits 
Control Signals (17) 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.823 L4- 24 
Arvind 
Jumps: MIPS-Controller-2 
State Control points next-state 
J0 A  PC next 
J1 B  IR next 
J2 PC  JumpTarg(A,B) fetch 
JR0 A  Reg[rs] next 
JR1 PC  A fetch 
JAL0 A  PC next 
JAL1 Reg[31]  A n e x t 
JAL2 B  IR next 
JAL3 PC  JumpTarg(A,B) fetch 
JALR0 A  PC next 
JALR1 B  Reg[rs] next 
JALR2 Reg[31]  A n e x t 
JALR3 PC  B fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.823 L4- 21 
Arvind 
Instruction Fetch &amp; ALU: MIPS-Controller-2
State Control points next-state 
fetch0 MA  PC next 
fetch1 IR  Memory spin 
fetch2 A  PC next 
fetch3 PC  A + 4 dispatch 
... 
ALU0 A  Reg[rs] next 
ALU1 B  Reg[rt] next 
ALU2 Reg[rd] func(A,B) fetch 
ALUi0 A  Reg[rs] next 
ALUi1 B  sExt16(Imm) next 
ALUi2 Reg[rd]  Op(A,B) fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.823 L4- 28 
Arvind 
Performance Issues 
Microprogrammed control 
 multiple cycles per instruction 
Cycle time ? 
tC &gt; max(treg-reg, tALU, tROM, tRAM) 
Given complex control, tALU &amp; tRAM can be broken 
into multiple cycles. However, tROM cannot be 
broken down. Hence 
tC &gt; max(treg-reg, tROM) 
Suppose 10 * tROM &lt; tRAM 
Good performance, relative to the single-cycle 
hardwired implementation, can be achieved 
even with a CPI of 10 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.823 L4- 4 
Arvind 
Microcontrol Unit Maurice Wilkes, 1954 
Embed the control logic state table in a memory array
op conditional
code flip-flop
Matrix A Matrix B 
Decoder Next state 
 address 
to Control lines  
ALU, MUXs, Registers 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.823 L4- 17 
Arvind 
Size of Control Store
size = 2(w+s) x (c + s) 
data status &amp; opcode 
addr 
next PC 
Control signals PC / 
w 
/ s 
/ c Control ROM 
MIPS: w = 6+2 c = 17 s = ? 
no. of steps per opcode = 4 to 6 + fetch-sequence 
no. of states  (4 steps per op-group ) x op-groups 
+ common sequences
= 4 x 8 + 10 states = 42 states  s = 6 
Control ROM = 2(8+6) x 23 bits  48 Kbytes 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.823 L4- 18 
Arvind 
Reducing Control Store Size 
Control store has to be fast expensive 
 Reduce the ROM height (= address bits) 
 reduce inputs by extra external logic 
each input bit doubles the size of the 
control store 
 reduce states by grouping opcodes 
find common sequences of actions 
 condense input status bits 
combine all exceptions into one, i.e., 
exception/no-exception 
 Reduce the ROM width
 restrict the next-state encoding 
Next, Dispatch on opcode, Wait for memory, ... 
 encode control signals (vertical microcode) 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.823 L4- 34 
Microprogramming thrived in the Arvind 
Seventies 
	Significantly faster ROMs than DRAMs were 
available 
	For complex instruction sets, datapath and 
controller were cheaper and simpler 
	New instructions , e.g., floating point, could 
be supported without datapath modifications 
	Fixing bugs in the controller was easier
 ISA compatibility across various models 
could be achieved easily and cheaply
Except for the cheapest and fastest machines, 
all computers were microprogrammed 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.823 L4- 29 
Arvind 
Horizontal vs Vertical  Code 
Bits per Instruction 
# Instructions 
 Horizontal code has wider instructions 
 Multiple parallel operations per instruction 
 Fewer steps per macroinstruction 
 Sparser encoding  more bits 
V e r t i c a l code has narrower instructions
	Typically a single datapath operation per instruction 
s e p a r a t e instruction for branches 
 More steps to per macroinstruction
M o r e  c o m p a c t  less bits
 Nanocoding 
 Tries to combine best of horizontal and vertical code 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.823 L4- 16 
Arvind 
Microprogram in the ROM Cont.
State Op zero? busy Control points next-state 
ALUi0 * * *  A  Reg[rs] ALUi1 
ALUi1 sExt * * B  sExt16(Imm) ALUi2 
ALUi1 uExt * * B  uExt16(Imm) ALUi2 
ALUi2 * * *  Op(A,B) 0 
... 
J0 * * * A  PC J1 
J1 * * * B  IR J2 
J2 * * *  0 
... 
beqz0 * * *  A  Reg[rs] 1 
beqz1 * * A  PC beqz2 
beqz1 * * .... fetch0 
beqz2 * * * B  sExt16(Imm) beqz3 
beqz3 * * *  A+B 0 
... 
JumpTarg(A,B) = {A[31:28],B[25:0],00} 
September 21, 2005  R e g [ r d ] fetch
P C JumpTarg(A,B) fetch
beqz
y e s 
no 
P C fetch</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.823 L4- 35 
Arvind 
Writable Control Store (WCS) 
	Implement control store with SRAM not ROM 
	MOS SRAM memories now almost as fast as control store 
(core memories/DRAMs were 2-10x slower) 
	Bug-free microprograms difficult to write 
	User-WCS provided as option on several 
minicomputers 
 Allowed users to change microcode for each process 
U s e r - W C S failed 
	Little or no programming tools support 
	Difficult to fit software into small space 
	Microcode control tailored to original ISA, less useful for 
others 
	Large WCS part of processor state - expensive context 
switches 
	Protection difficult if user can change microcode 
	Virtual memory required restartable microcode 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.823 L4- 7 
Arvind 
MIPS Instruction Formats 
6 5 5  5 5 6 
0 rs rt rd  0 func 
opcode rs rt immediate rd  (rs) func (rt) ALU 
rt  (rs) op immediate ALUi 
6 5 5 16 
Mem     M[(rs) + displacement] 
6 5 5 16 
6 5 5  16 
6 26 opcode rs rt displacement 
opcode rs offset BEQZ, BNEZ 
opcode rs JR, JALR 
opcode offset J, JAL 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.823 L4- 5 
Arvind 
Microcoded Microarchitecture
Memory 
(RAM) Datapath controller 
(ROM) 
Addr Data zero? busy? 
opcode 
enMem 
MemWrt holds fixed 
microcode instructions 
holds user program 
written in macrocode 
instructions (e.g., 
MIPS, x86, etc.) 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.823 L4- 15 
Arvind 
Microprogram in the ROM
State Op zero? busy Control points next-state 
fetch0 * * * MA  PC fetch1 
fetch1 * * yes .... fetch1 
fetch1 * *  Memory fetch2 
fetch2 * * * A  PC fetch3 
fetch3 ALU * * PC  A + 4 ALU0 
fetch3 ALUi * * PC  A + 4 ALUi0 
fetch3 LW * * PC  A + 4 LW0 
fetch3 SW * * PC  A + 4 SW0 
fetch3 J * *  A + 4 J0 
fetch3 JAL * * PC  A + 4 JAL0 
fetch3 JR * * PC  A + 4 JR0 
fetch3 JALR * * PC  A + 4 JALR0 
fetch3 beqz * * PC  A + 4 beqz0 
... 
ALU0 * * *  A  Reg[rs] ALU1 
ALU1 * * *  B  Reg[rt] ALU2 
ALU2 * * *  0 
September 21, 2005 n o  I R 
 P C 
 R e g [ r d ] func(A,B) fetch</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.823 L4- 14 
Arvind 
Microprogram in the ROM worksheet 
State Op zero? busy Control points next-state 
fetch0 * * * MA  PC fetch1 
fetch1 * * yes .... fetch1 
fetch1 * * n o  I R  Memory fetch2 
fetch2 * * * A  PC fetch3 
fetch3* * *  P C  A + 4 ? 
fetch3 ALU * * PC  A + 4 ALU0 
ALU0* * *  A  Reg[rs] ALU1 
ALU1 * * *  B  Reg[rt] ALU2 
ALU2 * * *  R e g [ r d ]  func(A,B) fetch0 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.823 L4- 12 
Arvind 
Microprogram Fragments (cont.)
LW: 	 A  Reg[rs]
B  Imm
MA  A + B
Reg[rt]  Memory
do instruction fetch 
J: 	 A  PC JumpTarg(A,B) = 
{A[31:28],B[25:0],00}B  IR
PC  JumpTarg(A,B)
do instruction fetch 
beqz:	 A  Reg[rs] 
If zero?(A) then go to bz-taken 
do instruction fetch 
bz-taken:	 A  PC
B  Imm &lt;&lt; 2
PC  A + B
do instruction fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 
Microprogramming
Arvind
Computer Science &amp; Artificial Intelligence Lab 
M.I.T.
Based on the material prepared by
Arvind and Krste Asanovic</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.823 L4- 23 
Arvind 
Branches: MIPS-Controller-2 
State Control points next-state 
BEQZ0 A  Reg[rs] next 
BEQZ1 fnez 
BEQZ2 A  PC next 
BEQZ3 B  sExt16(Imm&lt;&lt;2) next 
BEQZ4 PC  A+B fetch 
BNEZ0 A  Reg[rs] next 
BNEZ1 feqz 
BNEZ2 A  PC next 
BNEZ3 B  sExt16(Imm&lt;&lt;2) next 
BNEZ4 PC  A+B fetch 
September 21, 2005</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.823 L4- 19 
Arvind 
MIPS Controller V2

next | spin 
| fetch | dispatch 
| feqz | fnez Control ROM address 
data +1 Opcode ext 
PC (state) 
jump 
logic zero PC PC+1 absolute (start of a predetermined sequence) 
op-group 
busy PCSrcinput encoding 
reduces ROM height 
next-state encoding 
reduces ROM width 
September 21, 2005 JumpType = 
Control Signals (17)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
