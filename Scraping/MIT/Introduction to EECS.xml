<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/</course_url>
    <course_title>Introduction to EECS II: Digital Communication Systems</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Electrical Engineering </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>          6.02 Lecture 16: More on modulation/demodulation
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec16/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>Quadrature Demodulation  
If we let 
w[n]=I[n]+jQ[n]
then 
w[n]
6.02 Fall 2012 Lecture 16 Slide #18 =I[n]2+Q[n]2
=|x[nD]| c os2+sin2
=|x[nD]|x[n-D]cos( ) x[n-D]sin( ) 
I  jQ 
Constellation diagrams: 
transmitter receiver I Q 
I Q x[n] = { 0, 1 } 
OK for recovering x[n] if it 
never goes negative, as in  on-off keying</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>At the Receiver: Demodulation
In principle, this is (as easy as) modulation again: 
If the received signal is  
                         r[n] = x[n]cos( cn) = t[n],  
(no distortion or noise) then simply compute 
                        d[n] = r[n]cos( cn) 
                               = x[n]cos2(cn) 
                               = 0.5 {x[n] + x[n]cos(2 cn)} 
If there is distortion (i.e., r[n]  t[n]), then write y[n] instead of  x[n] (and hope that in the noise-free case y[.] is related to x[.] by 
an approximately LTI relationship!) 
 
What does the spectrum of d[n], i.e., D( ), look like?  
What constraint on the bandwidth of x[n] is needed for perfect 
recovery of x[n]?           
6.02 Fall 2012 Lecture 16 Slide #10</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Fixing Phase Problems in the Receiver
So phase errors and channel delay both result in a scaling of the 
output amplitude, where the magnitude of the scaling cant necessarily be determined at system design time: 
channel delay varies on mobile devices 
phase difference between transmitter and receiver is arbitrary 
 One solution:  quadrature demodulation 
6.02 Fall 2012 Lecture 16 Slide #17 
cos(cn) LPF I[n] = x[n-D]cos( ) 
 LPF Q[n] = x[n-D]sin() From channel  =  - cD 
sin(cn)</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>AM Radio
 
AM radio stations are on 520  1610 kHz (medium wave)  in the US, with carrier frequencies of different stations spaced 10 kHz apart.  Physical effects very much affect operation. e.g., EM signals  at these frequencies propagate much further at night (by  skywave through the ionosphere) than during the day (100s  of miles by groundwave diffracting around the earths  surface), so transmit power may have to be lowered at night!   
6.02 Fall 2012 Lecture 16 Slide #23</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Phase Error In Demodulation
When the receiver oscillator is out of phase with the transmitter: 
d[n]=r[n]cos(cn)=x[n]cos(cn)cos(cn)
6.02 Fall 2012 Lecture 16 Slide #13 y[n]=x[n].cos()
So a phase error of  results in amplitude scaling by cos(). 
 Note: in the extreme case where =/pi1/2, we are demodulating by a sine rather than a cosine, and we get y[n]=0 . But  
cos(cn).cos( cn)=0.5{cos()+cos(2 cn)}
It follows that the demodulated output, after the LPF of  gain 2, is</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>QPSK Modulation  
6.02 Fall 2012 Lecture 16 Slide #20 
cos(cn) 
sin(cn) + t[n] I[n] 
Q[n] I Q 
(-1,1) (1,1) 
(-1,-1) (1,-1) We can use the quadrature scheme at the transmitter too: 
Samples from first bit stream 
Samples from second bit stream</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Fast Fourier Transform (FFT) to compute 
samples of the DTFT  for 
signals of finite dura  tion
P11 X(k)=x[m]ejkm, x[n]=
m=0 P
6.02 Fall 2012 Lecture 16 Slide #2 For an x[n] that is zero outside of the interval [0,L-1] , choose  
P  L (with P preferably a power of 2; well assume that its at 
least a multiple of 2, i.e., even):  
(P/2)1
X(k)ejkn  
k=P/2
where k = k(2 /P), and k ranges from P/2 to (P/2)1, or over 
any P successive integers. Simpler notation: X(k) = Xk</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Multiple Transmitters:  
 Frequency Division Multiplexing (FDM)
6.02 Fall 2012 xB[n] 
xR[n] 
xG[n] cos(Bn) 
cos(Rn) 
cos(Gn) 
Lecture 16 Slide #22 +
Channel performs addition by 
superposing signals (voltages) from different frequency bands. Choose bandwidths and cs so as to 
avoid overlap!  Once signals combine at a given frequency, cant be undone LPF cutoff needs to be half the minimum separation between carriers 
-
G -R -B B R G</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>produces 
6.02 Fall 2012 Lecture 16 Slide #15 
Note combining of signals around 0 
results in cancellation!</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Demodulation + LPF  
6.02 Fall 2012 Lecture 16 Slide #12 r[n] 
cos(c n) d[n] LPF x[n] 
Cutoff @  c 
Gain = 2</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.02 Fall 2012 Lecture 16 Slide #9 
0 Hz 1000 Hz 1000 Hz 
Not great band-limiting, but maybe we can get away with it!</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>http://en.wikipedia.org/wiki/Phase-shift_keying 
6.02 Fall 2012 Lecture 16 Slide #21 Phase Shift Keying underlies many 
 familiar modulation schemes
The wireless LAN  standard, IEEE 802.11b-1999 , uses a variety of different PSKs depending on the data-
rate required. At the basic-rate of 1 Mbit /s, it uses DBPSK (differential BPSK). To provide the extended-rate 
of 2 Mbit/s, DQPSK is used. In reaching 5.5 Mbit/s and the full-rate of 11 Mbit/s, QPSK is employed, but has 
to be coupled with complementary code keying . The higher-speed wireless LAN standard, 
IEEE 802.1
1g-2003  has eight data rates: 6, 9, 12, 18, 24, 36, 48 and 54 Mbit/s. The 6 and 9 Mbit/s modes
use OFDM  modulation where each sub-carrier is BPSK modulated. The 12 and 18 Mbit/s modes use 
OFDM with QPSK. The fastest four modes use OFDM with forms of quadrature amplitude modulation .
 Because of its simplicity BPSK is appropriate for low-cost passive transmitters, and is used in RFID  standards such as ISO/IEC 14443 which has been adopted for biometric passports , credit cards 
such as American Express 's ExpressPay , and many other applications.
 
Bluetooth  2 will use (p/4)-DQPSK at its lower rate (2 Mbit/s) and 8-DPSK at its higher rate (3 Mbit/s) when 
the link between the two devices is sufficiently robust. Bluetooth 1 modulates with Gaussian minimum-shift keying , a binary scheme, so either modulation choice in version 2 will yield a higher 
data-rate. A similar technology, IEEE 802.15.4  (the wireless standard used by ZigBee ) also relies on PSK. 
IEEE 802.15.4 allows the use of two frequency bands: 868915 MHz using BPSK and at 2.4 GHz  using 
OQPSK.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 16 Slide #1 
6.02 Fall 2012 
Lecture #16 
DTFT vs DTFS 
Modulation/Demodulation 
Frequency Division Multiplexing 
(FDM)</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Why the periodicity of x
6.02 Fall 2012 Lecture 16 Slide #6 x[n] is irrelevant in  
 many applications
h[.] x[n] y[n] 
 
Suppose x[n] is nonzero only over the time interval [0 , nx], 
and h[n] is nonzero only over the time interval [0 , nh] .  
 
In what time interval can the non-zero values of y[n] be guaranteed to lie? The interval [0 , n
x + nh] . 
Since all the action we are interested in is confined to this  interval, choose P  1  n
x + nh .  
 Its now irrelevant what happens outside [0,P1]. So we can use  the FFT to go back and forth between samples of X( ), (), Y() /nonmarkingreturn
in the frequency domain and time-domain behavior in [0,P-1].</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 20: Network routing (with failures)
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec20/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>11/26/12 
1 6.02 Fall 2012 Lecture 20, Slide #1 
6.02 Fall 2012 
Lecture #20 
 Failure-resilient Routing F  ailures
Problems:Links and switches could fail 
Advertisements could get lost 
Routing loop 
A sequence of nodes on forwarding path that has a 
cycle (so packets will never reach destination) 
Dead-end: route does not actually reach destination 
Loops and dead-ends lead to routes not being valid 
Solution 
HELLO protocol to detect neighbor liveness 
Periodic advertisements  from nodes 
Periodic integration at nodes 
Leads to eventual convergence to correct state 
(see Chapter 18) 
6.02 Fall 2012 Lecture 20, Slide #2 
Routing Loop in Link-State Protocol  
A B 
path 
a A. 
ils. D B to D is vi
Link AD fa
As LSA to B is lost. 
A now uses B to get to D. 
But B continues to use A. Routing loop! 
Must wait for eventual arrival  
of correct LSAs to fix loop. 
6.02 Fall 2012 Lecture 20, Slide #3 Distance-Vector: Pros, Cons, and Loops 
+ Simple protocol 
+ Works well for small networks 
- Works only on small networks 
L1 L3 SA C L1 
L2 L2 
E:cost=3 E:cost=X5 sL2 E Counting to infinity! L1 
E:cost=4 E:cost=X6 L2 L1 
L1 L3 B D L2 uppose link AC fails. 
When A discovers failure, it 
ends E: cost = INFINITY to B. 
B advertises E: cost=2 to A 
A sets E: cost=3 in its table 
Now suppose link BD fails. 
B discovers it, then sets 
E: cost = INFINITY. But what if A had advertised Sends info to A, A sets to B before B advertised to A? E: cost = INFINITY. 
6.02 Fall 2012 Lecture 20, Slide #4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>11/26/12 
2 Fixing Count to Infinity with Path Vector 
 Routing
In addition to (or instead of) reporting costs, 
advertise the path discovered incrementally by the 
Bellman-Ford update rule 
Called path-vector 
Modify Bellman-Ford update with new rule: a node 
should ignore any advertised route that contains 
itself in the advertisement 
6.02 Fall 2012 Lecture 20, Slide #5 Path Vector Routing 
E: l1; cost=2; path=[CE] E: l1; cost=1; path=[E] 
[CE] 
A C l1 /g3l1 l3 To reach E, come this way /g4 
[ACE] path = [E] l2 l2 [CE] 
[ACE] l2 [CE] E E: /g1Self/g2 
[BDE] [DE] l1 
[DE] l2 l1 [BDE] /g3To reach E, come this way /g4 
B l1 l3 D l2 path = [E] [DE] 
E: l2; cost=2;  path=[DE] E: l2; cost=1; path=[E] 
For each advertisement, run /g3integration step/g4 
E.g., pick shortest, cheapest, quickest, etc. 
Ignore advertisements with own address in path vector 
Avoids routing loops that /g3count to infinity /g4 
6.02 Fall 2012 Lecture 20, Slide #6 
Summary 
The network layer implements the glue that 
achieves connectivity 
Does addressing, forwarding, and routing 
Forwarding entails a routing table lookup; the 
table is built using routing protocol 
DV protocol: distributes route computation; 
each node advertises its best routes to 
neighbors 
Path-vector: include path, not just cost, in 
advertisement to avoid count-to-infinity 
LS protocol: distributes (floods) neighbor 
information; centralizes route computation 
using shortest-path algorithm 
6.02 Fall 2012 Lecture 20, Slide #7</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 21: Reliable transport
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec21/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>11/28/12 
2 #5 Revised Plan  
Transmitter
Each packet includes a sequentially increasing sequence numb
When transmitting, save (xmit time,packet) on un-ACKed list
When acknowledgement (ACK) is received from the destination
for a particular sequence number, remove the corresponding
entry from un-ACKed list
Periodically check un-ACKed list for packets sent awhile ago
Retransmit, update xmit time in case we have to do it again! 
awhile ago: xmit time &lt; now /g6 timeout 
Receiver
Send ACK for each received packet, reference sequence number
Deliver packet payload to application in sequence number order
By keeping track of next sequence number to be delivered to app, it
easy to recognize duplicate packets and not deliver them a second 
time. 
6.02 Fall 2012 Lecture 21, Slide er 
s Issues  
Protocol must handle lost packets correctly
Lost data: retransmission will provide missing data
Lost ACK: retransmission will trigger another ACK from receiver
Size of packet buffers
At transmitter
Buffer holds un-ACKed packets 
Stop transmitting if buffer space an issue 
At receiver
Buffer holds packets received out-of-order 
Stop ACKing if buffer space an issue 
Choosing timeout value: related to RTT
Too small: unnecessary retransmissions
Too large: poor throughput
Delivery stalled while waiting for missing packets 
6.02 Fall 2012 Lecture 21, Slide #6 
Throughput of Stop-and-W  ait
We want to calculate the expected time, T (in seconds)
between successful deliveries of packets.  If N data packetsare sent (N large), the time to send them will be N*T, soThroughput = N/NT = 1/T data packets per second
We cant just assume T = RTT because packets get lost
E.g.: N links in the round trip between sender and receiver
If the per-link probability of losing a data/ACK packet is p, then
the probability its delivered over the link is (1-p), and thus the
probability its delivered over N links is (1-p)N.
So the probability a data/ACK packet gets lost is L = 1  (1-p)N.
Now we can write an equation for T in terms of RTT and the
timeout, RTO: T=(1L)RTT+LRTO+T()
L=RTT+ RTO1L
6.02 Fall 2012 Lecture 21, Slide #7 6.02 Fall 2012 Lecture 21, Slide #8  The Best Case
Occurs when RTT is the same for every packet, so
timeout is slightly larger than RTT
L 1T=RTT+ RTT= RTT1L 1L
(1L)Throughput =RTT
If bottleneck link can support 100 packets/sec and the RTT
is 100 ms, then, using stop-and-wait, the maximum
throughput is at most only 10 packets/sec.
Urk!  Only 10% utilization
We need a better reliable transport protocol</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>11/28/12 
4 6.02 Fall 2012 Lecture 21, Slide #13  560 580 600 620 640 660 680
 800  820  840  860  880  900"trace2-seq"
"trace2-ack"
Time (ms) Data/ACK sequence number 
Data 
ACKs 
RTT 
RTO Window 
Rxmit ACKs for rxmitted 
packets (most probably) Data/ACK sequence tr  ace Sliding Window Implementation
Transmitter
Each packet includes a sequentially increasing sequence number  
When transmitting, save (xmit time,packet) on un-ACKed list
Transmit packets if len(un-ACKed list)  window size W
When acknowledgement (ACK) is received from the destination
for a particular sequence number, remove the corresponding
entry from un-ACKed list
Periodically check un-ACKed list for packets sent awhile ago
Retransmit, update xmit time in case we have to do it again! 
awhile ago: xmit time &lt; now /g6 timeout 
Receiver
Send ACK for each received packet, reference sequence number
Deliver packet payload to application in sequence number order
Save delivered packets in sequence number order in local buffer 
(remove duplicates).  Discard incoming packets which have already 
been delivered (caused by retransmission due to lost ACK). 
Keep track of next packet application expects.  After each reception, 
deliver as many in-order packets as possible. 
6.02 Fall 2012 Lecture 21, Slide #14 
6.02 Fall 2012 Lecture 21, Slide #15 
RTT Measurements
 
6.02 Fall 2012 http://nms.csail.mit.edu/papers/index.php?detail=208 
Lecture 21, Slide #16 
 Association for Computing Machinery. All rights reserved. This content is excluded from our
Creative Commons license. For more information, s ee http://ocw.mit.edu/fairuse.Courtesy of the Cooperative Association for Internet Data Analysis. Used with permission.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>11/28/12 
3 Idea: Sliding Window Protocol  
Use a window
SENDER RECEIVER Allow W packets outstanding (i.e.,
unackd) in the network at once
(W is called the window size).
Overlap transmissions with ACKs
Sender advances the window by 1 for
each in-sequence ack it receives
I.e., window slides
So, idle period reduces
Pipelining
Assume that the window size, W, isfixed and known
Later, we will discuss how one might
set it
W = 3 in the example on the left
6.02 Fall 2012 Lecture 21, Slide #9 6.02 Fall 2012 Lecture 21, Slide #10 dr 1 2 3 4 5 6 123 45window = 2-6 
Sn
Rcvr 
p1 a1 a2 
p2 Sliding Window in Action 
window = 1-5 
W = 5 in this example 
window = 2-6 
window = 3-7 
1 22 33  44  55 66  7 
Sndr 
a1 a2 a3 
Rcvr 
p1 p2 p3 Sliding Window in Action 
Window definition: If window is W, then max number of 
unacknowledged packets is W  
 This is a fixed-size sliding window
6.02 Fall 2012 Lecture 21, Slide #11 6.02 Fall 2012 Lecture 21, Slide #12 
Sender Receiver 
1 
2 
3 
4 5 
6 
7 
8 
9 
10 X 
1 
2 
3 
4 5 
6 
7 
9 
10 
1
1
11 
12 
13 
14 
3
3
3
3
3
3
3
 
4
4
3
2
2
2
2
ee e
eeeeee
4
 
4
8 11 
12 
13 8 
LectuLecttttLectLectuLectut
uuuuuuuuuuuurererererererrrerrer
 LLLLL
 0
0
0
1
2222222 2
222222
TIMEOUT 
8
9
1
1
1
1
8888888
1
1RXMIT ACKs 
Packet lost Sender`s  window size = 5</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>11/28/12 
5 6.02 Fall 2012 Lecture 21, Slide #17 17 
Ping 
latency Delay (milliseconds) AT&amp;T Wireless on iPhone 3G 
mu: 1697.2 ms 
stddev: 2346.5 ms min:155.6 ms 
max:12126.6 ms 
Time (s) 6.02 Fall 2012 Lecture 21, Slide #18 /g3/g5/g6/g4/g1/g8/g7/g9/g1/g2/g3/g3/g1/g8/g6/g7/g9/g1Data from Verizon Wireless 3G network 
mu: 1554.8 ms 
stddev: 1563.8 ms min: 82.5 ms 
max: 9912.4 ms 
RTT  s can be highly variable
6.02 Fall 2012 Lecture 21, Slide #19 
2000 4000 6000 RTT value (ms) Mean &gt; 1.5 seconds 
Std dev &gt; 1.5 seconds In this data set, if we pick a timeout of 6 seconds, then P(spurious rxmit ) is 
about 3%. CDF of RTT over V  erizon Wireless 3G Network
Cumulative probability (CDF) Estimating RTT   from Data
Gather samples of RTT by comparing time when ACK arriveswith time corresponding packet was transmitted
Sample of random variable with some unknown distribution (not
necessarily Gaussian!)
Chebyshevs inequatility tells us that for a random variable X
with mean  and finite variance 2:
1P(Xk)k2
To reduce the chance of a spurious (i.e., unnecessary)
retransmission  packet wasnt lost, just the round trip time for 
packet/ACK was long  we want our timeout to be greater than 
most observed RTTs 
So choose a k that makes the chances small
We need an estimate for  and 
6.02 Fall 2012 Lecture 21, Slide #20</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>11/28/12 
1 6.02 Fall 2012 Lecture 21, Slide #1 
6.02 Fall 2012 
Lecture #21: Reliable Data Transport 
Redundancy via careful retransmission
Sequence numbers &amp; acks
Two protocols: stop-and-wait &amp; sliding window
Timeouts and round-trip time (RTT) estimation
6.02 Fall 2012 Lecture 21, Slide #2 The Problem  
Given: Best-effort network in which
Packets may be lost arbitrarily
Packets may be reordered arbitrarily
Packet delays are variable (queueing)
Packets may even be duplicated
Sender S and receiver R want to communicate reliably
Application at R wants all  data bytes in exactly the same
order that S sent them
Each byte must be delivered exactly once
These functions are provided by a reliable transport
protocol
Application /g3layered above/g4 transport protocol
6.02 Fall 2012 Lecture 21, Slide #3 Proposed Plan  
Transmitter
Each packet includes a sequentially increasing sequence number  
When transmitting, save (xmit time,packet) on un-ACKed list
When acknowledgement (ACK) is received from the destination
for a particular sequence number, remove the corresponding
entry from un-ACKed list
Periodically check un-ACKed list for packets sent awhile ago
Retransmit, update xmit time in case we have to do it again! 
awhile ago: xmit time &lt; now /g6 timeout 
Receiver
Send ACK for each received packet, reference sequence number
Deliver packet payload to applicationStop-and-W  ait Protocol
Sender Receiver S R S R
Data 1 Data 1 
 1 Data 1 ACK 1 RTT  Xound-trip time Data 2 
XACK 2 Timeout 
Retransmit  Data 1 
Data 3 Data 1 ACK 1 TT = r
ACK 3 R
Duplicate Normal behavior Data loss + 
packet reception (no losses) retransmission 
Wanted exactly once, got at least once 
6.02 Fall 2012 Lecture 21, Slide #4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>11/28/12 
6 6.02 Fall 2012 Lecture 21, Slide #21 
|H| 
 decreasesExponential Weighted Moving Aver  age (EWMA) 
[A   low-pass filter  see frequency response]
srtt  *rtt_sample + (1- )*srtt
6.02 
 = 0.1  = 0.5 
Responds too quickly? 
Fall 2012 Lecture 21, Slide #22 Response to One Long RTT Sample  
 = 0.1  = 0.5 
Doesnt respond quickly enough? 
6.02 Fall 2012 Lecture 21, Slide #23 
RTT   changes from 1 to 2 T  imeout Algorithm
EWMA for smoothed RTT (srtt)
srtt /g5 /g1*rtt_sample + (1- /g1)*srtt
Typically 0.1  /g1  0.25 on networks prone to congestion.
TCP uses /g1=0.125.
Use another EWMA for smoothed RTT deviation (srttdev)
Mean linear deviation easy to compute (but could also do std 
deviation)
dev_sample = |rtt_sample  srtt|
srttdev /g5 /g2*dev_sample + (1- /g2)*srttdev
TCP uses /g2= 0.25
Retransmit Timeout, RTO
RTO = srtt + ksrttdev
k = 4 for TCP
Makes the /g3tail probability /g4 of a spurious retransmission low  
On successive retransmission failures, double RTO
(exponential backoff)
6.02 Fall 2012 Lecture 21, Slide #24</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 11: LTI channel and intersymbol interference
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec11/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>12</slideno>
          <text>Channels as LTI Systems
 
Many transmission channels can be effectively modeled as 
LTI systems.  When modeling transmissions, there are few simplifications we can make: 
	 Well call the time transmissions start t=0; the signal before 
the start is 0. So x[m] = 0 for m &lt; 0. 
	 Real-word channels are causal : the output at any time 
depends on values of the input at only the present and past times. So h[m] = 0 for m &lt; 0. 
These two observations allow us to rework the convolution sum when its used to describe transmission channels: 
	  n n 
y[n] = x[k]h[n  k] =x[k]h[n  k] =x[k]h[n  k] =x[n  j]h[ j]
 
k= k=0	 k=0 j=0 
6.02 Fall 2012 start at t=0 causal 	 j=n-k Lecture 11, Slide #13</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Back To Convolution
 
From last lecture: If system S is both linear and time-invariant (LTI), then we can use the unit sample response h[n] to predict the response to any input waveform x[n]: 
Sum of shifted, scaled unit sample 
Sum of shifted, scaled unit sample functions responses, with the same scale factors 
 
S  
x[n] = x[k][n  k] y[n] = x[k]h[n  k] 
k= k= 
CONVOLUTION SUM 
Indeed, the unit sample response h[n] completely characterizes 
the LTI system S, so you often see 
h[.] x[n] y[n] 
6.02 Fall 2012 Lecture 11, Slide #7</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Constructing the Eye Diagram
 
(no need to wade through all this unless you 
really want to!)
 
1. Generate an input bit sequence pattern that contains all possible 
combinations of B bits (e.g., B=3 or 4), so a sequence of 2BB bits. 
(Otherwise, a random sequence of comparable length is fine.) 
2. Transmit the corresponding x[n] over the channel (2BBN samples, if 
there are N samples/bit) 3. Instead of one long plot of y[n], plot the response as an eye diagram: 
a.	 break the plot up into short segments, each containing 
KN samples, starting at sample 0, KN, 2KN, 3KN,  (e.g., K=2 or 
3) 
b. plot all the short segments on top of each other 
6.02 Fall 2012 Lecture 11, Slide #6</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Series  Interconnection of LTI Systems
 
x[n] h1[.] h2[.] w[n] y[n] 
y = h2  w = h2 (h1  x)=(h2  h1 ) x
 
(h2 *h1)[.]
 x[n] y[n] 
(h1 *h2)[.]
 x[n] y[n] 
h2[.]
 x[n] h1[.] 
 y[n] 
6.02 Fall 2012 Lecture 11, Slide #15</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Deconvolving Output of
 
Echo Channel
 
Channel, 
h1[.] Receiver filter,  h
2[.] x[n] y[n] z[n] 
Suppose channel is LTI with  
h1[n]=[n]+0.8 [n-1] 
Find h2[n] such that z[n]=x[n] 
(h2*h1)[n]= [n] 
Good exercise in applying  Flip/Slide/Dot.Product 
6.02 Fall 2012 Lecture 11, Slide #16</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Properties of Convolution
 
  
(x  h)[n]  x[k]h[n  k] = h[m]x[n  m] 
k= m= 
The second equality above establishes that convolution is commutative: 
x  h = h  x 
Convolution is associative: 
x (h1  h2) =(x  h1 ) h2 
Convolution is distributive: 
x (h1 + h2 )= (x  h1) + (x  h2) 
6.02 Fall 2012 Lecture 11, Slide #14</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>(side by side)
 
y[n] = 
  
(x  h)[n] = x[k]h[n  k] = h[m]x[n  m] = (h  x)[n ]
 
k= m= 
Input term x[0] at Unit sample response 
time 0 launches term h[0] at time 0  
scaled unit sample contributes scaled input 
response x[0]h[n] at  h[0]x[n] to output 
output 
Input term x[k] at Unit sample response  
time k launches term h[m] at time m  
scaled shifted unit contributes scaled shifted 
sample response  input h[m]x[n-m] 
x[k]h[n-k] at output to output 
6.02 Fall 2012 Lecture 11, Slide #10</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Parallel  Interconnection of LTI Systems
 
h1[.] 
x[n] y1[n] 
h2[.] + 
y2[n] y[n] 
y = y1 + y2 = (h1  x) + (h2  x) =(h1 + h2 ) x
 
(h1+h2)[.]
 x[n] y[n] 
6.02 Fall 2012 Lecture 11, Slide #18</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>To Convolve (but not to Convolute!)
 
  
 x[k]h[n  k] = h[m]x[n  m] 
k= m= 
A simple graphical implementation: 
Plot x[.] and h[.] as a function of the dummy index 
(k or m above) 
Flip (i.e., reverse) one signal in time,  
slide it right by n (slide left if n is ve), take the 
dot.product with the other.
 
This yields the value of the convolution at 
the single time n. 
flip one &amp; slide by n . dot.product with the other 
6.02 Fall 2012 Lecture 11, Slide #11</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Example: ringing channel
 
6.02 Fall 2012 
Lecture 11, Slide #5</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Deconvolving Output of
 
Channel with Echo
 
Channel, 
h1[.] Receiver 
filter,  h2[.] x[n] y[n] 
+ z[n]+v[n] 
w[n] 
Even if channel was well modeled as LTI and h1[n] 
was known, noise on the channel can greatly degrade the result, so this is usually not practical.  
6.02 Fall 2012 Lecture 11, Slide #17</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Example
 
	 From the unit sample response h[n] to the unit step response 
s[n] = (h *u)[n] 
	 Flip u[k] to get u[-k] 
	 Slide u[-k] n steps to right (i.e., delay u[-k]) to get u[n-k]), 
place over h[k] 
	 Dot product of  h[k] and u[n-k] wrt k: 
n 
s[n] = h[k] 
k= 
6.02 Fall 2012 Lecture 11, Slide #12</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Choosing Samples/Bit
 
Oops, no eye!
 ye! 
Given h[n], you can use the eye diagram to pick the 
number of samples transmitted for each bit (N): 
Reduce N until you reach the noise margin you feel 
is the minimum acceptable value. 
6.02 Fall 2012 Lecture 11, Slide #4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Eye Diagrams
 
000 100 010 110 001 101 011 111 
These are overlaidEye diagrams make it easy to find two-bit-slot segmentsthe worst-case signaling conditions  
of step responses, plottedat the receiving end. without the stems of  
the stem plot on the left6.02 Fall 2012 Lecture 11, Slide #2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
Lecture #11 
 Eye diagrams 
 Alternative ways to look at convolution 
6.02 Fall 2012 Lecture 11, Slide #1</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>A Complementary View of Convolution 
So instead of the picture: 
  
x[n] = x[k][n  k] y[n] = x[k]h[n  k] 
k= k= 
we can consider the picture: h[.] 
h[.]=+h[-1][n+1]+h[0] [n]+h[1] [n-1]+ x[n] y[n] 
 
from which we get  y[n] = h[m]x[n  m]
 
m= 
(To those who have an eye for these things, my apologies  
6.02 Fall 2012 for the varied math font --- too hard to keep uniform!) Lecture 11, Slide #9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 7: Viterbi decoding
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec07/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #11 Hard-decision Bra  nch Metric
BM = Hamming distance 
between expected parity bits and received parity bits 
Compute BM for each transition 
arc in trellis 
Example: received parity = 00 
BM(00,00) = 0 
BM(01,00) = 1 
BM(10,00) = 1 
BM(11,00) = 2 
Will be used in computing 
PM[s,i+1] from PM[s,i]. 
 
00 
01 10 11 
0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 ime:     i             i+1 
00 
0 
2 
2 
0 
1 1 
1 
1 TState</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #4 Trellis View at Transmitter  
00 
01 10 11 
0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 
x[n-1]x[n-2] 
x[n] 0 1 1 1 0 
 0 
0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 1/10 0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 Codeword 00 11 01 10 01 11 
1/00 
1
 
1/01 
1/01
1
1/
 
0/
0/10 
0/1
0/00 
1/11 
1/11
0/11
0/
0/11 
0
1/
1/00 
1/0
1/01 
1
1/
1
1/01
1
0/10 
0/
0/10
0/10
0/
time</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #12 Computing P  M[s,i+1]
Starting point: weve computed  
PM[s,i], shown graphically as label itrellis box for each state at time i. 
 
Example: PM[00,i] = 1 means there 
was 1 bit error detected when 
comparing received parity bits to what would have been transmitted when sending the most likely message, considering all messages that place the transmitter in state 0at time i. 
 
Q: Whats the most likely state s for 
the transmitter at time i? 
A: state 00 (smallest PM[s,i]) n 
0 
1 
3 
3 
2 
00 
01 10 11 
0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10 Time:     i             i+1 State 00 
0 
2 
2 
0 
1 1 
1 
1</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #17 Post-decoding BER v.   or BSC error prob.
All codes except (7,4) Hamming code 
are rate-1/2 (so dont assume its bad; it actually is better than (8,4) rect parity and one of the conv. codes  Bottom 2 curves: good conv codes Pink curve: bad conv code What makes a code good?</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #16 Hard-Decision V  iterbi Decoding
A walk through the trellis  
Path metric: number of errors on maximum-likelihood path 
to given state (min of all paths leading to state) 
Branch metric: for each arrow, the Hamming distance 
between received parity and expected parity 
0 
 
 
 
00 
01 10 11 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 00 Rcvd: 01 01 10 01 10</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #2  Convolutional Codes
Coding review 
Decoding via Viterbi algorithm</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #18 Soft Decoding Beats Hard Decoding  
2 dB improvement</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #6  Receiver
For the code 
p0 = x[n]+x[n-1]+x[n-2] 
p1 = x[n]           +x[n-2]  
Received: 
000101100110 
Some errors have occurred  
Whats the 4-bit  
message? 
: 0111 Most likely: 0111 
 ssage whose i.e., message whose   
codeword rd is closest is closest   
to rcvd bits 
 
Msg Codeword Received Hamming 
distance 
0000 000000000000 5 
0001 000000111011 - 
0010 000011101100 - 
0011 000011010111 - 
0100 001110110000 - 
0101 001110001011 - 
0110 001101011100 - 
0111 001101100111 
000101100110 000101100110  2 2 
1000 1000 111011000000 111011000000 - 
1001 1001 111011111011 111011111011 - 
1010 1010 111000101100 111000101100 - 
1011 1011 111000010111 111000010111 - 
1100 1100 110101110000 110101110000 - 
1101 1101110101001011 110101001011 - 
1110 110110011100 - 
1111 110110100111 - 
Initial and final state: 00</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #1 
6.02 Fall 2012 
Lecture #7 
 Viterbi decoding of convolutional codes 
         Path and branch metrics 
  Hard-decision &amp; soft-decision decoding 
 Performance issues: decoder complexity, post-
decoding BER, free distance concept</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #5 Decoding:  
Finding the Maxim um-Likelihood (ML) Path  
Given the received voltages, the receiver must find the most-
likely sequence of transmitter states, i.e., the path through the trellis that minimizes the distance between the received parity voltages and the voltages the transmitter would have sent had it followed that state sequence. 
One solution: Viterbi decoding 
00 
01 10 11 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 
00 
11 
11 
00 
01 10 
01 
10 Rcvd: 0.1,0.1 0.4,1.2 0.2,0.99 0.7,0.05 0.11,1.05 0.82,0.4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #3 Key Concept for Coding and Decoding:  
T  rellis
Example: K=3, rate- convolutional code 
g0 = 111: p0[n] = 1*x[n] + 1*x[n-1] + 1*x[n-2] 
g1 = 101: p1[n] = 1*x[n] + 0*x[n-1] + 1*x[n-2] 
States labeled with x[n-1] x[n-2] 
Arcs labeled with x[n]/p0p1 
00 
 10 
01 
 11 
0/00 
1/11 
1/10 
0/01 0/11 1/00 0/10 1/01 STARTING STATE 
S
0/01
1
0/01
1
1/00 
0/10 
2K-1 
states 
00 
01 10 11 
0/00 
1/11 
0/11 
1/00 
1/01 0/10 
0/01 
1/10  
 time x[n-1]x[n-2]</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 7, Slide #19 Spot Quiz Ti  me
1.What are the path metrics for the empty boxes 
(top to bottom order)? 
2.What is the most-likely state after time step 6? 
3.If the decoder had stopped after time step 2 and 
returned the most-likely message, what would the 
bits of the message be (careful about order!)?</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 3: Errors, channel codes
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec03/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #4 or  Mud Pulse Telemetry , anyone?!  
 
This is the most common method of data transmission used by 
MWD (Measurement While Drilling) tools. Downhole a valve is 
operated to restrict the flow of the drilling mud (slurry) according to the digital/nobreakspaceinformation to be transmitted. This creates pressure fluctuations representing the information. The pressure fluctuations propagate within the drilling fluid towards the surface where they are received from pressure sensors. On the surface, the received pressure signals are processed by computers to reconstruct the information. The technology is available in three varieties -/nobreakspace positive pulse,/nobreakspacenegative/nobreakspacepulse, 
and/nobreakspacecontinuous wave. 
 
(from Wikipedia)</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #15 Binary entropy function  
Heads   (or C=1) with  
probability     
 Tails  (or C=0) with 
probability  p
1ph(p)
h(p)
p
H(C)=plog2p(1p)log2(1p)=h(p)1.0 
0.5 1.0 
   0 
  0 0.5 1.0</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.02 Fall 2012 Lecture 8, Slide #2 
 6.02 Fa ll 2012 
 LeLeLLeLeLettcture 3, Slide #6 
Single Link Communication Model  
Digitize 
(if needed) Original source 
Source coding Source binary digits 
(message bits) 
Bit stream Render/display,  
etc. Receiving app/user 
Source decoding 
Bit stream 
Channel 
Coding 
(bit error  
correction) Recv 
samples 
+ 
Demapper Mapper 
+ 
Xmit 
samples 
Bits Signals 
(Voltages) 
over 
physical link 
Signals  
(Voltages)Channel 
Decoding 
(reducing or 
removing  
bit errors) 
Render/display,  
etc. 
Receiving app/user 
 g a
Source decoding 
etc
de
BBiitt ssttream 
 B
Digitize 
(if needed) 
Original sour ce 
iti
Source  coding 
SSoouurrccee bbiinnaarryy ddiiggiittss 
 SS
(message bits) 
BBiitt sttreamm 
 BBEnd-host 
computers 
(Bits</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #13 Evaluating conditional entropy and 
mutual information  
H(X|Y=yj)=p(xi
i=1m
 |yj)log21
p(xi|yj)

To compute conditional entropy: 
H(X|Y)=H(X|Y=yj)p(
i=1m
 yj)
because 
  p(xi,yj)=p(xi)p(yj|xi)
=p(yj)p(xi|yj)
I(X;Y)=I(Y;X)H(X,Y)=H(X)+H(Y|X)
=H(Y)+H(X|Y)
so 
 mutual information is symmetric</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #28 Minimum HD of Linear Code  
(n,k) code has rate k/n 
Sometimes written as (n,k,d), where d is the 
minimum HD of the code. 
The weight of a code word is the number of 1s in it.  
The minimum HD of a linear code is the minimum weight found in its nonzero codewords</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #23 Minimum Hamming Distance of Code vs.  
Detection &amp; Correction Capabilities  
If d is the minimum Hamming distance between codewords, we can 
detect all patterns of &lt;= (d-1) bit errors  
If d is the minimum Hamming distance between codewords, we can 
correct all patterns of             
or fewer bit errors  d1
2</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #14 e.g., Mutual information between  
input and output of  
binary symmetric channel (BSC)  
Channel 
X{0,1} Y{0,1}
p
With probability    the input binary digit gets flipped 
before being presented at the output.         p
I(X;Y)=I(Y;X)=H(Y)H(Y|X)
=1H(Y|X=0)pX(0)H(Y|X=1)pX(1)
=1h(p)Assume 0 and 1 
are equally likely</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #5 or  Mud Pulse Telemetry , anyone?!  
 
This is the most common method of data transmission used by 
MWD (Measurement While Drilling) tools. Downhole a valve is 
operated to restrict the flow of the drilling mud (slurry) according to the digital
/nobreakspaceinformation to be transmitted. This 
creates pressure fluctuations representing the information. The pressure fluctuations propagate within the drilling fluid towards the surface where they are received from pressure sensors. On the surface, the received pressure signals are processed by computers to reconstruct the information. The technology is available in three varieties -/nobreakspace positive pulse,/nobreakspacenegative/nobreakspacepulse, 
and/nobreakspacecontinuous wave. 
 
(from Wikipedia)</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #11 Replication Code to reduce decoding error  
Replication factor, n (1/code_rate) Prob(decoding error) over BSC w/ p=0.01 
Code: Bit b coded as bbb (n times) 
Exponential fall-off (note log scale) But huge overhead (low code rate) 
We can do a lot better!</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #16 So mu tual informa tion between input and 
output of the BSC with equally likely 
inputs looks like this:  
0.5 1.0 1.0 1h(p)
p
For low-noise channel, significant reduction in uncertainty 
about the input after observing the output.  For high-noise channel, little reduction.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #1 
6.02 Fall 2012 
Lecture #3 
 Communication network architecture 
 Analog channels 
 The digital abstraction 
 Binary symmetric channels 
 Hamming distance 
 Channel codes</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #7 
 6.02 Fall 2012
 Lecture 3, Slide # 7 
Network Communication Model  
Three Abstraction Layers: Packets, Bits, Signals  
Digitize 
(if needed) Original source 
iti
Source coding 
Source binary digits 
(message bits) 
Packets Render/display,  
etc. Receiving app/user 
 g a
Source decoding 
etc
de
Bit stream End-host 
computers 
Packetize 
Pa
Switch 
Switch Switch Switch Buffer + stream 
S
 S
S
uffer + s
LINK LINK LINK LINK 
Packets /barb2right Bits /barb2right Signals /barb2right Bits /barb2right Packets Bit stream</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #12 Mutual Information  
Channel 
XY
Noise I(X;Y)=H(X)H(X|Y)
How much is our uncertainty about     
reduced by knowing     ? X
Y
Evidently a central question in communication or,  more generally, inference. Thank you, Shannon!</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #29 Examples: What are n, k, d here?  
{000, 111} 
 
{0000, 1100, 0011, 1111}  
{1111, 0000, 0001} 
{1111, 0000, 0010, 1100} Not linear 
codes! 
N
c
 The HD of a linear code is 
the number of 1s in the non-zero codeword with the 
smallest # of 
1s (3,1,3). Rate= 1/3. 
 
(4,2,2). Rate = .  
(7,4,3) code. Rate = 4/7.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #2 The System, End-to-End  
Digitize 
(if needed) Original source 
iti
Source coding 
Source binary digits 
(message bits) 
Bit stream 
COMMUNICATION NETWORK Render/display,  
etc. Receiving app/user 
 g a
Source decoding 
etc
de
Bit stream 
The rest of 6.02 is about the colored oval 
Simplest network is a single physical communication link 
Well start with that, then get to networks with many links</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #20 The magic of asymptotically error-free 
transmission at any rate  R&lt;C
Shannon showed that one can theoretically transmit information  
(i.e., message bits) at an average rate           per use of the channel, with arbitrarily low error . 
 (He also showed the converse, that transmission at an average rate          incurs an error probability that is lower-bounded  by some positive number.) R&lt;C
RC
The secret: Encode blocks of     message bits into   -bit codewords, so              , with    and     very large .  k n
R=k/n n k
Encoding blocks of     message bits into   -bit codewords to protect against channel errors is an example of                            channel coding  k n</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #26 Binary Arithmetic  
Computations with binary numbers in code 
construction will involve Boolean algebra, or 
algebra in GF(2) (Galois field of order 2), or modulo-2 algebra:  
 
             0+0=0,     1+0=0+1=1,        1+1=0 
           
                  0*0=0*1=1*0 =0,      1*1=1</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #17 Channel capacity  
C=maxI(X;Y)=maxH(X)H(X|Y)} {To characterize the channel, rather than the input and output, define 
    where the maximization is over all possible distributions of    .    
X
This is the most we can expect to reduce our uncertainty  about     through knowledge of   , and so must be  the most 
information we can expect to send through the channel on average, per use of the channel. Thank you, Shannon! X Y
Channel 
XY
Noise</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #8 Digital Signaling: Map Bits to Signals 
Key Idea: Code or map or modulate  the desired bit sequence 
onto a (continuous-time) analog signal, communicating at 
some bit rate (in bits/sec).  To help us extract the intended bit sequence from the noisy received signals, well map bits to signals using a fixed set of discrete values.  For example, in a bi-level signaling (or bi-level 
mapping ) scheme we use two voltages:  
 V0 is the binary value 0   V1 is the binary value 1 
If V0 = -V1 (and often even otherwise) we refer to this as bipolar signaling.  At the receiver, process and sample to get a voltage Voltages near V0 would be interpreted as representing 0 
Voltages near V1 would be interpreted as representing 1 
If we space V0 and V1 far enough apart, we can tolerate some degree of noise --- but there will be occasional errors!</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #3 Physical Communication Links are 
Inherently Analog  
Analog = continuous-valued, continuous-time 
 
 Voltage waveform on a cable 
 Light on a fiber, or in free space  Radio (EM) waves through the atmosphere  Acoustic waves in air or water  Indentations on vinyl or plastic 
 Magnetization of a disc or tape 
  
 Sources unknown. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #24 How to Construct Codes?  
Want: 4-bit messages with single-error correction (min HD=3) 
 How to produce a code, i.e., a set of codewords, with this property?</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #19 What channel capacity tells us about how fast 
and how accurately we can communicate</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #18 C=1h(p) /barb4right 
0.5 1.0 1.0 Channel capacity pe.g., capacity of the binary symmetric channel  
C=maxH(Y)H(Y|X)} { Easiest to compute as                                           , still over all  
possible probability distributions for    . The second term doesnt  depend  on this distribution, and the first term is maximized  when 0 and 1 are equally likely at the input. So invoking our mutual information example earlier:  
Channel 
XY
p
X</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.02 Fall 2012 Lecture 3, Slide #27 Linear Block Codes  
Block code: k message bits encoded to n code bits, 
i.e., each of 2k messages encoded into a unique n-bit 
combination via a linear transformation, using GF(2) 
operations: 
                           C=D.G  
C is an n-element row vector containing the codeword 
D is a k-element row vector containing the message 
G is the kxn generator matrix  
Each codeword bit is a specified linear combination of message bits.   
Key property: Sum of any two codewords is also a 
codeword /barb2right necessary and sufficient for code to be linear.   (So the all-0 codeword has to be in any linear code --- why?)   
       More on linear block codes in recitation &amp; next lecture!!</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 4: Linear block codes, parity relations
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec04/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>9</slideno>
          <text>Matrix Notation
Task: given k-bit message, compute n-bit codeword.  We can 
use standard matrix arithmetic (modulo 2) to do the job.  For example, heres how we would describe the (9,4,4) rectangular code that includes an overall parity bit. 
1 0 0 0 1 0 1 0 1
0 1 0 0 1 0 0 1 1
D1D2D3D4 [] =D D D D[ P P P P P ]0 0 1 0 0 1 1 0 1 1 2 3 4 1 2 3 4 5
0 0 0 1 0 1 0 1 1
6.02 Fall 2012 Lecture 4, Slide #10 



1k kn 1n 
message generator code word 
vector matrix vector 
The generator matrix, Gkxn = IkkAk(nk)

D1xkGkxn=C1xn</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Let  s do some rectangular parity decoding
6.02 Fall 2012 Lecture 4, Slide #12 Received codewords 
1 0 1 
0 1 0 0 1 
1. Decoder action: ________________ 
0 0 0 1 1 1 1 1 
2. Decoder action: ________________ 
0 0 1 0 1 0 0 0 
3. Decoder action: ________________ D1 D2 P1 D3 D4 P2 P3 P4</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 4, Slide #17 
Syndrome Decoding: Idea 
After receiving the possibly corrupted message (use 
 to indicate possibly erroneous symbol), compute a 
syndrome bit (Ei) for each parity bit 
   
If all the E
i are zero: no errors 
Otherwise use the particular combination of the Ei 
to figure out correction   E
1 = D1 + D2 + D4 + P1 
E2 = D1 + D3 + D4 + P2 
E3 = D2 + D3 + D4 + P3 0 = D1+D2+D4+P1 
0 = D1+D3+D4+P2 
0 = D2+D3+D4+P3 
Index 1 2 3 4 5 6 7
Binary 
index001 010 011 100 101 110 111
(7,4) 
codeP1 P2 D1 P3 D2 D3 D4</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.02 Fall 2012 Lecture 4, Slide #4 If d is the minimum Hamming distance between 
codewords, we can:  
 detect all patterns of up to t bit errors 
    if and only if  d  t+1 
 
correct all patterns of up to t bit errors  
    if and only if   d  2t+1 
detect all  patterns of up to t
D bit errors  
    while correcting  all patterns of tC (&lt;tD) errors  
    if and only if     d  tC+tD+1   
 
e.g.:                                                               d=4,          
              tC=1, tD=2 Minimum Hamming Distance of Code vs.  
Detection &amp; Correction Capabilities</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Logic Behind Hamming Code Construction
Idea: Use parity bits to cover each axis of the 
binary vector space 
That way, all message bits will be covered with a unique 
combination of parity bits 
6.02 Fall 2012 Lecture 4, Slide #16 Index 1 2 3 4 5 6 7 
Binary index 001 010 011 100 101 110 111 
(7,4) code P1 P2 D1 P3 D2 D3 D4 
P1 with binary index 001 covers 
P1 = D1+D2+D4  
P2 = D1+D3+D4 D1 with binary index 011 
P3 = D2+D3+D4 D2 with binary index 101 
D4 with binary index 111</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Decoding Rectangular P  arity Codes
Receiver gets possibly corrupted word, w. 
Calculates all the parity bits from the data bits. 
If no parity errors, return rc bits of data. 
Single row or column parity bit error /barb2right rc data 
bits are fine, return them 
If parity of row x and parity of column y are in 
error, then the data bit in the (x,y) position is 
wrong; flip it and return the rc data bits 
All other parity errors are uncorrectable.  Return 
the data as-is, flag an uncorrectable error 
6.02 Fall 2012 Lecture 4, Slide #11</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Towards More Efficient Codes:  
(7,4,3) Hamming Code Example 
Use minimum number of parity bits, each covering 
a subset of the data bits. 
No two message bits belong to exactly the same 
subsets, so a single-bit error
6.02 Fall 2012 Lecture 4, Slide #15  will generate a unique 
set of parity check errors. 
Suppose we check the 
parity and discover that P1 Modulo-2 Dand P3 indicate an error? addition, P 1 1 P2     bit D2 must have flipped aka XOR  
D What if only P2 indicates 4 
D2 Dan error? 
P3     P2 itself had the error! 1 = D1+D2+D4 
P2 = D1+D3+D4 PP 3 3 = D2+D3+D4 
D
1
+D</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>(n,k) Systematic Linear Block Codes 
Split data into k-bit blocks 
Add ( n-k) parity bits to each block using (n-k) linear 
equations, making each block n bits long 
Every linear code can be represented by an equivalent 
systematic form --- ordering is not significant, direct 
inclusion of k message bits in n-bit codeword is. 
Corresponds to using invertible transformations on 
rows and permutations on columns of G to get  
G = [I | A] --- identity matrix in the first k columns 
6.02 Fall 2012 Lecture 4, Slide #7 Message bits Parity bits k 
n n-k</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Rectangular Code Corrects Single Errors
Claim: The min HD of the rectangular code with r 
rows and c columns is 3.  Hence, it is a single 
error correction (SEC) code. 
Code rate = rc / (rc + r + c). 
6.02 Fall 2012 Lecture 4, Slide #9 D1 D2 
D5 D6 
P3 
P5 P1 
P2 D3 D4 
D7 D8 
D9 D10 D11 D12 
P4 P7 If we add an overall parity bit P, 
we get a (rc+r+c+1, rc, 4) code 
 
Improves error detection but not 
correction capability P
Proof: Three cases. 6 
(1) Msgs with HD 1 /barb2right differ in 1 row and 1 col parity 
(2) Msgs with HD 2 /barb2right differ in either 2 rows OR 2 cols 
or both /barb2right HD  4 
(3) Msgs with HD 3 or more /barb2right HD  4 P</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Linear Block Codes
Block code: k message bits encoded to n code bits 
i.e., each of 2k messages encoded into a unique n-bit  
codeword via a linear transformation.  
 
Key property: Sum of any two codewords is also a codeword /barb2right necessary and sufficient for code to be 
linear. 
 (n,k) code has rate k/n. Sometime written as (n,k,d), where d is the minimum  Hamming Distance of the code. 
 
6.02 Fall 2012 Lecture 4, Slide #5</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Hamming Codes
Hamming codes correct single errors with the 
minimum number of parity bits: 
                n = 2n-k  1 
 (7,4,3) 
 (15,11,3) 
 (2m 1,2m -1-m,3)  
 
--- perfect codes (but not best!) 
6.02 Fall 2012 Lecture 4, Slide #14</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Constr  aints for more than single-bit errors
Code parity constraint inequality for single-bit errors 
1+ n  2n-k 
Write-out the inequality for t-bit errors  
6.02 Fall 2012 Lecture 4, Slide #18</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.02 Fall 2012 Lecture 4, Slide #8 Example: Rectangular Parity Codes 
D1 D2 
D3 D4 
P3 P4 P1 P1 is parity bit 
for row #1 Idea: start with rectangular 
array of data bits, add parity checks for each row and column.  Single-bit error in data will show up as parity 
P2 (n,k,d)=? 
errors in a particular row and column, pinpointing the 
P4 is parity bit 
bit that has the error. for column #2 
0 1 1 0 1 1 0 1 1 
1 1 0 1 0 0 1 1 1 
1 0 1 0 1 0 
Parity for each row Parity check fails for Parity check only fails 
and column is row #2 and column #2 for row #2  
correct  no errors  bit D4 is incorrect  bit P2 is incorrect</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Single Link Communication Model  
End-host 
6.02 Fall 2012 Lecture 4, Slide #2 
Digitize 
(if needed) Original source 
Source coding 
Source binary digits 
(message bits) 
Bit stream Render/display, 
etc. Receiving app/user 
Source decoding 
Bit stream 
Channel 
Coding 
(bit error  
correction) Recv 
samples 
+ 
Demapper Mapper 
+ 
Xmit 
samples 
Bits Signals 
(Voltages) 
over 
physical link 
Channel 
Decoding 
(reducing or 
removing  
bit errors) 
computers 
Bits</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Elementary Combinatorics
Given n objects, in how many ways can we choose 
m of them? 
 
If the ordering of the m selected objects matters, then   
         n(n-1)(n-2)  (n-m+1) = n!/(n-m)!  
 
If the ordering of the m selected objects doesnt 
matter, then the above expression is too large by a 
factor m!, so  
 n n!n choose m =    =m(nm)!m!                          
6.02 Fall 2012 Lecture 4, Slide #19</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>How Many Parity Bits Do Really We Need? 
We have n-k parity bits, which collectively can 
represent 2n-k possibilities 
For single-bit error correction, parity bits need to 
represent two sets of cases: 
Case 1: No error has occurred (1 possibility) 
Case 2: Exactly one of the code word bits has an 
error (n possibilities, not k) 
So we need n+1  2n-k  
                        n  2n-k  1 
Rectangular codes satisfy this with big margin --- inefficient 
6.02 Fall 2012 Lecture 4, Slide #13</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Gener  ator Matrix of Linear Block Code
 
Linear transformation: 
                                     C=D.G  C is an n-element row vector containing the codeword 
 
D is a k-element row vector containing the message  G is the kxn generator matrix  
 
Each codeword bit is a specified linear combination of 
message bits.   Each codeword is a linear combination of rows of G. 
 
6.02 Fall 2012 Lecture 4, Slide #6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 F #1 
6.02 Fall 2012 
Lecture #4 
Linear block codes 
Rectangular codes 
Hamming codes 
all 2012 Lecture 4, Slid e</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Error-Correcting Codes occur in many 
other contexts too  
e.g., ISBN numbers for books,  
      0-691-12418-3  
(Luenbergers Information Science) 
1D1+ 2D2+3D3++10D10 = 0 mod 11 
Detects single-digit errors, and transpositions   
6.02 Fall 2012 Lecture 4, Slide #20</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 22: Sliding window analysis, Little's law
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec22/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>12/3/12 
3 Example  
Q: The senders window size is 10 packets.  At what 
approximate rate (in packets per second) will the protocol deliver a multi-gigabyte file from the sender to the receiver? Assume that there is no other traffic in the network and packets can only be lost because the queues overflow. 
A: 10 packets / 21 ms, = 476 packets/second 
6.02 Fall 2012 Lecture 22, Slide #10 
 6.02 Fall 2012 Lecture 22, Slide #11 Example (cont.)  
Q: You would like to roughly 
double the throughput of our sliding window transport protocol. To do so, you can apply one  of the 
following techniques: a.Double window size W 
b.Halve the propagation delay of the links 
c.Double the rate of the link between the Switch and Receiver 
Q: For each of the following sender window sizes (in packets), list which of the above technique(s), if any, can approximately double the throughput:  W=10, W=50, W=30. 
6.02 Fall 2012 Lecture 22, Slide #12  Solutions to Example
Note that BW-delay product on given path = 20 packets 
W=10 
Doubling window size ~doubles throughput (BW-delay product is  
20 on path) 
Halving RTT ~doubles throughput (since now BW-delay product 
would be 10, equal to window size) 
Doubling bottleneck link rate wont change throughput much! 
W=50 
Doubling window size wont change throughput (were already 
saturating the bottleneck link) 
Halving RTT wont change throughput (same reason) 
Doubling bottleneck link speed will ~double throughput because 
new bw-delay product doubles to 40, and W=50 &gt; 40 
W=30 (trickiest case) 
Doubling window size or halving RTT: no effect 
Doubling bottleneck link changes BW-delay product to 40.  W is 
still lower than 40, so throughput wont double. But itll certainly  
increase, by perhaps about 50% more from befor e 
6.02 Fall 2012 Lecture 22, Slide #13 
RTT Measurements 
Courtesy of the Cooperative Association for Internet Data Analysis. Used with permission.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>12/3/12 
2 6.02 Fall 2012 Lecture 22, Slide #6  Sliding Window Implementation
Transmitter 
Each packet includes a sequentially increasing sequence number  
When transmitting, save (xmit time,packet) on un-ACKed list 
Transmit packets if len(un-ACKed list)  window size W 
When acknowledgement (ACK) is received from the destination 
for a particular sequence number, remove the corresponding 
entry from un-ACKed list 
Periodically check un-ACKed list for packets sent awhile ago 
Retransmit, update xmit time in case we have to do it again! 
awhile ago: xmit time &lt; now /g4 timeout 
Receiver 
Send ACK for each received packet, reference sequence number 
Deliver packet payload to application in sequence number order 
Save delivered packets in sequence number order in local buffer 
(remove duplicates).  Discard incoming packets which have already 
been delivered (caused by retransmission due to lost ACK). 
Keep track of next packet application expects.  After each reception, 
deliver as many in-order packets as possible. 
6.02 Fall 2012 Lecture 22, Slide #7 Little/g2/g2s Law  
n(t) = # pkts at time t in queue 
H G H D F G H C D E F G H B C D E F G H A B C D E F G H 
A B C D E F G H 
0 t T 
P packets are forwarded in time T (assume T large) 
Rate = /g1 = P/T 
Let A = area under the n(t) curve from 0 to T 
Mean number of packets in queue = N = A/T 
A is aggregate delay weighted by each packets time in queue. 
So, mean delay D per packet = A/P 
Therefore, N = /g1D   /g3 Littles Law 
For a given link rate, increasing queue size increases delay  
6.02 Fall 2012 Lecture 22, Slide #8 How to Set the Window Size to Maximize  Throughput?
Apply Littles Law  
Host A Host B 
If we can get Idle to 0, will achieve goal 
W = #packets in window 
B = rate of slowest (bottleneck) link in 
Time 
packets/second 
RTTmin= Min RTT along path, in the 
Idle  absence of any queueing (in seconds) 
If W = BRTTmin, then path is fully 
utilized (if no losses occur) 
BRTTmin is the /g1bandwidth-delay 
product/g2 
A key concept in the performance of 
windowed transport protocols 
6.02 Fall 2012 Lecture 22, Slide #9  Throughput of Sliding Window Protocol
If there are no lost packets, protocol delivers W packets every 
RTT seconds, so throughput is W/RTT 
Goal: to achieve high utilization, select W so that the 
bottleneck link is never idle due to lack of packets 
Without packet losses: 
Throughput = W/RTTmin if W  BRTTmin,  
       = B otherwise 
If W &gt; BRTTmin, then W = BRTTmin + Q, where Q is the queue 
occupancy 
With packet losses: 
Pick W &gt; BRTTmin to ensure bottleneck link is busy even if there 
are packet losses 
Expected # of transmissions, T, for successful delivery of pkt and 
ACK satisfies: T = (1-L) 1 + L(1 + T), so T = 1/(1/g4L), 
where L = Prob(either packet OR its ACK is lost) 
Therefore, throughput = (1/g4L)*B 
If W &gt;&gt; BRTTmin, then delays too large, timeout too big, and 
other connections may suffer</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>12/3/12 
1 6.02 Fall 2012 Lecture 22, Slide #1 
6.02 Fall 2012 
Lecture #22 
Sliding window protocol analysis 
Bandwidth-delay product &amp; queues 
Packet loss performance 
Littles law  
6.02 Fall 2012 Lecture 22, Slide #2 Sndr 
Rcvr window = 1-5 
1 2 3 4 5 
p1 a1 
x 6 1 2 3 4 5 
a3 
p3 window = 2-6 Sliding Window: Handling P  acket Loss
6.02 Fall 2012 Lecture 22, Slide #3 Sliding Window: Handling P  acket Loss
Timeout 
1 2 3 4 5 6 7 8 9 10 11 12 2 
Sndr 
a1 a3 a4 a5 a6 a7 a8 a9 a10  a11  a12  a2 
x 
Rcvr 
p1 p3 p4 p5 p6 p7 p8 p9 p10  p11  p12  p2 
Data packet 2 is lost. The receiver must save packets all later 
packets until packet 2 arrives, to deliver them to the application in 
proper order. Note that with our definition of the window, theres no limit to the number of packets that might arrive out of order.  
Q: Can the receiver discard these later packets (3, 4, , 12?) 
6.02 Fall 2012 Lecture 22, Slide #5  680
 660
Window 
 640
Data 
RTT 
ACKs 
 620RTO 
 600Rxmit ACKs for rxmitted 
packets (most probably) 
 580
 560
 800  820  840  860  880  900"trace2-ack"Data/ACK sequence number 
Time (ms) Data/ACK sequence tr  ace
"trace2-seq"</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>12/3/12 
4 http://nms.csail.mit.edu/papers/index.php?detail=208
 Association for Computing Machinery. All rights reserved. This content is excluded from
our Creative Commons license. For more information, see http://ocw.mit.edu/fairuse.
6.02 Fall 2012  Lecture 22, Slide #14 
latency mu: 1697.2 ms 
stddev: 2346.5 ms min:155.6 ms Delay (milliseconds) max:12126.6 ms 
15 Time (s) 
6.02 Fall 2012 Lecture 22, Slide #15 AT&amp;T Wireless on iPhone 3G Ping 
Mean &gt; 1.5 seconds 
Std dev &gt; 1.5 seconds In this data set, if we pick a timeout of 6 seconds, then P(spurious rxmit ) is 
about 3%.  
2000 4000 6000 RTT value (ms) CDF of RTT over V  erizon Wireless 3G Network
Cumulative probability (CDF) 
6.02 Fall 2012 Lecture 22, Slide #16</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 23: A brief history of the Internet
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec23/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>12/10/12 
3 Kahn  s Rules for Interconnection
Each network is
independent and must not  
be required to change
Best-effort
communicationCerf RFC 968 
Boxes (then calledgateways) connect
networks
No global control at
operations level (why?)
Courtesy of Vint Cerf. Used with permission.
Lecture 23, Slide #9 6.02 Fall 2012 
 6.02 Fall 2012 Lecture 23, Slide #10 Solution  
Gateways 
Courtesy of Scott Shenker. Used with permission.
Slide: Scott Shenker, UC Berkeley 
6.02 Fall 2012 Lecture 23, Slide #11 11 The Internetworking V  ision
Bob Kahn &amp; Vint Cerf imagined there would be
only a few networks and thus only a few gateways
The choice for network identification (8 bits) allows
up to 256 distinct networks. This size seems sufficient 
for the foreseeable future.
They were a little wrong!
Gateways would translate between networks
Evolved in the 1974 Cerf/Kahn paper as a universal
network layer, later called the Internet Protocol, or IP
We now think of it as all routers supporting IP
6.02 Fall 2012 Lecture 23, Slide #12  Handling Heterogeneity
Make it very easy to be a node or link on the network (best-
effort)
Universal network layer: standardize addressing and
forwarding
Switches maintain no per-connection state on behalf of endpoints
Original addressing model (then called a TCP address
because IP and TCP hadnt been split into different layers):
NETWORK TCP IDENTIFIER8 16
TCP address
Image by MIT OpenCourseWare.Cerf RFC 968</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>12/10/12 
4 1970s: Internetworking 
Classic Internet layering 1978: Layering!  TCP and IP
hourglass model split; TCP at end points, IP in 
the network 
IP network layer: simple best-
effort delivery
In retrospect: Packet switching
(&amp; TCP/IP) won because it is
good enough for almost everyapplication (though optimal fornothing!)
Competitor to TCP/IP: ISO,
standardizing 7-layer OSI stack
6.02 Fall 2012 Lecture 23, Slide #13 
6.02 Fall 2012 Lecture 23, Slide #14  Most Useful Lessons
One should architect systems for flexibility  youll almost never 
know what apps make it succeed. 
 (Even if it means sacrificing some performance!) 
Il semble que la perfection soit atteinte non quand il n'y a plus 
rien  ajouter, mais quand il n'y a plus rien  retrancher. 
Perfection is achieved, not when there is nothing more to add, 
but when there is nothing left to take away 
 -- Antoine de Saint-Exupery 
Or, 
When in doubt, leave it out 
6.02 Fall 2012 Lecture 23, Slide #15 1980s: Handling Growth with 
Topological Addressing 
1978-79: ARPANET moves to link-state routing
Per-node routing entries dont scale well
Solution: Organize network hierarchically
Into /g1areas/g2 or /g1domains /g2
Similar to how the postal system works
Hide detailed information about remote areas
For this approach to work, node addresses must
be topological
Address should tell network where in the network the
node is
I.e., address is a location in the network
Three classes of addresses in the 80s: Class A,
Class B, and Class C
Not used any more, though the dotted decimal notation of
IPv4 addresses makes it look like the dots matter
6.02 Fall 2012 Lecture 23, Slide #16 Ideal Case: Classic /g1/g1Area Routing /g2 
Area 1 
Area 2 Area 3 
Area 4 
Addresses are: 2.&lt;xyz&gt; Addresses are: 1.&lt;xyz&gt; 
Addresses are: 4.&lt;xyz&gt; Addresses are: 3.&lt;xyz&gt; 
And one could have areas  Only maintain routing table  
entries for other area identifiers Border routers 
within areas, etc.  Association for Computing Machinery. All rights
reserved. This content is excluded from ourCreative Commons license. For more information,see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>12/10/12 
6 6.02 Fall 2012 Lecture 23, Slide #21 1990s  
1990: no more ARPANET
1991: Tim Berners-Lee releases
WorldWideWeb
Mid-1990s: NSFNet backbone ends
Commercial ISPs take off
Classless addressing for scale
And the rise of NATs
BGP4: Path vector protocol between
competing ISPs, who must yet
cooperate
1991-1994: IPng &amp; IPv6 design starts
1993: search engines (Excite)
Mid-1990s: E-commerce starts
1998: Google reinvents search
1998: Content distribution networks
like Akamai
1996-2001: .com bubble starts &amp;bursts
6.02 Fall 2012 Lecture 23, Slide #22 2000s: The Internet Matures (Top 5 List)  
2000-2001: .com bust
And 9/11 happens 
Power of content distribution networks to handle load 
1.The rise of peer-to-peer networks
Gnutella, Freenet, distributed hash tables (e.g., Chord), BitTorrent, and 
of course, Napster 
2.Security threats and defenses
2000: Large-scale distributed denial-of-service (DDoS) attacks start 
2003: SQL slammer worm 
Spam  phishing and pharming  complex ecosystem 
Route hijacking by errors or malice 
3.User-generated content &amp; social networks
Blogs, Youtube, Facebook, and Twitter (UGC-meets-social) 
4.The rise of wireless and mobile data
5.Cloud computing and large-scale datacenters (Amazon, Google,
Microsoft, Facebook, etc.)
Almost everything moves to the Internet: telephony, video,entertainment 
6.02 Fall 2012 Lecture 23, Slide #23 2010s: The Decade Ahead  
Even more massive growth (largely from video, entertainment,  
and collaboration) &amp; internationalization
Combating complexity: new methods to make things simpler
(software-defined networks)
Wireless: the spectrum crisis, coping with mobility &amp;variability
Physical embedding &amp; embodiment: sensors &amp; actuators over
the network, mobile robots and autonomous agents, vehicles, 
embedded devices
Network security &amp; privacy; censorship
6.02 Fall 2012 Lecture 23, Slide #24 6.02 in One Slide  
How to design digital communication networks. 
Three layers of abstraction: bits, signals, packets. 
A unique storyline: vertical study across all layers 
Bits: Binary representation.  Compression (source coding).  
Bit errors and error correction codes (channel coding) 
Signals : Noise. LTI models. Frequency-domain analysis.  
Modulation &amp; demodulation. Packets: MAC protocols for shared media. Packet-switching 
&amp; queues.  Routing protocols.  Reliable transport. Two big themes: 
 Reliability  Sharing  CERN. All rights reserved. This  content  
is excluded from our Creative Commons  
license. For more information, see
http://ocw.mit.edu/fairuse .</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>12/10/12 
7 6.02 Fall 2012 Lecture 23, Slide #25  What Next?
Many UROP opportunities!
Networks and computer systems
6.033 (computer systems), 6.829 (computer networks), 6.824
(distributed systems), 6.263 (analysis of networks), 6.266
(network algorithms)
Security
6.857  (computer and network security), 6.858 (computer
systems security)
Signal processing &amp; digital communications
6.003 (signals and systems), 6.011 (communications, control,
and signal processing)
Advanced communication &amp; information theory
6.450 &amp; 6.451 (digital communications), 6.441 (info theory)
6.02 Fall 2012 Lecture 23, Slide #26 Thank you!  
Lectures
George Verghese
Hari Balakrishnan
Recitations
Yury Polyanskiy
Jacob White
Victor Zue
TAs
Rui Hu
Shao-Lun Huang
Ruben Madrigal
Kyu Seob Kim
Eduardo Sverdlin-Lisker
Cassandra Xia</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>12/10/12 
5 6.02 Fall 2012 Lecture 23, Slide #17 1980s: Rapid Growth  
1981-89: Dave Clark of MIT is Internets Chief Architect
Co-author of the end-to-end arguments (w/ Saltzer/Reed)
Ensures consistency of design and vision
We reject kings, presidents, and voting. We believe in rough 
consensus and running code.
1982: US DoD standardizes on TCP/IP
Berkeleys computer systems research group produces BSD
&amp; sockets
1983: MIT Project Athena  large-scale campus-area
networking
1984: Domain Name System (DNS) introduced
1985: NSFNet picks TCP/IP as standard
6.02 Fall 2012 Lecture 23, Slide #18  Growth Problems: Congestion
1986: Congestion collapse episodes on the Internet
Problems with bad timeout settings
Window size not appropriate for network state
Athena network file system congestion problems (bad timeout
settings)
Congestion avoidance and control 
RTT estimation using EWMA + new 
timeout method 
TCP congestion control by Van
Jacobson (concurrent work on
DECBit scheme by Ramakrishnan
&amp; Jain)
Adapt the window size to
congestion: If congested, decreasewindow; else increase. Use
 Van Jacobson. All rights reserved. This
exponential back-offs on timeouts content is excluded from our Creative
By the end of the 1980s,Commons license. For more information,
see http://ocw.mit.edu/fairuse.essentially all running TCPs had
congestion control
6.02 Fall 2012 Lecture 23, Slide #19 1990s 
1990: no more ARPANET
1991: Tim Berners-Lee releases
WorldWideWeb
Mid-1990s: NSFNet backbone ends
Commercial ISPs take off
Classless addressing for scale
And the rise of NATs
BGP4: Path vector protocol between
competing ISPs, who must yet
cooperate

e
6.02 Fall 2012 Lecture 23, Slide #20 1990s: Handling Growth with CIDR  
IPv4 Addresses &amp; Address  Prefixes
18.31.0.82 is actually the 32 bit string
00010010001111100000000001010010 
Routers have forwarding table entries corresponding to an
address prefix (a range of addrs w/ common prefix bitstring)
18.0.0.0/8 stands for all IP addresses in the range 00010010
000 to 00010010 111 (i.e., 224 addresses of the form
00010010*) 
18.31.0.0/17 stands for a range of 215 consecutive IP
addresses of the form 00010010001111100* (1st 17 bits are
the same for each address in that range)
Hence, subnetworks may be of size 1, 2, 4, 8,  (maxing outat 2
24 usually), and may be recursively divided further
Forwarding uses longest prefix match
At each router, routes are of the form For this range of
addresses, use this route
Pick the route that has the longest matching prefix w/ dest addr CERN. All rights reserved. This content is
xcluded from our Creative Commons
license. For more information, see
http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>12/10/12 
2 In the Beginning  
Kleinrocks group at UCLA tried to log on to SRI computer:
His recollection of the event
We set up a telephone connection between us and the guys
at SRI...
We typed the L and we asked on the phone
Do you see the L?
Yes, we see the L, came the response
We typed the O, and we asked
Do you see the O?
Yes, we see the O.
Then we typed the G
and the system crashed! 
Image in the public domain. Source: Wikipedia.
6.02 Fall 2012 Lecture 23, Slide #5 6.02 Fall 2012 Lecture 23, Slide #6 September 1971 
1970, ARPANET hosts start using NCP; first two 
cross-country lines (BBN-UCLA and MIT-Utah) 
/g1Hostile overlay /g2 atop telephone network 
Ran a distance-vector routing protocol 
6.02 Fall 2012 Lecture 23, Slide #7 1970s: Packet networks   Internetworking  
1972: successful ARPANET demo at conference (except it
failed when demod to skeptics from AT&amp;T!)
1972: modified ARPANET email program
1972: CYCLADES network (Louis Pouzin et al.): best-
effort datagrams; sliding window protocol; distance-
vector routing; time sync
1973: Ethernet (MAC protocol inspired by Aloha -- CSMA) 
1973-74: Xerox PUP (used distance-vector protocol)
1973: ARPANET becomes international
1973-75: Internetworking effort (Cerf, Kahn, et al.)
Developed TCP and IP (originally intertwined)  TCP uses
sliding window 
6.02 Fall 2012 The Problem  
Many different packet-switching networks
Only nodes on the same network could communicate
Slide: Scott Shenker, UC Berkeley 
Courtesy of Scott Shenker. Used with permission.
Lecture 23, Slide #8 Image in the public domain, from the ARPANET Completion Report, January 1978.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>12/10/12 
 
6.02 Fall 2012 Lecture 23, Slide #1 
6.02 Fall 2012 
Lecture #23 
A Brief History of the Internet  The Dawn of Packet Switching 
ARPA: 1957, in response to Sputnik 
Paul Baran (RAND Corp) 
	 Early 1960s: New approaches for 
survivable comms systems; "hot potato 
routing " and decentralized 
architecture, paper on packet switching over digital links 
Donald Davies (UK), early 1960s 
 Coins the term "packet" 
Len Kleinrock (MIT thesis): "Information 
flow in large communication nets ", 
1961 
J. Licklider &amp; W. Clark (MIT), On-line 
Man Computer Communication (1962) 
&amp; Lickliders vision of a galactic 
network 
RAND Corporation, On Distributed L. Roberts (MIT then ARPA), first Communications: Introduction to 
Distributed Communications Networks, ARPANET plan for time-sharing 
RM-3420-PR, 1964. Reprinted with remote computers 
permission. 
6.02 Fall 2012 	 Lecture 23, Slide #2 
ARPANET 
BBN team that implemented 
the interface message processor (IMP) 
Photographs  source unknown. All rights reserved. This content is excluded from our 
Creative Commons license. For more information, see http://ocw.mit.edu/fairuse. 
 1967: Connect computers at key research sites across the US using 
telephone lines 
 Interface Message Processors (IMP) ARPA contract to BBN 
 Sen. Ted Kennedy sends a somewhat confused telegram to BBN on 
winning the contract 
Congratulations  on interfaith message processor" 
6.02 Fall 2012 Lecture 23, Slide #3 Initial Baby Steps 
6.02 
 Alex McKenzie. All rights reserved. This content is excluded from our Creative 
Commons license. For more information, see http://ocw.mit.edu/fairuse.
 Fall 2012 Lecture 23, Slide #4 
1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 13: Frequency response of LTI systems
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec13/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Connection between CT and DT
 
The continuous-time (CT) signal 
sampled every T seconds, i.e., at a sampling 
frequency of fs = 1/T, gives rise to the discrete-time 
(DT) signal 
So  =  
and  =  corresponds to  = / T or 
f = 1/(2T) = fs/2 
6.02 Fall 2012 Lecture 13 Slide #7                        x(t) = cos(t) = cos(2ft) 
              x[n] = x( nT) =  cos(nT) = cos(n)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Sinusoidal Inputs and LTI Systems
 
h[n] 
A very important property of LTI systems or channels: 
If the input x[n] is a sinusoid of a given amplitude, 
frequency and phase, the response will be a sinusoid at the 
same frequency, although the amplitude and phase may be altered.  The change in amplitude and phase will, in general, depend on the frequency of the input. 
6.02 Fall 2012 Lecture 13 Slide #2</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Example: Deconvolving Output of
 
Channel with Echo
 
Channel, 
h1[.] Receiver 
filter,  h2[.] x[n] y[n] z[n] 
Suppose channel is LTI with  
1[n]=[n]+0.8 [n-1] 
 jm H1() = ?? = h1[m]e
m 
= 1+ 0.8ej = 1 + 0.8cos( )  j0.8sin( )So: 
|H1()| = [1.64 + 1.6cos()]1/2 EVEN function of ; 
&lt;H1() = arctan [(0.8sin()/[1 + 0.8cos()] ODD . 
6.02 Fall 2012 Lecture 13 Slide #13                    |H
                   &lt;H</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
Lecture #13 
 Frequency response  
 Filters 
 Spectral content 
6.02 Fall 2012 Lecture 13 Slide #1</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Exercise: Frequency response of h[n-D]
 
Given an LTI system with unit sample response h[n]  and associated frequency response H( ), 
determine the frequency response H
D() of an LTI 
system whose unit sample response is  
hD[n] = h[n-D] . 
Answer: HD() = exp{-jD}.() 
so : |HD()| = |()| , i.e., magnitude unchanged
 &lt;HD() = -D + &lt;() , i.e., linear phase term added 
6.02 Fall 2012 Lecture 13 Slide #20                &lt;H</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>The $4.99 version of a Low-pass Filter,
 h[n] and H()
 
6.02 Fall 2012 Lecture 13 Slide #17</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Useful Filters
 
6.02 Fall 2012 Lecture 13 Slide #24</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Spectral Content of Various Sounds
 
6.02 Fall 2012 Lecture 13 Slide #6 Human Voice
Cymbal Crash
Snare Drum
Bass Drum
Guitar
Bass Guitar
Synthesizer
Piano
13.75 Hz-
27.5 Hz27.5 Hz-
55 Hz55 Hz-
110 Hz110 Hz-
220 Hz220 Hz-
440 Hz440 Hz-
880 Hz880 Hz-
1,760 Hz1,760 Hz-
3,520 Hz3,520 Hz-
7,040 Hz7,040 Hz-
14,080 Hz14,080 Hz-
28,160 Hz
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>A little elaboration on                Properties of H() 
Repeats periodically on the frequency () axis, with period 2, 
because the input ejn is the same for  that differ by 
integer multiples of 2. So only the interval  in [-,] is of interest!
6.02 Fall 2012 Lecture 13 Slide #8</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Convolution  in Time &lt;---&gt;
 
Multiplication  in Frequency
 
x[n] h1[.] h2[.] y[n] 
x[n] y[n]
 (h2*h1)[.] 
In the frequency domain (i.e., thinking about input-to-output 
frequency response): 
x[n] H1() H2() y[n] 
i.e., convolution in time has become multiplication 
H()=H
2()H1() in frequency! 
6.02 Fall 2012 Lecture 13 Slide #12</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Complex Exponentials as 
Eigenfunctions  of LTI System 
x[n]=ejn h[.] y[n]=H()ejn 
Eigenfunction: Undergoes only scaling -- by the frequency 
response H() in this case: 
 jmH() h[m]e
m 
= h[m]cos( m)  jh[m]sin(m)
   
m m 
This is an infinite sum in general, but is well behaved if 
h[.] is absolutely summable, i.e., if the system is stable.
 
We also call H() the discrete-time Fourier transform (DTFT) of the time-domain function h[.] --- more on the DTFT later. 
6.02 Fall 2012 Lecture 13 Slide #3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>From Complex Exponentials to Sinusoids
 
cos(n)=(ejn+e-jn))/2 
So response to a cosine input is: 
Acos(0n+0) |H(0)|Acos(0n+0+&lt;H( 0)) H() 
(Recall that we only need vary  in the interval [ ,].) 
This gives rise to an easy experimental way to determine  
the frequency response of an LTI system. 
6.02 Fall 2012 Lecture 13 Slide #4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Loudspeaker Frequency Response
 
100
97
94
91
88
85
82
79
76
73
70
10 100 1,000 10,000 100,000-3dB @ 56.5Hz -3dB @ 12.5k HzSPL (dB)
Frequency (Hz)SPL versus Frequency
(Speaker Sensitivity = 85dB)
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Design ideal lowpass filter with cutoff 
frequency C and H( )=1 in passband
 
h[n] = 1  H()ejnd 2&lt;2&gt; 
1 C
jn = 1e d  2C 
sin(Cn)= , n  0 
n 
(extends to  in time ,=C / , n = 0 falls off only as 1/n)) DT sinc function 
6.02 Fall 2012 Lecture 13 Slide #19</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>A Frequency-Domain view of Deconvolution
 
Channel, 
H1() Receiver 
filter, H2() x[n] y[n] z[n] 
Noise w[n] 
Given H1(), what should H2() be, to get z[n]=x[n]? 
H2()=1/H1() Inverse filter 
= (1/|H1()|). exp{j&lt;H1()} 
Inverse filter at receiver does very badly in the presence of noise  
that adds to y[n]:      filter has high gain for noise precisely at frequencies where  
channel gain|H
1()| is low (and channel output is weak)! 
6.02 Fall 2012 Lecture 13 Slide #14</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>A little elaboration on                Properties of H() 
Repeats periodically on the frequency () axis, with period 2, 
because the input ejn is the same for  that differ by 
integer multiples of 2. So only the interval  in [-,] is of interest! 
 = 0, i.e., ejn = 1, corresponds to a constant (or DC, which 
stands for direct current, but now just means constant) input, 
so H(0) is the DC gain of the system, i.e., gain for constant inputs. 
H(0) =  h[m] --- show this fr om the definition!
6.02 Fall 2012 Lecture 13 Slide #9</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Symmetry  Properties of H() 
 jmH() h[m]e
m 
= h[m]cos( m)  jh[m]sin(m)
   
m m 
= C()  jS() 
For real h[n]: 
Real part of H( ) &amp; magnitude  are EVEN functions of . 
Imaginary part &amp; phase are ODD functions of . 
For real and even h[n] = h[n], H() is purely real. 
For real and odd h[n] = h[n], H() is purely imaginary.
 
6.02 Fall 2012 Lecture 13 Slide #11</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>A little elaboration on                Properties of H() 
Repeats periodically on the frequency () axis, with period 2, 
because the input ejn is the same for  that differ by 
integer multiples of 2. So only the interval  in [-,] is of interest! 
 = 0, i.e., ejn = 1, corresponds to a constant (or DC, which 
stands for direct current, but now just means constant) input, 
so H(0) is the DC gain of the system, i.e., gain for constant inputs. 
H(0) =  h[m] --- show this fr om the definition! 
 =  or , i.e., Aejn=(-1)nA, corresponds to the 
highest-frequency variation possible for a discrete-time signal, so H()=H(-) is the high-frequency gain of the system.
 H() = 
  (-1)m h[m] --- show from definition!
6.02 Fall 2012 Lecture 13 Slide #10</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Causal approximation to ideal lowpass filter
 
n Determine &lt;HC() 
6.02 Fall 2012 Lecture 13 Slide #22 
0               300                600 
                                        n    hC[n]= h[n-300] |HC[]| 
               0</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Frequency Response of Channels
 
6.02 Fall 2012 Lecture 13 Slide #25</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 2: Compression: Huffman and LZW
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec02/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #9 Trace-back  
A 0.1    B  0.3           0.3          0.4          0.6 0 
            0 0               0 0          1 
B 0.3    D  0.3          0.3          0.3           0.4 1 
            0 1               0 1          0 0 C 0.2    C  0.2          0.2          0.3              1 0               1 0          0 1 
D 0.3    A  0.1          0.2 
            1 1 0            1 1 
E 0.1    E  0.1 
            1 1 1</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #17  What is the Entropy of English?
Image in the public domain. Source: Wikipedia.
Taking account of actual individual symbol probabilities, 
but not using context, entropy = 4.177 bits per symbol</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.02 Fall 2012 ecture 2, Slide #24 LZW Encoding 
STRING = get input symbol 
WHILE there are still input symbols DO 
   SYMBOL = get input symbol    IF STRING + SYMBOL is in the STRINGTABLE THEN 
       STRING = STRING + SYMBOL    ELSE 
       output the code for STRING        add STRING + SYMBOL to STRINGTABLE 
       STRING = SYMBOL 
   END END 
 
output the code for STRING 
From http://marknelson.us/1989/10/01/lzw-data-compression/ L                      S=string, c=symbol (character) of text 
1.If S+c is in table, set S=S+c and read in next c. 
2.When S+c isnt in table: send code for S, add S+c to table. 
3.Reinitialize S with c, back to step 1. Code  Dr. Dobb's Journal. All rights reserved. This content
is excluded from our Creative Commons license. For more
information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>0 0
 
         0           0
 
         
 Reduction

A 0.1 B  0.3  
B 0.3 D 0.3  
C 0.2 C 0.2  
D 0.3 A 0.1  
E 0.1 E 0.1 
 
0.3 0.4 0.6 
0.3 0.3 0.4 
0.2 0.3 
0.2 
6.02 Fall 2012 Lecture 2, Slide #5</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #27 LZW Decoding 
Read CODE 
STRING = TABLE[CODE] // translation table 
 
WHILE there are still codes to receive DO 
    Read CODE from encoder     IF CODE is not in the translation table  THEN 
        ENTRY = STRING + STRING[0] 
    ELSE 
        ENTRY = get translation of CODE 
    END     output ENTRY 
    add STRING+ENTRY[0] to the translation table 
    STRING = ENTRY 
END 
(Ignoring special case in IF): 
1.Translate received code to output the corresponding table entry E=e+R (e is first symbol of entry, R is rest) 
2.Enter S+e in table. 
3.Reinitialize S with E, back to step 1.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #12 Another way to think about 
Entropy and Coding  
Consider a source S emitting one of symbols  
    s1, s2, , sN at each time, with probabilities  
    p1, p2, , pN respectively, independently of    
    symbols emitted at other times. This is an iid 
    source --- the emitted symbols are independent  
    and identically distributed 
In a very long string of K emissions, we expect to 
typically get Kp1, Kp2, , KpN instances of the 
symbols s1, s2, , sN respectively. (This is a very 
simplified statement of the law of large numbers.) 
A small detour to discuss the LLN</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>0 0  
 
                   0 0 0
 
         
 Trace-back

A 0.1A 0.1 BB 0.3 
B 0.3 D 0.3 
C 0.2 C 0.2 
D 0.3 A 0.1 
E 0.1 E 0.1 
 
0.3 0.4 0.60.3 0.4 0.6 0 
0.3 0.3 0.4 1 
0.2 0.3 
0.2 
6.02 Fall 2012 Lecture 2, Slide #6</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #14 Application  
        Symbol source = American electorate           
           s1=Obama , s2=Romney,   p2 = 1-p1 
 
Poll K people, and suppose M say Obama.  
    Then reasonable estimate of p1 is M/K (i.e., we are   
    expecting M=Kp1). For this example, suppose  
    estimate of p1 is 0.55.  
The fractional one-std uncertainty in this estimate 
of p1 is approximately sqrt{0.45*0.55/K} (note: we 
are looking at concentration around p1, not Kp1) 
    For 1% uncertainty, we need to poll 2,475 people                                         
    (not anywhere near 230 million!)</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #13  The Law of Large Numbers
The expected or mean number of occurrences of 
symbol s1 in K independent repetitions is Kp1, 
where p1 is the probability of getting s1 in a single 
trial 
The standard deviation (std) around this mean is  
                           sqrt{Kp1(1-p1)} 
So the fractional one-std spread around around the  
mean is  
                           sqrt{(1-p1)/(Kp1)} 
    i.e., goes down as the square root of K.  
Hence for large K, the number of occurrences of s1 
is relatively tightly concentrated around the mean 
value of Kp1</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #23 T  ry out LZW on
                        
 
          abcabcabcabcabcabcabc 
 
 
 
 
     (You need to go some distance out on this to  
     encounter the special case discussed later.)</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #18 In fact, English text has lots of 
 context
Write down the next letter (or next 3 letters!) in the 
snippet 
Nothing can be said to be certain, except death and ta_  
     But x has a very low occurrence probability 
     (0.0017) in English words 
Letters are not independently generated! 
 Shannon (1951) and others have found that the 
entropy of English text is a lot lower than 4.177 
Shannon estimated 0.6-1.3 bits/letter using human expts.  
More recent estimates: 1-1.5 bits/letter</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Bounds on Expected Code Length 
 
		We limit ourselves to instantaneously decodable 
(i.e., prefix-free) codes --- these put the symbols at 
the leaves of a code tree. 
		If L is the expected length of the code, the 
reasoning on the previous slide suggests that we 
need H(S)  L. The proof of this bound is not hard, see for example the very nice book by Luenberger, Information Science, 2006. 
	Shannon showed how to construct codes satisfying
 L  H(S)+1 (see Luenberger for details), but did not 
have a construction for codes with minimal
 expected length. 
		Huffman came up  with such a construction. 
6.02 Fall 2012 	 Lecture 2, Slide #3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
 
Lecture #2 
 
 More on entropy, coding and Huffman codes  Lempel-Ziv-Welch adaptive variable-length compression 
 
6.02 Fall 2012 Lecture 2, Slide #1</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #19 What exactly is it we want to 
 determine?
Average per-symbol entropy over long sequences: 
        H  = limK&gt; H(S1,S2, S3,  ,SK)/K  
 
where Sj denotes the symbol in position j in the text.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>2H(S)  2L  2H(S)+1  


 
Huffman Coding

		Given the symbol probabilities, Huffman finds an 
instantaneously decodable code of minimal 
expected length L, and satisfying 
H(S)  L  H(S)+1 
		Instead of coding the individual symbols of an iid 
source, we could code pairs sisj, whose 
probabilities are pipj . The entropy of this super-
source is 2H(S) (because the two symbols are 
independently chosen), and the resulting Huffman 
code on N2 super-symbols satisfies 
H(S)  L  H(S)+1 
    where L still denotes expected length per symbol  
    codeword. So now H(S)  L  H(S)+(1/ 2) 
		Extend to coding K at a time 
6.02 Fall 2012 	 Lecture 2, Slide #4</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #25 Example: Encode 
abbbabbbab  
1.Read a; string = a 
2.Read b; ab not in table 
output 97 , add ab to table, string = b 
3.Read b; bb not in table 
output 98 , add bb to table, string = b 
4.Read b; bb in table, string = bb 
5.Read a; bba not in table output 257 , add bba to table, string = a 
6.Read b, ab in table, string = ab 
7.Read b, abb not in table 
output 256 , add abb to table, string = b 
8.Read b, bb in table, string = bb 
9.Read a, bba in table, string = bba 
10.Read b, bbab not in table 
output 258 , add bbab to table, string = b 
 256 ab 
257 bb 
258 bba 
259 abb 
260 bbab 
261 
262</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #28 A special case:  cScSc
Suppose the string being examined at the source is cSc, where 
c is a specific character or symbol, S is an arbitrary (perhaps null) but specific  string (i.e., all c and S here denote the same 
fixed symbol, resp. string).  
Suppose cS is in the source and receiver tables already, and 
cSc is new, then the algorithm outputs the address of cS, 
enters cSc in its table, and holds the symbol c in its string, anticipating the following input text.  
The receiver does what it needs to, and then holds the string 
cS in anticipation of the next transmission. All good. 
But if the next portion of input text is Scx, the new string at the source is cScx ---not in the table, so the algorithm outputs 
the address of cSc and makes a new entry for cScx. 
The receiver does not yet have cSc in its table, because its one 
step behind! However, it has the string cS, and can deduce 
that the latest table entry at the source must have its last symbol equal to its first . So it enters cSc in its table, and then 
decodes the most recently received address.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #30 
Pop Quiz  
 
 
Which of these (A, B, C) is a valid Huffman code tree? 
 
   
What is the expected length of the code in tree C above? 
X  
p=0.4 Y  
p=0.3 Z, 
p=0.3 A. 
X, p=0.4 
Y, p=0.3 
Z, p=0.2 B. 
Z
.3 
W, p=0.1 
X, p=0.4 
Y, p=0.2 
Z, p=0.3 C. 
Z
.2 
W, p=0.1</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #20 Lempel-Ziv-W  elch (1977,78,84)
Universal lossless compression of sequential 
(streaming) data by adaptive variable-length coding  
Widely used, sometimes in combination with 
Huffman (gif, tiff, png, pdf, zip, gzip, )   
Patents have expired --- much confusion and distress over the years around these and related 
patents 
Ziv was also (like Huffman) an MIT graduate 
student in the golden years of information theory,  
early 1950s 
Theoretical performance: Under appropriate 
assumptions on the source, asymptotically attains 
the lower bound H  on compression performance</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #26  Encoder Notes
The encoder algorithm is greedy  its designed to find the 
longest possible match in the string table before it makes a transmission. 
The string table is filled with sequences actually found in the 
message stream. No encodings are wasted on sequences not 
actually found in the input data. 
Note that in this example the amount of compression 
increases as the encoding progresses, i.e., more input bytes 
are consumed between transmissions. 
Eventually the table will fill and then be reinitialized, 
recycling the N-bit codes for new sequences. So the encoder 
will eventually adapt to changes in the probabilities of the symbols or symbol sequences.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #15 Back to another way to think 
 about Entropy and Coding
In a very long string of K emissions, we expect to 
typically get Kp1, Kp2, , KpN instances of the 
symbols s1, s2, , sN respectively, and all ways of 
getting these are equally likely 
The probability of any one such typical string is  
              p1^(Kp1).p2^(Kp2) pN^(KpN)  
    so the number of such strings is approximately 
    p1^(-Kp1).p2^(-Kp2) pN^(-KpN). Taking the log2 of  
    this number, we get KH(S).  
So the number of such typical sequences is 2KH(S). 
It takes KH(S) binary digits to count this many 
sequences, so an average of H(S) binary digits per symbol to code the typical sequences .</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #21 Characteristics of LZW  
Universal lossless compression of sequential 
(streaming) data by adaptive variable-length coding 
Universal: doesnt need to know source statistics in 
advance. Learns source characteristics in the course of 
building a dictionary for sequential strings of symbols encountered in the source text 
Compresses streaming text to sequence of dictionary 
addresses --- these are the codewords sent to the receiver  
Variable length source strings assigned to fixed length 
dictionary addresses (codes) 
Starting from an agreed core dictionary of symbols, 
receiver builds up a dictionary that mirrors the senders, with a one-step delay, and uses this to exactly recover the source text (lossless) 
Regular resetting of the dictionary when it gets too big 
allows adaptation to changing source characteristics</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Entropy and Coding

		The entropy H(S) of a source S at some time 
represents the uncertainty about the source output 
at that time, or the expected information in the emitted symbol. 
		If the source emits repeatedly, choosing 
independently at each time from the same fixed 
distribution, we say the source generates independent and identically distributed (iid) symbols. 
		With information being produced at this average 
rate of H(S) bits per emission, we need to transmit 
at least H(S) binary digits per emission on average 
(since the maximum information a binary digit can 
carry is one bit). 
6.02 Fall 2012 	 Lecture 2, Slide #2</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #10 The Huffmann  Code  
A 0.1    B  0.3           0.3          0.4          0.6 0 
            0 0    
B 0.3    D  0.3          0.3          0.3           0.4 1 
            0 1 C 0.2    C  0.2          0.2          0.3              1 0  
D 0.3    A  0.1          0.2 
            1 1 0 
E 0.1    E  0.1     
            1 1 1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Trace-back  
A 0.1 B  0.3 
B 0.3 D 0.3 
C 0.2 C 0.2 D 0.3 A 0.1 E 0.1 E 0.1 0.3 0.4 0.6 0 
1 
0.3 0.3 0.4 1 
0 0 
0.2 0.3 
0 1 
0.2 
6.02 Fall 2012 Lecture 2, Slide #7</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #16  Some limitations
Symbol probabilities  
may not be known 
may change with time 
Source  
may not generate iid symbols, e.g., English text.  
Could still code symbol by symbol, but this wont be  
efficient at exploiting the redundancy in the text.  
Assuming 27 symbols (lower-case letters and space), could 
use a fixed-length binary code with 5 binary digits (counts 
up to 25 = 32).  
Could do better with a variable-length code because even  
assuming equiprobable symbols,                        H =  log
227 = 4.755 bits/symbol</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #29 A   couple of concluding thoughts
LZW is a good example of compression or 
communication schemes that transmit the 
model (with auxiliary information to run the model), rather than transmit the data 
Theres a whole world of lossy compression! 
(Perhaps well say a little later in the course.)</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.02 Fall 2012 Lecture 2, Slide #8 Trace-back  
A 0.1    B  0.3           0.3          0.4          0.6 0 
                                0 0          1 
B 0.3    D  0.3          0.3          0.3           0.4 1 
                                0 1          0 0 C 0.2    C  0.2          0.2          0.3                                  1 0          0 1 
D 0.3    A  0.1          0.2 
                                1 1 
E 0.1    E  0.1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 6: convolutional codes
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec06/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>7329 images, e.g.:

 
Image in the public domain. Source: NASA 
6.02 Fall 2012 Lecture 6, Slide #5</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #19 In the absence of noise   
Decoding is trivial: 
p0[n] = x[n] + x[n-1] + x[n-2] 
p1[n] = x[n] + x[n-2] 
Can you see how to recover the input x[.] from the 
parity bits p[.] ? 
In the presence of errors in the parity stream, 
message bits will get corrupted at about the same 
rate as parity bits, with this simple-minded 


recovery.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Spot Quiz!
Consider the convolutional code given by 
 p0[n] = x[n] + x[n-2] + x[n-3] 
 p1[n] = x[n] + x[n-1] + x[n-2] 
 p2[n] = x[n] + x[n-1] + x[n-2] + x[n-3] 
1.Constraint length, K, of this code = _____ 
2.Code rate = ____ 
3.Coefficients of the generators = ____, ____, ____ 
4.No. of states in state machine of this code = ____ 
 
6.02 Fall 2012 Lecture 6, Slide #20</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #18  Encoding &amp; Decoding Convolutional Codes
Transmitter (aka Encoder) 
Beginning at starting state, processes message bit-by-bit 
For each message bit: makes a state transition, sends p0p1 
Pad message with K-1 zeros to ensure return to starting state 
 
Receiver (aka Decoder) 
Doesnt have direct knowledge of transmitters state transitions; 
only knows (possibly corrupted) received parity bits, pi 
Must find most likely sequence of transmitter states that could 
have generated the received parity bits, pi 
If BER &lt; , P(more errors) &lt; P(fewer errors) 
When BER &lt; , maximum-likelihood message sequence is 
the one that generated the codeword (here, sequence of 
parity bits) with the smallest Hamming distance from the received codeword (here, parity bits)
 
I.e., find nearest valid codeword closest to the received 
codeword  Maximum-likelihood (ML) decoding</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Error Control Codes for Interplanetary 
Space Probes

 
		 Early Mariner probes, 1962-1967 (Mars, Venus)  
no ECC 
		 Later Mariner and Viking probes, 1969-1976  
(Mars, Venus)   linear block codes, e.g., 
 (32,6,16) bi-orthogonal or Hadamard code  
		 codewords comprise: the all-0 word, the all-1 word, and 
the other codewords all have sixteen 0s, sixteen 1s. The 
complement of each codeword is a codeword. 
6.02 Fall 2012 	 Lecture 6, Slide #2</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #14 Example: Transmit message 1011  
0 
 0 1
11 
Processing x[0] 
1 
 0 0
10 
Processing x[1] 
0 
 1 1
00 
Processing x[2] 
1 
 0 1
01 
Processing x[3] 
p0[n] = x[n] + x[n-1] + x[n-2] 
p1[n] = x[n] + x[n-2] p0[n] p1[n] 
x[n-2] x[n-1] x[n] 
Xmit seq: 1, 1, 1, 0, 0, 0, 0, 1,  
(codeword)</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #15 State-Machine View 
Example: K=3, rate- convolutional code 
There are 2K-1 states 
States labeled with (x[n-1], x[n-2]) value 
Arcs labeled with x[n]/p0[n]p1[n] 
msg=101100; xmit = 11 10 00 01 01 11 
00 
 10 
01 
 11 
0/00 
1/11 
1/10 
0/01 0/11 1/00 0/10 1/01 STARTING STATE 
S
1
1
0/01
1
0/01
1
1/00 
1/00
0/10 
p0[n] = x[n] + x[n-1] + x[n-2] 
p1[n] = x[n] + x[n-2] 
(Generators: g0 = 111, g1 = 101) 
The state machine is the same 
for all K=3 codes.  Only the pi 
labels change depending on number and values for the generator polynomials.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #9  Convolutional Codes 
(P  eter Elias, 1955)
Like the block codes discussed earlier, send parity 
bits computed from blocks of message bits 
Unlike block codes, generally dont send message bits, send 
only the parity bits! (i.e., non-systematic) 
The code rate of a convolutional code tells you how many 
parity bits are sent for each message bit.  Well mostly be 
talking about rate 1/r codes, i.e., r parity bits/message bit . 
Use a sliding window to select which message bits are 
participating in the parity calculations.  The width of the window (in bits) is called the codes constraint length K. 
p0[n] = x[n] + x[n-1] + x[n-2] 
p1[n] = x[n] + x[n-2] 
Addition mod 2 
(aka XOR)</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #16 Trellis  View  
State machine unfolded in time (fill in details using 
notes as guide, for the example considered here!)  
00 
01 
10 
11 
00 
01 
10 
11 
00 
01 
10 
11 
00 
01 
10 
11 
n=0            n=1          n=2           n=3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
 
Lecture #6 
 Convolutional codes  State-machine view &amp; trellis 
 
6.02 Fall 2012 Lecture 6, Slide #1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>More powerful codes needed for higher 
data rates with limited transmitter power
 
 
		 Space probe may have a 20W transmitter to cover 
tens of billions of kilometers! 
	 Part of the secret is the antenna --- directs the beam to 
produce the same received intensity as an omnidirectional 
antenna radiating in the megawatts 
	 Also cryogenically-cooled low-noise amplifiers, 
sophisticated receivers, and data coding and error-correction schemes. These systems can collect, detect, lock onto, and amplify a vanishingly small signal that reaches Earth from the spacecraft, and can extract data from the signal virtually without errors. (JPL quote) 
		 Convolutional codes with Viterbi decoding  
Voyager (1977) onwards, Cassini, Mars Exploration 
Rover,  
6.02 Fall 2012 	 Lecture 6, Slide #6</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #17 The P  arity Stream forms a Linear Code
Smallest-weight nonzero codeword has a weight 
that (locally in time) plays a role analogous to d, 
the minimum Hamming distance. Its called the free distance (fd) of the convolutional code. 
What is fd for our example?</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #12 Example  
Using two generators: 
g0 = 1, 1, 1, 0, 0,   abbreviated as 111 for K=3 code 
g1 = 1, 0, 1, 0, 0,   abbreviated as 110 for K=3 code 
 
Writing out the equations for the parity sequences: 
p0[n] = x[n] + x[n-1] + x[n-2] 
p1[n] = x[n] + x[n-2] 
 
Let x[n] = [1,0,1,1,]; x[n]= 0 when n&lt;0: 
p0[0] = (1 + 0 + 0) mod 2 = 1,  p1[0] = (1 + 0) mod 2 = 1
p0[1] = (0 + 1 + 0) mod 2 = 1,  p1[1] = (0 + 0) mod 2 = 0
p0[2] = (1 + 0 + 1) mod 2 = 0,  p1[2] = (1 + 1) mod 2 = 0
p0[3] = (1 + 1 + 0) mod 2 = 0,  p1[3] = (1 + 0) mod 2 = 1
 
Transmit: 1, 1, 1, 0, 0, 0, 0, 1,</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Saturn and Titan from Cassini, 
 
August 29, 2012

 
Image in the public domain. Source: NASA 
6.02 Fall 2012 Lecture 6, Slide #7</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 6, Slide #13 Shift-Register View  
One often sees convolutional encoders described with a block 
diagram like the following:          
Message bit in, parity bits out 
Input bits arrive one-at-a-time from the left 
The box computes the parity bits using the incoming bit and the 
K-1 previous message bits 
At the end of the bit interval, the saved message bits are shifted 
right by one, and the incoming bit moves into the left position. 
x[n-1] 
 x[n-2] x[n] 
+ 
+ 
mod 2 
mod 2 p0[n] p1[n] 
The values in 
the registers define the state of the encoder</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Phoning home using a K=15, rate=1/6 convolutional code 
82,950 bps 
(Cassini Saturn probe, Mars Pathfinder, Mars Rover) 
Image in the public domain. Source: NASA 
6.02 Fall 2012 Lecture 6, Slide #8</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Mariner 9 (400 million km trip)

 
		 Spacecraft control was through the central 
computer and sequencer which had an onboard 
memory of 512 words. The command system was 
programmed with 86 direct commands, 4 quantitative commands, and 5 control commands. Data was stored on a digital reel-to-reel tape 
recorder. The 168 meter 8-track tape could store 
180 million bits recorded at 132 kbits/s. Playback could be done at 16, 8, 4, 2, and 1 kbit/s using two tracks at a time. Telecommunications were via dual S-band 10 W/20 W transmitters and a single 
receiver through the high gain parabolic antenna, the medium gain horn antenna, or the low gain omnidirectional antenna. (NASA) 
6.02 Fall 2012	 Lecture 6, Slide #4  	 F</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Bi-orthogonal Codes

 
		 e.g., used on Mariner 9 (1971, Mars orbit) to 
correct picture transmission errors.  
		 Data word length: k=6 bits, for 64 grayscale values. 
		 Usable block length n around 30 bits. Could have done 5
repetition code, but comparable rate with better error 
correction from a [32, 6, 16] Hadamard code.  
	 Used through the 1980s. 
		 The efficient decoding algorithm was an important 
factor in the decision to use this code. The circuitry 
used was called the "Green Machine".  
		 More generally for such codes,  
d=2^(k-2)n=2^(k-1), 
6.02 Fall 2012 	 Lecture 6, Slide #3</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 8: Noise
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec08/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>21</slideno>
          <text>ImplicationsforSignalingRate  
	 Asthenoiseintensityincreases,weneedtoslow 
downthesignalingrate,i.e.,increasethenumber 
ofsamplesperbit(K),togethigherenergyinthe (MK)samplesextractedfromabitinterval,ifwe wishtomaintainthesameerrorperformance. 
	 e.g.Voyager2wastransmittingat115kbits/swhenit
wasnearJupiterin1979.Lastmonthitwasover9billion
milesaway,13lighthoursawayfromthesun,twiceasfar awayfromthesunasPluto.Andnowtransmittingatonly160bits/s.ThereceivedpowerattheDeepSpaceNetwork antennasonearthwhenVoyagerwasnearNeptunewasontheorderof10^(16)watts!!20billiontimessmaller thananordinarydigitalwatchconsumes.Thepowernowisestimatedat10^(19)watts.
6.02 Fall 2012 	 Lecture 8, Slide #22</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Flippedbitscanhaveserious 
consequences!  
	 OnNovember30,2006,atelemeteredcommandtoVoyager2was 
incorrectlydecodedbyitsonboardcomputerinarandomerror 
asacommandtotur nontheelectricalheatersofthespacecraft's 
magnetometer.Theseheatersremainedtur nedonuntilDecember4, 
2006,andduringthattime,therewasaresultinghightemperature 
above130C(266F),significantlyhigherthanthemagnetometers 
weredesignedtoendure,andasensorrotatedawayfromthecorrect orientation.Ithasnotbeenpossibletofullydiagnoseandcorrectfor 
thedamagecausedtotheVoyager2'smagnetometer,although 
effortstodosoareproceeding. 
	 OnApril22,2010,Voyager2encounteredscientificdataformat 
problemsasreportedbytheAssociatedPress onMay6,2010. 
OnMay17,2010,JPLengineersrevealedthataflippedbitinan 
onboardcomputerhadcausedtheissue,andscheduledabitreset 
forMay19.OnMay23,2010,Voyager2hasresumedsending 
sciencedatafromdeepspaceafterengineersfixedtheflippedbit. 
6.02 Fall 2012 http://en.wikipedia.org/wiki/Voyager_2 Lecture 8, Slide #23</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>TheUbiquityofGaussianNoise  
Thenetnoiseobservedatthereceiverisoftenthesumofmany
small,independentrandomcontributionsfrommanyfactors.Iftheseindependentrandomvariableshavefinitemeanandvariance,theCentralLimitTheorem saystheirsumwillbea
Gaussian.
Thefigurebelowshowsthehistogramsoftheresultsof10,000
trialsofsumming100randomsamplesdrawnfrom [1,1]
usingtwodifferentdistributions.
1 11Triangular Uniform
1 10.5
PDF PDF
6.02 Fall 2012 Lecture 8, Slide #12</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Butwecandobetter!  
	 Whyjusttakeasinglesamplefromabitinterval?
	 Instead,averageM(L)samples:
y[n]=+Vp+w[n] so avg{y[n]}=+Vp+avg{w[n]}
	 avg{w[n]}isstillGaussian,stillhasmean0,butits
varianceisnow 2/Minsteadof2 SNRis
increasedbyafactorofM
	 Sameanalysisasbefore,butnowbitenergyEb=
M.EsinsteadofsampleenergyEs: 
1 Eb 1	 MBER = P(error ) = erfc( ) = erfc(Vp )
6.02 Fall 2012 	 2 N0 2  2 Lecture 8, Slide #21</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>SignaltoNoiseRatio(SNR)  10logX X 
100 10000000000 
90 1000000000 
80 100000000 
70 10000000 
60 1000000 
50 100000 
40 10000 
30 1000 
20 100 
10 10 
0 1 
10 0.1 
20 0.01 
30 0.001 
40 0.0001 
50 0.000001 
60 0.0000001 
70 0.00000001 TheSignaltoNoiseratio(SNR)isuseful
injudgingtheimpactofnoiseonsystemperformance:
PPsignalSNR = PPnoise 
SNRforpowerisoftenmeasuredindecibels(dB):
SNR(db) =10 log10    
 PPsignal 
PPnoise    
 
Caution:Formeasuringratiosof 
amplitudesratherthanpowers,take 20log
10(ratio).80 0.000000001 
90 0.0000000001 
100 0.00000000001 3.01dbisafactorof2
inpowerratio6.02 Fall 2012 Lecture 8, Slide #9</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>ttt

 
   

SingleLinkCommunicationModel  
6.02 Fall 2012 Lecture 8, Slide #2
 6.02 Fa ll 2012 
 LeLeLLLeLecture 8, Slide # 2 
Digitize
(ifneeded)Originalsource
SourcecodingSourcebinarydigits
(messagebits)
BitstreamRender/display,Receivingapp/user
Sourcedecoding
Bitstream
Channel
Coding
(biterror
correction)Recv
samples
+
DemapperMapper
+
Xmit
samples
Bits
(Voltages)
over
physicallink
etc.
SignalsSignals 
(Vo ltages) Channel
Decoding
(reducingor
removing
biterrors)
Render/display,
etc.
Receivingapp/user
 ga 
Source decoding
etc 
de 
BB iitt ssttream
 B 
Digitize
(ifneeded) 
Original sour ce 
iti 
Source coding 
SSoouurrccee bbiinnaarryy ddiiggiittss
 SS 
(message bits)
BBiittstreamm
 BB Endhost
computers
(Bits</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Fornow,assumenodistortion,only 
AdditiveZeroMeanNoise  
 Receivedsignal 
y[n]=x[n]+w[n] 
i.e.,receivedsamplesy[n]are 
thetransmittedsamplesx[n]+ zeromeannoisew[n]oneachsample,assumediid 
(independentandidenticallydistributedateachn)
 SignaltoNoiseRatio(SNR) 
 usuallydenotestheratioof 
(timeaveragedorpeak)signalpower ,i.e.,squaredamplitude
ofx[n]
to
noisevariance,i.e.,expectedsquaredamplitudeofw[n]
6.02 Fall 2012 Lecture 8, Slide #7</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Themoralofthestoryis  
ifyouredoingappropriate/optimalprocessingat
thereceiver,yoursignaltonoiseratio(andtherefore
yourerrorperformance)inthecaseofiidGaussiannoiseistheratioofbitenergy(notsampleenergy)tonoisevariance.
6.02 Fall 2012 Lecture 8, Slide #25</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>UsingthePDFinProbabilityCalculations  
WesaythatXisarandomvariablegovernedbythePDF
fX(x)ifXtakesonanumericalvalueintherangeof x1tox2 
withaprobabilitycalculatedfromthePDFof Xas:
 x2p(x1 &lt; X &lt; x2) = fX (x)dx 
x1 
APDFisnotaprobabilityitsassociatedintegrals are. 
Notethatprobabilityvaluesarealwaysintherangeof0to1. 
6.02 Fall 2012 Lecture 8, Slide #11</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>e


 
 
MappingSamplestoBitsatReceiver  
codeword 
bits in 
1001110101 DAC modulate generate 
digitized 
symbols n=sampleindex
x[n]j=bitindex
NOISY &amp; DISTORTING  ANALOG CHANNEL 
y[n] b[j]
demodulate 1001110101&amp;filter sample &amp; 
threshold 
codeword 
bitsout 16samplesperbit ADC 
6.02 Fall 2012 Lecture 8, Slide #6
 .02 Fa ll 2012 
 LeLeLeLeLectctctctcturururururuuuuuurrrruuuu ruuruuu ruuuuu eeeeeeeeee eeeeeeeeeeeeeeeee eeeeee 8,8888 SSSSSlililiiiliiliii de #6 1001110101b[j]
Threshold</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>MeanandVarianceofaRandomVariableX  
x 
ThemeanorexpectedvalueJXisdefinedandcomputedas:
= x fX (x)dx X  
Thevariance X2istheexpectedsquaredvariationordeviation
oftherandomvariablearoundthemean,andisthuscomputed
as:
2   X =  (x X )2 fX (x)dx  
Thesquarerootofthevarianceisthestandarddeviation,X 
6.02 Fall 2012 Lecture 8, Slide #13</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>NoiseModelforiidProcessw[n]  
	 Assumeeachw[n]isdistributedastheGaussian 
randomvariableWontheprecedingslide,butwith 
mean0,andindependentlyofw[.]atallother 
times. 
6.02 Fall 2012 	 Lecture 8, Slide #16</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MappingBitstoSamplesatTransmitter  
codeword 
bitsin
1001110101generate
digitized
symbolsx[n]
16samplesperbit
1001110101 
6.02 Fall 2012 Lecture 8, Slide #4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>rre

   SamplesafterProcessingatReceiver  
bits in 
1001110101 DAC modulate generate 
digitized 
symbols codeword 
x[n]
NOISY &amp; DISTORTING  ANALOG CHANNEL 
ADC demodulate y[n] Assumingnonoise,
onlyendtoend distortionfilter 
6.02 Fall 2012 Lecture 8, Slide #5
 .02 Fa ll 2012 
 Lecturururururururururrrrrurrrrrruuurrrrrrurururuurr eeeeeeeeeeeeeeeeeeeeeeeeeeeeeee eeeee 8, Slide #5 y[n]</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02Fall2012 
Lecture#8 
 Noise:badthingshappentogoodsignals!  
 Signaltonoiseratioanddecibel(dB)scale  
 PDFs,means,variances,Gaussiannoise  
 Biterrorrateforbipolarsignaling 
6.02 Fall 2012 Lecture 8, Slide #1</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Whatifthereceivedsignalisdistorted?  
	 Supposey[n]=x0[n] +w[n]inagivenbitslot(L 
samples),wherex0[n]isknown,andw[n]isstilliid 
Gaussian,zeromean,variance2. 
	 Computeaweightedlinearcombinationofthey[.] 
inthatbitslot: 
any[n]=anx0[n] +anw[n]
ThisisstillGaussian,meananx0[n],butnowthe
2varianceis2 (an) 
	 Sowhatchoiceofthe{an}willmaximizeSNR? 
Simpleanswer: an=x0[n] matchedfiltering 
	 ResultingSNRforreceiverdetectionanderror performanceis(x
0[n])2/2,i.e.,againtheratioof 
bitenergyEbtonoisepower. 
6.02 Fall 2012 	 Lecture 8, Slide #24</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>TheGaussianDistribution 
AGaussianrandom
variableWwith
meanJandvariance
2 hasaPDF
describedby 
1fW (w) = 
2 2 
6.02 Fall 2012 Lecture 8, Slide #15 e  w( )2
2 2
Lecture 8, S lide #15 

 
2 
 
2</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Backtodistinguishing1from0:  
	 Assumebipolarsignaling: 
TransmitLsamplesx[.]at+Vp (=V1)tosignala1
TransmitLsamplesx[.]atVp (=V0)tosignala0
	 Simplemindedreceiver:takeasinglesamplevalue 
y[nj]atanappropriatelychoseninstantnjinthe 
	 jthbitinterval.Decidebetweenthefollowingtwo 
hypotheses: 
y[nj]=+Vp+w[nj] (==&gt;1)
or
y[nj]=Vp+w[nj] (==&gt;0)
where w[nj]isGaussian,zeromean,variance2 
6.02 Fall 2012 	 Lecture 8, Slide #18</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>NoiseCharacterization:  
FromHistogramtoPDF  
Experiment:createhistogramsof
samplevaluesfromindependenttrialsofincreasinglengths.
Histogramtypicallyconvergestoa
shapethatisknownafternormalizationtounitareaasa probabilitydensityfunction(PDF) 
6.02 Fall 2012 Lecture 8, Slide #10</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>VisualizingMeanandVariance  
Changesinmeanshiftthe Changesinvariancenarrow
centerofmassofPDF orbroadenthePDF(but
areaisalwaysequalto1)
6.02 Fall 2012 Lecture 8, Slide #14</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 19: Network routing (without failures)
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec19/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>11/19/12 
5 6.02 Fall 2012 Lecture 19, Slide #22 A 
B C 
D E 6 
6 
5 4 
0 7 F 
G 2 2 
8 (6) 
(6) (8) 
(10) 
(13) (16) C 
(12) B F A (0) 
(11) 
(10) E 
D G Integration Step: Dijkstra /g1s Algorithm  
(Example) 
Suppose we want to find paths from A to other nodes 
6Dijkstras Shortest Path  Algorithm
Initially 
nodeset = [all nodes] = set of nodes we havent processed 
spcost = {me:0, all other nodes: }  # shortest path cost 
routes = {me:--, all other nodes: ?}  # routing table 
while nodeset isnt empty: 
find u, the node in nodeset with smallest spcost 
remove u from nodeset 
for v in [us neighbors]: 
d = spcost(u) + cost(u,v)    # distance to v via u 
if d &lt; spcost(v):                 # we found a shorter path! 
spcost[v] = d 
routes[v] = routes[u] (or if u == me, enter link from me to v) 
Complexity: N = number of nodes, L = number of links 
Finding u (N times): linear search=O(N), using heapq=O(log N) 
Updating spcost: O(L) since each link appears twice in neighbors 
.02 Fall 2012 Lecture 19, Slide #23 


6.02 Fall 2012 Lecture 19, Slide #24  Another Example
Finding shortest paths from A: 4 
B D 19 
11 15 A 13 
7 
C E 5 
route LSAs: 
  A: [(B,19), (C, 7)] 
  B: [(A,19), (C,11), (D, 4)] 
  C: [(A, 7), (B,11), (D,15), (E, 5)] 
  D: [(B, 4), (C,15), (E,13)]   E: [(C, 5), (D,13)] 
spcost 
Step u Nodeset 
A B C D E A B C D E 
0 [A,B,C,D,E] 0     -- ? ? ? ? 
1 A [B,C,D,E] 0 19 7   -- L0 L1 ? ? 
2 C [B,D,E] 0 18 7 22 12 -- L1 L1 L1 L1 
3 E [B,D] 0 18 7 22 12 -- L1 L1 L1 L1 4 B [D] 0 18 7 22 12 -- L1 L1 L1 L1 5 D [] 0 18 7 22 12 -- L1 L1 L1 L1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>11/19/12 
4 18 DV Example: final state 
{A: (L0,19),/nonmarkingreturn {A: (L1,22),/nonmarkingreturn
 B: (None,0),/nonmarkingreturn  B: (L0,4),/nonmarkingreturn C: (L2,19),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn D: (L2,4),/nonmarkingreturn  D: (None,0),/nonmarkingreturn
 E: (L2,17)/nonmarkingreturn  E: (L
}/nonmarkingreturn }/nonmarkingreturn
L0 4 B L2 L0 D 19 
L1 
 L1 LL0 
ne,0),/nonmarkingreturn A 11 15 1
,19),/nonmarkingreturn L1 
,7),/nonmarkingreturn L1 L
,22),/nonmarkingreturn7 L2 
L0 C L0 
L3 E ,12)/nonmarkingreturn 5 
{A: (L0,7),/nonmarkingreturn {A: (L
 B: (L2,19),/nonmarkingreturn  B: (L2,13)/nonmarkingreturn
2 
{A: (No 3 
 B: (L0
 C: (L1 1 
 D: (L1 E: (L1
}/nonmarkingreturn0,12),/nonmarkingreturn
Node A: no updates 1,17),/nonmarkingreturn
Node B: no updates  C: (None,0),/nonmarkingreturn  C: (L0,5),/nonmarkingreturn
Node C: no updates  D: (L2,15),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn
Node D: no updates  E: (L3,5)/nonmarkingreturn  E: (None,0)/nonmarkingreturn
Node E: no updates }/nonmarkingreturn }/nonmarkingreturn
6.02 Fall 2012 Lecture 19, Slide #Correctness &amp; P  erformance
Optimal substructure property fundamental to correctness of 
both Bellman-Ford and Dijkstras shortest path algorithms 
Suppose shortest path from X to Y goes through Z.  
Then, the sub-path from X to Z must be a shortest 
path.  
Proof of Bellman-Ford via induction on number of 
walks on shortest (min-cost) paths 
Easy when all costs &gt; 0 and synchronous model (see notes)  
Harder with distributed async model (not in 6.02)  
How long does it take for distance-vector routing 
protocol to converge? 
Time proportional to largest number of hops 
considering all the min-cost paths 
6.02 Fall 2012 Lecture 19, Slide #19 
6.02 Fall 2012 Lecture 19, Slide #20 Link-State Routing 
Advertisement step 
Send information about its links to its neighbors (aka link 
state advertisement or LSA): 
 
    [seq#, [(nbhr1, linkcost1), (nbhr2, linkcost2), ] 
 
Do it periodically (liveness, recover from lost LSAs)  
Integration 
If seq# in incoming LSA &gt; seq# in saved LSA for source node:  
    update LSA for node with new seq#, neighbor list     rebroadcast LSA to neighbors ( /g2 flooding ) 
Remove saved LSAs if seq# is too far out-of-date 
Result: Each node discovers current map of the network 
Build routing table 
Periodically each node runs the same shortest path algorithm 
over its map (e.g., Dijkstras alg) 
If each node implements computation correctly and each 
node has the same map, then routing tables will be correct 
6.02 Fall 2012 Lecture 19, Slide #21 LSA Flooding 
LSA: [F, seq, (G, 8), (C, 2)] 
6 2 
A C F 
4 
6 7 E 8 
0 2 
B D G 5 
Periodically originate LSA 
LSA travels each link in each direction 
Dont bother with figuring out which link LSA came from 
Termination: each node rebroadcasts LSA exactly once 
Use sequence number to determine if new, save latest seq 
Multiple opportunities for each node to hear any given LSA 
Time required: number of links to cross network</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>11/19/12 
1 6.02 Fall 2012 Lecture 19, Slide #1 
6.02 Fall 2012 
Lecture #19 
 addressing, forwarding, routing 
 distance-vector routing 
 link-state routing 
[Today: no failures] 
6.02 Fall 2012 Lecture 19, Slide #2 The Problem: Distributed Methods for 
Finding P  aths in Networks
L2 4 L0 B D 19 
L1 
15 A 11 13 
Link costs 7 
C E 
5 
Addressing  (how to name nodes?) 
Unique identifier for global addressing 
Link name for neighbors 
Forwarding  (how does a switch process a packet?) 
Routing  (building and updating data structures to ensure 
that forwarding works) 
Functions of the network layer 
6.02 Fall 2012 Lecture 19, Slide #3 Forwarding 
Switch 
Core function is conceptually simple 
lookup(dst_addr) in routing table returns route (i.e., outgoing 
link) for packet 
enqueue(packet, link_queue) 
send(packet) along outgoing link 
And do some bookkeeping before enqueue 
Decrement hop limit (TTL); if 0, discard packet 
Recalculate checksum (in IP, header checksum) 
6.02 Fall 2012 Lecture 19, Slide #4 B 
C D 
E A 4 
11 
5 13 15 19 
7 (Assume all costs  0) Shortest Path Routing 
Each node wants to find the path with minimum total cost  
to other nodes 
We use the term shortest path even though were interested 
in min cost (and not min #hops)  
Several possible distributed approaches 
Vector protocols, esp. distance vector (DV) 
Link-state protocols (LS)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>11/19/12 
2 6.02 Fall 2012 Lecture 19, Slide #5 Routing Table Structure 
Routing table @ node B 
Destination Link (next-hop) Cost 
A ROUTE L1 18 
B Self 0 
C L1 11 
D L2 4 
E ROUTE L1 16 B D 
A 4 
11 13 15 19 
7 L2 
L1 L0 
C E 5 Distributed Routing: A Common Plan  
Determining live neighbors 
Common to both DV and LS protocols 
HELLO protocol (periodic)  
Send HELLO packet to each neighbor to let them know whos at the 
end of their outgoing links 
Use received HELLO packets to build a list of neighbors containing 
an information tuple for each link: (timestamp, neighbor addr, link) 
Repeat periodically.  Dont hear anything for a while /g2 link is down, so remove from neighbor list. 
Advertisement step (periodic) 
Send some information to all neighbors 
Used to determine connectivity &amp; costs to reachable nodes 
Integration step 
Compute routing table using info from advertisements 
Dealing with stale data 
6.02 Fall 2012 Lecture 19, Slide #6 
Distance-Vector Routing   
DV advertisement 
Send info from routing table entries: (dest, cost) 
Initially just (self,0) 
DV integration step [Bellman-Ford] 
For each (dest,cost) entry in neighbors advertisement 
Account for cost to reach neighbor: (dest,my_cost) 
my_cost = cost_in_advertisement + link_cost 
Are we currently sending packets for dest to this neighbor? 
See if link matches what we have in routing table 
If so, update cost in routing table to be my_cost 
Otherwise, is my_cost smaller than existing route? 
If so, neighbor is offering a better deal!  Use it 
update routing table so that packets for dest are sent to this neighbor 
6.02 Fall 2012 Lecture 19, Slide #11 6.02 Fall 2012 Lecture 19, Slide #13 DV Example: round 2 
{A: (L0,19),/nonmarkingreturn {B: (L0,4),/nonmarkingreturn
 B: (None,0),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn
 C: (L1,11),/nonmarkingreturn  D: (None,0),/nonmarkingreturn
 D: (L2,4)/nonmarkingreturn  E: (L2,13)/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
L0 4 B L2 L0 D 19 
L1 L1 L2 L0 
,/nonmarkingreturnA 11 15 13 
/nonmarkingreturnL1 
L1 L1 7 L2 
C L0 
L0 L3 E {A: (None,0)
 B: (L0,19), C: (L1,7)/nonmarkingreturn}/nonmarkingreturn
5 
{A: (L0,7),/nonmarkingreturn {C: (L0,5),/nonmarkingreturn
Node A: update routes to BC, DC, EC  B: (L1,11),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn
Node B: update routes to AC, EC  C: (None,0),/nonmarkingreturn  E: (None,0)/nonmarkingreturn
Node C: no updates  D: (L2,15),/nonmarkingreturn }/nonmarkingreturn
Node D: update routes to AC  E: (L3,5)/nonmarkingreturn
Node E: update routes to AC, BC }/nonmarkingreturn</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>11/19/12 
3 6.02 Fall 2012 Lecture 19, Slide #14 DV Example: round 3 
{A: (L1,18),/nonmarkingreturn {A: (L1,22),/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
L0 4 B L2 L0 D 19 
L1 L1 L2 L0 
one,0),/nonmarkingreturn A 11 15 13 
1,18),/nonmarkingreturn L1 
1,7),/nonmarkingreturn L1 L1 7 L2 1,22),/nonmarkingreturnC L0 
L0 L3 E 1,12)/nonmarkingreturn 5  B: (None,0),/nonmarkingreturn  B: (L0,4),/nonmarkingreturn
 C: (L1,11),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn D: (L2,4),/nonmarkingreturn  D: (None,0),/nonmarkingreturn
 E: (L1,16)/nonmarkingreturn  E: (L2,13)/nonmarkingreturn
{A: (N
 B: (L C: (L
 D: (L
 E: (L
}/nonmarkingreturn{A: (L0,7),/nonmarkingreturn {A: (L0,12),/nonmarkingreturn
Node A: no updates  B: (L1,11),/nonmarkingreturn  B: (L0,16),/nonmarkingreturn
Node B: no updates  C: (None,0),/nonmarkingreturn  C: (L0,5),/nonmarkingreturn
Node C: no updates  D: (L2,15),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn
Node D: no updates  E: (L3,5)/nonmarkingreturn  E: (None,0)/nonmarkingreturn
Node E: no updates }/nonmarkingreturn }/nonmarkingreturn
6.02 Fall 2012 Lecture 19, Slide #15 DV Example: Break a Link 
B D 19 
L1 L1 L2 0 
A 11 15 13 
1  
L1 L1 
L2 7 
L0 C L0 E {A: (L1,18),/nonmarkingreturn {A: (L1,22),/nonmarkingreturn
 B: (None,0),/nonmarkingreturn  B: (L0,4),/nonmarkingreturn C: (L1,11),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn D: (L2,4),/nonmarkingreturn  D: (None,0),/nonmarkingreturn
 E: (L1,16)/nonmarkingreturn  E: (L2,13)/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
L0 4 L2 L0 
L
{A: (None,0),/nonmarkingreturn
 B: (L1,18),/nonmarkingreturn L
 C: (L1,7),/nonmarkingreturn
 D: (L1,22),/nonmarkingreturn
L3  E: (L1,12)/nonmarkingreturn 5 
}/nonmarkingreturn{A: (L0,7),/nonmarkingreturn {A: (L0,12),/nonmarkingreturn B: (L1,11),/nonmarkingreturn  B: (L0,16),/nonmarkingreturn
When link breaks: eliminate routes  C: (None,0),/nonmarkingreturn  C: (L0,5),/nonmarkingreturn
that use that link.  D: (L2,15),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn E: (L3,5)/nonmarkingreturn  E: (None,0)/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
16 DV Example: round 4 
{A: (None,),/nonmarkingreturn {A: (L1,22),/nonmarkingreturn
 B: (None,0),/nonmarkingreturn  B: (L0,4),/nonmarkingreturn
 C: (None,),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn
 D: (L2,4),/nonmarkingreturn  D: (None,0), E: (None,)/nonmarkingreturn  E: (L2,13)/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
L0 B L2 L0 D 19 
L1 L1 L2 0 
A 11 15 13 
1  
L1 L1 7 L2 
L0 C L0 
L3 E 5 4 
L
{A: (None,0),/nonmarkingreturn B: (L1,18),/nonmarkingreturn
L
 C: (L1,7),/nonmarkingreturn D: (L1,22),/nonmarkingreturn E: (L1,12)/nonmarkingreturn
}/nonmarkingreturn{A: (L0,7),/nonmarkingreturn {A: (L0,12),/nonmarkingreturn
Node A: update cost to B
C  B: (None,),/nonmarkingreturn  B: (L0,16),/nonmarkingreturn
Node B: update routes to AA, CD, ED  C: (None,0),/nonmarkingreturn  C: (L0,5),/nonmarkingreturn
Node C: update routes to BD  D: (L2,15),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn
Node D: no updates  E: (L3,5)/nonmarkingreturn  E: (None,0)/nonmarkingreturn
Node E: update routes to BD }/nonmarkingreturn }/nonmarkingreturn
6.02 Fall 2012 Lecture 19, Slide #/nonmarkingreturnDV Example: round 5 
{A: (L0,19),/nonmarkingreturn {A: (L1,22),/nonmarkingreturn
 B: (None,0),/nonmarkingreturn  B: (L0,4),/nonmarkingreturn
 C: (L2,19),/nonmarkingreturn  C: (L1,15),/nonmarkingreturn
 D: (L2,4),/nonmarkingreturn  D: (None,0),/nonmarkingreturn
 E: (L2,17)/nonmarkingreturn  E: (L2,13)/nonmarkingreturn
}/nonmarkingreturn }/nonmarkingreturn
L0 4 B L2 L0 D 19 
L1 L1 L2 L0 
),/nonmarkingreturn A 11  15 13 
,/nonmarkingreturnL1 
/nonmarkingreturn L1 L1 7 L2 ,/nonmarkingreturnC L0 
L0 /nonmarkingreturn L3 E 5 Update cost 
{A: (None,0 B: (L1, ) C: (L1,7), D: (L1,22) E: (L1,12)
}/nonmarkingreturn{A: (L0,7),/nonmarkingreturn {A: (L0,12),/nonmarkingreturn
Node A: update route to B
B  B: (L2,19),/nonmarkingreturn  B: (L1,17),/nonmarkingreturn
Node B: no updates  C: (None,0),/nonmarkingreturn  C: (L0,5),/nonmarkingreturn
Node C: no updates  D: (L2,15),/nonmarkingreturn  D: (L1,13),/nonmarkingreturn
Node D: no updates  E: (L3,5)/nonmarkingreturn  E: (None,0)/nonmarkingreturn
Node E: no updates }/nonmarkingreturn }/nonmarkingreturn
6.02 Fall 2012 Lecture 19, Slide #17</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 14: Spectral representation of signals
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec14/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 14 Slide #11  PC Magazine. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Spectral Content of Various Sounds
 
6.02 Fall 2012 Lecture 14 Slide #12 Human Voice
Cymbal Crash
Snare Drum
Bass Drum
Guitar
Bass Guitar
Synthesizer
Piano
13.75 Hz-
27.5 Hz27.5 Hz-
55 Hz55 Hz-
110 Hz110 Hz-
220 Hz220 Hz-
440 Hz440 Hz-
880 Hz880 Hz-
1,760 Hz1,760 Hz-
3,520 Hz3,520 Hz-
7,040 Hz7,040 Hz-
14,080 Hz14,080 Hz-
28,160 Hz
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.02 Fall 2012 Lecture 14 Slide #20  
 Spectrum of Digital Transmissions
 
6.02 Fa ll 2012 
Lecture 14 S lide #20</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Input/Output Behavior of 
LTI System in Frequency Domain
 
1 jnx[n] = X()e d  2&lt;2&gt; 
H() y[n] = 1 
2 H()X()e jn 
&lt;2&gt;  d 
1 jny[n] = Y ()e d  2&lt;2&gt; 
Y () = H ()X() 
Compare with y[n]=(h*x)[n] 
Again, convolution in time 
has mapped to multiplication in frequency 
6.02 Fall 2012 Lecture 14 Slide #7</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>A Frequency-Domain view of Deconvolution
 
Channel, 
H1() Receiver 
filter, H2() x[n] y[n] z[n] 
Noise w[n] 
Given H1(), what should H2() be, to get z[n]=x[n]? 
H2()=1/H1() Inverse filter 
= (1/|H1()|). exp{j&lt;H1()} 
Inverse filter at receiver does very badly in the presence of noise  
that adds to y[n]:      filter has high gain for noise precisely at frequencies where  
channel gain|H
1()| is low (and channel output is weak)! 
6.02 Fall 2012 Lecture 14 Slide #3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>The spectrum of the exponential signal (0.5)nu[n] is shown over the 
frequency range  = 2f in [-4,4], The angle has units of degrees. 
http://cnx.org/content/m0524/latest/6.02 Fall 2012 
Lecture 14 Slide #5 Courtesy of Don Johnson. Used with permission; available under a CC-BY license.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Core of the Story 
1. A huge class of DT and CT signals 
can be written --- using Fourier  transforms --- as a 
weighted sums of sinusoids (ranging from very slow to very fast)
 
or (equivalently, but more compactly) complex exponentials. 
The sums can be discrete  or continuous  (or both). 
2. LTI systems act very simply on sums of sinusoids: 
superposition  of responses to each sinusoid, with the 
frequency response determining the frequency-dependent  
scaling of magnitude, shifting in phase. 
6.02 Fall 2012 Lecture 14 Slide #9</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Magnitude and Angle
 
Y ()= H()X() 
|Y ()|=|H()|.|X()| 
and 
&lt;Y ()=&lt;H()+&lt;X() 
6.02 Fall 2012 Lecture 14 Slide #8</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Effect of Low-Pass Channel
 
6.02 Fall 2012
6 02 Fall 2012 Lecture 14 Slide #22</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Demo: Deconvolving Output of
 
Channel with Echo
 
Channel, 
h1[.] Receiver 
filter,  h2[.] x[n] y[n] z[n] 
Suppose channel is LTI with  
h1[n]=[n]+0.8 [n-1] 
 jm H1() = ?? = h1[m]e
m 
= 1+ 0.8ej = 1 + 0.8cos( )  j0.8sin( )So: 
|H1()| = [1.64 + 1.6cos()]1/2 EVEN function of ;/nonmarkingreturn
 &lt;H1() = arctan [(0.8sin()/[1 + 0.8cos()] ODD . 
6.02 Fall 2012 Lecture 14 Slide #2                    |H
                   &lt;H</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Loudspeaker Bandpass Frequency Response
 
6.02 Fall 2012F ll 2012 Lecture 14 Slide #10 100
97
94
91
88
85
82
79
76
73
70
10 100 1,000 10,000 100,000-3dB @ 56.5Hz -3dB @ 12.5k HzSPL (dB)
Frequency (Hz)SPL versus Frequency
(Speaker Sensitivity = 85dB)
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>x[n] and X( ) 
6.02 Fall 2012 Lecture 14 Slide #6</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 14 Slide #19  
 Spectrum of Digital Transmissions
 
6.02 Fa ll 2012 
Lecture 14 S lide #19 (scaled version of DTFT samples)</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
Lecture #14 
 Spectral content via the DTFT 
6.02 Fall 2012 Lecture 14 Slide #1</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>x[n] and X( ) 
6.02 Fall 2012 Lecture 14 Slide #15</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Signal x[n] that has its frequency content 
uniformly distributed in [ c , c]
 
1 jnx[n] = X()e d  2&lt;2&gt; 
1 C
jn = 1 e d  2C 
sin(Cn)= , n  0 
n DT sinc function 
(extends to  in time , /nonmarkingreturn=C / , n = 0 falls off only as 1/n) 
6.02 Fall 2012 Lecture 14 Slide #14</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>X() and x[n] 
6.02 Fall 2012 Lecture 14 Slide #16</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Fast Fourier Transform (FFT) to compute 
samples of the DTFT for 
signals of finite duration
P1 (P/2)1 
 jkm jknX(k ) =x[m]e , x[n]= 1  X(k )eP m=0 k=P/2 
where k = k(2 /P), P is some integer (preferably a power of 2) 
such that P is longer than the time interval [0,L-1] over which 
x[n] is nonzero, and k ranges from P/2 to (P/2)1 (for even P). 
Computing these series involves O(P2) operations  when P gets 
large, the computations get very s l o w. 
Happily, in 1965 Cooley and Tukey published a fast method for 
computing the Fourier transform (aka FFT, IFFT), rediscovering  a technique known to Gauss. This method takes O(P log P) operations. 
P = 1024, P2 = 1,048,576, P logP  10,2406.02 Fall 2012 Lecture 14 Slide #17</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>/nonmarkingreturn Connection between CT and DT
 
The continuous-time (CT) signal 
x(t) = cos( t) = cos(2ft) 
sampled every T seconds, i.e., at a sampling 
frequency of fs = 1/T, gives rise to the discrete-time 
(DT) signal 
x[n] = x(nT) = cos(nT) = cos(n) 
So  = /nonmarkingreturn 
and  =  corresponds to  = /T or f = 1/(2T) = fs/2 
6.02 Fall 2012 Lecture 14 Slide #13</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 1: Overview: information and entropy
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec01/</lecture_pdf_url>
      <lectureno>0</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #22 Significance of Entropy 
Entropy  (in bits) tells us the average amount of information (in 
bits) that must be delivered in order to resolve the uncertainty 
about the outcome of a trial.  This is a lower bound on the number of binary digits that must, on the average, be used to encode our messages. 
If we send fewer binary digits on average, the receiver will have 
some uncertainty about the outcome described by the message. 
If we send more binary digits on average, were wasting the 
capacity of the communications channel by sending binary digits we dont have to. 
Achieving the entropy lower bound is the gold standard for an 
encoding (at least from the viewpoint of information compression).</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #14 Claude E. Shannon, 1916-2001 
/g81/g89/g83/g87/g1/g14/g28/g51/g52/g32/g50/g51/g1/g52/g40/g32/g51/g41/g51/g64/g1/g6/g6/g1/g5/g32/g48/g52/g64/g1/g14/g10/g21/g1
/g1/g2/g1/g19/g25/g15/g6/g17/g14/g13/g7/g1/g5/g16/g5/g14/g25/g19/g13/g19/g1/g17/g10/g1/g18/g9/g14/g5/g25/g1/g5/g16/g8/g1/g19/g24/g13/g20/g7/g12/g13/g16/g11/g1/g1/g7/g13/g18/g7/g23/g13/g20/g19/g1/g1/g10/g46/g52/g50/g47/g31/g56/g30/g32/g31/g1/g28/g48/g48/g44/g41/g30/g28/g53/g47/g46/g1/g47/g33/g1/g3/g47/g47/g44/g32/g28/g46/g1/g1/g1/g28/g44/g39/g32/g29/g50/g28/g1/g52/g47/g1/g44/g47/g39/g41/g30/g1/g30/g41/g50/g30/g56/g41/g52/g51/g64/g1/g28/g46/g31/g1/g57/g41/g30/g32/g1/g57/g32/g50/g51/g28/g66/g1/g1/g1/g23/g32/g50/g60/g1/g41/g46/g37/g56/g32/g46/g53/g28/g44/g1/g41/g46/g1/g31/g41/g39/g41/g52/g28/g44/g1/g30/g41/g50/g30/g56/g41/g52/g1/g31/g32/g51/g41/g39/g46/g66/g1/g1/g69/g14/g47/g51/g52/g1/g41/g45/g48/g47/g50/g52/g28/g46/g52/g1/g14/g28/g51/g52/g32/g50/g51/g1/g52/g40/g32/g51/g41/g51/g1/g47/g33/g1/g52/g40/g32/g1/g30/g32/g46/g52/g56/g50/g60/g70/g1/g1/g81/g89/g84/g80/g1/g17/g40/g5/g64/g1/g14/g28/g52/g40/g1/g5/g32/g48/g52/g64/g1/g14/g10/g21/g1/g1/g1/g2/g16/g1/g5/g14/g11/g9/g6/g18/g5/g1/g10/g17/g18/g1/g20/g12/g9/g17/g18/g9/g21/g7/g5/g14/g1/g11/g9/g16/g9/g21/g7/g19/g1/g1/g21/g47/g1/g28/g46/g28/g44/g60/g61/g32/g1/g52/g40/g32/g1/g31/g60/g46/g28/g45/g41/g30/g51/g1/g47/g33/g1/g14/g32/g46/g31/g32/g44/g41/g28/g46/g1/g1/g1/g48/g47/g48/g56/g44/g28/g53/g47/g46/g51/g66/g1
Photograph  source unknown. All rights reserved.This
content is excluded from our Creative Commons license.
For more information, see http://ocw.mit.edu/fairuse.
/g1/g1MIT faculty /g11/g47/g41/g46/g32/g31/g1/g3/g32/g44/g44/g1/g13/g28/g29/g51/g1/g41/g46/g1/g81/g89/g84/g80/g66/g11956-1978 /g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g69/g2/g1/g45/g28/g52/g40/g32/g45/g28/g53/g30/g28/g44/g1/g52/g40/g32/g47/g50/g60/g1/g47/g33/g1/g30/g50/g60/g48/g52/g47/g39/g50/g28/g48/g40/g60/g70/g1/g81/g89/g84/g85/g71/g81/g89/g84/g89/g1
/g1/g1/g1/g1/g1/g1/g1/g21/g2/g1/g11/g4/g15/g8/g6/g11/g4/g16/g5/g4/g10/g1/g15/g8/g6/g13/g14/g19/g1/g13/g7/g1/g5/g13/g11/g11/g18/g12/g9/g5/g4/g16/g13/g12/g22/g1/g25/g28/g26/g27/g1
/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #26 Huffmans Coding Algorithm 
Begin with the set S of symbols to be encoded as binary strings, 
together with the probability p(s) for each symbol s in S.  
Repeat the following steps until there is only 1 symbol left in S:  
Choose the two members of S having lowest probabilities. 
Choose arbitrarily to resolve ties. 
Remove the selected symbols from S, and create a new node of 
the decoding tree whose children (sub-nodes) are the symbols 
you've removed. Label the left branch with a 0, and the right 
branch with a 1.  
Add to S a new symbol that represents this new node. Assign 
this new symbol a probability equal to the sum of the 
probabilities of the two nodes it replaces.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #28 Another Variable-length Code (not!) 
Heres an alternative variable-length for the example on the 
previous page: 
/g4/g9/g22/g9/g18/g1 /g3/g16/g7/g17/g8/g13/g16/g11/g1
/g2/g1 /g80/g1
/g3/g1 /g81/g1
/g4/g1 /g80/g80/g1
/g5/g1 /g80/g81/g1
Why isnt this a workable code?    The expected length of an encoded message is                 (.333+.5)(1) + (.083 + .083)(2) = 1.22 bits  which even beats the entropy bound</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #20 e.g., Binary entropy function  h(p)
Heads   (or C=1) with  
pr p obability     
Tails  (or C=0) with 
probability  1p
h(p)
p1.0 
0.5 1.0 
   0 
  0 0.5 1.0 
H(C)=plog2p(1p)log2(1p)=h(p)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #13 /g7/g28/g51/g52/g73/g33/g47/g50/g58/g28/g50/g31/g1/g81/g80/g80/g1/g60/g32/g28/g50/g51/g1
/g23/g41/g28/g1
/g21/g32/g44/g32/g48/g40/g47/g46/g32/g1/g74/g69/g10/g45/g48/g50/g47/g57/g32/g45/g32/g46/g52/g1/g41/g46/g1/g21/g32/g44/g32/g39/g50/g28/g48/g40/g60/g70/g64/g1/g48/g28/g52/g32/g46/g52/g1/g77/g1
/g81/g87/g84/g64/g84/g85/g86/g64/g1/g3/g32/g44/g44/g1/g81/g88/g87/g86/g75/g1
/g24/g41/g50/g32/g44/g32/g51/g51/g1/g52/g32/g44/g32/g39/g50/g28/g48/g40/g60/g1/g74/g14/g28/g50/g30/g47/g46/g41/g1/g81/g89/g80/g81/g75/g1
/g2/g14/g1/g50/g28/g31/g41/g47/g1/g74/g7/g32/g51/g51/g32/g46/g31/g32/g46/g1/g81/g89/g80/g86/g75/g1/g1
/g7/g14/g1/g50/g28/g31/g41/g47/g1/g74/g2/g50/g45/g51/g52/g50/g47/g46/g39/g1/g81/g89/g83/g83/g75/g1/g1
/g21/g32/g44/g32/g57/g41/g51/g41/g47/g46/g1/g29/g50/g47/g28/g31/g30/g28/g51/g53/g46/g39/g1/g29/g60/g1/g52/g40/g32/g1/g3/g3/g4/g1/g74/g81/g89/g83/g86/g75/g1
/g67/g1
/g3/g32/g44/g44/g1/g13/g28/g29/g51/g1/g39/g28/g44/g28/g59/g60/g1/g47/g33/g1/g50/g32/g51/g32/g28/g50/g30/g40/g32/g50/g51/g1
/g15/g60/g49/g56/g41/g51/g52/g64/g1/g3/g47/g31/g32/g64/g1/g9/g28/g50/g52/g44/g32/g60/g64/g1/g67/g1
/g1/g1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #15 A probabilistic theory requires at least a one-slide 
checklist on Probabilistic Models! 
/g22/g46/g41/g57/g32/g50/g51/g32/g1/g3/g1/g47/g33/g1/g32/g44/g32/g45/g32/g46/g52/g28/g50/g60/g1/g47/g56/g52/g30/g47/g45/g32/g51/g1 /g51/g81/g64/g1/g51/g82/g64/g1/g67/g64/g1/g51/g15/g64/g1/g67/g1/g66/g1/g16/g46/g32/g1/g28/g46/g31/g1/g47/g46/g44/g60/g1/g47/g46/g32/g1
/g47/g56/g52/g30/g47/g45/g32/g1/g41/g46/g1/g32/g28/g30/g40/g1/g32/g59/g48/g32/g50/g41/g45/g32/g46/g52/g1/g47/g50/g1/g50/g56/g46/g1/g47/g33/g1/g52/g40/g32/g1/g45/g47/g31/g32/g44/g66/g1
/g6/g57/g32/g46/g52/g51/g1/g2/g64/g1/g3/g64/g1/g4/g64/g1/g67/g1 /g28/g50/g32/g1/g51/g56/g29/g51/g32/g52/g51/g1/g47/g33/g1/g47/g56/g52/g30/g47/g45/g32/g51 /g66/g1/g24/g32/g1/g51/g28/g60/g1/g32/g57/g32/g46/g52/g1/g2/g1/g40/g28/g51/g1 /g47/g30/g30/g56/g50/g50/g32/g31/g1/g41/g33/g1/g52/g40/g32/g1
/g47/g56/g52/g30/g47/g45/g32/g1/g47/g33/g1/g52/g40/g32/g1/g32/g59/g48/g32/g50/g41/g45/g32/g46/g52/g1/g44/g41/g32/g51/g1/g41/g46/g1/g2/g66/g1
/g6/g57/g32/g46/g52/g51/g1/g33/g47/g50/g45/g1/g28/g46/g1/g69/g28/g44/g39/g32/g29/g50/g28/g70/g1/g47/g33/g1/g51/g32/g52/g51/g64/g1/g41/g66/g32/g66/g64/g1/g2/g1 /g17/g18/g1/g3/g1/g74/g56/g46/g41/g47/g46/g64/g1/g28/g44/g51/g47/g1/g58/g50/g41/g55/g32/g46/g1 /g2/g90/g3/g75/g1/g41/g51/g1/g28/g46/g1
/g32/g57/g32/g46/g52/g64/g1/g2/g1/g5/g16/g8/g1/g3/g1/g74/g41/g46/g52/g32/g50/g51/g32/g30/g53/g47/g46/g64/g1/g28/g44/g51/g47/g1/g58/g50/g41/g55/g32/g46/g1 /g2/g3/g75/g1/g41/g51/g1/g28/g46/g1/g32/g57/g32/g46/g52/g64/g1/g16/g17/g20/g1/g2/g1/g74/g30/g47/g45/g48/g44/g32/g45/g32/g46/g52/g64/g1
/g28/g44/g51/g47/g1/g58/g50/g41/g55/g32/g46/g1/g2/g30/g75/g1/g41/g51/g1/g28/g46/g1/g32/g57/g32/g46/g52/g66/g1/g20/g47/g1/g3/g1/g28/g46/g31/g1/g52/g40/g32/g1/g46/g56/g44/g44/g1/g51/g32/g52/g1/g24/g1/g28/g50/g32/g1/g28/g44/g51/g47/g1/g32/g57/g32/g46/g52/g51/g66/g1
/g17/g50/g47/g29/g28/g29/g41/g44/g41/g53/g32/g51/g1/g28/g50/g32/g1/g31/g32/g35/g46/g32/g31/g1/g47/g46/g1/g32/g57/g32/g46/g52/g51/g64/g1/g51/g56/g30/g40/g1/g52/g40/g28/g52/g1/g80/g1/g92/g1/g17/g74/g2/g75/g1/g92/g1/g81/g64/g1/g1/g17/g74 /g3/g75/g91/g81/g64/g1/g28/g46/g31/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1
/g17/g74/g2/g90/g3/g75/g91/g17/g74/g2/g75/g90/g17/g74/g3/g75/g1/g41/g33/g1/g2/g1/g28/g46/g31/g1/g3/g1/g28/g50/g32/g1/g45/g56/g52/g56/g28/g44/g44/g60/g1/g32/g59/g30/g44/g56/g51/g41/g57/g32/g64/g1/g41/g66/g32/g66/g1/g41/g33/g1/g2/g3/g91/g24/g66/g1/g1/g14/g47/g50/g32/g1
/g39/g32/g46/g32/g50/g28/g44/g44/g60/g64/g1/g17/g74/g2/g90/g3/g75/g91/g17/g74/g2/g75/g90/g17/g74/g3/g75/g73/g17/g74/g2/g3/g75/g66/g1/g1
/g6/g57/g32/g46/g52/g51/g1/g2/g64/g1/g3/g64/g1/g4/g64/g1/g5/g64/g1/g6/g64/g1/g32/g52/g30/g66/g64/g1/g28/g50/g32/g1/g51/g28/g41/g31/g1/g52/g47/g1/g29/g32/g1/g74/g45/g56/g52/g56/g28/g44/g44/g60/g75/g1/g41/g46/g31/g32/g48/g32/g46/g31/g32/g46/g52/g1/g41/g33/g1/g42/g47/g41/g46/g52/g1
/g48/g50/g47/g29/g28/g29/g41/g44/g41/g52/g60/g1/g47/g33/g1/g32/g57/g32/g50/g60/g1/g30/g47/g45/g29/g41/g46/g28/g53/g47/g46/g1/g47/g33/g1/g52/g40/g32/g51/g32/g1/g32/g57/g32/g46/g52/g51/g1/g33/g28/g30/g52/g47/g50/g51/g1/g41/g46/g52/g47/g1/g48/g50/g47/g31/g56/g30/g52/g1/g47/g33/g1
/g41/g46/g31/g41/g57/g41/g31/g56/g28/g44/g1/g48/g50/g47/g29/g28/g29/g41/g44/g41/g53/g32/g51/g64/g1/g51/g47/g1/g17/g74/g2/g3/g4/g5/g6/g75/g91/g17/g74/g2/g75/g17/g74/g3/g75/g17/g74/g4/g75/g17/g74/g5/g75/g17/g74/g6/g75/g64/g1/g1/g17/g74/g2/g3/g4/g5/g75/g91/g17/g74/g2/g75/g17/g74/g3/g75/g17/g74/g4/g75/g17/g74/g5/g75/g64/g1/g17/g74/g2/g5/g6/g75/g91/g17/g74/g2/g75/g17/g74/g5/g75/g17/g74/g6/g75/g64/g1/g32/g52/g30/g66/g1/g1
/g4/g47/g46/g31/g41/g53/g47/g46/g28/g44/g1/g48/g50/g47/g29/g28/g29/g41/g44/g41/g52/g60/g1 /g17/g74/g2/g64/g1/g39/g41/g57/g32/g46/g1/g52/g40/g28/g52/g1/g3/g1/g40/g28/g51/g1/g47/g30/g30/g56/g50/g50/g32/g31/g75/g1/g91/g1/g17/g74/g2/g72/g3/g75/g1/g91/g1/g17/g74/g2/g3/g75/g71/g17/g74/g3/g75/g66/g1
/g6/g59/g48/g32/g30/g52/g32/g31/g1/g57/g28/g44/g56/g32/g1/g47/g33/g1/g28/g1/g50/g28/g46/g31/g47/g45/g1/g57/g28/g50/g41/g28/g29/g44/g32/g1/g41/g51/g1/g52/g40/g32/g1/g48/g50/g47/g29/g28/g29/g41/g44/g41/g52/g60/g73/g58/g32/g41/g39/g40/g52/g32/g31/g1/g28/g57/g32/g50/g28/g39/g32/g66 /g1</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #21 Connection to (Binary) Coding 
Suppose p=1/1024, i.e., very small probability of getting a
Head, typically one Head in 1024 trials. Then
h(p)=(1/1024)log2(1024 /1)+(1023/1024)log2(1024 /1023)
=.0112 bits of uncertainty or information per trialon average 
So using 1024 binary digits (C=0 or 1) to code the results of
1024 tosses of this particular coin seems inordinately
wasteful, i.e., 1 binary digit per trial. Can we get closer to anaverage of .0112 binary digits/trial?
Yes! Confusingly, a binary digit  is also referred to as a bit!  
Binary coding: Mapping source symbols to binary digits</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Expected Information as  
Uncertainty or Entropy 
Consider a discr S ete random variable    , which may represent 
the set of possible symbols to be transmitted at a particular 
s1,s2,...,sN time, taking possible values                     ,  with respective 
pr pS(s1),pS(s2),...,pS(sN) obabilities                                              . 
The entropy H(S)S           of     is the expected (or mean or average)
value of the information obtained by learning the outcome Sof    : 
N N 1
H(S)=pS(si)I(S=si)=pS(si)log2
i=1 i=1 pS(si)
pS(si) When all the              are equal 1/N  (with value         ), then    
H(S)=log2N or  N=2H(S)
6.02 Fall 2012 This is the maximum attainable value! Lecture 1, Slide #19</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #27 Huffman Coding Example 
Initially S = { (A, 1/3)  (B, 1/2)  (C, 1/12)  (D, 1/12) } 
CD First iteration 0 1 
Symbols in S with lowest probabilities: C and D C D Create new node 
Add new symbol to S = { (A, 1/3)  (B, 1/2)  (CD, 1/6) } ACD 
Second iteration 0 1 
0 1 A Symbols in S with lowest probabilities: A and CD 
C D Create new node 
0 1 Add new symbol to S = { (B, 1/2)  (ACD, 1/2) } 
Third iteration B 0 1 
0 1 Symbols in S with lowest probabilities: B and ACD A 
Create new node C D 
Add new symbol to S = { (BACD, 1) } 
Done</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #7 Samuel/g1/g7/g66/g3/g66/g1/g14/g47/g50/g51/g32/g1
/g10/g46/g57/g32/g46/g52/g32/g31/g1/g74/g81/g88/g83/g82/g1/g47/g46/g58/g28/g50/g31/g51/g64/g1/g48/g28/g52/g32/g46/g52/g1/g77/g81/g64/g86/g84/g87/g1/g41/g46/g1/g81/g88/g84/g80/g75/g1/g52/g40/g32/g1/g45/g47/g51/g52/g1/g48/g50/g28/g30/g53/g30/g28/g44/g1/g33/g47/g50/g45/g1
/g47/g33/g1/g32/g44/g32/g30/g52/g50/g41/g30/g28/g44/g1/g52/g32/g44/g32/g39/g50/g28/g48/g40/g60 /g64/g1/g41/g46/g30/g44/g56/g31/g41/g46/g39/g1/g43/g32/g60/g51/g64/g1/g58/g41/g50/g32/g1/g28/g50/g50/g28/g46/g39/g32/g45/g32/g46/g52/g51/g64/g1
/g32/g44/g32/g30/g52/g50/g47/g45/g28/g39/g46/g32/g52/g51/g64/g1/g45/g28/g50/g43/g41/g46/g39/g1/g31/g32/g57/g41/g30/g32/g51/g64/g1/g50/g32/g44/g28/g60/g51/g64/g1/g67/g64/g1/g28/g46/g31/g1/g14/g47/g50/g51/g32/g1/g30/g47/g31/g32/g63/g1
/g24/g47/g50/g43/g32/g31/g1/g53/g50/g32/g44/g32/g51/g51/g44/g60/g1/g52/g47/g1/g32/g51/g52/g28/g29/g44/g41/g51/g40/g1/g52/g40/g32/g1/g52/g32/g30/g40/g46/g47/g44/g47/g39/g60/g1
/g2/g38/g32/g50/g1/g41/g46/g41/g53/g28/g44/g1/g51/g52/g50/g56/g39/g39/g44/g32/g51/g64/g1/g52/g32/g44/g32/g39/g50/g28/g48/g40/g60/g1/g58/g28/g51/g1/g49/g56/g41/g30/g43/g44/g60/g1/g28/g31/g47/g48/g52/g32/g31/g1/g28/g46/g31/g1/g58/g41/g31/g32/g44/g60/g1
/g31/g32/g48/g44/g47/g60/g32/g31/g1/g1
/g21/g50/g28/g46/g51/g73/g2/g52/g44/g28/g46/g53/g30/g1/g30/g28/g29/g44/g32/g1/g28/g55/g32/g45/g48/g52/g51/g1/g81/g88/g85/g87/g1/g74/g81/g86/g1/g40/g47/g56/g50/g51/g1/g52/g47/g1/g51/g32/g46/g31/g1/g89/g88/g1/g58/g47/g50/g31/g51/g1/g33/g50/g47/g45/g1/g18/g56/g32/g32/g46/g1
/g23/g41/g30/g52/g47/g50/g41/g28/g1/g52/g47/g1/g17/g50/g32/g51/g41/g31/g32/g46/g52/g1/g3/g56/g30/g40/g28/g46/g28/g46/g63/g75/g64/g1/g81/g88/g85/g88/g64/g1/g81/g88/g86/g85/g64/g1/g35/g46/g28/g44/g44/g60/g1/g51/g56/g30/g30/g32/g51/g51/g1/g41/g46/g1/g81/g88/g86/g86/g1/g74/g88/g1/g58/g47/g50/g31/g51/g71
/g45/g41/g46/g56/g52/g32/g75/g1
/g21/g50/g28/g46/g51/g73/g30/g47/g46/g53/g46/g32/g46/g52/g28/g44/g1/g22/g20/g1/g41/g46/g1/g81/g88/g86/g81/g1/g74/g32/g34/g32/g30/g53/g57/g32/g44/g60/g1/g32/g46/g31/g32/g31/g1/g52/g40/g32/g1/g17/g47/g46/g60/g1/g6/g59/g48/g50/g32/g51/g51/g75/g1
/g21/g50/g28/g46/g51/g73/g17/g28/g30/g41/g35/g30/g1/g81/g89/g80/g82/g1
/g21/g32/g44/g32/g39/g50/g28/g48/g40/g60/g1/g52/g50/g28/g46/g51/g33/g47/g50/g45/g32/g31/g1/g30/g47/g45/g45/g56/g46/g41/g30/g28/g53/g47/g46/g1/g74/g52/g50/g28/g46/g51/g73/g2/g52/g44/g28/g46/g53/g30/g1/g53/g45/g32/g1/g33/g50/g47/g45/g1/g81/g80/g1/g31/g28/g60/g51/g1
/g29/g60/g1/g51/g40/g41/g48/g1/g52/g47/g1/g45/g41/g46/g56/g52/g32/g51/g1/g29/g60/g1/g52/g32/g44/g32/g39/g50/g28/g48/g40/g75/g1/g28/g46/g31/g1/g30/g47/g45/g45/g32/g50/g30/g32/g64/g1/g28/g44/g51/g47/g1/g51/g48/g56/g50/g50/g32/g31/g1/g45/g28/g42/g47/g50/g1
/g31/g32/g57/g32/g44/g47/g48/g45/g32/g46/g52/g51/g1/g41/g46/g1/g6/g6/g1/g52/g40/g32/g47/g50/g60/g1/g76/g1/g48/g50/g28/g30/g53/g30/g32/g1/g74/g9/g32/g46/g50/g60/g64/g1/g12/g32/g44/g57/g41/g46/g64/g1/g9/g32/g28/g57/g41/g51/g41/g31/g32/g64/g1/g17/g56/g48/g41/g46/g64/g1/g67/g75/g1
/g1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #3 /g5/g41/g39/g41/g52/g28/g44/g1/g57/g51/g66/g1/g2/g46/g28/g44/g47/g39/g1/g4/g47/g45/g45/g56/g46/g41/g30/g28/g53/g47/g46/g1
/g2/g15/g2/g13/g16/g8/g1/g1/g4/g47/g45/g45/g56/g46/g41/g30/g28/g53/g46/g39/g1/g28/g1/g30/g47/g46/g53/g46/g56/g47/g56/g51/g73/g53/g45/g32/g1/g58/g28/g57/g32/g33/g47/g50/g45/g1/g1/g1/g1/g1/g1/g1/g1/g1/g1
/g74/g32/g66/g39/g66/g64/g1/g57/g47/g44/g52/g28/g39/g32/g1/g33/g50/g47/g45/g1/g28/g1/g45/g41/g30/g50/g47/g48/g40/g47/g46/g32/g75/g64/g1/g57/g41/g28/g1/g28/g45/g48/g44/g41/g52/g56/g31/g32/g1/g45/g47/g31/g56/g44/g28/g53/g47/g46/g1
/g74/g2/g14/g75/g1/g47/g50/g1/g33/g50/g32/g49/g56/g32/g46/g30/g60/g1/g45/g47/g31/g56/g44/g28/g53/g47/g46/g1/g74/g7/g14/g75/g1/g47/g50/g1/g66/g66/g66/g1
/g2/g46/g28/g44/g47/g39/g1/g32/g44/g32/g30/g52/g50/g47/g46/g41/g30/g51/g1
/g7/g41/g31/g32/g44/g41/g52/g60/g1/g52/g47/g1/g52/g40/g32/g1/g58/g28/g57/g32/g33/g47/g50/g45/g1
/g5/g10/g8/g10/g21/g2/g13/g1/g1/g4/g47/g45/g45/g56/g46/g41/g30/g28/g53/g46/g39/g1/g28/g1/g45/g32/g51/g51/g28/g39/g32/g1/g30/g47/g45/g48/g50/g41/g51/g41/g46/g39/g1/g28/g1/g31/g41/g51/g30/g50/g32/g52/g32/g73
/g53/g45/g32/g1/g51/g32/g49/g56/g32/g46/g30/g32/g1/g47/g33/g1/g51/g60/g45/g29/g47/g44/g51 /g1/g33/g50/g47/g45/g1/g51/g47/g45/g32/g1 /g51/g47/g56/g50/g30/g32/g1/g28/g44/g48/g40/g28/g29/g32/g52/g1
/g16/g38/g32/g46/g1/g30/g47/g31/g32/g31/g1/g47/g46/g52/g47/g1/g51/g47/g45/g32/g1/g47/g52/g40/g32/g50/g1/g51/g32/g49/g56/g32/g46/g30/g32/g1/g47/g33/g1/g51/g60/g45/g29/g47/g44/g51/g1/g52/g40/g28/g52/g68/g51/g1/g28/g31/g28/g48/g52/g32/g31/g1/g52/g47/g1
/g52/g40/g32/g1/g30/g47/g45/g45/g56/g46/g41/g30/g28/g53/g47/g46/g1/g30/g40/g28/g46/g46/g32/g44/g64/g1/g32/g66/g39/g66/g64/g1 /g29/g41/g46/g28/g50/g60/g1/g31/g41/g39/g41/g52/g51/g64/g1/g80/g1/g28/g46/g31/g1/g81/g66/g1
/g16/g38/g32/g46/g1/g41/g46/g57/g47/g44/g57/g41/g46/g39/g1/g28/g46/g28/g44/g47/g39/g1/g30/g47/g45/g45/g56/g46/g41/g30/g28/g53/g47/g46/g1/g28/g30/g50/g47/g51/g51/g1/g52/g40/g32/g1/g48/g40/g60/g51/g41/g30/g28/g44/g1/g30/g40/g28/g46/g46/g32/g44/g1/g1
/g7/g41/g31/g32/g44/g41/g52/g60/g1/g52/g47/g1/g52/g40/g32/g1/g45/g32/g51/g51/g28/g39/g32/g1
/g24/g32/g44/g44/g1/g51/g56/g41/g52/g32/g31/g1/g52/g47/g1/g50/g41/g31/g41/g46/g39/g1/g52/g40/g32/g1/g51/g52/g28/g39/g39/g32/g50/g41/g46/g39/g1/g39/g50/g47/g58/g52/g40/g1/g41/g46/g1/g30/g47/g45/g48/g56/g52/g28/g53/g47/g46/g28/g44/g1/g48/g47/g58/g32/g50/g64/g1
/g51/g52/g47/g50/g28/g39/g32/g64/g1/g29/g41/g39/g1/g31/g28/g52/g28/g64/g1/g67/g1
/g1</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #1 
6.02 Fall 2012 
Lecture #1 
Digital vs. analog communication
The birth of modern digital communication
Information and entropy
Codes, Huffman coding</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #18 /g17/g50/g47/g48/g32/g50/g53/g32/g51/g1/g47/g33/g1/g10/g46/g33/g47/g50/g45/g28/g53/g47/g46/g1/g31/g32/g35/g46/g41/g53/g47/g46/g1
/g2/g1/g44/g47/g58/g32/g50/g73/g48/g50/g47/g29/g28/g29/g41/g44/g41/g52/g60/g1/g47/g56/g52/g30/g47/g45/g32/g1/g60/g41/g32/g44/g31/g51/g1/g40/g41/g39/g40/g32/g50/g1/g41/g46/g33/g47/g50/g45/g28/g53/g47/g46/g1/g1
/g2/g1/g40/g41/g39/g40/g44/g60/g1/g41/g46/g33/g47/g50/g45/g28/g53/g57/g32/g1/g47/g56/g52/g30/g47/g45/g32/g1/g31/g47/g32/g51/g1/g46/g47/g52/g1/g46/g32/g30/g32/g51/g51/g28/g50/g41/g44/g60/g1/g45/g32/g28/g46/g1/g28/g1
/g45/g47/g50/g32/g1/g57/g28/g44/g56/g28/g29/g44/g32/g1/g47/g56/g52/g30/g47/g45/g32/g64/g1/g47/g46/g44/g60/g1/g28/g1/g45/g47/g50/g32/g1/g51/g56/g50/g48/g50/g41/g51/g41/g46/g39/g1/g47/g56/g52/g30/g47/g45/g32/g64/g1/g41/g66/g32/g66/g64/g1
/g52/g40/g32/g50/g32/g68/g51/g1/g46/g47/g1/g41/g46/g52/g50/g41/g46/g51/g41/g30/g1/g57/g28/g44/g56/g32/g1/g29/g32/g41/g46/g39/g1/g28/g51/g51/g32/g51/g51/g32/g31/g1/g74/g30/g28/g46/g1/g52/g40/g41/g46/g43/g1/g47/g33/g1
/g41/g46/g33/g47/g50/g45/g28/g53/g47/g46/g1/g28/g51/g1/g31/g32/g39/g50/g32/g32/g1/g47/g33/g1/g51/g56/g50/g48/g50/g41/g51/g32 /g75/g1
/g16/g38/g32/g46/g1/g56/g51/g32/g31/g1/g33/g28/g30/g52/g65/g1/g21/g40/g32/g1/g41/g46/g33/g47/g50/g45/g28/g53/g47/g46/g1/g41/g46/g1/g41/g46/g31/g32/g48/g32/g46/g31/g32/g46/g52/g1/g32/g57/g32/g46/g52/g51/g1/g41/g51/g1
/g28/g31/g31/g41/g53/g57/g32/g66/g1/g74/g4/g28/g56/g53/g47/g46/g65/g1/g52/g40/g47/g56/g39/g40/g1/g1/g41/g46/g31/g32/g48/g32/g46/g31/g32/g46/g30/g32/g1/g41/g51/g1/g51/g56/g36/g30/g41/g32/g46/g52/g1/g33/g47/g50/g1
/g28/g31/g31/g41/g53/g57/g41/g52/g60/g64/g1/g41/g52/g1/g41/g51/g1/g46/g47/g52/g1/g46/g32/g30/g32/g51/g51/g28/g50/g60/g64/g1/g29/g32/g30/g28/g56/g51/g32/g1/g58/g32/g1/g30/g28/g46/g1/g40/g28/g57/g32/g1/g17/g74/g2/g3/g4/g75/g91/g17
/g74/g2/g75/g17/g74/g3/g75/g17/g74/g4/g75/g1/g32/g57/g32/g46/g1/g58/g40/g32/g46/g1/g2/g64/g3/g64/g4/g1/g28/g50/g32/g1/g46/g47/g52/g1/g41/g46/g31/g32/g48/g32/g46/g31/g32/g46/g52/g1/g73/g73/g73/g1
/g41/g46/g31/g32/g48/g32/g46/g31/g32/g46/g30/g32/g1/g50/g32/g49/g56/g41/g50/g32/g51/g1/g52/g40/g32/g1/g48/g28/g41/g50/g58/g41/g51/g32/g1/g42/g47/g41/g46/g52/g1/g48/g50/g47/g29/g28/g29/g41/g44/g41/g53/g32/g51/g1/g52/g47/g1/g28/g44/g51/g47/g1/g33/g28/g30/g52/g47/g50/g66/g75/g1/g1</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.02 Fall 2012 Lecture 1, Slide #16 Measuring Information  
 Shannons (and Hartleys) definition of the information obtained on 
sibeing told the outcome     of a pr S obabilistic experiment     :
1
I(S=si)=log2
pS(si)
wher pS(si) e           is the pr S=siobability of the event              . 
The unit of measurement (when the log is base-2) is the bit 
(binary information uni t --- not the same as binary digit!). 
1 bit of information corresponds to 
pS(si)=0.5                   . So, for example, when the
outcome of a fair coin toss is revealed to us, we have received 1 bit of information. 
Information is the  resolution of uncertainty  
Shannon 
Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 15: Modulation/demodulation
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec15/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #19 
No longer confined to  
its 256-sample slot, so  causes intersymbol interference  (ISI).   
Corresponding pulse in  time, i.e., lowpass filtered  version of rectangular pulse</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Back to our  Audiocom lab example
6.02 Fall 2012 Lecture 15 Slide #13</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #11 
Magnitude of preceding DTFT 
https://ccrma.stanford.edu/~jos/sasp/Rectangular_Window.html Courtesy of Julius O. Smith. Used with permission.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #1 
6.02 Fall 2012 
Lecture #15 
Modulation 
 to match the transmitted signal to 
the physical medium Demodulation</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #25 
The lowpass filtered 
shaped pulse conforms  more tightly to the  256-sample slot,  and settles a little quicker After passing the two pulses through a 400 Hz cutoff lowpass filter:</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #3 
6.02 Fa ll 2012 
LLecture 15 Slide #3 
 Single Link Communication Model
Digitize 
(if needed) Original source 
Source coding Source binary digits 
(message bits) 
Bit stream Render/display,  
etc. Receiving app/user 
Source decoding 
Bit stream 
Channel 
Coding 
(bit error  
correction) Recv 
samples 
+ 
Demapper Mapper 
+ 
Xmit 
samples 
Bits Signals 
(Voltages) 
over 
physical link 
Signals  
(Voltages)Channel 
Decoding 
(reducing or 
removing  
bit errors) 
Render/display,  
etc. 
Receiving app/user 
 g a
Source decoding 
etc
de
BBiitt ssttream 
 B
Digitize 
(if needed) 
Original sour ce 
iti
Source  coding 
SSoouurrccee bbiinnaarryy ddiiggiittss 
 SS
(message bits) 
BBiitt sttreamm 
 BBEnd-host 
computers 
(Bits</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>How Low Can We   Go?
6.02 Fall 2012 Lecture 15 Slide #21</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #24 
|DTFT| of  
rectangular pulse 
Negative|DTFT| of  shaped pulse Frequency content of shaped pulse  only extends to here, around 1500 Hz In the spectral domain:</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #30 
Corresponding pulse in  
time, i.e., bandpass  Wont do  filtered version of  at all!! rectangular pulse</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #18 
Zooming in: 
0 400 Hz 256 = N</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #6 
 PC Magazine. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Phase   of the frequency response 
 is important too!
Maybe not if we are only interested in audio, 
because the ear is not so sensitive to phase 
distortions 
But its certainly important if we are using an 
audio channel to transmit non-audio signals such 
as digital signals representing 1s and 0s, not intended for the ear 
6.02 Fall 2012 Lecture 15 Slide #7</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #27 
 PC Magazine. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #35 
0 Hz 100 Hz, lower cutoff  
of bandpass filter Zooming in: 
1000 Hz 128, i.e. half height of original 
10,000 Hz, upper  cutoff of bandpass filter  
1000 Hz</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #14 
  
 x[n]=u[n]-u[n-256]</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #28 
Spectrum of  
rectangular pulse  after ideal bandpass filtering,  100 Hz  to 10,000 Hz  
10,000 Hz 
0</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Is Modulation Linear? Ti  me-Invariant? 
6.02 Fall 2012 Lecture 15 Slide #32 x[n] 
cos(cn) t[n] 
 as a system that takes input x[n] and produces  
output t[n] for transmission?  
Yes, linear!  No, not time-invariant!</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #33 
So for our rectangular pulse example: 
Time domain: 
Pulse modulated onto 1000 Hz carrier</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #12 
DTFT of x[n]= u[n]  u[n-10], 
rectangular pulse of length 10 starting at time 0  
http://cnx.org/content/m0524/latest/ Courtesy of Don Johnson. Used with permission; available under a CC-BY license.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>But loudspeakers are bandpass,  
  
 not lowpass
6.02 Fall 2012 Lecture 15 Slide #26</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>At the Receiver: Demodulation
In principle, this is (as easy as) modulation again: 
If the received signal is 
                         r[n] = x[n]cos( cn),  
then simply compute 
                        d[n] = r[n]cos(cn) 
                               = x[n]cos2(cn) 
                               = 0.5 {x[n] + x[n]cos(2 cn)} 
 What does the spectrum of d[n] look like?  
What constraint on the bandwidth of x[n] is needed 
for perfect recovery of x[n]?           
6.02 Fall 2012 Lecture 15 Slide #38</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #34 
Corresponding  
spectrum of signal modulated onto carrier 
0</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #23 
Slightly round the transitions 
from 0 to 1, and from 1 to 0,  by making them sinusoidal,  just 30 samples on each end.  A shaped pulse versus a rectangular pulse:</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #8 
To gauge how it will fare on  
lowpass and bandpass channels,  lets look at the spectral content  of a rectangular pulse,  
 x[n]=u[n]-u[n-256] ,  
 of the kind weve been using  in on-off signaling in our  Audiocom lab.  Any guesses as to spectral shape?</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #29 
Zooming in: 
100 Hz 0 10,000 Hz</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>6.02 Fall 2012 Lecture 15 Slide #36 
Pulse modulated  
onto 1000 Hz carrier makes it through the bandpass channel with very little  distortion</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>The Solution: Modulation
Shift the spectrum of the signal x[n] into the 
loudspeakers passband by modulation!  
6.02 Fall 2012 Lecture 15 Slide #31 x[n]cos( cn)=0.5x[n](ejcn+ejcn)
=0.5
2[X(')ej('+c)n
&lt;2&gt; d'+ X(")ej("c)n
&lt;2&gt; d"]
=0.5
2[X(c)ejn
&lt;2&gt; d+ X(+c)ejn
&lt;2&gt; d]
Spectrum of modulated signal comprises half-height 
replications of X() centered as c (i.e., plus and minus  
the carrier frequency). So choose carrier frequency comfortably in the passband, leaving room around it for the spectrum of x[n].</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Input/Output Behav  ior of 
LT  I System in Frequency Domain
6.02 Fall 2012 Lecture 15 Slide #5 H() x[n]=1
2X()ejn
&lt;2&gt; d y[n]=1
2H()X()ejn
&lt;2&gt; d
Y()= H()X()y[n]=1
2Y()ejn
&lt;2&gt; d
Spectral content  
of output Spectral content  of input 
Frequency response  of system</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Effect of Low-P  ass Channel
6.02 Fall 2012 Lecture 15 Slide #20 
6 02 Fall 2012</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Complementary/dual behav  ior in 
 time and frequency domains
Wider in time, narrower in frequency; and vice 
versa. 
This is actually the basis of the uncertainty principle 
in physics! 
Smoother in time, sharper in frequency; and vice versa 
Rectangular pulse in time is a (periodic) sinc in 
frequency, while rectangular pulse in frequency is 
a sinc in time; etc. 
  
6.02 Fall 2012 Lecture 15 Slide #22</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 17: Packet switching
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec17/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>11/7/12
4
Traffic Version Flow Label Class 
Length Next Header Hop Limit 
IPVersion6header
Destination Address 
Source Address 
6.02Fall20126 02 Fall 2012 Lecture 17 Slide #13 6 02 Fall 2012 Lecture 17 Slide #13 6 02 Fall 2012 Lecture 17 Slide #13 6 02 Fall 2012 Lecture 17 Slide #13 6.02 Fall 2012 Lecture17,Slide#13LectuLecture re1717, Fall ,SliSlide#1de #13 6.02 2012 3 6.02Fall2012 Lecture17,Slide#14
Router
PacketSwitching:Multiplexing/Demultiplexing 
Routerhasaroutingtablethatcontainsinformationabout
whichlinktousetoreachadestination
Foreachlink,packetsaremaintainedinaqueue
Ifqueueisfull,packetswillbedropped
Demultiplexusinginformationinpacketheader
HeaderhasdestinationQueue
6.02Fall2012 Lecture17,Slide#15WhyPacketSwitchingWorks: 
StatisticalMultiplexing 
Aggregatng mutpe conersatons smooths usage 
6.02Fall2012 Lecture17,Slide#16
10mswindows
100mswindowstes 
 tme 
tes  tme 
rac in  ireess LA during a  ecture 
Noticehowburstsbecomesmoother
(butdontcompletelydisappear)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Queues  









11/7/12
6.02Fall2012 Lecture17,Slide#17
1secondwindows100mswindows
tes 
 tme tes  tme 
Noticehowburstsbecomesmoother
(butdontcompletelydisappear)
QueuesareEssentialinaPacketSwitchedNetwork  
	 Queuesmanagepacketsbetweenarrivalanddeparture
	 Theyarea"necessaryevil" 
 Neededtoabsorbbursts
	 Buttheyadddelaybymakingpacketswaituntillinkis
available
	 Sotheyshouldntbetoobig
6.02Fall2012	 Lecture17,Slide#19
1secondwindows
6.02Fall2012 Lecture17,Slide#18
10secondwindows
Best Effort DeliveryModel 
NoGuarantees!
	 Noguaranteeofdeliveryatall!
	 Packetsgetdropped(duetocorruptionorcongestion)
 UseAcknowledgement/Retransmissionprotocoltorecover
	 Howtodeter minewhentoretransmit?T imeout?
	 Eachpacketisindividuallyrouted
	 Mayarriveatfinaldestinationreorderedfromthetransmitor der
	 Nolatencyguaranteefordelivery
	 Delaysthroughthenetworkvarypackettopacket
	 Ifpacketisr etransmittedtoosoonduplicate
6.02Fall2012	 Lecture17,Slide#20
5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>11/7/12
SharingtheNetwork 
Wehavemanyapplicationlevelcommunications,
whichwellcallconnections,thatneedtomappedontoasmallernumberoflinks
Howshouldweshar ethelinksbetweenallthe
connections?
Twoapproachespossible:
Circuitswitching (isochronous)
Packetswitching(asynchronous)
6.02Fall2012	 Lecture17,Slide#5
Multiplexing/Demultiplexing 
Switch 
0 1 2 34 5 Slots=Frames
0 1 2 34 5
Onesharingtechnique:timedivisionmultiplexing(TDM)
	 Timedividedintoframesandframesdividedintoslots
	 Numberofslots=numberofconcurrentconversations
	 Relativeslotpositioninsideaframedeter mineswhich 
conversationthedatabelongsto 
 E.g.,slot0belongstother edconversation
 Mappingestablishedduringsetup,removedatteardown
	 Forwardingstepatswitch:consulttable
6.02Fall2012	 Lecture17,Slide#7CircuitSwitching
 Firstestablisha
Callee Caller circuitbetweenend
points
	 E.g.,donewhenyou 
dialaphonenumber 
	 Messagepropagates
(1) 
DATA Establish fromcallertoward
callee,establishing 
somestateineach 
switch  (2) Then,endssendCommunicate
data("talk")toeach 
other  
(3) Teardown  Aftercall,teardown 
(close)circuit
	 Removestate
6.02Fall2012	 Lecture17,Slide#6
TDMSharesLinkEqually,ButHasLimitations
Switch 
Timeslots
frames= 0 1 2 34 5 0 1 2 34 5
	 SupposelinkcapacityisCbits/sec
	 EachcommunicationrequiresRbits/sec
	 #framesinone"epoch"(oneframepercommunication) 
=C/R 
	 MaximumnumberofconcurrentcommunicationsisC/R
	 WhathappensifwehavemorethanC/Rcommunications?
	 Whathappensifthecommunicationsendsless/mor ethanR
bits/sec?
Designisunsuitablewhentraf ficarrivesinbursts 
6.02Fall2012	 Lecture17,Slide#8
2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 5: Error correction, syndrome decoding
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec05/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>13</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #14 Coping with Burst Errors by Interleav  ing
Well, can we think of a way to turn a B-bit error burst 
into B single-bit errors? 
Row-by-row Col-by-col 
B transmission B transmission 
order order 
Problem: Bits from a Solution: interleave bits 
particular codeword are from B different codewords.  
transmitted sequentially, Now a B-bit burst produces 
so a B-bit burst produces 1-bit errors in B different 
multi-bit errors. codewords.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #12 Burst Errors 
Correcting single-bit errors is good 
Similar ideas could be used to correct 
independent multi-bit errors 
But in many situations errors come in 
bursts: correlated multi-bit errors (e.g., 
fading or burst of interference on wireless channel, damage to storage media etc.).  How does single-bit error correction help with that?</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Syndrome Decoding  Steps
Step 1: For a given code and error patterns Ei, precompute  
Syndromes and store them 
HEi=Si
Step 2: For each received word, HR=S
compute the Syndrome 
Step 3: Find l such that Sl == S and apply correction for error El 
C=R+El
6.02 Fall 2012 Lecture 5, Slide #8</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #11 Linear Block Codes: Wrap-Up 
(n,k,d) codes have rate k/n and can correct up to 
floor((d-1)/2) bit errors 
Code words are linear operations over message 
bits: sum of any two code words is a code word 
Message + 1 parity bit: (n+1,n,2) code 
Good code rate, but only 1-bit error detection 
Replicating each bit c times is a (c,1,c) code 
Simple way to get great error correction; poor code rate 
Hamming single-error correcting codes are  
(n, n-m, 3) where n = 2m - 1 for m &gt; 1 
Adding an overall parity bit makes the code (n+1,n-m,4) 
Rectangular parity codes are (rc+r+c, rc, 3) codes 
Rate not as good as Hamming codes 
Syndrome decoding: general efficient approach for decoding linear block codes</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #6 Simple-minded Decoding  
Compare received n-bit word R = C + E against 
each of  2k valid codewords to see which one is HD 
1 away 
Doesnt exploit the nice linear structure of the 
code!</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #4 Parity Check Matrix  
Can restate the codeword For (9,4,4) example 
Dgeneration process as a 1
parity check or D
2
nullspace check 1 1 0 0 1 0 0 0 0 D3
1 1 0 1 0 0 0 0 0 D
C.HT0  4= 1 0 1 0 0 0 1 0 0 P1=05x1  0 1 0 1 0 0 0 1 0 P2
HT 1 1 1 1 0 0 0 0 1  P3(nk)xnC1xn=0
P4
(n-k) x n  P5parity check The parity check matrix,  n1 matrix code word 
vector 
(transpose)</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #10  Syndrome Decoding  Steps (9,4,4) example
Correction: 
 Since received word Syndrome [1 0 0 1 1]
T matches the  
Syndrome of the error [0 1 0 0 0 0 0 0 0],  apply this error to the received word to recover the original codeword 
Received word 
1 1 1 1 0 0 0 0 0 =1 0 1 1 0 0 0 0 [] [ 0+]
0 1 0 0 0 0 0 0 0[
Corrected codeword 
Error pattern from  matching Syndrome ]</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Syndrome Decoding  Steps (9,4,4) example
Codeword generation: 
1 0 0 0 1 0 1 0 1 
0 1 0 0 1 0 0 1 1
1 1 1 1[] =1 1 1[ 1 0 0 0 0 0 ]0 0 1 0 0 1 1 0 1 
 
0 0 0 1 0 1 0 1 1 
Received word in error:generation: 
1 0 1 1 0 0 0 0 0 =1 1 1 1 0 0 0 0 [] 0 [ +]
0 1 0 0 0 0 0 0 0[
Syndrome computation  1
 0for received word
1 1 0 0 1
0 0 0 0 11
0 0 1 1 0 1 0 0 0 01   
1 0 1 0 0 0 1 0 0 0=0
   0 1 0 1 0 0 0 1 0 01   
1 1 1 1 0 0 0 0 1 0 1    
0
0
6.02 Fall 2012     Lecture 5, Slide #9 ]




=







11001
000000010
100001111010001010001000101000101100000010011Precomputed  Syndrome for  
a given error pattern</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #16 Summary: example channel coding steps 
1.Break message stream into k-bit 011011101101 
blocks. Step 1: k=4 
2.Add redundant info in the form of 0110 
(n-k) parity bits to form n-bit 1110 
1101 codeword.  Goal: choose parity Step 2: (8,4,3) code bits so we can correct single-bit 
errors. 01101111 
11100101 
3.Interleave bits from a group of B 11010110 
codewords to protect against B- Step 3: B = 3 
bit burst errors. 011111110001100111101110 
4.Add unique pattern of bits to Step 4: sync = 0111110  start of each interleaved 
codeword block so receiver can 011111001111011100011001111001110 
tell how to extract blocks from Sync pattern has five consecutive 1s.  To received bitstream. prevent sync from appearing in message, 
5.Send new (longer) bitstream to bit-stuff 0s after any sequence of four 
1s in the message.  This step is easily transmitter. reversed at receiver (just remove 0 after any sequence of four consecutive 1s in 
the message).</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #3 A closer look at the Parity Check Matrix  A
k
 Parity equation Pj=Diaij
i=1
k
Parity relation Pj+Diaij=0
i=1
A=[aij]
So entry aij in i-th row, j-th column of A specifies 
whether data bit Di is used in constructing parity bit Pj 
 
Questions: Can two columns of A be the same? Should two 
columns of A be the same? How about rows?</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #15 Framing 
Looking at a received bit stream, how do we know 
where a block of interleaved codewords begins? 
Physical indication (transmitter turns on, 
beginning of disk sector, separate control channel) 
Place a unique bit pattern (frame sync sequence) in the bit stream to mark start of a block 
Frame = sync pattern + interleaved code word block 
Search for sync pattern in bit stream to find start of frame 
Bit pattern cant appear elsewhere in frame (otherwise our 
search will get confused), so have to make sure no legal 
combination of codeword bits can accidentally generate 
the sync pattern (can be tricky) 
Sync pattern cant be protected by ECC, so errors may 
cause us to lose a frame every now and then, a problem that will need to be addressed at some higher level of the 
communication protocol.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #5 Extr  acting d from H
Claim: The minimum distance d of the code C is 
the minimum number of columns of H that are 
linearly dependent, i.e., that can be combined to give the zero vector 
Proof: d = minimum-weight nonzero codeword in C 
One consequence: If A has two identical rows, then 
A
T has two identical columns, which means d is no 
greater than 2, so error correction is not possible.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #2 Matrix Notation for Linear Block Codes  
Task: given k-bit message, compute n-bit codeword.  We can 
use standard matrix arithmetic (modulo 2) to do the job.  For example, heres how we would describe the (9,4,4) rectangular code that includes an overall parity bit. 
DG=C
1 0 0 0 1 0 1 0 1 
0 1 0 0 1 0 0 1 1
D D D D  [] 
1 2 3 4 =D1D2D3D4 [ P1P P2P3P0 0 1 0 0 1 1 0 14 5
 
0 0 0 1 0 1 0 1 1  1n 
1k kn code word vector 
message generator in the row space of G 
vector matrix 
 The generator matrix, Gkxn = IkAk k(nk)]</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Syndrome Decoding  Matrix Fo  rm
Task: given n-bit code word, compute (n-k) syndrome bits.  
Again we can use matrix multiply to do the job.   
received word R=C+E
(n-k) x 1 compute Syndromes  H RTSsyndrome on receive word  =
vector 
To figure out the relationship of Syndromes to errors: 
H(C+E)T=S HCTuse =0
HET=Sfigure-out error type 
from Syndrome 
Knowing the error patterns we want to correct for, we can compute k Syndrome vectoroffline  (or n, if you want to correct errors in the parity bits, but this is not needed) and then do a lookup after the Syndrome is calculated from a received word to find the error type that occurred  
6.02 Fall 2012 Lecture 5, Slide #7</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #1 
6.02 Fall 2012 
Lecture #5 
  Error correction for linear block codes 
-Syndrome decoding 
Burst errors and interleaving</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #13  Independent multi-bit errors
e.g., m errors in n bits 
n
 pm(1p)nm
m
n n! =m(nm)!(m)!
nn
n!2n e</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>6.02 Fall 2012 Lecture 5, Slide #17 Summary: example error correction steps 
1.Search through received bit 011111001111011100100001111001110 
stream for sync pattern, Step 1: sync = 0111110 
extract interleaved codeword 
block 011111110010000111101110 
2.De-interleave the bits to form Step 2: B = 3, n = 8 
B n-bit codewords 01100111 
3.Check parity bits in each code 11110101 
11000110 word to see if an error has Step 3: (8,4,3) code occurred.  If theres a single-
bit error, correct it. 010   110 0 
4.Extract k message bits from 101   111   001 
each corrected codeword and 11    01    10 
concatenate to form message Step 4 
stream. 
0110 1110 1101    11
   010111010
11    01    1011</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 12: Filters and composition
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec12/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Sinusoidal Inputs and LTI Systems
 
h[n] 
A very important property of LTI systems or channels: 
If the input x[n] is a sinusoid of a given amplitude, 
frequency and phase, the response will be a sinusoid at the 
same frequency, although the amplitude and phase may be altered. 
The change in amplitude and phase will, in 
general, depend on the frequency of the input. 
Lets prove this to be true  but use complex exponentials 
instead, for clean derivations that take care of sines and  
cosines (or sinusoids of arbitrary phase) simultaneously.
 
6.02 Fall 2012 Lecture 12, Slide #7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>But much more is true for 
Sinusoidal Inputs to LTI Systems
 
Sinusoidal inputs, i.e., 
x[n] = cos(n + ) 
yield sinusoidal outputs at the same fr
equency  rads/sample. 
And observe that such inputs are not even periodic 
in general! 
Periodic if and only if 2/ is rational, =P/Q for some 
integers P(&gt;0), Q. The smallest such P is the period. 
Nevertheless, we often refer to 2/ as the period of this 
sinusoid, whether or not it is a periodic discrete-time 
sequence. This is the period of an underlying 
continuous-time signal. 
6.02 Fall 2012 Lecture 12, Slide #5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Bounded-Input Bounded-Output (BIBO) 
Stability
 
What ensures that the infinite sum " 
y[n] = # h[m]x[n ! m] 
is well-behaved? m=!"
 
One important case: If the unit sample response is absolutely
 
"summable, i.e., # | h[m] |!&lt;!" 
m=!" 
and the input is bounded, i.e., | x[k] |!! M &lt; " 
Under these conditions, the convolution sum is well-behaved, 
and the output is guaranteed to be bounded. 
The absolute summability of h[n] is necessary and sufficient for this bounded-input bounded-output (BIBO) stability. 
6.02 Fall 2012 Lecture 12, Slide #2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Time now for a Frequency-Domain Story
 
in which
 
convolution
 
is transformed to 
multiplication,
 
and other
 
good things
 
happen
 
6.02 Fall 2012 Lecture 12, Slide #3</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>A related simple case:
 real discrete-time (DT) exponential 
inputs also produce exponential outputs
 
of the same type
 
 Suppose x[n] = rn for some real number r 
" 
 y[n] = # h[m]x[n ! m] 
m=!" 
= #" 
h[m]rn!m 
m=!" 
$ '
 
= &amp; #" 
h[m]r!m )rn
 
%m=!" (
 i.e., just a scaled version of the exponential input 
6.02 Fall 2012 Lecture 12, Slide #8</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Back to Sinusoidal Inputs 
Invoking the result for complex exponential inputs, it is 
easy to deduce what an LTI system does to sinusoidal inputs: 
|H(0)|cos(0n + &lt;H( 0)) cos(0n) H() 
This is IMPORTANT 
6.02 Fall 2012 Lecture 12, Slide #12</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>From Complex Exponentials to
 
Sinusoids
 
cos(n)=(ejn+e-jn))/2 
So response to this cosine 
 input is 
(H()ejn+H(-)e-jn))/2 = Real part of H()ejn 
= Real part of |H()|ej(n+&lt;H( )) 
cos(0n) |H(0)|cos(0n + &lt;H( 0)) H() 
6.02 Fall 2012 Lecture 12, Slide #13</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Useful Properties of ej
 
When  = 0: 
ej 0 =1 
When  = : 
ej! = e! j! = !1 
ej!n = e! j!n =(!1)n 
(More properties later) 
6.02 Fall 2012 Lecture 12, Slide #10</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Frequency Response
 
y[n] h[.] A(cosn + jsin n)=Aejn 
Using the convolution sum we can compute the systems 
response to a complex exponential (of frequency ) as input: 
y[n] = "h[m]x[n ! m] 
m 
= "h[m]Aej#(n!m) 
m 
$ ' 
= &amp;"h[m]e! j#m ) Aej#n 
( % m 
= H(#)* x[n] 
where weve defined the frequency response of the system as 
H(!) "$h[m]e# j!m 
m 
6.02 Fall 2012 Lecture 12, Slide #11</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Sometimes 
written Example h[n] and H( ) 
as H(ejn) 
6.02 Fall 2012 Lecture 12, Slide #14</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>A First Step
 
Do periodic inputs to an LTI system, i.e., x[n] such that 
x[n+P] = x[n] for all n, some fixed P 
(with P usually picked to be the smallest positive integer for which this is true) yield periodic outputs? If so, of period P? 
Yes! ---
use Flip/Slide/Dot.Product to see 
this easily: sliding by P gives the same picture back again, hence the same output value. 
Alternate argument: Since the system is TI, using 
input x delayed by P should yield y delayed by P. But x delayed by P is x again, so y delayed by P must be y. 
6.02 Fall 2012 Lecture 12, Slide #4</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 
Lecture #12 
 Bounded-input, bounded-output stability 
 Frequency response 
6.02 Fall 2012 Lecture 12, Slide #1</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Frequency Response of Moving Average
 
Filters
 
6.02 Fall 2012 
Lecture 12, Slide #15</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Examples
 
cos(3n/4) has frequency 3/4 rad/sample, and 
period 8; shifting by integer multiples of 8 yields the same sequence back again, and no integer smaller than 8 accomplishes this. 
cos(3n/4) has frequency  rad/sample, and is not periodic as 
a DT sequence because 8/3 is irrational, but we could 
still refer to 8
/3 as its period, because we can 
think of the sequence as arising from sampling the periodic continuous-time signal cos(3t/4) at integer t. 
6.02 Fall 2012 Lecture 12, Slide #6</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Complex Exponentials
 
A complex exponential is a complex-valued function of a 
single argument  an angle measured in radians. Eulers formula shows the relation between complex exponentials and our usual trig functions: 
ej! = cos(!) + j sin(!) 
1 1 1 1 e! j!ej!! cos(!) = 2 ej! + 2 e! j! sin(!) =
 2 j 2 j
 
In the complex plane, ej! = cos(!) + j sin(!) is a 
point on the unit circle, at an angle of  with respect 
to the positive real axis. cos and sin are projections on real and imaginary axes, respectively. 
Increasing  by 2 brings you back to the same point! 
ej!So any function of only needs to be studied for  in [-, ] . 
6.02 Fall 2012 Lecture 12, Slide #9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 9: Transmitting on a physical channel
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec09/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>6.02Fall2012 
Lecture#9 
 Asmallpostscriptonmultiplerandomvariables 
 Introductiontomodulationanddemodulation
 Input/outputdescriptionsofsystems
 Lineartimeinvariant(LTI)models
6.02 Fall 2012 Lecture 9, Slide #1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>TwoImportantFacts  
Wewritethesefortworandomvariables X andY, buttheresults 
extendtoM randomvariables.Also,g(.) and h(.) beloware
arbitraryfunctions.
1.Expectationisalwaysadditive,i.e.,
E[g(X,Y) + h(X,Y) ] = E[g(X,Y)]+ E[h(X,Y)] 
 Followsfromthefactthatintegrationisadditive;needsno 
assumptions(apartfromexistenceoftheexpectedvalue)  
 Inparticular, E[g(X) + h(Y) ] = E[g(X)]+ E[h(Y)] 
TheRHSonlyneeds1DPDFs,notjointPDFs!
2. ForINDEPENDENTrandomvariables,expectationis
alwaysmultiplicative.Infact, X andY areindependentifand
onlyif E[g(X)h(Y) ] = E[g (X)].E[h (Y)] 
forallchoicesoffunctionsg(.) andh(.). 
Again,theRHSneedsonly1DPDFs,notjointPDFs!
6.02 Fall 2012 Lecture 9, Slide #4</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>IdeasforDemodulation  
	 Foronoffkeying,itsuf ficestodetectwhentheres
signalandwhenthereisnt,sincewereonlytrying
todistinguish v0 = 0 v1 = V &gt; 0
Manywaystodothat,e.g.,takeabsolutevalueand 
thenlocalaverageoverhalfperiodofcarrier
	 Forbipolarkeying,weneedthesign:
v0 =V v1 = V &gt; 0 
6.02 Fall 2012 	 Lecture 9, Slide #14</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>FromBrantRocktower,radioagewassparked
ByCarolynY.Johnson,GlobeStaf f| July30,2006
MARSHFIELD,MA Acenturyago,radiopioneer
ReginaldA.Fessenden usedamassive420footradio
towerthatdwar fedBrantRocktosendvoiceandmusicto
shipsalongtheAtlanticcoast,inwhathasbecomeknownastheworld'sfirstvoiceradiobroadcast.Thisweek,Marshfieldwilllayclaimtoitslittleknownradioheritagewithathreedayextravaganzatocelebratethefeatincludingpilgrimagestothebaseofthelongdismantledtower,acocktailtobenamedtheFessendenFizz,andadramaticreenactmentofthehistoricmoment,called``MiracleatBrantRock.
AmplitudeModulation(AM)
6.02 Fall 2012 Lecture 9, Slide #11</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>UnitSample  
Anothersimplebutusefuldiscretetimesignalistheunit 
sample signalorfunction, [n],definedas
 0, n  0[n] = u[n]  u[n 1] = 
 1, n = 0 
6.02 Fall 2012 Lecture 9, Slide #24</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>TransmissionoveraChannel  
Signalx[n]fromdigitizedsymbolsattransmitter
Distortednoisefreesignaly[n]atreceiver
6.02 Fall 2012 Lecture 9, Slide #20</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>DigitizedSymbols
6.02 Fall 2012 Lecture 9, Slide #6 
 Fall 2012 
 Lecture 9, S li1001110101
Distortednoisefreesignaly[n]atreceiverSamplenumbern
(discretetimeindex)Samplevalue</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>UnitSample  
Anothersimplebutusefuldiscretetimesignalistheunit 
samplesignalorfunction, [n],definedas
 0, n  0[n] = u[n]  u[n 1] = 
1, n = 0  
Notethatstandardalgebraicoperationsonsignals 
(e.g.subtraction,addition,scalingbyaconstant) aredefinedintheobviousway,instantbyinstant. 
6.02 Fall 2012 Lecture 9, Slide #25</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Unit Step  
Asimplebutusefuldiscretetimesignalistheunit step 
signalorfunction,u[n],definedas
 0, n &lt; 0 u[ n] = 
 1, n  0
6.02 Fall 2012 Lecture 9, Slide #23</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>DealingwithMultipleRandomVariables  
 PDFofarandomvariableX:
fX(x)  0 and  fX(x) dx = 1 
(thisintegralisovertheentirerealline)
	 Thenaturalextensiontothecaseoftworandom 
variables XandYisthejointPDFofX and Y: 
fX,Y(x,y)  0 and fX,Y(x,y) dx dy = 1 
(2Dintegralcoverstheentirex,y plane)
	 ExpectedvalueofafunctionofX,Y: 
E[g(X,Y)] =  g(x,y) fX,Y(x,y) dx dy 
 Andsimilarlyformorerandomvariables
6.02 Fall 2012 	 Lecture 9, Slide #2</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>ASingleLink 
codeword
bitsin
1001110101generate
digitized
symbolsDAC modulate
NOISY &amp; DISTORTING  ANALOG CHANNEL 
ADCdemodulate
&amp;filtersample&amp;
threshold
1001110101  
codeword bitsout
bitratesamples 
6.02 Fall 2012 Lecture 9, Slide #8</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>TimeInvariantSystems  
Lety[n]betheresponseofStoinputx[n]. 
Ifforallpossiblesequencesx[n]andintegersN 
Sx[nN] y[nN]
thensystemSissaidtobetime invariant(TI). Atime
shiftintheinputsequencetoSresultsinanidenticaltimeshiftoftheoutputsequence.
Inparticular,foraTIsystem,ashiftedunitsample
function
[n  N ]attheinputgeneratesanidentically
shiftedunitsampleresponse h[n  N ] attheoutput.
6.02 Fall 2012 Lecture 9, Slide #29</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>rrre



  Demodulation  
6.02 Fall 2012 Lecture 9, Slide #16
 6.02 Fa ll 2012 
 LLLe tcttttttttttttttttuuuururrru uuuuru rru uu uuu u eeeettttttttttttt ururuurururuurrrururururururururururrrrururururuuuuururrruurrrururuuuuruu r eeeeeeeeeeeeeeeeeeeeeeeeeeeeee 999, SSSSSSSSlid c=2/16 
16samplespercycle 
1001110101
e #16 codeword 
bits in 
1001110101 
x[n] t[n]
t[n]z[n]
z[n]DAC modulate generate 
digitized 
symbols 
NOISY &amp; DISTORTING  ANALOG CHANNEL 
ADC demodulate</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>InourSignalDetectionSetting  
	 Lastlecturewediscussedaveragingmultiplerandom
variables:A = (w[1] + w{2] + .. + w[M]) / M 
andwantedthemeanandvarianceof A. 
Hereeachw[n] wastheadditivenoisecomponentofa
receivedsampleinafixedbitslot,andassumedtobea
zeromeanGaussianofvariance2,independentofall
otherw[.] .Thesew[.] constituteadditivewhite
Gaussiannoise(AWGN) whitehere=zeromeaniid  
	 Strictlyspeaking,weshouldhavebeenworkingwith
thejointPDFoftheM randomvariables,inan M
dimensionalspace.However,thefollowingfactssuffice 
togetusthroughwithjust1DPDFs:
6.02 Fall 2012 	 Lecture 9, Slide #3</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>UnitSampleandUnitStepResponses  
S[n] h[n]Unitsample Unitsampleresponse
Theunit sample response ofasystemSistheresponseof
thesystemtotheunitsampleinput. Wewillalwaysdenotetheunitsampleresponseash[n].
Similarly,theunit step response s[n]:
Su[n] s[n]Unitstep Unitstepresponse
6.02 Fall 2012 Lecture 9, Slide #26</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Modulation(attheTransmitter)  
Adaptsthedigitizedsignalx[n]tothecharacteristicsofthechannel.
e.g.,Acousticchannelfromlaptopspeakertomicrophoneisnotwellsuitedtotransmitting constant levels
vandvto0 1
represent0and1.Soinsteadtransmitsinusoidal pressurewavesignalsproportionaltospeakervoltages
v cos(2  ft) and vcos(2  ft)0 c 1 c 
where f isthecarrier frequency (e.g.,2kHz;wavelengthc 
at340m/s =17cm,comparablewithspeakerdimensions)and 
v0 = 0 v1 = V&gt; 0 (onoff or
amplitude keying)
oralternatively
v0 =V v1 = V&gt; 0 (bipolaror
phase-shiftkeying)
Couldalsokeythefrequency.
6.02 Fall 2012 Lecture 9, Slide #10</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>LinearSystems  
Lety1[n]betheresponseofStoanarbitraryinputx1[n]
andy2[n]betheresponsetoanarbitraryx2[n].
If,forarbitraryscalarcoefficients a andb, wehave:
S ay1[n]+ by2[n] ax1[n]+ bx2[n] 
thensystemSissaidtobelinear. Iftheinputisthe
weightedsumofseveralsignals,theresponseisthesuperposition(i.e.,weightedsum)oftheresponsetothosesignals.
Onekeyconsequence:Iftheinputisidentically0fora
linearsystem,theoutputmustalsobeidentically0.
6.02 Fall 2012 Lecture 9, Slide #30</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>6.02 Fall 2012
6.02 Fa ll 2012 UnitSample 
Decomposition  
Adiscretetimesignalcanbedecomposed
intoasumoftimeshifted,scaledunitsamples.
Example:inthefigure,x[n]isthesumof 
x[2][n+2]+x[1][n+1]++x[2][n2]. 
Ingeneral:
 
x[n] = x[k]
l i [
n  k] 
k= 
Foranyparticularindex,onlyonetermofthissumisnonzero 
Lecture 9, Slide #27</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>6.02 Fall 2012
6.02 Fa ll 2012 
D 
e 
s 
t 
s 
I 
t 
s 
x UnitStep 
Decomposition  
Digitalsignalingwaveformsare
easilydecomposedintotimeshifted,scaledunitsteps(eachtransitioncorrespondstoanothershifted,scaledunitstep).
Inthisexample,x[n]isthe
transmissionof1001110using4samples/bit:
x[n] = u[n] u[n  4]+ u[n 12] u[n  24]  
Lecture 9, Slide #28</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>ASingleLink  
codeword
bitsin
1001110101 DAC
NOISY &amp; DISTORTING  ANALOG CHANNEL modulategenerate
digitized
symbols
ADCdemodulate
&amp;filtersample&amp;
threshold1001110101 
codeword bitsout
6.02 Fall 2012 Lecture 9, Slide #5</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>ttttttttttt

Modulation  codeword
bitsin
1001110101 
6.02 Fall 2012 Lecture 9, Slide #13
 6.02 Fa ll 2012 
 LeLLLLLLLLLLLLLLLLL cture 9, S lide #13  
cos(cn)t[n]generate
digitized
symbolsx[n]
1001110101c=2/16 
16samplespercycle</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>ASingleLink  
codeword
 clocked,discretetime
bitsin
1001110101
continuoustimeDAC modulategenerate
digitized
symbols
NOISY &amp; DISTORTING  ANALOG CHANNEL 
ADCdemodulate
&amp;filtersample&amp;
threshold1001110101 
codeword 
clocked,discretetime
 bitsout
DAC:Digitaltoanalogconverter 
ADC:Analogtodigitalconverter 
6.02 Fall 2012 Lecture 9, Slide #7</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Assumingno Demodulation  
distortionor
noiseonchannel,sowhatwastransmittedisreceived
z[n] = t[n]cos(cn) 
z[n] = x[n]cos(cn)cos(cn) 
z[n] = 0.5x[n](1+ cos( 2cn)) 
z[n] = 0.5x[n] + 0.5x[n]cos( 2cn) t[n]
cos(cn)z[n]
6.02 Fall 2012 Lecture 9, Slide #15</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>ModelingChannelBehavior  
codeword
bitsingenerate
digitized
symbolsx[n]
1001110101 DAC modulate
NOISY &amp; DISTORTING  ANALOG CHANNEL 
ADCdemodulate
&amp;filtery[n]codeword1001110101
bitsoutsample&amp;
threshold
6.02 Fall 2012 Lecture 9, Slide #19</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>OurfocuswillbeonLTIModels  
	 LTI=Linearand TimeInvariant
	 Gooddescriptionoftimeinvariantsystemsfor
smalldeviationsfromanominaloperating
equilibrium
	 Lotsofstructure,detailedanalysispossible,
amenabletodevelopmentofgoodcomputational
tools,
	 Majorarenaforengineeringdesign
6.02 Fall 2012 	 Lecture 9, Slide #31</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>eee

  

Filtering:Removingthe2 ccomponent  
codeword 
bits in 
1001110101 generate 
digitized 
symbols 
NOISY &amp; DISTORTING  ANALOG CHANNEL =2/16c
z[n] 16samplespercyclet[n]r[n]
demodulate filter DAC modulate 
x[n] t[n] 
ADC 
6.02 Fall 2012 Lecture 9, Slide #18
 .02 Fa ll 2012 
 Lecturrrrrreeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee 9,99999999999999999999999999999999999999 Slide #18 r[n]
1001110101</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 10: Linear time-invariant (LTI) systems
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec10/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #18 If system S is both linear and time-invariant (LTI), then we can 
use the unit sample response to predict the response to any 
input waveform x[n]:        Indeed, the unit sample response h[n] completely characterizes the LTI system S, so you often see 
S x[n]= x[k][nk]
k=
 y[n]= x[k]h[nk]
k=
Sum of shifted, scaled unit samples Sum of shifted, scaled responses Modeling LT  I Systems
h[.] CONVOLUTION SUM 
x[n] y[n]</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Linear Systems
Let y1[n] be the response of S to an arbitrary input x1[n] 
and y2[n] be the response to an arbitrary x2[n]. 
 
If, for arbitrary scalar coefficients a and b, we have: 
     then system S is said to be linear.  If the input is the weighted sum of several signals, the response is the superposition (i.e., same weighted sum) of the response to those signals.  One key consequence: If the input is identically 0 for a linear system, the output must also be identically 0. 
6.02 Fall 2012 Lecture 10, Slide #5 S ax1[n]+bx2[n] ay1[n]+by2[n]</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #12  so the corresponding response is  
y[n]
=s[n]
s[n4]
+s[n12]
s[n24]x[n]
=u[n]
u[n4]
+u[n12]
u[n24]
Note how we have invoked linearity and time invariance!</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #8              h[n]                             s[n]</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Properties of Convolution
 
(xh)[n]x[k]h[nk]=h[m]x[nm]
k= m=
The second equality above establishes that convolution is 
commutative:  
xh=hx
 Convolution is associative:   
x(h1h2)=xh1h2()
 Convolution is distributive: 
xh1+h2=(x() h1)+(xh2)
6.02 Fall 2012 Lecture 10, Slide #21</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #2 Modeling Channel Behavior  
codeword  
bits in 
codeword  bits out 1001110101 
DAC 
ADC NOISY &amp; DISTORTING  ANALOG CHANNEL modulate 
1001110101demodulate 
&amp; filter generate 
digitized  
symbols 
sample &amp; 
threshold x[n] 
y[n]</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Time Invariant Systems  
Let y[n] be the response of S to input x[n]. 
 If for all possible sequences x[n] and integers N       then system S is said to be time invariant (TI).  A time shift in the input sequence to S results in an identical time shift of the output sequence.  In particular, for a TI system, a shifted unit sample function              at the input generates an identically shifted unit sample response               at the output.  
6.02 Fall 2012 Lecture 10, Slide #4 S x[n-N] y[n-N] 
[nN]
h[nN]</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #23 Spot Quiz  
0.5 1 
0 1 2 3 4 5     n Unit step response: s[n] 
0.5 1 
0 1 2 3 4 5 6 7 8 9   n 
x[n] S x[n] y[n] input response 
Find y[n]: 
 1. Write x[n] as a function of  unit steps  2. Write y[n] as a function of  unit step responses  3. Draw y[n]</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Receiv  ing the Response
6.02 Fall 2012 Lecture 10, Slide #15 
Digitization threshold = 0.5V</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #1 
6.02 Fall 2012 
Lecture #10 
Linear time-invariant (LTI) models 
Convolution</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #20       Convolution
Evaluating the convolution sum 
 
 y[n]=x[k]h[nk]
 k=
for all n defines the output signal y in terms of the input x and unit-sample response h. Some constraints are needed to ensure this infinite sum is well behaved, i.e., doesnt blow up --- well discuss this later.  We use     to denote convolution, and write y=x  h. We can thus 

write the value of y at time n, which is given by the above sum, as 
y[n]=(xh)[n]
 Instead youll find people writing                            , where the poor index n is doing double or triple duty. This is awful notation, but a super-majority of engineering professors (including at MIT) will inflict it on their students.                                    Dont stand for it! 
 y[n]=x[n]h[n]</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #7 
[n]=u[n]u[n1]
 h[n]=s[n]s[n1]Relating h[n] and s[n] of an LTI System  
S u[n] s[n] Unit step signal Unit step response S [n] h[n] Unit sample signal Unit sample response 
from which it follows that 
 (assuming                  , e.g., a causal LTI  system; more  
generally, a right-sided unit sample response)  s[n]=h[k]
k=n

s[]=0</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #19       Convolution
Evaluating the convolution sum 
 
 y[n]=x[k]h[nk]
 k=
for all n defines the output signal y in terms of the input x and unit-sample response h. Some constraints are needed to ensure this infinite sum is well behaved, i.e., doesnt blow up --- well discuss this later.  We use     to denote convolution, and write y=x  h. We can then 

write the value of y at time n, which is given by the above sum, 
y[n]=(xh)[n] as                          . We could perh y[n]=xh[n] aps even write</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #11  Unit Step Decomposition
Rectangular-wave digital 
signaling waveforms, of the sort we have been considering, are easily decomposed into time-shifted, scaled unit steps --- each transition corresponds to another shifted, scaled unit step.  e.g., if x[n] is the transmission of 1001110 using 4 samples/bit: 
x[n]
=u[n]
u[n4]
+u[n12]
u[n24]
6.02 Fa ll 2012 

s
w
e
s
t
s
e
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Unit Sample and Unit Step Responses
6.02 Fall 2012 Lecture 10, Slide #6 S Unit sample 
[n] h[n] Unit sample response 
The unit sample response of a system S is the response of 
the system to the unit sample input.  We will always denote the unit sample response as h[n]. 
S u[n] s[n] Unit step Unit step response Similarly, the unit step response s[n]:</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Tr  ansmission Over a Channel
6.02 Fall 2012 Lecture 10, Slide #14 
Ignore this  
notation for now, will explain shortly</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #10 
             h[n]                          s[n]</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Series  Interconnection of LT  I Systems
6.02 Fall 2012 Lecture 10, Slide #22 h1[.] x[n] h2[.] y[n] 
y=h2w=h2h1x()=h2h1()x
(h2/g1h1)[.] 
 x[n] y[n] w[n] 
(h1/g1h2)[.] 
 x[n] y[n] 
h2[.] 
 x[n] h1[.] 
 y[n]</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Unit Sample 
Decomposition  
6.02 Fall 2012 Lecture 10, Slide #17 
 6.02 Fa llA discrete-time signal can be decomposed 
into a sum of time-shifted, scaled unit samples.  Example: in the figure, x[n] is the sum of  x[-2][n+2] + x[-1] [n+1] +  + x[2][n-2]. 
 In general: 

x[n]=x[k][nk]
k=
For any particular index, only one term of this sum is non-zero 
 2012 
li</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Example  
6.02 Fall 2012 Lecture 10, Slide #13</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>6.02 Fall 2012 Lecture 10, Slide #9 
             h[n]                          s[n]</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>          6.02 Lecture 18: MAC protocols
        </lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-02-introduction-to-eecs-ii-digital-communication-systems-fall-2012/resources/mit6_02f12_lec18/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.02 Introduction to EECS II: Digital Communication Systems
Fall 2012
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>11/14/12 
1 6.02 Fall 2012 Lecture 18, Slide #1 
6.02 Fall 2012 
Lecture #18 
 Shared medium /g2 Media access (MAC) protocol 
 Time division multiplexing 
 Contention protocols: Aloha 
6.02 Fall 2012 Lecture 18, Slide #2 Shared Media Networks 
/g7/g9/g20/g11/g15/g15/g13/g20/g11/g1
 /g4/g20/g12/g11/g18/g16/g11/g20/g1
/g8/g13/g18/g11/g15/g11/g19/g19/g1
/g15/g17/g10/g9/g15/g1/g1
/g9/g18/g11/g9/g1
/g16/g11/g20/g22/g17/g18/g14/g19/g1
/g1
/g8/g13/g13/g8/g13/g13/g13/g13/g8/g8/g8/g8/g13/g8/g13/g8/g13/g13/g8/g13/g8/g8/g8/g8/g8/g13/g8/g13/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g8/g13/g8/g13/g8/g13/g8/g13/g8/g13/g13/g18/g18/g18/g11/g18/g11/g18/g18/g18/g18/g18/g18/g18/g11/g18/g11/g18/g11/g18/g11/g18/g11/g18/g18/g15/g11/g15/g15/g15/g15/g15/g11/g15/g11/g11/g11/g11/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g19/g1
/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g17/g15/g17/g15/g17/g15/g17/g15/g17/g15/g17/g15/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g17/g10/g9/g10/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g9/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g9/g10/g9/g10/g9/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g10/g9/g9/g10/g10/g9/g9/g9/g10/g9/g10/g9/g9/g10/g9/g9/g9/g9/g9/g9/g10/g10/g9/g9/g9/g9/g9/g10/g9/g9/g9/g10/g10/g10/g9/g10/g10/g10/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g15/g1/g15/g15/g1
/g9/g9/g9/g9/g9/g9/g9/g9/g18/g9/g9/g9/g9/g9/g9/g9/g9/g9/g11/g9
60 2F l l 20122222
/g16/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g11/g20/g22/g17/g18 /g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g18/g14
/g8/g13/g18/g11/g15/g11/g19/g19/g1/g5/g2/g6/g19/g1
/g3/g11/g15/g15/g21/g15/g9/g18/g1/g1
/g22/g13/g18/g11/g15/g11/g19/g19/g1
Shared Communications Channels  
Basic idea in its simplest form: avoid collisions between 
transmitters  collision occurs if transmissions are concurrent 
Wanted: a communications protocol (rules of engagement) 
that ensures good performance 
Nodes may all hear each other perfectly, or not at all, or 
partially 
6.02 Fall 2012 Lecture 18, Slide #3 Shared channel, e.g., wireless or cable 
channel 
interface 
packet 
queues 
6.02 Fall 2012 Lecture 18, Slide #4 Good Performance: What are the Metrics?  
High utilization 
Channel capacity is a limited resource /g2 use it efficiently 
Ideal: use 100% of channel capacity in transmitting packets 
Waste: idle periods, collisions, protocol overhead 
Fairness 
Divide capacity equally among requesters 
But not every node is requesting all the time 
Bounded wait 
An upper bound on the wait before successful transmission 
Important for isochronous communications (e.g., voice/video) 
Dynamism and scalability 
Accommodate changing number of nodes, ideally without 
changing implementation of any given node 
Not all protocols do well on all these metrics Individual images  source unknown. All rights reserved. This content is excluded from our
Creative Commons license. For more information, see http://ocw.mit.edu/fairuse .</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>11/14/12 
6 
What Went Wrong?  
 
ny successive failures /g2 p very small /g2 no xmit atteStarvation
Too ma mpts 
Result: significant long-term unfairness 
Try a reduction rule with a lower bound: p /g1 max(pmin, p/2) 
Choosing pmin &lt;&lt; 1/max(N) seems to work best 
But theres another problem: capture effect 
Some node captures the  
network for a period, starving the others  Significant short-term  
unfairness! 
Utilization = .71, Fairness = .99 
python PS7_stabaloha.py r n 6 t 1000 --pmin=.05 
6.02 Fall 2012 Lecture 18, Slide #21  Limiting the Capture Effect
Capture effect 
A successful node maintains a high p (avg. near 1) 
Starves out other nodes for short periods 
Try an incryp p ease rule with an upper bound: pp /g1 min(p (pmax max,p ),2*p) 
Utilization = .41, Fairness = .99 
python PS7_stabaloha.py r n 10 t 10000 --pmin=.05 --pmax=0.8 
6.02 Fall 2012 Lecture 18, Slide #22 
x
x
 Node Probabilities: pmax=.25, pmin=.01
(different run from prev  ious page)
Y-axis is per-node transmission probability 
6.02 Fall 2012 Bottom panel: per-node throughput Lecture 18, Slide #23 
 6.02 Fall 2012 Lecture 18, Slide #24 Aloha in Pictures: Collisions 
Collisions! Packet is 5 slots long in this picture 
A collision occurs when multiple 
transmissions overlap in time (even partially) 
Throughput = Uncollided packets per second 
Utilization = Throughput / Channel Rate</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>11/14/12 
4 6.02 Fall 2012 Lecture 18, Slide #13 time slot Collision Channel idle  Success, Idleness, Collisions
Throughput = Uncollided packets per time interval 
Utilization of the above pic = Throughput / Channel Rate 
= 13/20 = .65  Slotted Aloha
Aloha protocol  each node independently implements: 
 
 If a node is backlogged, it sends a packet  
 in the next time slot with probability p. 
 
Assume (for now) each packet takes exactly one time slot to 
transmit ( slotted  Aloha) 
Utilization when N nodes are backlogged?  The probability 
that exactly one node sends a packet. 
prob(send a packet) = p 
prob(dont send a packet) = 1-p 
prob(only one sender) = p(1-p)N-1 
There are (N choose 1) = N  ways to choose the one sender 
U=Np(1p)N1
slotted Aloha
6.02 Fall 2012 Lecture 18, Slide #14 
g  Maximizing Utilization
TTo determine maximum: 
s set dU/dp = 0, solve for p. 
 R Result: p = 1/N , so 
1Umax=(1)N1
N
1As N, Umax  37%e
 11 1ln (1)N=(N1) ln(1)N N
1 1 1=(N1)( 2N23N3)N
1 1 1=1++ +N6N2+ 12N3
=1 as N
6.02 Fall 2012 Lecture 18, Slide #15 
 6.02 Fall 2012 Lecture 18, Slide #16 
Simulation of Slotted Aloha (N=10)  
Utilization = .38, Fairness = .98 
Top: success 
Bottom: failure</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>11/14/12 
2  Utilization
Utilization measures the throughput of a channel: 
  
total throughput over all nodesUchannel= maximum data rate of channel 
Example: 10 Mbps channel, four nodes get throughputs of 1, 
2, 2 and 3 Mbps.  So utilization is (1+2+2+3)/10 = 0.8. 
0  U  1 .  Utilization can be less than 1 if 
The nodes have packets to transmit (nodes with packets in their 
transmit queues are termed backlogged), but the protocol is 
inefficient. 
There is insufficient offered load, i.e., there arent enough 
packets to transmit to use the full capacity of the channel. 
With backlogged nodes, perfect utilization is easy: just let one 
node transmit all the time!  But that wouldnt be fair 
6.02 Fall 2012 Lecture 18, Slide #5 Fairness  
Many plausible definitions.  A standard recipe:  
Measure throughput of nodes = xi, over a given time interval 
Say that a distribution with lower standard deviation is fairer 
than a distribution with higher standard deviation. 
Given number of nodes, N, fairness F is defined as 
 2 x N
i
 F=i=1()
 N2
i Nx
i=1 
1/N  F  1 , where F=1/N implies single node gets all the 
throughput and F=1 implies perfect fairness. 
Well see that there is often a tradeoff between fairness and utilization, i.e., fairness mechanisms often impose some 
overhead, reducing utilization 
6.02 Fall 2012 Lecture 18, Slide #6 
 Channel Sharing Protocols
Protocol /g3 rules of engagement for good performance 
Known as media access control (MAC) or multiple access control 
Time division 
Share time slots between requesters 
Prearranged: time division multiple access (TDMA) 
Not prearranged: contention protocols (e.g., Alohanet).   
Frequency division 
Give each transmitter its own frequency, receivers choose 
station 
Cf. lab for PS 6  use different carrier frequencies &amp; recv filters 
Code division 
Uses unique orthogonal pseudorandom code for each transmitter  
Channel adds transmissions to create combined signal 
Receiver listens to one dimension of combined signal using dot 
product of code with combined signal 
Not covered in 6.02 
6.02 Fall 2012 Lecture 18, Slide #7 6.0Abstraction for Shared Medium  
Time is divided into slots of equal length 
Each node can start transmission of a packet only at the 
beginning of a time slot 
All packets are of the same size and hence take the same 
amount of time to transmit, equal to some integral multiple of  
time slots. 
If the transmissions of two or more nodes overlap, they are 
said to collide and none of the packets are received correctly.  
Note that even if the collision involves only part of the packet, 
the entire packet is assumed to be lost. 
Transmitting nodes can detect collisions, which usually 
means theyll retransmit that packet at some later time. 
Each node has a queue of packets awaiting transmission.  A 
node with a non-empty queue is said to be backlogged. 
Depending on context, nodes may hear each other perfectly (eg, Ethernet), or 
not at all (e.g., satellite ground stations), or partially (e.g., WiFi devices or cell 
phones). For now, assume all nodes want to send packets to a fixed master (eg, base station) 
2 Fall 2012 Lecture 18, Slide #8</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>11/14/12 
Time Division Multiple  Access (TDMA)
Suppose that there is a centralized resource allocator and a 
way to ensure time synchronization between the nodes  for example, a cellular base station.  
For N nodes, give each node a unique index in the range 
[0,N-1].  Assume each slot is numbered starting at 0. 
 
Node i gets to transmit in time slot t if, and only if, t mod N = 
i.  So a particular node transmits once every N time slots.  
No packet collisions!  But unused time slots are wasted, 
lowering utilization.  Poor when nodes send data in bursts or 
have different offered loads. 
6.02 Fall 2012 Lecture 18, Slide #9 6.02 Fall 2012 Lecture 18, Slide #10 TDMA   for GSM Phones
First slot is used in cell 
phones to contact tower for slot assignment.  Tower can determine appropriate timing 
advance for each user 
(accounts for varying distance from tower) so that transmissions won/g1t 
overlap at the tower. 
The Aloha Protocol 
Context: Norm Abramson, Hawaii 
Developed scheme to connect islands via 
satellite network 
6.02 Fall 2012 Lecture 18, Slide #11 Contention Procotols:  Aloha (Simplest Example)
To improve performance when there are burst data patterns or 
skewed loads, use a contention protocol where allocation is not pre-determined. 
Alohanet, designed by Norm 
Abramson et al. (Hawaii), was a satellite-based data network connecting computers on the Hawaiian islands.  One frequency was used to send data to the satellite, which rebroadcast it on a different frequency to be received by all stations.  Stations could only hear the satellite, so had to decide independently when it was their turn to transmit. 
6.02 Fall 2012 Lecture 18, Slide #12 
 Mozzerati  (top), source unknown (bottom). CC BY-SA. This content is excluded from our
Creative Commons license. For more information, see http://ocw.mit.edu/fairuse .[link over Mozzerati to http://en.wikipedia.org/wiki/File:Tdma-frame-structure.png]
Satellite clip art is in
the public domain.
Satellite clip art is in
the public domain.
3 ..
Map  Europa Technologies, TerraMetrics, Google, and NASA. All rights reserved. This content is
excluded from our Creative Commons license. For more information, see http://ocw.mit.edu/fairuse .Map  Europa Technologies, TerraMetrics, Google, and NASA. All
rights reserved. This content is excludedfrom our Creative Comm-
ons license. For more information, see http://ocw.mit.edu/fairuse .</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>11/14/12 
7 Dear Professor Balakrishnan, 
 
I just wanted to let you know how my IAP has been going because 
I've been using a few 6.02 concepts . I'm interning at Quizlet.  
 
 I also worked on making a testing tool. I set up a node server to 
manage many headless browsers (using phantomjs). I realized that 
phantomjs wasn't built to manage the number of connections I required. Then I noticed that this was similar to the problem of connecting multiple clients to the same router. So I set up a systesimilar to one of the 6.02 labs in which everyone was assigned a random time to start their connection, over a period of time.  When 
browser started initialising, I put on a lock that made other browsers attempting to connect wait another random time period. Once the "messy" part of the initialisation was done, I unset the locin order to allow other clients to connect.  
 
I had to implement various other 6.02ish features like exponential 
back off, and I've also noticed that node.js is (of course) a very 6.02
type of project. For example, it has a concept of heartbeats. 
 
Best, Chase (Fall 2011 student that sat in the front row and whom 
you approached at the gym) 
6.02 Fall 2012 Lecture 18, Slide #25 m 
a  
k 
  Spot Quiz
1.In Aloha, each node maintains a variable, p. What does p 
represent?  [P(x) is probability of event x] 
A.P(node being backlogged) 
B.P(backlogged node sends in a timeslot)  
C.P(packet transmission is received correctly) 
D.P(time slot is kept idle) 
2.In stabilized Aloha, the value of p never goes below pmin. 
Why should pmin not be too small? 
A.To increase the utilization 
B.To avoid extreme unfairness 
C.To reduce the problems caused by the capture effect 
D.To reduce the number of collisions 
3.Slotted Aloha, packet size 1 slot, N backlogged nodes, each node has a fixed p. Calculate: P( collision in a timeslot) 
 /nonmarkingreturn
6.02 Fall 2012 Lecture 18, Slide #26 
 Spot Quiz
1.In Aloha, each node maintains a variable, p. What does p 
represent?  [P(x) is probability of event x] 
A.P(node being backlogged) 
B.P(backlogged node sends in a timeslot)  
C.P(packet transmission is received correctly) 
D.P(time slot is kept idle) 
2.In stabilized Aloha, the value of p never goes below pmin. 
Why should pmin not be too small? 
A.To increase the utilization 
B.To avoid extreme unfairness 
C.To reduce the problems caused by the capture effect 
D.To reduce the number of collisions 
3.Slotted Aloha, packet size 1 slot, N backlogged nodes, each 
node has a fixed p. Calculate: P( collision in a timeslot) 
 Soln: P(collision) = P(2 xmits) = 1  P(no xmit)  P(1 xmit) = 
     P(no xmit) = (1- p)N; P(1 xmit) = Np(1-p)(N-1) 
          Therefore, P(collision) = 1 - (1- p)N - Np(1-p)(N-1)./nonmarkingreturn
6.02 Fall 2012 Lecture 18, Slide #27</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>11/14/12 
5  Stabilization: Selecting the Right p
Setting p = 1/N maximizes utilization, where N is the number 
of backlogged nodes. 
With bursty traffic or nodes with unequal offered loads (aka 
skewed loads), the number of backlogged is constantly 
varying. 
Issue: how to dynamically adjust p to achieve maximum 
utilization? 
Detect collisions by listening, or by missing acknowledgement 
Each node maintains its own estimate of p 
If collision detected, too much traffic, so decrease local p 
If success, maybe more traffic possible, so increase local p 
Stabilization is, in general, the process of ensuring that a system is operating at, or near, a desired operating point. 
Stabilizing Aloha: finding a p that maximizes utilization as 
loading changes. 
6.02 Fall 2012 Lecture 18, Slide #17 6.02 Fall 2012 Lecture 18, Slide #18  Binary Exponential Backoff
Decreasing p on collision 
Estimate of N (# of backlogged nodes) too low, p too high 
To quickly find correct value use multiplicative decrease: 
 p /g1 p/2 
k collisions in a row: p decreased by factor of 2/g2k 
Binary : 2, exponential: k, back-off: smaller p /g2 more time 
between tries 
 
Increasing p on success 
While we were waiting to send, other nodes may have emptied 
their queues, reducing their offered load. 
If increase is too small, slots may go idle 
Try multiplicative increase: p /g1 min(2*p,1)  
Or maybe just: p /g1 1 to ensure no slots go idle 

6.02 Fall 2012 Lecture 18, Slide #19 
Simulation of Stabilized  Aloha
Some nodes did well 
Others didnt 
Utilization = .33, Fairness = .47 
python PS7_stabaloha.py r n 10 t 1000 
6.02 Fall 20 Node probabilities over time 
(different run from prev  ious page)
Y-axis is per-node transmission probability 
12 Bottom panel: per-node throughput Lecture 18, Slide #20</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
