<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/</course_url>
    <course_title>Topics in Theoretical Computer Science: An Algorithmist's Toolkit</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Mathematics </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Multiplicative weights</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe24/</lecture_pdf_url>
      <lectureno>24</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3. Let jt denote the outcome of event t. Update the weights as follows: 
w t1(1  )M(i,jt)/ if M(i, jt)  0 wit = { wii
t1(1 + )M (i,jt)/ if M(i, jt) &lt; 0 
A similar analysis to before gives: 
Theorem Let Dt denote the probabilit y distribution {p1t ,...,pnt } with which we pick experts to make a 
prediction for event t. Let M (Dt,jt) denote the expected value of our penalt y when following the distribution 
Dt for event t and when the actual outcome is jt . Then for   1/2 and for all T and i, 
T log(n)   
M(Dt,jt)  + (1 + ) M(i, jt) + (1  ) M(i, jt) 
t=1 t:M(i,jt )0 t:M(i,jt)&lt;0 
Corollary For any , for   min(1/2, /4), for T = 162log(n)/2 rounds and for all i, the average 
penalt y we get per round obeys: 
T TM(Dt,jt) M(i, jt)t=1 t=1  + T T 
and in particular our average penalt y per round is at most  bigger than the average penalt y of the best 
expert. 
Applications of Multiplicativ e Weights 
Our rst application of the Multiplicativ e Weights algorithm will be to zero-sum games. In a zero-sum game, 
we have a row player, R, and a column player, C. If R plays strategy i and C plays strategy j, then R pays 
C the amoun t M(i, j). Players can also play mixed strategies, i.e. probabilit y distributions over the sets of 
pure strategies. We will extend our payo notation so that M(D, P ) denotes the expected amoun t that R 
pays C when R plays the mixed strategy D and C plays the mixed strategy R. Recall that von Neumanns 
Minimax Theorem states that 
minD maxj M(D, j)= maxP miniM (i, P ) 
We will denote the above quantity by ; it is known as the value of the game. 
We are now ready to state the zero-sum game problem : given the sets of strategies for R and C and the 
payos M(i, j), estimate the value of the game . Our approac h will be to associate elemen ts of the curren t 
problem to appropriately chosen elemen ts of the Multiplicativ e Weights algorithm, then directly apply what 
we already know about Multiplicativ e Weights to conclude that we do indeed get a good appro ximation to 
 in a reasonable amoun t of time. The details of the argumen t will be presented next lecture. 
24-3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Decem ber 8, 2009 
Lecture 24 
Lecturer: Jonathan Kelner Scrib e: Dimiter Ostrev 
Multiplicativ e Weights 
In this lecture we will introduce Multiplicativ e Weights, a simple technique with many applications. We 
start with an example. 
Example Supp ose Mr. X wants to bet on football games but does not know much about football himself. 
Before each game, X can check the predictions of n experts. Is there an algorithm that allows Mr. X to 
perform well in the long run? 
Two potential ideas are: 
(1) For each game, bet according to what the majority of experts predict 
(2) Wait a few game s to see which of the experts get it right most of the time and then follow their advice 
These strategies work well in some cases but not in others: (1) fails when only a few experts make good 
predictions, and (2) fails when there is an expert that performs well for the rst few games and then never 
makes a correct prediction again. Instead, we will consider a combination of the two approac hes: for each 
game, we will consider the opinion of all experts, but each experts opinion will be weighted according to 
his past performance. More precisely , let wit denote the weight of expert i after t games, and consider the 
following algorithm: 
1. Set wi 0 = 1 for i =1, ..., n 
2. Make a prediction for game t based on a weighted majority of experts where expert i gets weight 
wit1/ j wjt1 
3. After game t update the weights as follows: if expert is prediction for game t was wrong then set 
t tw= (1  )w t1; otherwise set w= w t1 
i i ii 
For this algorithm, we have the following: 
Theorem Let mt
i denote the number of mistakes that expert i makes in the rst t games and mt denote 
the number of mistak es that Mr. X makes in the rst t games. Then for all i and t, 
2log(n) m t   + 2(1 + )m t 
i 
and in particular, this holds for the i that minimizes mit . 
Proof Dene k = i wik . If Mr. X makes a mistak e at game k, then a weighted majority of the experts 
must have made a wrong prediction for game k. The weights of all these experts drop by a factor of (1  ) 
and so we have k  (1  /2)k1 . Then over the rst t games we have 
t tt  (1   )m 0 = n(1   )m 
2 2
On the other hand we have wt = (1  )mit and so
i 
24-1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>t 
i t  wit = (1  )m 
Therefore, 
t t 
i n(1  2)m  (1  )m 
Rearranging this inequality gives 
log(n) log(1  ) m t + m t
i log(1  /2) log(1  /2) 
This bound is slightly stronger than the one in the statemen t of the theorem. Using the inequalit ies 
/2 log(1  /2) and  + 2 log(1  ) converts it to the required form and completes the proof. 
Next, we will modify our algorithm to get rid of the factor of 2 on the right hand side of the bound above. 
Consider the following: 
1. Set wi 0 = 1 for i =1, ..., n 
2. To make a prediction for game t, do the following: for i =1, ...n, follow expert is prediction with 
probabilit y pt
i = wit1/ j wjt1 
3. After game t update the weights as follows: if expert is prediction for game t was wrong then set 
wt = (1  )w t1 else set wt = w t1 
i i ii 
For this algorithm, we have the following: 
Theorem Let mit denote the number of mistak es that expert i makes in the rst t games and let mt denote 
the random variable equal to the number of mistak es that Mr. X makes in the rst t games. Then for &lt; 1/2 
and for all i and t, 
E(m t)  log
 (n) +(1+ )mit 
and in particular, this holds for the i that minimizes mit . 
The proof of this Theorem is similar to before and we will omit it. Instead, we will introduce our most 
general version of the multiplicativ e weights algorithm. In the example above, we had only two possibilities 
for the relation between event outcomes and expert predictions: the outcome of game t either matc hed expert 
is prediction or it did not. Our measure of performance for individual experts and for the algorithm as a 
whole was simply counting wrong predictions. We want to generalize the algorithm to allow for an arbitrary 
set P of possible outcomes to events. In this setting, we will measure the performance of the algorithm as 
follows: we will say that at each step, following expert is prediction when the true outcome is j incurs a 
penalt y of M(i, j). More precisely , we have the following: 
0. The input of the algorithm consists of: a set P of possible outcomes to events. For i =1, ...n and for 
j  P a number M(i, j) from the interval [l, ]. We will refer to  as the width; we will also have the 
restriction l&lt;. 
1. Set wi 0 = 1 for i =1, ..., n 
2. To make a prediction for event t, do the following: for i =1, ...n, follow expert is prediction with 
probabilit y pt
i = wit1/ j wjt1 
24-2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Linear algebra review, adjacency and Laplacian matrices associated with a graph, example Laplacians</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe1/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>5  X(1) 
v = 	X(2)  
X(3) 
The action of LG on v is then 
  	  
1 0 X(1) X(1)  X(2)  X(1)  X(2)  
 1 
  =   =  X(2)  [ X(1)+X (3) LGv = 12 1 X(2) 2X(2)  X(1)  X(3)  2 2 ]  
0 11 X(3) X(3)  X(2) X(3)  X(2) 
For a general Laplacian, we will have 
[LGv]i =[di  (X(i)  average of X on neigh bors of i)] 
Remark For any G, 1 = (1,..., 1) is an eigen vector of LG with eigen value 0, since for this vector X(i) 
always equals the average of its neigh bors values. 
Prop osition 6 We will see later the following results about the eigenvalues i and corresponding eigenve c
tors vi of LG: 
	Order the eigenvalues so 1  ...  n, with corresponding eigenve ctors v1,...,vn. Then v1 = 1 and 
1 = 0. So for all ii  0. 
	One can get much information about the graph G from just the rst few nontrivial eigenve ctors. 
Matlab Demonstration 
As remark ed before, vectors v  Rn may be construed as maps Xv : V  R. Thus each eigen vector assigns a 
real number to each vertex in G. A point in the plane is a pair of real numbers, so we can embed a connected 
graph into the plane using (Xv2 ,Xv3 ): V R2 . The following examples generated in Matlab show that 
this embedding provides represen tations of some planar graphs. 
Figure 1: Plots of the rst two nontrivial eigen vectors for a ring graph and a grid graph 
1-3 Image courtesy of Dan Spielman. Used with Permission.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>4 Let G =(V, E) be a graph, where |V | = n and |E| = m. We will for this lecture assume that G is 
unweighted, undirected, and has no multiple edges or self loops. 
Denition 4 For a graph G, the adjacency matrix A = AG is the n  n matrix given by 
1 if (i, j)  E Ai,j =0 otherwise 
For an unweighted graph G, AG is clearly symmetric. 
Denition 5 Given an unweighte d graph G, the Laplacian matrix L = LG is the n  n matrix given by 
Li,j = 
 
 1 if (i, j)  E 
di if i = j 
0 otherwise 
wher e di is the degree of the ith vertex. 
For unweighted G, the Laplacian matrix is clearly symmetric. An equiv alent denition for the Laplacian 
matrix is 
LG = DG  AG, 
where DG is the diagonal matrix with ith diagonal entry equal to the degree of vi, and AG is the adjacency 
matrix. 
Example Laplacians 
Consider the graph H with adjacency matrix 
  
AH =  01010 
10100 
01011 
10100 
00100  
This graph has Laplacian   2 10 10 
12 10 0 
0 13 1 1 
0 2 0   LH = 
1 
0 1 
1 0 0 1 
Now consider the graph G with adjacency matrix 
 010 
101 AG = 
010 
This graph has Laplacian   1 10 
12 1 
0 1   LG = 
1 
LG is a matrix, and thus a linear transformation. We would like to understand how LG acts on a vector 
v. To do this, it will help to think of a vector v  R3 as a map X : V  R. We can thus write v as 
1-2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit September 10, 2009 
Lecture 1 
Lecturer: Jonathan Kelner Scribe:  Jesse Geneson (2009) 
1 Overview 
The classs goals, requirements, and policies were introduced, and topics in the class were described. Every
thing in the overview should be in the course syllabus, so please consult that for a complete description. 
2 Linear Algebra Review 
This course requires linear algebra, so here is a quick review of the facts we will use frequently. 
Denition 1 Let M by an n  n matrix. Suppose that 
Mx = x 
for x  Rn , x = 0, and   R. Then we call x an eigenvector and  an eigenvalue of M. 
Proposition 2 If M is a symmetric n  n matrix, then 
	If v and w are eigenvectors of M with dierent eigenvalues, then v and w are orthogonal (v  w = 0). 
	If v and w are eigenvectors of M with the same eigenvalue, then so is q = av + bw, so eigenvectors 
with the same eigenvalue need not be orthogonal. 
	M has a full orthonormal basis of eigenvectors v1,...,v n. All eigenvalues and eigenvectors are real. 
 M is diagonalizable: 
  M = VV T
where V is orthogonal (VV T = In	), with columns equal to v1,...,v n, and  is diagonal, with the 
corresponding eigenvalues of M as its diagonal entries. So M = n
i=1 ivivT 
i. 
In Proposition 2, it was important that M was symmetric. No results stated there are necessarily true 
in the case that M is not symmetric. 
Denition 3 We call the span of the eigenvectors with the same eigenvalue an eigenspace. 
3 Matrices for Graphs 
During this course we will study the following matrices that are naturally associated with a graph: 
 The Adjacency Matrix 
     
 The Laplacian Matrix 
 The Normalized Laplacian Matrix 
1-1 The Random Walk Matrix</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Figure 2: Handmade graph embedding (left) and plot of the rst two nontrivial eigen vectors (righ t) for an 
interesting graph due to Spielman 
Figure 3: Handmade graph embedding (left) and plot of rst two nontrivial eigen vectors (righ t) for a graph 
used to model an airfoil 
1-4 Image courtesy of Dan Spielman. Used with Permission.
Image courtesy of Dan Spielman. Used with Permission.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Nonblocking routing networks, local and almost-linear time clustering and partitioning, Lovasz-Simonovits Theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe7/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Pro of 
This proof was not covered in its entirety in todays lecture, but it was covered in Lecture 7 of the 2007 
version of the course: 
We will only prove the case x[0,m], the second case should be analogous. As in the previous claim we 
can assume without loss of generalit y that x= k, and that (u1,v1),...,(uk,vk) are exactly the set of edges 
starting from W = {u1,...,u k). We then have: 
k k 
t(ui,vi)= t1(vi,u i). 
i=1 i=1 
Break the edges of the right into two sets: 
W1 = {(vi,u i): ui,vi W,vi /negationslash= ui}.W 2 = {(vi,u i): ui W,vi /negationslashW}{self loops}. 
We claim that: 
1.  
(v,u)W1 t1(v,u)  1
2 It1(x2Gx). 
2.  
(v,u)W2 t1(v,u)  1
2 It1(x+2Gx). 
Note that out of the x= k edges starting at W, x/2 are self loops, and at least Gxedges leave W, therefore, 
the number of edges in W1 is at most x/2 Gx. Note that if we let ci be1if ei Wi and 0 otherwise, we 
have that 
ik 
=1 ci x/2 Gx. And then, using Claim 8, we can obtain the following weaker bound: 
t1(v,u) It1(x2Gx). 
(v,u)W1 
We need to move the 1/2 factor outside of It1 someho w. Instead of doing that, we will carefully choose 
other values for ci to obtain the wanted bound. For that simply let ci be 1/2 if ei Wi or if ei is a self loop 
in W and 0 otherwise. Then, since every vertex has the same number of self loops as edges leaving it (and 
they all have the same  value), we also obtain under this new set of weights, that 
ik 
=1 ci  x/2 Gx. 
Using Claim 8 and that 2ci 1, for every i,wehave: 
k k   1  1 t1(v,u)= cit1(vi,u i)= cit1(vi,u i)  It1(x2Gx).2 2 (v,u)W1 i=1 i=1 
The second claim follows in a similar way, and combining both of them we obtain the result. 
Using the previous theorem, we are ready to state and prove Lovasz-Simono vits Theorem. 
Theorem 11 (Lovasz-Simono vits) For all initial distribution p0 and every t, 
  1 t x It(x) min( x, 2mx)1  2 + .2 G 2m 
Sketch of Pro of Consider the curve 
R0 = min(  x,  
2mx)+ x.2m 
It is easy to show that I0(x) R0(x), for all x[0,2m]. Ifweset 
1   
Rt(x)= Rt1(x2Gx)+ Rt1(x+2Gx) ,2 
7-7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>conductance cut will be an obstacle for the mixing time. This means that the random walk has trouble 
leaving the cluster. So, a good guess for the cluster is the set of vertices for which a random walk starting 
at v will have the highest probabilities after a given number of steps. Using this idea, a good primitiv e to 
construct an almost linear algorithm will be the following. 
Appro ximate, for every vertex in the graph, the probabilit y that a random walk starting from v is in 
that vertex after certain time, select the k highest valued vertices and check if they dene a good cut. Repeat 
this until you get a good cut or you reach a predetermined limit. 
Note that this metho d is similar to the proof of Cheegers inequalit y, where we ordered the entries of v2 
to obtain a cut. Here, however, use use a probabilit y vector instead of the eigenvector v2. 
We need a bound that says that this idea works. Unfortunately , all the bounds we have proven thus far 
are global, involving 2 of the whole graph. We desire bounds on a local feature of the graph. Furthermore, 
we cant really compute all of the necessary probabilities, because it will take too long. We therefore need 
to approximate the probabilities, without knowing the size of the cluster in advance. 
4.3 Lovasz-Simono vits 
The Lovasz-Simono vits Theorem will give us the bound we need for the algorithm. It relies on an interesting 
idea: measure the progress of the walk not by one number but by a whole curve. The better the random walk 
is to reaching the stable distribution, the closer the curve will be to a straigh t line. Dieren t points on the 
curve will corresp ond to dieren t size partitions. Before stating the theorem, we will need some denitions. 
Let G be the digraph obtained from the original graph where we rst replace each undirected edge uv 
by two directed arcs (u,v) and (v,u), and then we add self-lo ops loops to each node until every node v of G 
has dv /2 self-lo ops (i.e. half of the edges leaving v are self-lo ops). 
Instead of focusing on the vertices, we will mainly study the edges of G. Suppose that we have a certain 
probabilit y distribution p on vertices. Dene 
p(u)(u)= , for every node u,du 
(u,v)= (u), for every arc (u,v). 
Note that (u,v) represen ts the mass about to be sent over arc (u,v) and that for every node u, (u) 
approac hes 1/2m as the walk converges (here, 2m is the number of arcs in the digraph). Therefore, as the 
walk converges, (e)goesto1/2m for every arc e. 
We will dene a curve that measures how close we are to convergence and also contains additional 
information. 
Denition 7 (Lovasz-Simono vits curv e) For a given , order the arcs such that (e1) (e2) ...  
(e2m). We dene I :[0, 2m] [0, 1] as follows: For each k {0,..., 2m}, 
k 
I(k)= (ei). 
i=1 
We extend I to the complete interval by interp olating it piecewise linearly. 
Intuitively I(k) measures how much probabilit y is transp orted over the k most utilized edges. Here we 
describ e some of the properties of the L-S curve. 
	As the walk converges I should eventually converge to a straigh t line. 
	I(0)=0,I(2m)=1. 
	The slope of I between k and k + 1 is given by I(k +1) I(k)= (ek+1). 
	Since  depends only on the start vertex, it does not matter how we order edges out of any particular 
node u, and therefore the slope of I is constan t for all the intervals corresp onding to arcs leaving u. 
7-5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Figure 1: Buttery network for 8 inputs. 
Although in a Buttery network, routing is easy (just follow the binary expansion of the label of the 
desired output), its not possible to avoid congestion (e.g. in Figure 1, if you try to route 000 to 000 and 
110 to 001, the route will collide on the third layer). Actually, for some permutations you can get up to 
N collisions. While buttery networks do not successfully avoid congestion, they are the base for a type 
of nonblocking routing network called a multibuttery. 
3.2 Multibuttery networks 
The construction of multibuttery networks uses bipartite expanders, which we discussed last class. We 
recall the denition here: 
Denition 1 A d-regular bipartite graph is an (,)-expander if every set S on the left with |S|n has 
N(S) |S|. 
Thus, intuitively, a regular bipartite graph is an (,)-expander if any small collection of vertices on the left 
has a proportionately large number of neighbors. 
A d-multibuttery is a layered graph constructed similarly to the buttery networks, but such that 
each node now has d edges out to the up block of the next layer and d edges out to the down block. 
These networks are carefully constructed such that the graph induced by the vertices of a block and its up 
neighbor block and the graph induced by the block and its down neighbor block are both (,)-expanders 
for &gt;d / 2,  1/2. 
A 2-multibuttery for N = 8 allowing 4 pairs of inputs/outputs is shown in Figure 2. 
Note that d-multibutteries have O(N log N) vertices and that the vertices have bounded degree (at most 
4d). then the only condition needed for this to be a good nonblocking routing network is that we can route 
any permutation of inputs and outputs easily. 
7-2 3.1 Buttery networks 
A rst attempt to nd a small network in which routing is easy is to consider a Buttery network. A Buttery 
network for N inputs and outputs consists on log(N) layers of N nodes each. The rst layer (or zeroth layer) 
correspond to the N inputs and the last layer corresponds to the N outputs. The i-th layer is divided into 
blocks of N/2i elements. Each block splits into two blocks: up and down in the next layer. Each node 
has two neighbors in the next layer, one in the corresponding up block and one in the corresponding down 
block in such a way that every node in the (i + 1)-th layer has only 2 neighbors in the i-th layer. 
This can be easily done by labeling each node in the network with a pair (i, r) where i corresponds to the 
number of the layer and r is the position of the node in the layer written in binary. Then each node (i, r)i s 
connected to (i +1,r) and (i +1,r(i+1)), where r(i+1) denotes r with the (i + 1)-th element complemented. 
000
001
010
011
100
101
110
111
0 1 2 3Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Figure 2: 2-Multibuttery network for 4 inputs. 
Each layer of the network has N elemen ts, but the multibuttery network we describ ed will only route 
2N inputs to 2N outputs. In a nal implemen tation, we would connect the actual inputs and outputs to 
/2 successiv e multibuttery inputs/outputs. 
Claim 2 A d-Multibuttery with N nodes in each layer can route any permutation of 2N inputs to 2N 
outputs. 
To prove this claim, we will need Halls Theorem. 
Theorem 3 (Hall) A bipartite graph G has a perfect matching if and only if every set S on the left has at 
least |S|neighb ors on the right. 
Using Halls Theorem, we now prove the above claim. 
Pro of We only need to prove that in each pair of consecutiv e layers, any block can successfully route its 
signals to both the up neighbor block and the down neighbor block. It suces to prove this for the rst 
layer, since the proof for any other layer is identical. Note that in the rst layer, at most N calls will need 
to go to the top half, and at most N calls need to go to the bottom half. Let S be the set of nodes in the 
rst layer which need to go up, and let T be the set of nodes in the top half of the second layer. We will 
show that it is possible to choose edges to match each vertex of S with a vertex of T . 
Since |S|N, and the graph induced by S T is an (,)-expander by construction, it follows that 
|N(S) T ||S|&gt; |S|, and so, by Halls theorem, there is a perfect matching between S and T .We 
use that matching to route the calls that need to go up. The calls that need to go down are routed in an 
analogous way. 
The previous claim guaran tees that a nonblocking routing exists, and it can be found by solving a match
ing problem. However, in real life we dont want to use a complicated global computation for routing: We 
need a simple distributed algorithm. Such an algorithm exists and is simple to describ e. Consider a pair of 
blocks S and T that we want to match (as in the proof of the claim). The algorithm is as follows: 
Algorithm : 
S1 S. 
while Si /negationslash= 
Every node of Si sends a proposal to all neighbors in T .
Every node of T receiving exactly one proposal accepts it.
Every node in Si that got at least one accepted proposal picks one arbitrarily and matches to it.
Si+1 is the set of unmatc hed remaining nodes in Si.
7-3 0 1 2 3
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Septem ber 30, 2009 
Lecture 7 
Lectur er: Jonathan Kelner Scrib e: Alan Deckelb aum (2009) 
1 Administrivia 
There were two administrativ e announcemen ts made at the beginning of class today. 
	The rst problem set will be posted online tonigh t. It will be due two weeks from today, on Octob er 
15. 
	Some people have asked Professor Kelner if he could post typo-free versions of the powerpoint slides 
online. Therefore, it is requested that the scribe email Professor Kelner a list of the known typos in 
the slides. 
2 Outline 
In todays lecture we are going to: 
	Discuss nonblocking routing networks 
	Start the description of a metho d for local and almost linear-time clustering and partitioning based on 
the Lovasz-Simono vits Theorem. 
Nonblocking routing networks are an example application of expander graphs. Furthermore, many of 
the techniques that we discuss today for analyzing routing networks can also be applied to error-correcting 
codes. In the second half of class, we discuss local clustering and partitioning, and we will nish the analysis 
in Tuesdays class. 
3 Non blocking routing networks 
Suppose you have a network with 2N terminals, consisting of N input terminals (drawn on the left side) 
and N output terminals (drawn on the right side). We want to design a network that connects them, in 
such a way that every permutation can be routed. That is, for any one-to-one function f from the input 
terminals to the output terminals, we would like there to be a path in the network from each input node i to 
the output node f(i). Furthermore, we ask that for any such f, the paths can be chosen to be vertex disjoin t. 
The motivation is that we would like for each input terminal to be able to talk to a dieren t output terminal 
so that none of the intermediate routers are overloaded. This kind of networks is called a nonblo cking routing 
network and it is generally useful for comm unication. 
We also want the network to have some nice features. First, we want that each node has bounded degree 
(If we dont ask this, then a simple solution is to put a wire from every input to every output. Of course, 
this is not very plausible for large N, for example the phone network in the U.S.). Second, we want that the 
number of nodes in the graph is O(n log n) (so, that the network is not too big). And nally we want a fast, 
decentralized algorithm for nding routes. 
7-1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>The slope is nondecreasing, so I is concave. 
We will prove some claims and Theorems about I, and then we will state and prove the Lovasz-Simono vits 
Theorem. 
Claim 8 For any c1,...,c 2m  1, 
2m 2m 
ci(ei)  I ci . 
i=1 i=1 
Pro of Think of the cis as weights for the (ei). Since the (ei) are non-increasing, moving some weight 
from some j to some i&lt;j only makes the sum bigger. So the sum is biggest when the rst bunch of cis 
are 1, the next one is the remaining, and the rest of them are 0, which is exactly the value of the right hand 
side. 
In what follows, let t and It be  and I at time t in the random walk. 
Claim 9 For all x and t, It(x)  It1(x). 
Pro of This claim states that the value of the curve at any point never increases as t increases. Let 
ei =(ui,vi), so that (u1,v1)  (u2,v2) ...  (u2m,v2m) It suce to prove the claim in the case where 
x = k and W is the vertex set {u1,...,u k} such that (u1,v1),...,(uk,vk) are precisely the set of edges 
leaving W. We then have: 
k k 
It(k)= t(ui,vi)) = t(ui)
i=1 i=1
k 
= t1(vi,u i), as the mass leaving W at t is the amoun t entering W at t 1 
i=1 
k 
 It1( 1)= It1(k),
i=1
where the last inequalit y follows from the previous claim. 
Now we will prove something a little stronger: We will prove that the curve It has to lie below It1 by 
an amoun t depending on G. 
Theorem 10 For every initial distribution p0,all t, and every x [0,m], 
1   
It(x)  It1(x 2Gx)+ It1(x+2Gx) ,2 
and for every x [m,2m], 
1   
It(x)  It1(x 2G(2m x)) + It1(x+2G(2m x)) .2 
Before beginning the proof, we note that the above result has a geometric interpretation. The rst 
equation above states that the value of It(x) lies below the midpoint of the line connecting It1(x 2Gx) 
and It1(x+2Gx). Recalling that the graph of I is always concave, this implies the above result that the 
value of I(x) at time tis no greater than the value of I(x) at time t1. Furthermore, if It diers signican tly 
from the straigh t line (which I converges to in the limit) and if G(x) is large, then It(x) will decrease by 
a signican t amoun t in the next step (once again, by concavity). Thus, the theorem matches our intuition 
that low-conductance cuts around x cause the walk to mix more slowly. 
7-6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Claim 4 The previous algorithm nds a matching of S and T in O(log n) steps. 
To prove this claim, rst we will need to prove the following two lemmas: 
Lemma 5 For any set S of size |S|N, the numb er of vertic es in T with exactly one neighb or in S is at 
least (2 d)|S| 
Pro of Let A be the vertices in T with exactly one neighbor in S, and let B be the remaining vertices in 
T which are neighbors of S. Since the graph induced by S T is a (,)-expander, |A B||S|. Also, 
we know that the number of edges from S to T is at most |A|+2|B|. Thus, using the fact that the number 
of edges from S to T is exactly d|S|, we know: 
|A|+ |B|= |A B||S| 
d|S|= e(S,T ) |A|+2|B| 
and hence 
|A||S||B||S| d|S||A|
2 
and thus |A|2|S|d|S|. 
Given the above lemma nd the fact that any node in the left side can receiv e at most d acceptances in 
any round of the protocol, we conclude: 
Lemma 6 For all i, 
|Si+1|2(1 /d). |Si| 
This last lemma guaran tees that the algorithm converges in O(log n) steps, as desired. 
4 Local and almost linear-time clustering and partitioning 
4.1 Motiv ation 
In these days, graphs are getting really big. For example, circuits layouts have 50 million transistors; scientic 
computing has hundreds of millions of variables; the Internet has billions of nodes, etc. So any algorithm 
that performs a task in these graph in time such as n2 will be very bad in practice. Even a running time 
such as n1.5 tends to not be good enough. In some cases, like in Internet, even visiting every node of the 
graph once is an impossible task. For that reason, we are interested in developing algorithms for certain 
applications that runs in almost linear time (i.e. O(npolylog( n))), or algorithms that are local, i.e., that do 
not need to visit the entire graph. (Note that log factors tend to be fairly small, even in the cases mentioned 
above, and oftentimes logarithmic factors depend on the specic model of computation being analyzed.) 
4.2 Local Clustering 
Given a vertex v in a graph we would like to know if v is contained in a cluster, where a cluster is a set that 
denes a cut of low conductance. We also want the running time for this algorithm to depend on cluster 
size and not in the size of the graph. A good example for this is nding a cluster of web pages around the 
mit.edu domain, where we dont want this task to depend on the number of sites recently created on the 
other side of the world. 
The goal for this part of the lecture will be to describ e an algorithm that runs in time almost linear in 
K that outputs a cluster of size at least K/2 around starting vertex, if that cluster exist. 
The approac h we will use will rely on what we know about cuts, eigenvalues and random walks. First 
observ e that if v is contained in a cluster and you start running a random walk from v, then the low 
7-4</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>for x  [0,m] and 
1   
Rt(x)= Rt1(x  2G(2m  x)) + Rt1(x +2G(2m  x)) ,2 
for x  [m, 2m], then it is easy to show that 
  1 t x Rt(x)  min( x, 2m  x)1  2 
G + .2 2m 
Using that all the curves dened so far are concave and the previous theorem, we have: 
It(x)  Rt(x), 
for all t, which proves the theorem. 
From here, we can derive the following Corollary: 
Corollary 12 For W a set of vertic es, and x = wW dw, 
    t   1 2  p t(w)  (w)  min( x, 2m  x)1  G .   2 wW 
We can use this Corollary for local clustering in the following way. If after O((log m/ G)2) steps a set of 
vertices contains a constan t factor more than what it would have under stationary distribution, then we can 
get a cut C such that (C)  G. (The cut can be obtained by mapping the probabilities to real line and 
cut like we did with v2 some lectures ago). 
The problem with this approac h is that computing all the probabilities will be too slow. In particular, 
after a constan t number of steps we have too many nonzero values. One solution proposed by Lovasz and 
Simono vits is to simply zero out the smallest probabilities and prove that it doesnt hurt much. However, 
the analysis can get messy . Instead, in next lecture we will speak about a dieren t approac h that uses a 
slightly dieren t vector, called PageRank. 
7-8</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Monte Carlo methods continued, approximate DNF counting, approximating the permanent of 0-1 matrices</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe5/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Figure 3: Type A path of length 2. 
Lemma 10 Let s  Mn. Then at most O(n2) other nodes s  Mn  Mn1 have s as their partner. 
Proof There are three possible types of nodes s that have s as their partner. The rst is if s = s 
(hence the type A path is empt y). The second can be obtained by a Reduce transition (the nodes s with 
augmen ting path of length 1). The third can be obtained by a Reduce and Rotate pair of transitions. 
There is only partn er for the rst, O(n) for the second, and O(n2) for the third. Therefore there are at most 
O(n2) nodes s that can count s as their partner. 
Now we wish to count the number of canon ical paths for type B. 
Lemma 11 Let T be a transition (i.e. an edge of Cn). Then the numb er of pairs s,t  Mn that contain T 
on their type B canonic al path is bounde d by |Cn|. 
Proof We will provide an injection T (s,t) that maps to matc hings in Cn = Mn  Mn1. As before, let 
d = st be the symmetric dierence of the two matc hings s and t (recall that these can be broken down into 
disjoin t alternating cycles C1,...,Cr). Now we proceed along the unwind ing of these cycles until we reach 
the transition T . At this point we stop and say that the particular matc hing we are at, where all cycles up 
to this point agree with s and all cycles after this point agree with t, is the matc hing that T (s,t) maps to. 
It is clear that this is ne when T is a Reduce or Augmen t transition, since these only occur at the 
beginning or end of an unwinding. The only problem is when T is a Rotate transition, because then there 
exists a vertex u (the pivot of the rotation) that is matc hed to a vertex v with (u,v)  s and is also matc hed 
to a vertex w with (u,w)  t. This is because up to T we agree with s, and after T we agree with t. But 
what we can do at this point is notice that one of these two edges (whic h we denote by es,t) always has 
the start vertex of the curren t cycle as one of its end-p oints. Therefore by remo ving it we end up with a 
matc hing again . This is further illustrated in Figure 6. 
Theorem 12 The conductanc e of our graph has the following bound 
1(Cn)= n6 
5-7 
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Figure 2: C2 for the graph G = K2,2. 
4.2.6 Canonical Paths 
We still need to dene the canonical paths pv,w for our graph Cn. For each node s  Mn Mn1, we associate 
with it a partner s  Mn, as follows: 
 If s  Mn, s = s. 
 If s  Mn1 and has an augmen ting path of length 1, augment to get s. 
 If s  Mn1 and has a shortest augmen ting path of length 3, augmen t to get s. 
Now for nodes s,t  Mn  Mn1, we show how to provide a canonical path ps,t which consists of three 
segmen ts (and each segmen t can be one of two dieren t types). 
 s  s (Type A) 
 s  t (Type B) 
 t  t (Type A) 
Type A paths are paths that connect a vertex s  Mn  Mn1 to its partner s  Mn. Clearly , if s  Mn 
then the type A path is empt y. Now if s  Mn1 and has an augmen ting path of length 1, then our canonical 
path is simply the edge that performs the Augmen t operation. If s  Mn1 and has a shortest augmen ting 
path of length 3, then our canon ical path is of length 2: rst a Rotate, then an Augmen t (see Figure 3 for 
an example). 
For a Type B path, both s and t are in Mn. We let d = s  t, the symmetric dier ence of the two 
matc hings (those edges which are not commo n to both matc hings). It is clear that since s and t are perfect 
matc hings, d consists of a collection of disjoin t, even-length, alternating (from s or from t) cycles of length 
at least 4. 
Our canonical path from s to t will in a sense unwind each cycle of d individually . Now, in order 
for the path to be canonical, we need to provide some ordering on the cycles so that we process them in 
the same order each time. However, this can be done easily enough. In addition, we need to provide some 
ordering on the vertices in each cycle so that we unwind each cycle in the same order each time. Again, this 
can be done easily enough . All that remain s is to describ e how the cycles are unwound , which can be done 
much more eectiv ely with a picture than by text. See Figures 4 and 5. 
We must now bound the number of canon ical paths that pass through each edge. First we consider the 
type A paths. 
5-6 
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Figure 4: Unwind ing a single cycle (type B path). 
Proof By Lemma 8, we have (G)  1 . As shown in Lemma 10, there are at most O(n2) canonical 2bdmax 
paths to s  Mn from Mn1, and at most O(n2) canonical paths from t  Mn to Mn1. In addition we 
showed in Lemma 11 that the number of type B paths through a particular transition T is bounded by 
= V (where V is the vertex set of Cn). Therefore as a whole, the number of canonical paths |Mn  Mn1| || 
2 4through a particular transition T is bounded by nV 2, which implies b = n. || n 1  
Since dmax = O(n2), the conductance is bounded from below by  nand our random walk mixes in 6 
polynomial time. 
5-8 
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>If there are n boolean literals, then there are 2n possible assignmen ts. Of these 2n assignmen ts, we want 
to know how many of them satisfy a given DNF formula F . Unfortunately , computing the exact number of 
solutions to a given DNF formula is #P -Hard1 . Therefore we simply wish to give an -appro ximation for the 
number of solutions a given DNF formula that succeeds with probabilit y 1   and runs in time polynomial 
in n, m, log , and 1/. 
Navely, one could simply try to use the Monte Carlo metho d outlined above to appro ximate the number 
of solutions. However the number of satisfying assignments migh t be exponentially small, requiring expo
nentially many samples to get a tight bound. For example the DNF formula F =(x1  x2   xn) has 
only 1 solution out of 2n assignmen ts. 
3.1 Reducing the Sample Space 
Instead of picking assignmen ts uniformly at random and testing each clause, we will instead sample from the 
set of assignmen ts that satisfy at least one clause (but not uniformly). This algorithm illustrates the general 
strategy of sampling only the important space. 
Consider a table with assignmen ts on one side and the clauses C1,C2,...,Cm on the other, where each 
entry is 0 or 1 depending on whether the assignme nt satises the clause. Then, for each assignmen t, we color 
the entry for rst clause which it satises yellow (if such a clause exists). We color the remaining entries 
satised clauses blue, and we set these entries to 0. See Figure 1. 
Figure 1: A table of assignmen ts versus clauses and how to color it. 
We will sample uniformly from the space of blue and yellow colored entries, and then test whether weve 
sampled a yellow entry. We then multiply the ratio we get by the total number of blue and yellow entries, 
which we can easily compute. 
Let clause Ci have ki literals. Then clearly the column corresp onding to Ci has 2nki satisfying assign
ments (whic h we can easily compute). We choose which clause to sample from with probabilit y proportional 
to 2nki . Then we pick a random satisfying assignmen t for this clause and test whether it is the rst satised 
clause in its row (a yellow entry), or if there is a satised clause that precedes it (a blue entry). The total 
size of the space were sampling from is just i 2nki . 
1To see this, understand that the negation of a DNF formula is just a CNF formula by application of De Morga ns laws. 
Therefore counting soluti ons to a DNF formula is equiv alent to counting (non-)solutions of a CNF formula, which is the canon ical 
example of a #P -Hard problem. 
5-2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>4.2.4 How to Sample (Appro ximately) Uniformly 
We still have to show how to sample uniformly from Ck = Mk  Mk1. We will only show how to sample 
approximately uniformly from this set. As it turns out, this result is good enough for our purp oses. 
The main idea here is to construct a graph whose vertex set is Ck, and then do a random walk on this 
graph which converges to the uniform distribution. We have to show two things: that the random walk 
converges in polynomial time, and that the stationary distribution on the graph Ck is uniform. To show 
that the random walk mixes quickly, we bound the conductance (Ck) by the metho d of canonic al paths. 
Lemma 8 Let G =(V,E) be a graph for which we wish to bound (G). For every v,w  V , we specify a 
canonic al path pv,w from v to w. Supp ose that for some constant b and for all e  E, we have 
I[e  pv,w]  b|V |
v,wV 
that is, at most b|V | of the canonic al paths run through any given edge e. Then (G)  1 , where dmax 4bdmax is the maxi mum degree of any vertex. 
Proof As before, the conductance of G is dened as 
e(S)(G) = min  . 
SV min vS d(v), vS d(v) 
Let S  V . We will show that (S)  1 . Without loss of generalit y, assume that |S||V |/2. Then 4bdmax 
the number of canonical paths across the cut is at least |S||S||S||V |/2. For each edge along the cut there 
can be no more than b|V | paths through each edge, the number of edges e(S) is at least |S| . 	 2b 
In addition we can bound min vS d(v), vS d(v) by |S|dmax. Thes e bounds give us 
(S) |S|/2b 1 . |S|dmax  2bdmax 
as claimed. 
Since the spectral gap is at least (G)2, as long as b and dmax are bounded by polynomials, a random 
walk on G will converge in polynomial time. 
4.2.5 The Graph Ck 
We will only do Cn. It should be clear later how to extend this construction for all k. Recall that our 
vertices corresp ond to matc hings in Mn  Mn1. We show how to connect our vertices with 4 dieren t types 
of directed edges: 
	Reduce (Mn  Mn1): If m  Mn, then for all e  m dene a transition to m = m  e  Mn1 
	Augmen t (Mn1  Mn): If m  Mn1, then for all u and v unmatc hed with (u,v)  E, dene a 
transition to m = m +(u,v)  Mn. 
	Rotate (Mn1  Mn1): If m  Mn1, then for all (u,w)  m,(u,v)  E with v unmatc hed, dene 
a transition to m = m +(u,v)  (u,w)  Mn1. 
	Self-Lo op: Add enough self-lo ops so that you remain where you are with probabilit y 1/2 (this gives 
us a unifor m stationary distribution). 
Note that this actually provides an undirected graph since each of these steps is reversible. 
Example 9 In Figur e 2 we show C2 for the graph G = K2,2. The two leftmost and two rightmost edges are 
Augment /Reduce pairs, while the others are Rotate transitions. The self-loops are omitte d. 
5-5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Our probabilit y of picking a yellow entry is at least 1/m, where m is the number of clauses, so we can 
take enough samples in polynomial time. Therefore, this algorithm is an FPRAS for counting the solutions 
to the DNF formula. 
4 Appro ximating the Permanen t of a 0-1 Matrix 
Denition 4 (Determinan t) For a given n  n matrix M, the deter minant is given by 
n
det(M )= sgn( ) Mi,(i). 
Sn i=1 
The formula for the permanen t of a matrix is largely the same, with the sgn() omitted. 
Denition 5 (Permanen t) For a given n  n matrix M, the permanent is given by 
n
per(M )= Mi,(i). 
Sn i=1 
However, while the determinan t of a matrix is easily computable  O(n3) by LU decomp osition  
calculating the permanen t of a matrix is #P -Complete. As we will show, computing the permanen t of a 0-1 
matrix reduces to the problem of nding the number of perfect matc hings in a bipartite graph. 
4.1 The Permanen t of a 0-1 Matrix and Perfect Matc hings 
Given an n  n 0-1 matrix M, we construct a subgraph G of Kn,n, as follows. Let the vertices on the left be 
v1,v2,...vn and let the vertices on the right be w1,w2,...wn. There is an edge between vi and wj if and 
only if Mij is 1. 
Supp ose  is a permutation of {1, 2,...n}. Then the product i Mi(i) is 1 if the pairing (vi,w(i)) is a 
perfect matc hing, and 0 otherwise. Therefore, the permanen t of M equals the number of perfect matc hings 
in G. As an example, we look at a particular 3  3 matrix and the corresp onding subgraph of K3,3. 
 
110
 110 
011
 
  1 
1 0 1 
1 1 0 
0 1  
  
 
  1 
1 
0 1 
1 
1 0 
0 
1  
  
Calculating the permanen t of a 0-1 matrix is still #P -Complete. As we will see, there is an FPRAS for 
appro ximating it. 
5-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>4.2 An FPRAS for Appro ximating the Permanen t of a Dense Graph 
4.2.1 Some History 
	1989: Jerrum and Sinclair showed how to appro ximate the permanen t of a dense graph (all vertices 
have degree at least n/2). At the time, it was not known if this result could be extended to the general 
case. 
	2001: Jerrum, Sinclair and Vigoda showed how to appro ximate the permanen t of an arbitrary graph 
(and therefore for any matrix with nonnegativ e entries). 
We will show today the result of 1989 for appro ximating the permanen t of a dense graph. 
4.2.2 General Strategy 
We cant do the nave Monte Carlo here, since the probabilit y of picking a perfect matc hing from the set 
of all permutations can be exponentially small. Therefore we will instead consider the set of all (possibly 
partial) matc hings, not just perfect ones. Let Mk be the set of all partial matc hings of size k. Now supp ose 
that we had a black box that samples uniformly at random from Mk  Mk1 for any k. Then by the Monte 
Carlo metho d, by testing mem bership in Mk, we can determine the ratio rk = |Mk| . |Mk1|
If we assume that for all k,1/  rk   for some polynomially-sized , then we can estim ate each rk 
to within relativ e error  =1/n2 using polynomially many samples. Therefore our estimate of the number 
of perfect matc hings is just 
n
|Mn| = |M1| 
i=2 ri. 
If all of our appro ximations were within a (1  1 
2 ) factor, then our total error is at most (1  1 
2 )n  (1  1 ).n	 n n 
4.2.3 Bounding the rk 
We rst begin with a crucial lemma. 
Lemma 6 Let G be a bipartite graph of minimum degree  n/2. Then every partial matching in Mk1 has 
an augmenting path of length  3. 
Proof Let m  Mk1 be a partial matc hing. Let u be an unmatc hed node of m. Now supp ose that there 
are no augmen ting paths of length 1 starting from u in this matc hing m (i.e. there is no unmatc hed node 
v such that there is an edge connecting u and v). Then by our degree conditions, u must be connected to 
at least n/2 of the matc hed nodes vi. Likewise if we pick an unmatc hed node v, if it has no augmen ting 
paths of length 1, then it must be connected to at least n/2 of the matc hed nodes u
j . But by the pigeonhole 
principle, there must exist i and j such that (u
j ,vi)  m. The path (u,vi,u
j ,v) is an augmen ting path of 
length 3. 
Theorem 7 Let G be a bipartite graph of minimum degree  n/2. Then 1/n2  rk  n2 for all k. 
Proof We rst prove that rk  n2 . Consider the function f : Mk  Mk1, which maps m  Mk to its 
(arbitrarily-c hosen) canonical represe ntative in Mk1 (i.e. uniquely choose a submatc hing of m). For any 
m  Mk1, it must be the case that |f1(m)| (n  k + 1)2  n2 . Thus |Mk| n2|Mk1|. 
Now we show that 1/n2  rk. Fix some m  Mk. By Lemma 6, every partial matc hing in Mk1 has 
an augmen ting path of length  3. There are at most k partial matc hings in Mk1 that can by augmen ted 
by a path of length 1 to equal m. In addition, there are at most k(k  1) matc hings in Mk1 that can be 
augmen ted by a path of length 3 to equal m. Thus |Mk1| (k + k(k  1))|Mk| = k2|Mk| n2|Mk|. 
5-4</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Figure 5: Unwindin g a collection of cycles (type B path). 
Figure 6: The encoding T (s,t). 
5-9
Figure by MIT OpenCourseWare.
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>LLL algorithm for lattice basis reduction, application to integer programming</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Remark. 
	For linear programming (LP), the running time of the algorithm will grow exponentially in n, but 
polynomially in m (the number of constrains) and the number of bits in the inputs. 
	For convex programming, the running time is polynomial in log(R/r). 
	As before, we could also ask for maximum of c  x over all x  K  Zn, which is equivalent to the 
feasibility problem, as we can do a binary search on the whole range of c  x. 
The main idea of Lenstras algorithm is the following. The main diculty of integer programming comes 
from the fact that K may not be well-rounded, therefore it could be exponentially large but still contain no 
integral point, as illustrated in the following gure: 
Figure 1: A not-well-rounded convex body 
Our rst step is thus to change the basis so that K is well-rounded, i.e., K contains a ball of radius 1 
and is contained in a ball of radius c(n) for some function that depends only on n. Such a transformation 
will sends Zn to some lattice L. Now our convex body is well-rounded but the basis of lattice L may be 
ill-conditioned, as shown in the following gure: 
Figure 2: A well-rounded convex body and an ill-conditioned lattice basis 
20-5 x2x1 + x2
x1
Figure by MIT OpenCourseWare.
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>2. Factor polynomials over the integers or rationals. Note that this problem is harder than the same task 
but over reals, e.g. it needs to distinguish x2 1 from x2 2. 
3. Given an approximation of an algebraic number, nd its minimal polynomial.	 For example, given 
0.645751 outputs x2 +4x3. 
4. Find integer relations among a set of numbers. A set of real numbers {x1,...,x n} is said to have an 
integer relation if there exists a set of integers {a1,...,a n} not identically zero such that a1x1 + + 
anxn = 0. As an example, if we are given arctan(1) ,arctan(1 /5) and arctan(1 /239), we should output 
arctan(1) 4arctan(1 /5) + arctan(1 /239) = 0. How would you nd this just given these numbers as 
decimals? 
5. Appro ximate to SVP, CVP and some other lattice problems. 
6. Break a whole bunch of cryptosystems. For example, RSA with low public exponent and many knapsac k 
based cryptographic systems. 
7. Build real life algorithms for some NP-hard problems, e.g. subset sum problem. 
4.2 Integer Programming in Bounded Dimension 
4.2.1 Linear, Con vex and Integer Programming 
Consider the following feasibilit y version of the linear programming problem: 
	Linear Programming (feasibilit y)
Given: An mn matrix A and a vector bRn
Goal: Find a point xRn s.t. Axb, or determine (with a certicate) that none exists
One can show that other versions, such as the optimization version, are equivalent to feasibilit y version. 
If we relax the searching regions from polytop es to convex bodies, we get convex programming. 
	Convex Programming (feasibilit y)
Given: A separation oracle for a convex body K and a promise that
	K is contained in a ball of singly exponential radius R 
 if K is non-empt y, it contains a ball of radius r which is at least 1/(singly exponential) 
Goal: Find a point xRn that belongs to K, or determine (with a certicate) that none exists 
Integer programming is the same thing as above, except that we require the program to produce a point 
in Zn, not just Rn . Although linear programming and convex programming are known to be in P, integer 
programming is a well-kno wn NP-complete problem. 
4.2.2 Lenstras algorithm 
Theorem 3 (Lenstra) If our polytop e/convex body is in Rn for any constant n, then ther e exists a poly
nomial time algorithm for inte ger programming. 
20-4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>/productdisplay 3.1 LLL produces a short vector 
We rst show that reduced basis gives a short vector. 
n1 
2 Claim 2 If b1,...,b n is a reduced basis, then ||b1||  2 1(L). 
Pro of Note that 
4 ||b  
i ||2 = ||Si bi||2  ||Si bi+1||2 
3 
4 4 4 = ||bi 
+1 + i+1 ,ibi  ||2 = ||bi 
+1||2 + i2
+1 ,i||bi  ||2 
3 3 3 
4 1  ||b  
i+1||2 + ||bi  ||2 ,3 3 
which gives ||bi 
+1||2  1 ||bi ||2 . By induction on i,wehave 2 
1 1 ||bi  ||2  ||b1 ||2 = ||b1||2 .2i1 2i1 
Recall that b  L, ||b||  min i ||b||. Therefore 1(L)  min i ||b||, which combined with the inequalit y i	 i 
above yields 
||b1||2  min{2i1||bi  ||2} 2n1 min{||bi  ||2} 2n11(L)2 
i	 i 
as desired. 
3.2 Con vergence of LLL 
Now we show that the LLL algorithm terminates in polynomial time. Note that in each iteration of LLL, 
Step 1 takes polynomial time and Step 2 takes O(1) times. What we need to show is that we only need 
to repeat Step1and Step2ap olynomial number of times. To this end, we dene a potential function as 
follows: n 
D(b1,...,b n)= ||bi  ||ni . 
i=1 
It is clear that Step 1 does not change D since we do not change the Gram-Sc hmidt basis. 
We are going to show that each iteration of Step 2 decreases D by a constan t factor. In Step 2, we swap i 
and i+ 1 only when ||b||2 &gt;4/3||Si bi+1||2  4/3||b ||2 . Therefore each swapping decreases D by a factor  i	 i+1
of at least 2/ 3, as desired. 
It is left to show that D can be upper-and lower-bounded. Since ||b||||bi||, the initial value of D cani /producttextbe upper bounded by (max i ||bi||)n(n1)/2 . On the other hand, we may rewrite D as n |det( i)|, where i=1 
i is the lattice spanned by b1,...,b i. Since we assume that the lattice basis vectors are integer-v alued, so 
D is at least 1. 
In sum, the algorithm must terminate in log (max i ||bi||)n(n1)/2 = poly(n) iterations. 2/ 3
4	Application of LLLLenstras Algorithm for Integer Program
ming 
4.1 Applications of LLL 
LLL algorithm has many important applications in various elds of computer science. Here are a few (many 
taken from Regevs notes): 
1. Solve integer programming in bounded dimension as we are going to see next. 
20-3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>2 LLL Algorithm 
2.1	 Reduced Basis 
In order to nd a short vector in the lattice, we would like to perform a discrete version of GS procedure. 
To this end, we need to formalize the notion of being orthogonal in lattice problems. One way to do this 
is to say that the result of our procedure is almost orthogonalized so that doing Gram-Sc hmidt does not 
change much. 
Denition 1 (Reduced Basis) Let {b1,...,b n} be a basis for a lattic e L and let M be its GS matrix 
dene d in Section 1. {b1,...,b n} is a reduced basis if it meets the following two conditions: 
 Condition 1: all the non-diagonal entries of M satisfy |ik| 1/2. 
 Condition 2: for each i, ||Si bi||2  4 ||Si bi+1||2, wher e Si is the ortho gonal complement of (i.e., the 3 
subsp ace ortho gonal to) span(b1,...,b i1), and Si is the projection operator to Si. 
Remark The constan t 4/3 here is to guaran tee polynomial-time termination of the algorithm, but the 
choice of the exact value is somewhat arbitrary . In fact, any number in (1,4) will do. 
Remark Condition 2 is equivalent to ||b + i+1 ,ib 
i ||2  3 ||b 
i ||2 and one may think it as requiring i+1	 4 
that the projections of any two successiv e basis vectors bi and bi+1 onto Si satisfy a gapped norm ordering 
condition, analogous to what we did in Gausss algorithm for 2D case. 
2.2	 The algorithm 
Given {b1,...,b n}, the LLL algorithm works as below. 
LLL Algorithm for SVP 
Repeat the following two steps until we have a reduced basis 
Step 1: Gauss Reduction 
Compute the GS matrix M
for i=1 to n
for k = i 1to1
m nearest integer to ik
bi  bi  mb k
end 
end 
Step 2: Swapping 
if exists i s.t. ||Si bi||2 &gt; 4 ||Si bi+1||2 
3 
then	 swap bi and bi+1
go to Step 1
Analysis of LLL Algorithm 
The LLL algorithm looks pretty intuitive, but it is not obvious at all that it converges in polynomial number 
of steps or gives a good answer to SVP. Well see that it indeed works. 
20-2 3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>/productdisplay It turns out that the lattice points are still well-separated and we can remedy the lattice basis by a basis 
reduction procedure of LLL (i.e., discrete Gram-Sc hmidt). Finally we chop the lattice space up in some 
intelligen t way and search for lattice points in K. 
Note that in the rst step of Lenstras algorithm, what we need is an algorithmic version of Fritz Johns 
theorem. As we saw in the problem set, there is an ecien t algorithm which, for any convex body K specied 
by a separation oracle, constructs an ellipsoid E such that 
E(P/prime)  K  O(n 3/2)E(P/prime). 
Next let T : Rn  Rn be the linear transformation such that E(P/prime) is transformed to B(P,1). NowKis 
sandwic hed between two reasonably-sized balls: 
B(P,1)  TK  B(P,R), 
where R = O(n3/2) is the radius of the outer ball. 
Let L = TZn with basis Te1,...,T en. Our goal is to nd a point (if it exists) in TK  TZn = TK  L. 
Our next step is to apply the basis reduction in LLL algorithm. We will need the following two lemmas in 
analyzing Lenstras algorithm. The proofs of the lemmas are left as exercises. 
Lemma 4 Let b1,...,b n be any basis for L with ||b1||2  ||bn||2 . Then for every x  Rn, ther e exists 
a lattic e point y such that 
1 ||x y||2  (||b1||2 +  + ||bn||2)4 
1  n||bn||2 .4 
Lemma 5 For a reduced basis b1,...,b n ordered as above, 
n 
||bi||  2n(n1)/4det(L). 
i=1 
Conse quently, if we let H = span(b1,...,b n1), then 
2n(n1)/4||bn||  dist(H,bn) ||bn||. 
Let b1,...,b n be a reduced basis for L. Applying Lemma 4 gives us a point y  L such that ||y P||  
1 n||bn||.2 
 case 1: y  TK. We nd a point in TK  L. 
 case 2: y/ TK, hence y/ B(P,1). Consequen tly, ||y P||  1 and ||bn||  2. n 
This means that the length of bn is not much smaller than R. In the following we partition L along the 
sublattice orthogonal to bn and then apply this process recursiv ely. /uniontext Let L/prime be the lattice spanned by b1,...,b n1 and let Li = L/prime + ibn for each i Z. Clearly L = iZ Li. 
From Lemma 5 the distance between two adjacen t hyperplanes is at least 
dist(bn,span(b1,...,b n1))  2n(n1)/4||bn||
2  2n(n1)/4||bn|| = c1(n), n 
where c1(n) is some function that depends only on n. This implies that the convex body TK can not 
intersect with too many hyperplanes. That is 
|{i Z : Li  B(P,R) /negationslash= }|  2R/c1(n)= c2(n) 
for some function c2(n) that depends only on n. Now we have reduced our original searching problem in 
n-dimensional space to c2(n) instances of searching problems in (n 1)-dimensional space. Therefore we 
can apply this process recursiv ely and the total running time will be a polynomial in the input size times a 
function that depends only on n. 
20-6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 18.409 An Algorithmists Toolkit Nov. 19, 2009 
Lecture 20 
Lectur er: Jonathan Kelner 
Brief Review of Gram-Sc hmidt and Gausss Algorithm 
Our main task of this lecture is to show a polynomial time algorithm which approximately solves the Shortest 
Vector Problem (SVP) within a factor of 2O(n) for lattices of dimension n. It may seem that such an algorithm 
with exponential error bound is either obvious or useless. However, the algorithm of Lenstra, Lenstra and 
Lovasz (LLL) is widely regarded as one of the most beautiful algorithms and is strong enough to give some 
extremely striking results in both theory and practice. 
Recall that given a basis b1,...,b n for a vector space (no lattices here yet), we can use the Gram-Sc hmidt 
process to construct an orthogonal basis b1,...,b 
n such that b 
1 = b1 and 
b 
k = bk [projection of bk onto span(b1,...,b k1)] for all 2 k n (note that we do not normalize b 
k). In 
particular, we have that for all k: 
 span(b1,...,b k) = span(b1,...,bk ), 
 bk = /summationtextk
i=1 kib 
i , and 
 kk =1. 
The above conditions can be rewritten as B = MB, where basis vectors are rows of B and B, and 
 11 0 0 ... 0   1 0 0 ... 0  
 21 22 0 ... 0   21 1 0 ... 0  
M =    . . . ...    =    . . . . ..    . 
n1 n2 n3 ... nn n1 n2 n3 ... 1 
Obviously det(M) = 1, and thus vol(B) = vol(B). However, the entries of M are not integers, and thus 
L(B) /negationslash). We have proved last time that = L(B
for any bL, ||b||min i{||b||}.i 
Well use this to prove useful bound for the shortest vector on lattice. 
Recall also that last time we saw the Gausss algorithm which solves SVP for d = 2. There are two key 
ingredien ts of the algorithm. The rst is a denition of reduced basis which characterizes the discrete 
version of bases being orthogonal: namely , 
a basis {u,v}for a 2-d lattices is said to be reduced,if |u||v|and |uv| |u|2 .2 
The second is an ecien t procedure that produces a reduced basis. The procedure consists of two stages: 
First is a Euclid-lik e process which subtracts a multiple of the shorter vector from the longer one to get a 
vector as short as possible. The second stage is, if the length ordering is broken, we swap the two vectors 
and repeat, otherwise (i.e., |u||v|) the procedure ends. To make the above procedure obviously terminate 
in polynomial time, we change the termination criterion to be (1 /epsilon1)|u||v|. This only gives us a (1 /epsilon1)
approximation, but is good enough. The basic idea of LLL algorithm is to generalize Gausss algorithm to 
higher dimensions. 
20-1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Concentration of measure and the isoperimetric inequality, Johnson-Lindenstrauss theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>Pro of 
We will need the following denition. 
Denition 5 (Mo dulus of Con vexit y) The modulus of convexity  for a spher e is 
(/epsilon1)=inf 1 | x + y |: x, y Sn1 , |x y|/epsilon1.2 
It is a matter of two dimensional geometry to compute 
/epsilon12 
(/epsilon1)=1  1 /epsilon12/8.4 
(where the inequalit y comes from the Taylor series). 
This quantity measures how much more curved the sphere is than required by convexity. Namely , by 
convexity we are guaran teed that (/epsilon1) 1 (which we would obtain in the L1 or L norm). If (/epsilon1) is smaller, 
it means that longer segmen ts lie well inside the convex body. 
We would like to apply Brunn-Mink owski, but we dont have any result of that sort for the surface of the 
sphere. We will pass to a spherical shell, for which we can apply Brunn-Mink owski. Namely , if A  Sn1 
consider B =[ 1
2 , 1]A the union of the sets xA for 1
2  x  1. Note that Vol(B)  Vol(A)/2, where the 
volume of B is taken in Rn1 normalized so that Bn has volume 1 and the volume of A is taken in Sn1 
normzlied so that Sn1 has volume 1. The choice of 1/2 in particular is not important. All that matters 
is that neighborhoods of [ 1
2 , 1]A centrally project to reasonable neighborhoods of Sn1; ifwetook(0, 1]A, 
neighborhoods near the origin could project to almost all of Sn1 . 
xTo go from a set B  Bn to an A  Sn1 we take |x| : x B . Note that if we dene B =[ 1
2 , 1]A, 
take B/epsilon1, and then convert this back to a subset of Sn1, we do not necessarily obtain A/epsilon1. A point within /epsilon1 
of 21 A may project back to a point on Sn1 as far as 2/epsilon1 from A. In fact this is the worst that can happen, 
so that B/epsilon1 is carried back into A2/epsilon1. We would like to say that the volume of B/epsilon1 Bn is at least the volume 
of A2/epsilon1, so that we can convert a bound on the size of B/epsilon1 from Brunn-Mink owski into a bound on the size of 
A2/epsilon1 A/epsilon1. This isnt quite trueB/epsilon1 may contain points of norm &lt; 1/2. However, all points in B/epsilon1 have norm 
at least 1/2 /epsilon1, so it turns out this does not have a signican t eect (Vol [ 1
2 /epsilon1, 1
2 ]A is very small). 
We will show that Vol(B/epsilon1 Bn) 1 e2n/Vol(B). This will give us the desired result, since then 
2n(2/epsilon1) n/epsilon12/2e eVol(A2/epsilon1) &gt; (1 + /epsilon1)Vol(B/epsilon1 Bn) 1  1 2Vol(B) Vol(A) 
which is what we wanted. 
To bound the volume of B/epsilon1 Bn, let C be the set of points of Bn at least /epsilon1 away from every point of B. 
For any x  B and any y  C, by the denition of modulus of convexity |x+
2 y|  1 (/epsilon1) (the worst case 
is that both lie in Sn1). This implies that B C (1 )Bn, so that Vol(B C)1/n (1 ). Now by 
Brunn-Mink owski, 
(1 ) Vol(B C)1/n Vol(B)1/n + Vol(C)1/n. 
By easy calculus or the power-mean inequalit y, and the inequalit y ex 1 x, we conclude 
Vol(B)1/2Vol(C)1/2 (1 )n 
Vol(C) (1 )2n/Vol(B) e 2n/Vol(B) 
Taking complemen ts in Bn , 
2neVol(B/epsilon1 Bn)=1 Vol(C) 1  Vol(B) 
as desired. 
16-3</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Theorem 1 For any Awith Vol(A)=1/2, Vol(A/epsilon1) 1 en/epsilon12/2 . 
Pro of If Awere a spherical cap, then A/epsilon1 would be the complemen t of the spherical cap at height /epsilon1, which 
has volume 1 en/epsilon12/2 . But by the isoperimetric inequalit y this is the minim um possible value of Vol(A/epsilon1). 
This theorem shows that for spheres in high enough dimension almost all of the volume of the sphere lies 
within /epsilon1of any set containing at least half the volume of the sphere. In fact almost all of the volume of the 
sphere lies within /epsilon1of any set containing any constan t fraction of the volume of the sphere (although the 
constan ts in the theorem would change). 
We will now go on to use this result to conclude that Lipschitz functions are almost always close to their 
median. 
Lipschitz Functions and Concen tration of Measure 
Denition 2 (1-Lipsc hitz) A function f: Sn1R is 1-Lipschitz if |f(a)f(b)||ab|for all a,bSn1 . 
It turns out that many reasonable functions are Lipschitz. For example, distance from a xed set is 
Lipschitz. 
Dene a median Mof a Lipschitz function to be a value Msuch that Vol({x: f(x) M}) Vol({x: f(x) M})= 
1/2. 
If we take f were one of the coordinate functions (which are Lipschitz), then the statemen t that most of 
the volume of a sphere lies near any hyperplane through the origin becomes the statemen t that the value of 
f is almost always near its median. We will see that in fact all Lipschitz functions are almost always near 
their median. 
Theorem 3 If f is Lipschitz, M is its median, and /epsilon1&gt;0, then 
Vol({x: |f(x) M|&gt;/epsilon1}) 2e n/epsilon12/2 . 
Pro of The set A= f(x) M has volume at least 1/2. The set f(x) M+ /epsilon1contains A/epsilon1. Therefore 
by the isoperimetric inequalit y, f(x)  M+ /epsilon1 holds for at least 1 en/epsilon12/2 of the volume of the sphere. 
Similarly , f(x) M/epsilon1holds for 1 en/epsilon12/2 of the volume of the sphere. Therefore in total |f(x) M|&gt;/epsilon1 
for at most 2en/epsilon12/2 of the volume of the sphere (since at every point where this inequalit y holds at least 
one of the previous two must fail). 
Although the range of a 1-Lipsc hitz function may have diameter 2, this result shows that 1-Lipsc hitz 
functions are almost constan t over most of their domain. We call this result concen tration of measure. 
Note that this result doesnt rely on the exact form of the isoperimetric inequalit y; it would be ne if the 
bound on the ratio Vol(A/epsilon1)/Vol(A) was somewhat weaker. 
The Isoperimetric Inequalit y 
We will prove a weaker statemen t than the full isoperimetric inequalit y because it is somewhat easier. 
Normally we would have to use a symmetrization argumen t, but after weakening the constan tants we will 
be able to apply Brunn-Mink owski. 
Theorem 4 For any ASn1 and any /epsilon1&gt;0 
2en/epsilon12/16 
Vol(A/epsilon1) &gt;1  .Vol(A) 
16-2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Nov. 5, 2009 
Lecture 16 
Lectur er: Jonathan Kelner Scrib e: Paul Christiano 
The Cherno Bound as Concen tration of Measure 
We have already seen some ways in which convex bodies are related to probabilit y. For example, we can 
think of the Cherno bound as the statemen t that for any unit vector a and real t,if x is chosen uniformly 
at random from the cube then 
Pr [|a  x| &gt;t]  2e 6t2 . 
Since |a  x| is the distance of x from the hyperplane orthagonal to a, this says that all but 2e6t2 of the 
volume of the cube lies at distance at most t from this hyperplane. Since a was arbitrary , we can conclude 
that 1  2e6t2 of the volume of the cube lies within t of any hyperplane through the origin. 
On the sphere we also observ ed that almost all of the volume lies very close to any hyperplane through the 
origin. In light of the probabalistic implications of this assertion for the cube we are motivated to consider 
them for the sphere. First we will need to derive a stronger statemen t for the sphere. 
The Isoperimetric Inequalit y on the Sphere 
We will consider the analogue of the isoperimetric question for subsets of the surface of the sphere. This 
requires analogues of the notions of distance, volume, and surface area. 
We dene the distance d(x, y) between points x, y  Sn1 to be their distance in the usual Euclidean 
metric in Rn . 
For volumes, we use the unique rotationally invariant measure on the surface of the sphere. The volume 
Vol(A) of a region A on the surface of the sphere is the volume of the union in Rn of all segmen ts connecting 
the origin to a point of A, normalized so that the volume of the whole sphere is 1. Alternativ ely, this is Haar 
measure when the sphere is given the natural Lie group structure. (You can do anything reasonable and get 
the same measure.) 
For surface areas, we use the same denition as in Rn . Namely , for a set A  Sn1, dene A/epsilon1 to be the 
set of points in Sn1 at a distance of less than /epsilon1 from some point of A. The surface area of A may be dened 
as /epsilon1A/epsilon1. We wont work with this quantityinstead we will derive bounds on Vol(A/epsilon1) itself for /epsilon1&gt; 0. 
Now the isoperimetric question is: among sets with a xed Vol(A), what is the minimal possible value of 
Vol(A/epsilon1)? 
The answer is the analogue of a ball: a spherical cap. More precisely , dene 
C(r, v)=  
x  Sn1 : d(x, v)  r  
. 
This is the ball of radius r centered at v in the metric we have dened on the sphere. This result 
is precisely analogous to the isoperimetric inequalit y in Rn . (The statemen t itself will be slightly more 
complicated because the optimal ratio A/epsilon1/A depends on the volume of A: a small cap is basically a ball in 
Rn1, while a very large cap has a very small surface area) 
For convenience, we will also dene the cap at height t: 
ct = c(t, v)=  
x  Sn1 : x  v  t  
. 
We have seen previously that the volume of a section of the sphere at height t is exponentiall small. From 
this it follows that the volume of c(t, v) is exponentially small in t. In fact, Vol(c(t, v))  ent2/2 . 
We will prove an approximation to this result soon, but rst we consider some consequences. 
16-1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Johnson-Lindenstrauss 
Johnson-Lindenstrauss can be proved by manipulating Gaussians, but it is quite easy with concen tration of 
measure. For now we will just give the setup and outline some applications. 
This is the rst example we have seen of the notion of metric embeddings, which turn out to be generally 
algorithmically useful. Given a metric d on a nite set of points X, we would like to nd a map f : X  Rn 
such that d(x, y)  d(f(x),f(y)) for the normal Euclidean metric d on Rn . More precisely , for any map 
f : X  Rn we dene the distortion D to be the ratio between the largest and smallest values of d(x,y) 
d(f(x),f (y)) 
as x and y vary. We would like to nd an embedding with 1 + /epsilon1 distortion. 
The Johnson-Lindenstrauss Theorem states that if the metric on X arises from an embedding of X into 
any Euclidean space, then X can be embedded with distortion at most 1 + /epsilon1 in Rk for k = O(/epsilon12 log |X|). 
More concretely , this embedding is given by projection onto a random k-dimensional subspace, and the ratio 
d(x, y)/d(f (x),f(y)) is very nearly O( k/n). 
This result is extremely useful in a number of situations. If I wish to answer some question about a 
xed set of points which depends only on their pairwise distances, then Johnson-Lindenstrauss allows us to 
reduce the problem to one in logarithmic dimension (for xed /epsilon1) by randomly projecting. If our algorithm 
has bad dependence on the dimension, this may reduce the runtime considerably (for example, exponential 
dependence becomes polynomial). Similarly , if I am dealing with a stream of very high-dimensional data and 
I do not have storage space to record it all, Johnson-Lindenstrauss allows us to retain a very small fraction 
of this data while preserving the answer to any question which depends only on distances. 
We will prove this result in the next lecture. 
16-4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Sparsification (combinatorial and spectral), effective resistance, matrix pseudoinverses and tail bounds</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe9/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>/summationdisplay 
/summationtext 2.3 Pseudoin verses 
In our analysis, we will come across the need to invert a singular matrix. Since this is obviously not 
possible, we redene our question to one that makes more sense. Let M be a n  n symmetric matrix. We 
can diagonalize M: 
n 
M = iviviT 
i=1 
If all the eigenvalues are nonzero, then it obviously invertible, and M1 = /summationtext
in 
=1 1 
i vivit 
The case we worry about is when there is a zero eigenvalue. But this is okay too: when M is degenerate, 
we dene the pseudoinverse by throwing away the zero eigenvalues and eigenvectors. In that case, we have 
/summationdisplay 1 M+ = i viviT 
i|i/negationslash=0 
The pseudoin verse has many nice properties. Of these, we use: 
 ker(L)= ker(L+ 
 MM+ = i|i/negationslash iT = the projection onto the nonzero eigenvectors. =0 viv
It is easy to see that MM+ = I when restricted to the image of M. 
2.4 Eectiv e Resistance 
We mentioned earlier that Spectral Sparsication also samples edges with dieren t probabilit y. It turns 
out that the correct way to do this is to sample each edge with probabilit y proportional to its eectiv e 
resistance. 
The basic idea is to treat each edge as a resistor with resistance 1. If the edge had a capacit y of c, we give 
it a resistance of 1/c. After calculating these values, we sample the edge (u,v) with probabilit y proportional 
to the eectiv e resistance between nodes u and v. 
Studen ts may recall learning metho ds to solve circuits from their previous classes. For example, studen ts 
may use a combination of Ohms law and Kirchos law, as well as the rules for calculating eectiv e resistances 
of resistors in series and parallel. To those who are comfortable with solving circuits, this may be a good 
way to think about the problem. However, the studen ts who dont like solving circuits are in luck too: now 
that we have the tools of Spectral Graph Theory , we can solve circuits with only linear algebra! In fact, we 
will combine our frequen t use of the graph Laplacian with the pseudoin verse dened above. 
Let U be the edge-v ertex adjacency matrix, C be the diagonal matrix with the various capacitances, and 
re =1/ce. 
That is, we dene U as in: 
 
 1 if v is the head of e 
U(e,v)= 1 if v is the tail of e  0 otherwise 
Then, we have that L = UT CU. From ohms law, we have i = CUv for i  RE , and v  Rv . From the 
conserv ation of curren t, we have iext = UT i, for iext  RV . Finally , we have iext = Lv, and v = L+iext 
We dene U(e,v) to be the adjacency matrix with 1 values. Let ue be the eth row, and v = L+iext. 
We have 
Reff (e)= ueL+ ueT 
and as a result, 
Reff (e)=(UL+UT )e,e 
Thus, calculating the eectiv e resistance of an edge is as simple as calculating the pseudoin verse of the 
Laplacian. Simple! 
9-5</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>/summationtext 
/summationdisplay 
/summationdisplay 
/summationdisplay 2 Theorem 2 (Cherno Bound) Let X1,...,X n be random variables so that Xi [0, 1], and let X = Xi. 
Then, 
Pr[|X E[X]|/epsilon1X] 2e (1)/epsilon12E[X] 
Proof The only dierence here is that the random variables Xi are no longer discrete variables, but lie in 
the interval [0, 1]. The proof is carried out the same as with the regular Cherno bound. 
What this allows us to do is to scale our random variables without changing the error bounds. Returning 
to our case, we assign to every edge e a random variable Ye and a weight we.If e is in a cut of size c,we 
require that we c. We will set Ye = 1 with probabilit y p/we; and Ye = 0 with probabilit y 1p/we. Instead 
of counting how many edges cross a cut (S,S), we will compute a weighted sum: 
YS = weYe 
e(S,S) 
The expectation is still correct; if there are c edges across the cut (S,S)in G, then 
E[YS ]= we p = pc. 
e(S,S) we 
This scheme gives us an advantage: if an edge is presen t in only cuts of large size, we can keep it with low 
probabilit y, which corresp onds to setting we to be large. On the other hand, if an edge is presen t in cuts of 
small size, we will keep it with high probabilit y, which corresp onds to setting we to be small. In this way, we 
can approximate cut problems while throwing away more edges which are presen t in only cuts of high size. 
Thus, a natural choice for we would be the size of the smallest cut containing e. Unfortunately , we 
do not know we; however, it is possible to approximate it quickly. The nal result is an /epsilon1 multiplicativ e 
approximation based on this scheme. We refer the reader to [1] for details. 
Spectral Sparsiers 
The construction shown above is known as a Combinatorial Sparsier . In the upcoming section and following 
lecture, we will see how to impro ve upon it with the spectral metho ds that we have been learning. 
Let G =(V,E) be our original graph. Recall that the laplacian has the property that 
x T LGx = (xi xj )2 , 
(i,j)E 
for some x Rn, and the sum is being taken over all edges in G.If x takes value 1 on the set S and 1on 
the S, this equation becomes 
x T LGx =4e(S). 
Let G/prime be a combinatorial sparsier of the graph G. The condition that all cuts in G are approximated 
with a multiplicativ e error of at most /epsilon1 by cuts in G/prime can be restated as 
(1 /epsilon1)x T LG x x T LGx (1 + /epsilon1)x T LG x, (2) 
for all x that take on only the values 1 and 1. This is true for all such discrete values of x. 
On the other hand, consider if Eq. (2) is true for all x Rn. Note that in this case we can limit ourselv es 
to the instances x  [1, 1]n by normalization. We now have a good denition for a spectral version of 
sparsication: 
Denition 3 A Spectral Sparsier G/prime of a graph G is one for which the relation 
(1 /epsilon1)x T LG x T LGx (1 + /epsilon1)x T LG x 
for all x [0, 1]n 
9-2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit 10/8/2009 
Lecture 9 
Lectur er: Jonathan Kelner 
At the end of the previous lecture, we began to motivate a technique called Sparsication. In this lecture, 
we describ e sparsiers and their use, and give an overview of Combinatorial and Spectral Sparsiers. We 
also dene Spectral Sparsiers, and create tools and language with which to construct and analyze them. 
1 Sparsication 
Suppose we are given a graph G =(V,E). We would like to solve some cut problem (i.e. min-cut, s-t min 
cut, sparsest cut) and so on. The running time of algorithms for these problems typically depends on the 
number of edges in the graph, which might be as high as O(n2). Is there any way to approximate our graph 
with a sparse graph G/prime in which all cuts are approximately of the same size? 
We will describ e two ways of sparsifying our graph. The rst is the metho d of Benczur-Karger, and 
relies on random sampling of edges. The second technique is Spectral Sparsication, and uses spectral 
techniques to impro ve upon Benczur-Kargers algorithm. 
1.1 First Try 
Our rst attempt at sparsifying will use random sampling. Lets start by sampling each edge with probabilit y 
p. Then, if a cut has c edges crossing it in G, the expected value of edges crossing it in the new graph G/prime is 
pc. Our algorithm will solve the cut problem in G/prime. Say the answer is a cut with value S/prime; then our algorithm 
will output the estimate S = S/prime/p for the original graph G. 
Denoting the number of edges between S and S  by e(S)= pc, we have the following concen tration result 
due to Cherno s inequalit y: 
P (|eG (S) pc|/epsilon1pc) e /epsilon12 pc/2 . (1) 
So our result will be close to the correct answer provided pc is large. In particular, picking 
d log n p =( ),/epsilon12c 
will make the right side of Eq. (1) at most nd . Summarizing, we can choose p to get an /epsilon1 multiplicativ e 
approximation with probabilit y at least 1 nd . 
Is it possible to choose p to get this multiplicativ e approximation for all cuts, rather than just one as 
above? The answer is yes; the main ingredien t is a result of Karger that the number of small cuts in a graph 
is not too large: 
Theorem 1 (Karger) If G has a min-cut of size c, then the numb er of cuts of value c or less is at most 
2n . 
1.2 Second try 
The problem with this proposal is that it breaks for small cuts. Say c is small, but an edge e is only involved 
in cuts of size k. What we want to do is to sample these edges with a small probabilit y of failure. 
The idea that we use is to sample edges, but with a weight of 1/p. This metho d is called importance 
sampling. To do this, we need a slightly modied version of the Cherno bound: 
9-1</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg 
10 111 
0 1 /followsequal 
21 1 
as well as 
1 /parenleftbigg 
11 /parenrightbigg/parenleftbigg 
10 /parenrightbigg 
 /followsequal 21 1 0 1
After this experimen tation, we claim that the following is the right denition of order.
Denition 4 We write that M /followsequal N if 
x T Mx  XT Nx x  Rn 
Note that this denition of order has the following properties: 
1. If M /followsequal N and N /followsequal M, then M = n 
2. M /followsequal 0if M is a positive semidenite matrix. 
3. M /followsequal N if M  N is positive semidenite 
4. If M1 /followsequal N1 and M2 /followsequal N2, then
M1 + M2 /followsequal N1 + N2
These properties suce for our purposes, and with this, we can dene an associated order on graphs as 
well. 
Denition 5 Given graphs G and H, say that G /followsequal H if LG /followsequal LH . 
Claim 6 Let G =(V,EG,wG) and H =(V,EH ,wH ) be weighte d graphs on the same vertex set such that 
wG(i,j)  wH (i,j) for all edges (i,j)  E. Then, G /followsequal H 
2.2 Towards Spectral Sparsication 
With this order relation on graphs, we can now restate the goal of spectral sparsication: Given a dense 
graph G, we want to create a sparse graph H where 
Lh /precedesequal LG /precedesequal (1 + /epsilon1)LH 
By sparse, we mean that H has polylog( n) edges, where n is the number of nodes. We will show in this 
and the next lecture how to construct spectral sparsiers with O(nlogn) edges in Polynomial time. This 
can actually be impro ved to a linear time construction, but will use geometric techniques that we will learn. 
Moreo ver, it is possible to construct O(n) edge sparsiers in polynomial time. The benets of this are that 
the problem is more geometrically avored. It is also a nice example of how generalizing can make things 
easier sometimes. 
The algorithm that we propose is very simple. It is similar in structure to the B-K algorithm, but we use 
dieren t probabilities for sampling the edges. 
 Compute probabilit y pe for each edge e. 
 Sample each edge uniformly with probabilit y pe, and if an edge is selected, include it with weight 1/pe. 
These probabilities are based on a linear algebra sense of importance, and have a nice interpretation in terms 
of eectiv e resistance of circuits. To proceed with our analysis, however, we need to develop the ideas of 
pseudoin verses, calculating eectiv e resistances, and a matrix version of the Cherno Bound. 
9-4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>It is clear from this denition that spectral sparsiers are combinatorial sparsiers. A natural question 
is then to ask if all combinatorial sparsiers also spectral sparsiers. 
The answer is no, and we provide a proof by counterexample. Consider the graph G/prime with vertex set 
{1,2,...,n } and an edge between i,j when i j mod n  k. G is G/prime with the edge (1,n/2) added. The 
graph looks something like the gure below. 
Then, for an appropriate /epsilon1, G/prime is a combinatorial sparsier of G. Indeed, the min cut in G cuts (k) 
edges; the min cut in G/prime cuts one less. With /epsilon1= (1/k), we have that G/prime is a combinatorial sparsier of G. 
On the other hand, G/prime is not a spectral sparsier of G.Let 
x= /parenleftbig 
01 ... n/2  1 n/2  1 ... 10 /parenrightbig 
. 
Then, we have that 
x T LG x=(nk3) 
since each vertex contributes ( /summationtext
ik 
=1 k2) to the sum. On the other hand, 
x T LGx=(nk3)+( n  1)2 
2 
If k is constan t, we get that we need /epsilon1= (1/n) for G/prime to be a spectral sparsier of G. 
2.1 Order Relations on Laplacians 
In order to dene spectral approximations, we rst need to dene the appropriate vocabulary . Earlier, we 
made error approximations based on cut size. In the spectral case, we will be using the laplacian of the 
graph instead -so a nice way to compare laplacians would be idea. That is to say, we want a good relation 
/followsequal on symmetric matrices that is an ordering on them, and also is somewhat consisten t with the notions of 
cuts. 
How will we dene this ordering? An immediate idea is the following: 
M /followsequal N  mi,j  ni,j i,j 
Upon second though t, we realize that this is no good for our purposes. For one, spectral graph theory is 
all about eigenvalues, and this relation tells us nothing about the eigenvalues of the matrix! Furthemore, the 
values of individual entries are highly dependen t on choice of basis, which would be bad. If such a denition 
were used, a process like diagonalizing the Laplacians could possibly aect the graph orders. 
We try again with another denition: 
M /followsequal N if the ith eigenvalue of M is  the ith eigenvalue of N for all indices i 
This is better in that it is basis independen t -but it is too basis independen t. Under this denition, we 
have both 
9-3</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>/radicalBigg 
 logq E[/bardbl   S /bardbl2]  kn  1  /epsilon1/2 q 
for q = O(n log n//epsilon12). Thus, we see that our construction yields a spectral sparsier as desired. 
From the algorithmics of the construction, it is easy to see that this is a poly-time procedure. The whole 
procedure is constructiv e, and uses the standard linear algebra operations. The bottlenec k in this procedure 
comes from computing eectiv e resistances, and in particular, the matrix inversions and multiplications. We 
claim that the procedure can be impro ved to nearly linear time. Doing so would involve two components: 
	Close to linear algorithms for solving linear equations of the form Lx = b for a laplacian L. 
	A way to compute all the eectiv e resistances by solving logarithmically many linear systems. This 
uses the Johnson-Lindenstrauss Lemma. 
References 
[1]	 Randomized Appro ximation Schemes for Cuts and Flows in Capacitated Graphs,  A. Benczur, D. Karger, 
man uscript. 
9-7</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>/summationdisplay /radicalBigg 
/summationdisplay /summationdisplay 
/summationdisplay /summationdisplay 2.5 Error Bounds 
The last tool that we need to build is a way to dene error bounds for matrices. In particular, we will use 
the following theorem. 
Theorem 7 For distributions on vectors y wher e /bardbl y /bardbl t and /bardbl Eyyt /bardbl2 1 (wher e we are using the l2 
norm) then: 
E /bardbl EyyT  1 q 
yiyiT /bardbl2 kt log q 
q qi=1 
This is a concen tration of measure theorem, and we claim that it is similar to the Cherno bound. 
Now, onto approximation. For our sparisier H to approximate the original dense graph G, we want that 
xT LH x1  /epsilon1   1+ /epsilon1 xT LGx 
for all vectors x. Rather, it is sucien t to show that 
zT MT LH Mz1  /epsilon1   1+ /epsilon1 zT MT LGMz 
for all vectors z, provided that x  (LG)  x  range(M). Choose M so that MT LGM is a projection. 
Then, it suces to show that 
/bardbl MT LH M  MT LGM /bardbl2 /epsilon1 
From before, we have that LG = UT CU. Choose M = L+UT C1/2 . Then, we have G
= MT LGM = C1/2UL+UT C1/2 = G
Now, recall that LG = UT CU. Ifwelet de be the weight of e in the sparsier H, set Se,e = d
cee . Then, 
we can write 
LH = UT CSU = UT C1/2SC1/2U 
yielding 
MT LH M =S 
We need to choose a diagonal S such that the number of nonzero elemen ts of S is O(nlogn//epsilon12) With this 
choice, we have 
/bardbl S   /bardbl2 /epsilon1 
Dene e as the eth column of : that is, e =(,e). Then, S= /summationtext Se,eeeT ,so 
/bardbl e /bardbl2= e,e = ceReff (e) 
(this is because  = 2 = C1/2(UL+UT )C1/2) /radicalBig G n1 c2Reff (e)We then set e = ceReff (e) e with /bardbl e /bardbl= n  1. Choose edges with probabilit y pe = n1 . 
Recall that 
ceReff (e)= e,e = n  1 
e e 
Then, we nd that 
E[eeT ]= peeeT = eeT = 
e e 
Sample q times with replacemen t, and set S(e,e)= 1  the number of times that e is chosen. qceReff (e) 
Then, from the theorem above, we have 
9-6</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Courant-Fischer and Rayleigh quotients, graph cutting, Cheerger&#8217;s Inequality</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe3/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>/summationtext 
/summationtext /summationtext /summationtext . /summationtext /summationtext 
/prime 
/parenleftbigg /parenrightbigg 
/summationtext /summationtext 
/summationtext/prime 
/summationtext
/summationdisplay /summationdisplay 
/summationdisplay 
/summationdisplay /summationdisplay /summationdisplay 
/summationdisplay /summationdisplay 
/parenleftbig /parenrightbig 
/summationdisplay 4.2.3 Step 3: Breaking the Sum in Half 
We would like to break the summations in half so that we do not have to deal with separate cases with 
positive and negativ e numbers. Let E/prime be the edges (i,j) with i,j m, and let E/prime be the edges (i,j) with  + 
i,j m. We then have 
(yi yj )2 + (yi yj )2 
(i,j)E (i,j)E/prime (yi yj )2 
= /prime
 +(i,j)E
2 m 2 + n y2 
i yi i=1 yi i=m i 
Note that ym appears twice in the summation on the denominator, which is ne since ym = 0. We also know 
that for any a,b,c,d&gt;0, 
a+ b ab min ,, c+ d cd 
so it is enough to bound 
(yi yj )2 (yi yj )2 
/prime
 +(i,j)E (i,j)Eand . m n 2 2 
i=1 y yi i i=m 
Since the two values are essentially the same, we will focus only on the rst one. 
4.2.4 The Main Lemma 
Let Ci be the number of edges crossing the point xi, i.e. the number of edges in the cut if we were to take 
S = {1,...,i }. Recall that 
e(S)= (G) = min , 
SV min(|S|,|S|) 
so by taking S = {1,...,i },weget Ci i for in/2 and Ci (ni) for in/2. 
The main lemma we use to prove Cheegers Inequalit y is as follows. 
Lemma 8 (Summation by Parts) For any z1 ...zm =0, 
m 
(i,j)E/prime
 |zi zj | |zi|. 
i=1 
Proof For each (i,j) E/prime with i&lt;j, write  
j1 
|zi zj |= zj zi =(zi+1 zi)+(zi+2 zi+1)+ +(zj zj1)= (zk+1 zk). 
k=i 
Summing over (i,j) E/prime , we observ e that each term zk+1 zk appears exactly Ck times. Therefore, 
m1 m1 
|zi zj |= Ck(zk+1 zk)  k(zk+1 zk). 
/prime
k=1 k=1 (i,j)E
Note that zi zm =0, so |zi|= zi for 1 im. Then we can evaluate the last summation above as 
m1 
|zi zj | k(zk+1 zk) 
/prime
k=1 (i,j)E
=  (z2 z1)+2(z3 z2)+3(z4 z3)+ +(m1)(zm zm1) 
= (z1 z2 zm1 +(m1)zm) 
m 
=  |zi|. 
i=1 
3-8</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>/summationdisplay 
/summationdisplay /summationdisplay /summationdisplay 18.409 An Algorithmists Toolkit Septem ber 17, 2009 
Lecture 3 
Lectur er: Jonathan Kelner Scrib e: Andre Wibisono 
1 Outline 
Todays lecture covers three main parts: 
 Couran t-Fischer formula and Rayleigh quotien ts 
 The connection of 2 to graph cutting 
 Cheegers Inequalit y 
2 Couran t-Fisc her and Rayleigh Quotien ts 
The Couran t-Fischer theorem gives a variational formulation of the eigenvalues of a symmetric matrix, which 
can be useful for obtaining bounds on the eigevalues. 
Theorem 1 (Couran t-Fisc her Form ula) Let A be an n  n symmetric matrix with eigenvalues 1  
2 ... n and corresponding eigenve ctors v1,...,v n. Then 
xT Ax 1 = min x T Ax= min , 
/bardblx/bardbl=1 x/negationslash=0 xT x 
xT Ax 2 = min x T Ax= min , 
/bardblx/bardbl=1 x/negationslash=0 xT x 
xv1 xv1 
xT Ax n = max = max x T Ax= max . 
/bardblx/bardbl=1 x/negationslash=0 xT x 
In gener al, for 1  k  n, let Sk denote the span of v1,...,v k (with S0 = {0}), and let Sk  denote the 
ortho gonal complement of Sk. Then 
xT Ax k = min x T Ax= min . 
/bardblx/bardbl=1 x/negationslash=0 xT x 
xSk
1 xSk
1 
Proof Let A = QT Q be the eigendecomp osition of A. We observ e that xT Ax = xT QT Qx = 
(Qx)T (Qx), and since Q is orthogonal, /bardblQx/bardbl= /bardblx/bardbl. Thus it suces to consider the case when A= is a 
diagonal matrix with the eigenvalues 1,..., n in the diagonal. Then we can write 
  1 x1 n  .  .  2 x T Ax= /parenleftbig 
x1  xn /parenrightbig
 ..  ..  = ixi . 
 xi=1 n n 
We note that when A is diagonal, the eigenvectors of A are vk = ek, the standard basis vector in Rn, i.e. 
(ek)i =1 if i = k, and (ek)i = 0 otherwise. Then the condition x Sk
1 implies x ei for i=1,...,k 1, 
so xi = /angbracketleftx,ei/angbracketright= 0. Therefore, for xSk
1 with /bardblx/bardbl= 1, we have 
n n n 
x T Ax= ix 2 
i = ix 2 
i k x 2 
i = k/bardblx/bardbl2 = k. 
i=1 i=k i=k 
3-1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>/summationdisplay /summationdisplay /summationdisplay Theorem 5 For any x  1, x1  x2  ...  xn, ther e is some i for which 
xT Lx ({1,...,i })2 
 . xT x 2dmax 
This is great because it not only implies Cheegers inequalit y by taking x = v2, but it also gives an actual 
cut. It also works even if we have not calculated the exact values for 2 and v2; we just have to get a good 
approximation of v2 and we can still get a cut. 
4.2 Proof of Cheegers Inequalit y 
4.2.1 Step 1: Prepro cessing 
First, we are going to do some prepro cessing. This step does not reduce the generalit y of the proof much, 
but it will make the actual proof cleaner. 
 For simplicit y, suppose n is odd. 
 Let m =(n +1)/2. 
 Dene the vector y by yi = xi  xm. 
We can observ e that ym = 0, half of the vertices are to the left of ym, and the other half are to the right of 
ym. 
Claim 6 
xT Lx yT Ly xT x yT y 
Proof First, the numerators are equal by the operation of the Laplacian, 
x T Lx = (xi  xj )2 = /parenleftbig 
(yi + xm)  (yj + xm) /parenrightbig2 = (yi  yj )2 = y T Ly. 
(i,j)E (i,j)E (i,j)E 
Next, since x  1, 
y T y =(x + xm1)T (x + xm1)= x T x +2xm(x T 1)+ x 2 (1T 1)= x T x + nx 2  x T x.m m 
Putting together the two computations above yields the desired inequalit y. 
4.2.2 Step 2: A Little More Prepro cessing 
We do not want edges crossing ym = 0 (because we will later consider the positive and negativ e vertices 
separately), so we replace any edge (i, j) with two edges (i, m) and (m, j). Call this new edge set E/prime . 
Claim 7 /summationtext /summationtext 
(i,j)E (yi  yj )2
(i,j)E/prime (yi  yj )2 
/summationtext  /summationtext .2 2 
iV yi iV yi 
Proof The only dierence in the numerator comes from the edges (i, j) that we split into (i, m) and (m, j). 
In that case, it is easy to see that (also noting that ym =0) 
(yj  yi)2  (yj  ym)2 +(ym  yi)2 . 
3-7</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>C (see  Figure 2 for an illustration). Let p and q be the points that minimize 
Since C
/summationtext 
/summationtext
/summationdisplay /summationdisplay 
/summationtext /summationtext 3.3 Interlude on Relaxations 
nThe idea to drop the constrain t x {1,1}mentioned in the previous section is actually a recurring 
technique in algorithms, so it is worthwhile to give a more general explanation of this relaxation technique. 
A common setup in approximation algorithms is as follows: we want to solve an NP-hard question which 
takes the form of minimizing f(x) subject to the constrain t x C. Instead, we minimize f(x) subject to a 
weaker constrain t x C/prime f 
in C and C/prime, respectively. 
smaller 
f(x) C C/prime 
p 
q q /prime 
Figure 2: Illustration of the relaxation technique for approximation algorithms. 
For this relaxation to be useful, we have to show how to round q to a feasible point q/prime C, and prove 
f(q/prime) f(q) for some constan t  1. This implies f(q/prime) f(q) f(p), so this process gives us a 
-appro ximation. 
3.4 Solving the Relaxed Program 
Going back to our integer program to nd the cut of minim um ratio, now consider the following relaxed 
program, 
(xi xj )2 
(i,j)Emin /summationtext . 
xRn (xi xj )2 
i&lt;j 
Since the value of the objective function only depends on the dierences xi xj , we can translate x Rn 
such that x 1, i.e. in 
=1 xi =0. 
Then observ e that n 
(xi xj )2 = nxi 2 , 
i&lt;j i=1 
which can be obtained either by expanding the summation directly , or by noting that x is an eigenvector of 
the Laplacian of the complete graph Kn with eigenvalue n (as we saw in Lecture 2). Therefore, using the 
Rayleigh quotien t, 
(i,j)E (xi xj )2
(i,j)E (xi xj )2 2min /summationtext = min /summationtextn 2 = . 
xRn i&lt;j (xi xj )2 xRn n i=1 xi n 
x1 
3-5 CC/prime, we know that f(q)f(p).</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/braceleftBigg 
/summationdisplay 
/parenleftBigg /parenrightBigg 
/summationtext 
/summationtext 
/summationtext The cut of minim um ratio is the cut that minimizes (S).The isop erimetric number of a graph G is 
the value of the minimum cut, 
(G) = min (S). 
SV 
As we can see from the denition above, the cut ratio is trying to minimize the number of edges across the 
cut, while penalizing cuts with small number of vertices. This criterion turns out to be a good one, and is 
widely used for graph cutting in practice. 
3.2 An Integer Program for the Cut Ratio 
Now that we have a good denition of graph cutting, the question is how to nd the optimal cut in a 
reasonable time. It turns out that we can cast the problem of nding cut of minim um ratio as an integer 
program as follows. 
Associate every cut S S  with a vector x {1, 1}n, where 
1, if i S, and xi = 1, if i   S. 
Then it is easy to see that we can write 
1 e(S)= (xi xj )2 .4 (i,j)E 
For a boolean statemen t A, let [A] denote the characteristic function on A,so[A]=1i f A is true, and 
[A]=0 if A is false. Then we also have 
  
 S] [i S, j  
2 = xj ]= 4|S||S|= /summationdisplay 
[i S] /summationdisplay 
[j  = /summationdisplay 
S]= 1 /summationdisplay 
[xi /negationslash1 /summationdisplay 
(xi xj )2 . 
iV jV i,jV i,jV i&lt;j 
Combining the two computations above, 
(i,j)E (xi xj )2 e(S)min /summationtext = min . 
x{ 1,1}n (xi xj )2 SV |S||S|i&lt;j 
Now note that if |V |= |S|+ |S|= n, then 
n min(|S|, |S|) |S||S|n min(|S|, |S|),2 
so we get 
1 e(S) /summationtext 
(i,j)E (xi xj )2 2e(S) 2 (G) = min  min /summationtext  min = (G). n SV n min(|S|, |S|) x{ 1,1}n (xi xj )2 SV n min(|S|, |S|) ni&lt;j 
Therefore, solving the integer program 
(xi xj )2 
min (i,j)E 
x{ 1,1}n (xi xj )2 
i&lt;j 
allows us to approximate (G) within a factor of 2. The bad news is that it is NP-hard to solve this program. 
nHowever, if we remove the x {1, 1}constrain t, we can actually solve the program. Note that removing 
nthe constrain t x {1, 1}is actually the same as saying that x  [1, 1]n, since we can scale x without 
changing the value of the objective function. 
3-4</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>/summationdisplay /summationdisplay 
/summationdisplay /summationdisplay /summationdisplay 
/summationtext 
/summationtext/summationtext 
/summationtext 
/summationtext 
/prime 
/summationtext
/braceleftBigg /summationtext /summationtext 
/summationtext/prime 
/summationtext/bracerightBigg 4.2.5 Using the Main Lemma to Prove Cheegers Inequalit y 
Now we can nally prove Cheegers inequalit y.
Proof of Cheegers Inequalit y: This proof has ve main steps.
1. First, we normalize y such that /summationtextm
i=1 yi 2 =1. 
2. Next, this is perhaps a somewhat nonintuitive step, but we want to get squares into our expression, so 
we apply the main lemma (Lemma 8) to a new vector z with zi = yi 2 . We now have 
m 
2 2 2 
j | i |= . 
(i,j)E/prime
 |y y |yi 
i=1 
3. Next, we want something that looks like (yi yj )2 instead of yi 2 yj 2, so we are going to use the 
Cauchy-Schwarz inequalit y. 
 1/2  1/2 
/summationdisplay /summationdisplay /summationdisplay /summationdisplay 
(i,j)E/prime 
 |y 2 
i y 2 
j |= 
(i,j)E/prime 
 |yi yj ||yi + yj | 
(i,j)E/prime 
 (yi yj )2   
(i,j)E/prime 
 (yi + yj )2  . 
4. We want to get rid of the (yi + yj )2 part, so we bound it and observ e that the maxim um number of 
times any yi 2 can show up in the summation over the edges is the the maxim um degree of any vertex. 
m 
(yi + yj )2 2 (yi 2 + yj 2) 2 dmax yi 2 2dmax. 
(i,j)E/prime
(i,j)E/prime
i=1 
5. Putting it all together, we get 
/parenleftBig /parenrightBig2 2 2(yi yj )2 |yy| 2 
 /prime
/prime
i j (i,j)E (i,j)E . m 2 (yi + yj )2 2dmax i=1 y /prime
i (i,j)E
Similarly , we can also show that 
(yi yj )2 2 
+(i,j)E . n 2 2dmax yi i=m 
Therefore, 
(yi yj )2 (yi yj )2T Lx yT Ly 2 /prime
 +(i,j)E (i,j)E xmin   , . m 
i=1 yn 2 2 2dmax xT x yT y yi i i=m 
4.2.6 So who is Cheeger anyway? 
Je Cheeger is a dieren tial geometer. His inequalit y makes a lot more sense in the continuous world, and his 
motivation was in dieren tial geometry . This was part of his PhD thesis, and he was actually investigating 
heat kernels on smooth manifolds. A heat kernel can also be though t of as a point of heat in space, and the 
question is the speed at which the heat spreads. It can also be though t of as the mixing time of a random 
walk, which will be discussed in future lectures. 
3-9</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>/summationtext 
/summationtext /parenleftbigg /parenrightbigg It is easy to see that iV xi = 0, since there are equal numbers of nodes on the left and right subtrees of 
the root, so x  1. Calculating the Rayleigh quotien t of x gives us 
(i,j)E (xi  xj )2 2 1 /summationtext = = O . x2 n  1 niV i 
Thus we get 2  O(1/n), again with little eort. It turns out in this case that our approximation is correct 
within a constan t factor, and we did not even need to diagonalize a big matrix. 
3 Graph Cutting 
The basic problem of graph cutting is to cut a given graph G into two pieces such that both are prett y 
big. Graph cutting has many applications in computer science and computing, e.g. for parallel processing, 
divide-and-conquer algorithms, or clustering. In each application, we want to divide the problem into smaller 
pieces so as to optimize some measure of eciency , depending on the specic problems. 
3.1 How Do We Cut Graphs? 
The rst question to ask about graph cutting is what we want to optimize when we are cutting a graph. 
Before attempting to answer this question, we introduce several notations. Let G =(V, E) be a graph. Given 
a set S  V of vertices of G, let S = V \ S be the complemen t of S in V . Let |S| and |S| denote the number 
of vertices in S and S, respectively. Finally , let e(S) denote the number of edges between S and S. Note 
that e(S)= e(S). 
Now we consider some possible answers to our earlier question. 
Attempt 1: Min-cut. Divide the vertex set V into two parts S and S  to minimize e(S). This approac h 
is motivated by the intuition that to get a good cut, we do not want to break too many edges. However, 
this approac h alone is not sucien t, as Figure 1(a) demonstrates. In this example, we ideally want to cut 
the graph across the two edges in the middle, but the min-cut criterion would result in a cut across the one 
edge on the right. 
Attempt 2: Appro ximate bisection. Divide the vertex set V into two parts S and S, such that |S| and 
|S| are approximately n/2 (or at least n/3). This criterion would take care of the problem mentioned in 
Figure 1(a), but it is also not free of problems, as Figure 1(b) shows. In this example, we ideally want to 
cut the graph across the one edge in the middle that separates the two clusters. However, the approximate 
bisection criterion would force us to make a cut across the dense graph on the left. 
(a) Problem with min-cut (b) Problem with appro ximate bisection 
Figure 1: Illustration for problems with the proposed graph cutting criteria. 
Now we propose a criterion for graph cutting that balances the two approac hes above. 
Denition 3 (Cut Ratio) The cut ratio  of a cut S  S  is given by 
e(S)(S)= min(|S|, |S|) . 
3-3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Putting all the pieces together, we get 
e(S)(G) = min 
SV min(|S|,|S|) 
n e(S) min2 SV |S||S| 
n /summationtext 
(i,j)E (xi xj )2 
= min /summationtext 2 x{ 1,1}n (xi xj )2 
i&lt;j 
n /summationtext 
(i,j)E (xi xj )2 
 min /summationtext 2 xRn i&lt;j (xi xj )2 
n /summationtext 
(i,j)E (xi xj )2 
= min /summationtextn 22 xRn n i=1 xi x1 
2 = .2 
4 Cheegers Inequalit y 
In the previous section, we obtained the bound (G) 2/2, but what about the other direction? For that, 
we would need a rounding metho d, which is a way of getting a cut from 2 and v2, and an upper bound on 
how much ithe rounding increases the cut ratio that we are trying to minimize. In the next section, we will 
see how to construct a cut from 2 and v2 that gives us the following bound, which is Cheegers Inequalit y. 
Theorem 4 (Cheegers Inequalit y) Given a graph G, 
(G)2 
2 2(G),2dmax 
wher e dmax is the maximum degree in G. 
As a side note, the dmax disapp ears from the formula if we use the normalized Laplacian in our calcu
lations, but the proof is messier and is not fundamen tally any dieren t from the proof using the regular 
Laplacian. 
The lower bound of (G)2/2dmax in Cheegers Inequalit y is the best we can do to bound 2. The square 
factor (G)2 is unfortunate, but if it were within a constan t factor of (G), we would be able to nd a constan t 
approximation of an NP-hard problem. Also, if we look at the examples of the path graph and the complete 
binary tree, their isoperimetric numbers are the same since we can cut exactly one edge in the middle of the 
graph and divide the graphs into two asymptotically equal-sized pieces for a value of O(1/n). However, the 
two graphs have dieren t upper bounds for 2, O(1/n2) and O(1/n) respectively, which demonstrate that 
both the lower and upper bounds of 2 in Cheegers inequalit y are tight (to a constan t factor). 
4.1 How to Get a Cut from v2 and 2 
Let xRn such that x1. We will use x as a map from the vertices V to R. Cutting R would thus give a 
partition of V as follows: order the vertices such that x1 x2 ...xn, and the cut will be dened by the 
set S = {1,...,k } for some value of k. The value of k cannot be known a priori since the best cut depends 
on the graph. In practice, an algorithm would have to try all values of k to actually nd the optimal cut 
after embedding the graph to the real line. 
We will actually prove something slightly stronger than Cheegers Inequalit y: 
3-6</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>/summationdisplay /summationdisplay 
/summationtext /parenleftbigg /parenrightbigg On the other hand, plugging in x = ek  Sk
1 yields xT Ax =(ek)T Ae k = k. This shows that 
k = min x T Ax. 
/bardblx/bardbl=1 
xSk
1 
Similarly , for /bardblx/bardbl =1, 
n n 
x T Ax = ix 2 
i  max x 2 
i = max/bardblx/bardbl2 = max. 
i=1 i=1 
On the other hand, taking x = en yields xT Ax =(en)T Ae n = max. Hence we conclude that 
max = max x T Ax. 
/bardblx/bardbl=1 
The Rayleigh quotien t is the application of the Couran t-Fischer Formula to the Laplacian of a graph. 
Corollary 2 (Rayleigh Quotien t) Let G =(V, E) be a graph and L be the Laplacian of G. We already 
know that the smal lest eigenvalue is 1 =0 with eigenve ctor v1 = 1. By the Cour ant-Fischer Formula, 
xT Ax /summationtext 
(i,j)E (xi  xj )2 
2 = min = min /summationtext 2 , 
=0 x=0 
xv1 x1 x/negationslashxT x /negationslashiV xi 
xT Ax /summationtext 
(i,j)E (xi  xj )2 
max = max = max /summationtext . 
=0 x=0 x/negationslashxT x /negationslashiV x2 
i 
We can interpret the formula for 2 as putting springs on each edge (with slightly weird boundary 
conditions corresp onding to normalization) and minimizing the potential energy of the conguration. 
Some big matrices are hard or annoying to diagonalize, so in some cases, we may not want to calculate 
the exact value of 2. However, we can still get an approximation by just constructing a vector x that has 
a small Rayleigh quotien t. Similarly , we can nd a lower bound on max by constructing a vector that has 
a large Rayleigh quotien t. We will look at two examples in which we bound 2. 
2.1 Example 1: The Path Graph 
Let Pn+1 be the path graph of n +1 vertices. Label the vertices as 0, 1,...,n from one end of the path to the 
other. Consider the vector x  Rn+1 given by xi =2i  n for vertices i =0, 1,...,n . Note that /summationtext
in 
=0 xi =0, 
so x  1. Calculating the Rayleigh quotien t for x gives us 
(i,j)E (xi  xj )2 4n 4n 1 /summationtext = /summationtextn = = O . x2 (2i  n)2 (n3) n2 
iV i i=0
Thus we can bound 2  O(1/n2). We knew this was true from the explicit formula of 2 in terms of sines 
and cosines from Lecture 2, but this is much cleaner and more general of a result. 
2.2 Example 2: A Complete Binary Tree 
Let G be a complete binary tree on n =2h  1 nodes. Dene the vector x  Rn to have the value 0 on the 
root node, 1 on all nodes in the left subtree of the root, and 1 on all nodes in the right subtree of the root. 
3-2</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Spectral sparsification (cont.), introduction to convex geometry</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Preconditioning on Laplacians, ultra-sparsifiers</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe23/</lecture_pdf_url>
      <lectureno>23</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/parenleftbig /parenrightbig Although we wont go into detail, it turns out that this exact algorithm actually runs in O(m4/3) time. 
The eigenvalues of the tree have a structure such that error is halved in O(m1/3) iterations, not just O(m1/2) 
iterations as Chebyshev polynomials show. 
Instead, well add a few more edges to make Vaidyas augmen ted spanning trees, which will impro ve 
the condition number substan tially. In the problem set, youll go into more detail into this, and into how to 
apply this recursiv ely to get nearly linear recovery time. 
4.1.3 Constructing ultra-sparsiers 
We will take a spanner T of G, and add a small number s more edges to get H. We partition T into t subtrees 
of balances path lengths. We then add one well-chosen bridge edge between every pair of subtrees. This 
can be done so that 
1/2 1/2(LH LGLH )  O(n/t). 
The ultra-sparsier H will have n  1+ s edges, for s  2 t in general or s  O(t) for planar graphs. 
With more cleverness, Spielman and Teng showed that s can be impro ved to O(t) for general graphs. 
5 Conclusion 
Its interesting that previously we used linear algebra to speed up graph theory , but now were using graph 
theory to speed up linear algebra. 
23-4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>/summationtext 
/summationdisplay /summationdisplay /summationdisplay 
/summationdisplay 4.1 Ultra-Sparsication 
What we cover now is a metho d to speed up conjugate gradien t by using an even sparser H. All the metho ds 
discussed henceforth are easily applicable to solving Mx = b for any M that is weakly diagonal ly dominant 
(not just graph Laplacians), i.e. for all i it holds that |Mi,i| |Mi,j |. The H we will precondition with j/negationslash=i 
now we call ultra-sp arsiers as they will only have (1 + o(1))n edges! You can think of H as essentially being 
a spanning tree of G with only a few extra edges. 
Theorem 1 Given a graph G with n vertic es and m edges, it is possible to obtain a graph H with n + 
t logO(1) n edges such that LH /precedesequalLG /precedesequal(n/t)LH , indep endent of m. 
We will not prove Theorem 1 here. In the problem set you will show a weaker version where the (n/t)is 
replaced with (n/t2). Getting to (n/t) requires similar ideas but gets slightly more complicated. 
The main benet to ultra-sparsication is that for many algorithms, the ultra-sparse graph acts like a 
graph with many fewer vertices. The ultra-sparse graph is a tree with relativ ely few additional edges linking 
nodes of the tree. For intuition on this, note that paths without branching can usually be condensed into a 
single edge. Furthermore, linear systems on trees can be solved in linear time. 
The result will be that we can solve diagonally dominan t linear systems in nearly linear time. This 
lecture will focus on Laplacians, but the problem set has a question on how to extend it to general diagonally 
dominan t systems. 
4.1.1 Embedding of graphs 
Recall from problem set 1 that: 
Lemma 2 Let Pu,v be a path from u to v of length k, and let Eu,v be the graph that just has one edge from 
u to v. Then 
Eu,v /precedesequalkPu,v. 
Now, suppose that we have two graphs G and H and an embedding of G onto H such that each edge in G 
maps to a path in H. For (i, j) G, dene stretch(i, j) to be the length of (i, j)s embedded path in H. 
Then 
G = Ei,j /precedesequal stretch(i, j)image(i, j) /precedesequal stretch(i, j)H. 
(i,j)E(G) (i,j)E(G) (i,j)E(G) 
If H is a subgraph of G, this means 
/summationdisplay 
H /precedesequalG /precedesequal stretch(i, j)H. 
(i,j)E(G) 
4.1.2 Spanning tree preconditioners 
For trees T , we can solve LT x = b in linear time. So it would be really nice if H were a low average-stretc h 
spanning tree. We could then precondition on H and take O(m) time per iteration. 
Turns out that that low average-stretc h spanning trees exist and can be found ecien tly: 
Theorem 3 Any graph G has a spanning tree T into which it can be emb edded such that 
stretch(i, j) m logc n. 
(i,j)E(G) 
This already is strong enough to give a non-trivial result. If we use such a spanning tree as a precondi
tioner, we take O(m) per iteration and take O(m1/2 iterations (because the condition number is O(m)), for 
O(m3/2) time. 
23-3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Decem ber 3, 2009 
Lecture 23 
Lectur er: Jonathan Kelner 
1 Outline 
Last lecture discussed the conjugate gradien t algorithm for solving linear systems Ax = b. This lecture will 
discuss preconditioning , a metho d for speeding up the conjugate gradien t algorithm for specic matrices. 
2 Last Lecture 
Last lecture we describ ed the conjugate gradien t algorithm for solving linear systems Ax = b for positive 
denite matrices A. The time bound for conjugate gradien t depends on tting low-degree polynomials to 
be small on the eigenvalues of A and large at 0. Using Chebyshev polynomials, this gives a running time of 
maxO(1/2), where  = is the condition numb er of A.min For certain matrices A, tighter bounds can be achieved. For example, if the eigenvalues of A tend to 
be clustered together, then polynomials can more easily be small at all the eigenvalues. But in the worst 
case and the general case, conjugate gradien t takes (1/2) time. For ill-conditione d (or badly-c onditione d, 
poorly-c onditione d, etc.) matrices, this can be quite slow. What do we do then? 
3 Preconditioning 
3.1 Motiv ating example 
Some matrices A with terrible condition number can still be solved fairly easily. For example, consider the 
matrix 
  
10000 777 123 
A =  0.1 1 0.2 . 
0.002 0.001 0.01 
This has condition number   1000000, and condition number 1012 once you compute AT A to get a 
positive denite matrix to perform conjugate gradien t on. But if you normalize the diagonal to get 
  10.0777 0.0123 
D1A =  0.1 1 0.2  , 
0.20.1 1 
you nd a well-conditioned matrix. So you can use conjugate gradien t to quickly solve D1Ax = D1b 
instead. When we do this, we call D a preconditioner for A. 
Theres no reason that preconditioners have to be diagonal; they just have to be easily invertible. The 
next section discusses the general problem of nding preconditioners. 
3.2 In general 
The problem is that we want to solve Ax = b but A is ill-conditioned, so conjugate gradien t doesnt work 
directly . However, we also know some other positive denite matrix M that approximates A and is easy to 
invert. Then we instead use conjugate gradien t on M1Ax = M 1b.If M approximates A, then M1A 
should have low condition number and conjugate gradien t will be fast. This idea has few complications: 
23-1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>How do we nd M ? Theres no general answer to this question, since its impossible for most A. 
However, most problems you want to solve have structure, which often allows you to nd a good M. 
The second part of this lecture discusses how to nd a good M when A is a Laplacian. 
	It could hard to compute M1A. If M and A are sparse, you dont want to compute the dense 
matrix M 1A. Fortunately , you dont need to. Conjugate gradien t only computes vector products, 
which you can compute in succession. 
	M 1A may not by symmetric or positiv e denite. You need it to be positive denite for 
conjugate gradien t to be proven correct. Fortunately , this can be worked around, as shown below: 
3.3 Dealing with M1A being asymmetric 
While M 1A may not be symmetric, both M and A are. So we can factor M = EET . Then E1AET has 
the same eigenvalues as M 1A, since if M1Av = v, then 
E1AET (ET v)= ET M 1Av = ET v. 
So rather than solving M1Ax = M 1b, we can solve E1AET x= E1b and return x = ET x. This can 
be done with conjugate gradien t, since it uses a positive denite matrix. 
Now, we might not know how to factor M = EET . Fortunately , if we look at how conjugate gra
dient works, it never actually requires this factorization. Every time E is used, it will come in the pair 
(aET )(E1b)= aM 1b. 
This completes our sketch of how preconditioning algorithms work, once you nd a preconditioner. We 
will spend the rest of lecture on nding preconditioners for Laplacians. 
4 Preconditioners on Laplacians 
Recall from previous lectures that any graph G can be sparsie d. This means that we can nd a graph H 
with O(n) edges such that 
(1  /epsilon1)Lh /precedesequal LG /precedesequal (1 + /epsilon1)Lh. 
Then Lh is a good preconditioner for LG. This is because all the eigenvalues of L1LG lie in [1  /epsilon1, 1+ /epsilon1],H 
so L1LG has constan t condition number. H 
We can use this to solve Laplacian linear systems for all graphs as if they are sparse and only multiply the 
number of iterations by log factors. Each step of conjugate gradien t requires solving a sparse linear system 
on H, and it only takes logarithmically many iterations to converge. 
But to do this, we need to nd H. Our previous metho d required a linear system solver to get H,sowe 
cant use it. There is a way to get a slightly weaker spectral sparsier in nearly linear time, though. We give 
a sketch of the algorithm, but dont go into much detail: 
We know that random sampling does a good job of sparsifying expanders. The problem with random 
sampling is when cuts have very few edges crossing them. So we rst break the graph into well-connected 
clusters with our fast local partitioning algorithm. Inside each cluster, we randomly sample. We then 
condense the clusters and recurse on the edges between clusters. 
So in nearly linear time, we can get a graph with O(n) edges and a 1 + /epsilon1 spectral approximation to G. 
But for use as a preconditioner, we dont need a 1 + /epsilon1 approximation. We can relax the approximation ratio 
in order to dramatically cut the number of edges in the sparsier. This will cause us to take more iterations 
of conjugate gradien t, but be able to perform each iteration quickly. We dub these incredibly sparse matrices 
ultra-sp arsiers . 
23-2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>(Lazy) random walks, their stationary distribution and l2-convergence, normalized Laplacian, conductance, Monte Carlo methods</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe4/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>3 Claim 2 W   = .
Proof Let k  V(G). We have
n  1  1 1[W  ]k = Wk,ii =   d(k)=   d(k)= (k). d(v) d(k) d(v)i=1 vV (G)(i,k)E(G) vV (G) 
This statemen t is equivalent to the matrix W having eigenvalue 1, with corresp onding eigenvector  
(note that, since  is a multiple of the vector of node degrees, D  1, we could also take the latter as the 
eigenvector). 
The natural next step at this point would be to claim that the random walk of a graph Galways converges 
to the stationary distribution . This however turns out to be false. It is easy to see that for a bipartite 
graph G. Consider for example the case G = C6, the cycle on 6 vertices, and let the vertex set of G be 
V(G)= {1,2,...,6}. Assume without loss of generalit y that the random walk starts at time t0 = 1 at vertex 
6. Then, at time t, the curren t vertex is odd if and only if t is odd. Therefore, the walk does not converge 
to any distribution. 
Lazy Random Walks 
There is an easy way to x the above periodicity problem. We introduce a modied version of the original 
walk, which we call lazy random walk. In a lazy random walk at time t: 
 we take a step of the original random walk with probabilit y 1/2, 
 we stay at the curren t vertex with probabilit y 1/2. 
We can show that the above modication breaks the periodicity of the random walk. The transition proba
bilities are encoded in the following matrix: 
W/prime =(W + I)/2=(I + A D1)/2, 
where I denotes the identity matrix. 
The fact that W and W/prime are not symmetric matrices makes their analysis complicated. We will thus 
dene new matrices. The normalize d walk matrix is dened as 
N = D1/2  W  D1/2 = D1/2  A D1/2 . 
The normalize d lazy walk matrix is dened as 
N/prime = D1/2  W/prime  D1/2 =(I + D1/2  A D1/2)/2. 
Claim 3 The matric es N and W have the same eigenvalues and relate d eigenve ctors. 
Proof Suppose that v is an eigenvector of N, with eigenvalue . Let q = D1/2  v. Then, 
N  v =  v = D1/2  W  D1/2  v = D1/2  W  q. 
Multiplying by D1/2 on the left we obtain 
W  q =  D1/2  v =  q. 
Therefore, q is an eigenvector of W with eigenvalue . 
Observ e that, by Claim 2, W has eigenvector D  1, with eigenvalue 1. Therefore, by Claim 3, the 
normalized walk matrix N has eigenvector D1/2  1, with eigenvalue 1. 
4-2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Assume that the square corresp onds to [1, 1] [1, 1]. If you pick a point in the square uniformly at 
random, the probabilit y that you pick one inside the circle is equal to /4. Suppose that you pick n points 
in [1, 1] [1, 1], uniformly at random. Then, 
E[number of points inside circle] = n /4 
So, you can return the estimate 
= (number of points inside circle) 4/n. 
A natural question is how close this estimate would be to the right answer. 
In order to answer the above question, we will introduce the Cherno bound. Suppose we have a random 
variable r {0, 1}, such that Pr[r =1]= p, and Pr[r =0]=1 p. Assume that we draw n independen t 
samples r1,...,r n, and let R = i ri. By the linearit y of expectation, we have 
E[R]= E[ ri]= E[ri]= n p 
i i 
We will say that R/epsilon1 -appr oximates E[R]if 
(1 /epsilon1)E[R] R (1 + /epsilon1)E[R] 
This is a multiplicativ e error measure. 
Theorem 8 (One version of the Cherno bound) The probability that R fails to /epsilon1-appr oximate E[R] 
is 
Pr [|R E[R]|/epsilon1E[R]] 2e np/epsilon12/12 =2e E[R]/epsilon12/12 . 
Some notes on the above bound: 
	The bound is near tight. 
	It is necessary for the trials to be independen t, in order for the bound to hold. 
	It provides a multiplicativ e, but not an additiv e error guaran tee. 
	For xed /epsilon1, it falls o exponentially in n. So, if we have failure probabilit y 1/2, we can impro ve it to 
1/2k by performing m = n k trails. 
	Therefore, smaller n requires more trials. 
	If we want /epsilon1-appro ximation with probabilit y 1 , then we need 
log(1/)N  . p/epsilon12 
That is, we need enough trials to get (log(1/)//epsilon12) successes. 
4-5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>4 Connections to Laplacians 
Weve used the Laplacian L. The normalized Laplacian Lis dened as 
L= D1/2 LD1/2 . 
Claim 4 N = I L . 
Therefore, the eigenvalues of N are given by 1 (eigen values of L). So, it makes sense to order them in the 
opposite way 
1= 1 2 ... n 
We can now translate our theorems about the eigenvalues of Laplacians to theorems about is. We have 
 For each i, i [1,1]. 
 If G is connected, then 2 &lt;1. 
 The 1 eigenvalues occur only for bipartite graphs. 
Let /prime 
i be the eigenvalues of N/prime . Then 
 For each i, /prime 
i [0,1]. 
 If G is connected, then /prime 
2 &lt;1. 
5 2 Con vergence 
Dene the spectral gap to be 
:= 1 /prime 
2. 
For probabilit y distributions p,q, we dene their /lscript2 distance to be 
/bardblpq/bardbl2 = (p(i) q(i))2 . 
i 
The following theorem gives a bound on the rate of convergence of the lazy random walk to the stationary 
distribution . 
Theorem 5 Let p0 be an arbitr ary initial distribution, and pt be the distribution after t steps of the lazy 
random walk. Then, 
max x d(x)/bardblpt /bardbl2 (1 )t  .miny d(y) 
Proof [Proof for regular graphs] Observ e that for a matrix M = Q1  Q, we have Mk = Q1 k Q. 
Thus, for an eigenvector v of M, Mk v = k v. 
Recall that N/prime =(I + D1/2 AD1/2)/2. Since G is regular, D = dI, for some integer d&gt;0. Thus, 
1 N/prime = I + A d 
and the stationary distribution is simply the uniform distribution on V(G) 
1  = 1. n 
4-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Let ci = viT p0, where vi denotes the eigenvector corresp onding to the i-th eigenvalue. We have 
n n 
N/primek p0 = ci k
i vi = c1 v1 + ci k
i vi 
i=1 i=2 
Since c1 = v1 T p0 =1/n, it follows that 
n  n n 
/bardblpk /bardbl2 = /bardbl ci k
i vi/bardbl2 =  ci 2 2
ik k 
2  ci 2 
i=2 i=2 i=2 
n 
 k 
2 (viT p0)2 k 
2 =(1 )k . 
i=1 
Using a similar argumen t, we can also show an analogous bound for /lscript convergence. 
Theorem 6 For any vertex v V(G), 
|pt(v) (v)|(1 )t  d(v) 
miny d(y) 
6 Conductance 
Cheegers inequalit y carries over too, by replacing the isoperimetric number by a new parameter, which we 
call conductanc e . 
Denition 7 (Conductance) For S V(G), let 
e(S)(S)=    . min vS d(v), vSd(v) 
We dene the conductanc e to be 
(G) = min (S). 
SV 
Using the above denition, Cheegers inequalit y now becomes: 
(1) 2(G) 1 /prime 
2 (1) (G). 
The parameter (G) is related to the rate of convergence to the stationary distribution. In particular, 
bounds on (G) let us prove that a walk mixes quickly. 
The intuitive interpretation of the connection between conductance and the rate of convergence is as 
follows. If a graph has high conductance, it is well-connected. Therefore, a large amoun t of probabilit y mass 
can very quickly move from one part of the graph to another. 
7 Introduction to Mon te Carlo metho ds 
Assume that we want to estimate  =3.1415 ... by throwing darts in the following dartboard: 
4-4</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Back to the dartboard example, if we want to estimate  within, say, 5%, with probabilit y at least 0.99, 
then we have /epsilon1 =0.05,  =1/100. Therefore, we need 
log(100) N   (/4)(0.05)2 
Observ e that it is easy to make  smaller, but it is harder to make /epsilon1 smaller. 
If we are bad darts, then we run into trouble. This happens if we have a big dartboard, and a small 
circle. 
In particular, if p is exponentially small, then we need exponentially many trials to expect a constan t number 
of successes. 
We can also run into trouble if it is hard to throw darts at all. That is, if it is hard to draw samples 
uniformly at random from the ambient space. We will develop some techniques for xing the above problems 
in certain scenarios. 
4-6</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Random sampling from a convex body (cont.), grid walk, introduction to concentration of measure</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>7 The Rest of the Details 
1. We need to make sure we can arrange for our convex body K to be polynomially well-rounded to make 
sure the diameter isnt too big. The rough idea is that if K is far from isotopic (i.e. not well-rounded) 
we can nd a point far from the origin using the ellipsoid algorithm and use this to construct a better 
John Ellipse; see the problem set for details. 
2. We need to show that isoperimetry of our graph is properly related to isoperimetry of the body near 
the boundary . This is where we use rounding of the corners of K. 
3. Finally , we need to show that we dont reject too many samples. 
4. Once weve done all of the above, we get an algorithm for sampling from any convex body K, and can 
use this to estimate the volume as per our sketch at the beginning of these notes. 
8 Concen tration of Measure and Geometric Probabilit y Theory 
8.1 The Cherno (Hoeding-Azuma-Bernstein-...) Bound 
The question here is how to think of a convex body relating in some way to probabilit y theory . Weve been 
doing a lot of things with convex bodies and probabiliies and there should be a lot of overlap. We sort of 
did this last time with isoperimetry but now we will be much more concrete. Well think of points in the 
convex body as being points of a probabilit y distrib ution. Well have interesting, very stron g theorems that 
go both ways in implications, that will appear unlikely. The main point is that we keep coming up with 
the phenomenon that volume in convex bodies is counterintuitiv ely distributed, more or less pervasively for 
high-dimensional spaces. Weve seen this over and over but well make it more concrete now. We already 
have the rst concen tration of measure theorem from earlier in the semeste r: the Cherno bound. 
Theorem 1 Let x  {1}n be indep endent random variables with p[xi = 1] = .5, and a1,...,an satisfying 
ai 2 =1 (some can be negative). Then 
Pr n
aixi 
i=1 &gt;i 2 2et/2 
I assert this is the same bound we already did. Now, lets change it so that the xi are anywhere in 
[1/2, 1/2]. The bound is still true up to some constan ts. Lets see what this means geometrically . 
Claim 2 aixi = ax = distanc e of x from hyperplane Ha = {x|ax =0}.   
Pictorially , that means I can take the unit cube in Rn, pick any hyperplane at all, and we cut the cube 
with it, and that gives us some intersection with the interior of the cube. What it says is that no matter 
how we choose this hyperplane, almost all of the cube is prett y close to this hyperplane. I claim this is our 
rst concen tration of measure theorem. 
Another way we can phrase this, so you get a hint of where Im going with this, is to say that 
Vol(S) 
Vol([1/2, 1/2]n)  1  2e6t2 
Here, we dene S to be the set of all points within distance t of Ha. 
So what is a little neigh borhood of S? Ive shown that its prett y big as a function of the volumes 
involved. This is not exactly the isoperimetric inequalit y because were looking at big sets, not small sets, 
and the set, not its complemen t. Its a dieren t parameter regime but the same kind of question. Someho w 
we have three phenomena in this course that all come out to be the same thing: isoperimetric inequalities, 
Cherno bounds, and this phenomenon of volume in convex bodies. 
15-5</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>5 Run a random walk on K, and at any cube generate a random point p + k. If this point is not in K, 
then start the random walk over. Provided that there are not too many cubes near the boundary or entirely 
in K, in expectation we will not need to run the random walk many times before obtaining a random point 
in K. Then this denes a random walk on the graph G = H  K, and again self-lo ops can be added to 
each vertex to ensur e that G is regular and that the random walk is aperiodic. 
Consider an arbitrary cube C that contains points in K, and points not in K. A point in K  C is 
generated with probab ility equal to the fraction of volume in C also in K, once the node corresp onding to 
the cube C is reached. Because all points in K can be generated, then this random walk generates points in 
K uniformly at random. 
Mixing 
The above random walk will generate a random sample from any body K. Convexity is not needed to ensure 
that this random walk produces a random sample, but is needed to ensure that the random walk mixes 
quickly. Consider the body K given in the gure below. Intuitiv ely, this walk mixes slowly for the same 
reasons that a random walk on a graph containing two cliques connected by a long path does. 
To bound the mixing time for a random walk on a convex body K, we need to bound the isoperimetric 
number or conductance of G. Then for any set S of nodes in G such that |S| |V (G)| , the isoperimetric 2 
number for S is |E
|(
SS
| )| . Each cube contains the same volume, and the size of S is proportional to the volume 
of Q(S)-V oln(Q(S )), the space enclosed by the cubes corresp onding to nodes in S. Similarly , each edge 
leaving S corresp onds to a face on the surface of Q(S), and each face has the same surface area. Then 
the number of edges leaving S is proportional to V oln1(dQ(S )). If the space Q(S) does not intersect the 
boundary of K, then this is exactly the isoperimeteric number of the graph. To incorp orate the boundary , 
we need a Relativ e Isoperimetric Inequalit y. 
K
Q(S)
Theorem Let K  Rn be a convex body with diameter d. Let S be an n  1 dimensional surface that 
cuts K into two pieces A and B. Then 
min{V oln(A),V oln(B)} dV oln1(S) 
15-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>6 Again, if A does not intersect the boundary (and is round enough ) then this is appro ximately the standard 
isoperimeteric inequalit y. Also, we can dene the isoperimeteric constan t (or Cheeger constan t) for any body 
X (not necess arily convex) as the minim um  such that 
min{V oln(A),V oln(B)} V oln1(S) 
Isoperimetric inequalities arise naturally in bounding the mixing time of any diusion process. 
Appro ximate Proof 
The n  1-dimensional volume is more subtle to work with, and this theorem can be proven by proving a 
related theorem. 
Theorem Let K  Rn be a convex body with diameter d. Decomp ose K into A  B  S, where 
dist(A, B)  t. Then 
d min{V oln(A),V oln(B)} t V oln(S) 
The original theorem is proven by decreasing t to zero. Let E be the smalles t volume ellipse containing 
K. Then there are two cases to consider. 
Case 1: The ellipse E is needle-lik e, which we den e to mean all but at most 1 axis of E is of radius  t 
for a small enough . In this case, the theorem is true by inspection. 
Case 2: The ellipse E is not needle-lik e. Then we can apply a symmetrization procedure until the ellipse 
is needle-lik e. Supp ose that there exists a counter-example to our theorem, then by the Ham Sandwhic h 
Theorem there exists a hyperplan e that simultaneously cuts A into A1,A2 and B into B1,B2 such that A1 
and A2 have equal volume, and so do B1 and B2. Then the hyperplane cuts S into S1,S2 and one of the 
convex bodies A1  B1  S1 OR A2  B2  S2 is a counter-example that is closer to needle-lik e. 
dA BS
Iterating this procedure, we can eventually reduce all but at most one dimension to  t, and this 
produces a contradiction because the theorem is true when the bounding ellipse is needle-lik e. 
15-4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit November 3rd, 2009 
Lecture 15 
Lecturer: Jonathan Kelner Scrib e: Justin Thaler 
1 Outline 
In this lecture we will design a metho d to randomly sample from a convex body, and this method will be a 
subroutine in appro ximately computing volume 
2 Reminder from Last Time 
In the last lecture, we showed that no deterministic algorithm that queries a membership oracle only a 
polynomial number of times, can appro ximate the volume of a convex body to within a factor of m where2n 
m is the number of membership queries. However, a randomized algorithm can approximately compute 
the volume of a convex body and the procedure will be similar to the Jerrum and Sinclair metho d for 
appro ximating the permanen t. The algorithm will construct a series of convex bodies, and the ratio of 
successiv e convex bodies in the series can be well-appro ximated. These appro ximations will be used to 
appro ximate the volume of the origin al convex body K, even if K has exponentially small volume compared 
to the bounding sphere that is guaranteed to contain K. 
Last time, we presen ted the following sketch of how our randomized algorithm will work. 
1. Change coordinates s.t. K is well-round ed, i.e. B  K  nB 
2. Let  =1+1/n, and let Ki = K  iB. Compute i = V (Ki1) 
V (Ki) 
3. Return V (B) 1 
i 
Note that Step 3 works because K0 = K  B = B. Also note that the rst part isnt too hard using the 
ellipsoid algorithm, which we mention briey near the end of these notes. 
3 Grid Walk 
To approximately compute volume in Step 2 of our sketch above, we need a metho d to sample randomly 
from a convex body K. To do this, we will use a random walk and we will need to bound the mixing time of 
the random walk. There are a number of random walks that can be used to sample randomly from a convex 
body, and the most basic is the Grid Walk. 
Dene a grid graph H, such that nodes are points in Zn . The dene the graph G = H  K as the 
subgraph of nodes in H that are also contained in the convex body K. For sucien tly small , a random 
vertex in G is roughly a random point in K 
The graph G can contain an exponential number of vertices. Consider the convex body K, the hyper-cub e 
[1, 1]n . We must show that even if the number of vertices is exponential, that the random walk mixes in 
polynomial time. Returning to the example of the hyper-cub e, the hyper-cub e is the graph product of n line 
graphs. Each line graph has a mixing time c2 if there are c nodes. Then a random walk on the grid graph 
is a choice of the direction to walk in, and a random step on the corresp onding line graph. The walk mixes 
in O(nc2logn) steps, because in expectation O(nlogn) steps are needed to ensure that a step is taken in all 
directions (this is an instance of the coup on-collector problem), and when O(c2) steps have been taken in 
each direction, a random walk on the hyper-cub e is mixed. 
15-1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>4 Problems with our Approac h 
We only sample grid points. A rst-cut x is to generate a random vertex p in G, and then add a random 
,  
2] to the point p. However p + k is not necessarily in K. We could generate 2vector k in the cube [
another k in the cube, and try again -but if the nodes in G are generated uniformly at random, then a cube 
containing points in K and points not in K will generate the points in K disprop ortionately often compared 
to points in K that are in a cube that only contains points in K. This problem can be avoided by restarting 
the random sample procedure when a point p + k is generated that is not in K. A more serious problem is 
that not all points in K can be generated. 
Also, the graph G is bipartite because the full grid H is bipartite, and G is a subgraph of H. Then a 
random walk will be periodic. Also, not all nodes in G have the same degree and the limiting distribution is 
not necessarily uniform on the nodes in G. These problems can be xed by adding self-lo ops at each node, 
and adding extra self-lo ops at any node that is not connected to 2n nodes in G. The degrees in the graph 
can be made equal, and the limiting distribution will be uniform on the nodes. 
The graph G need not even be connected. 
Intuitiv ely, this problem arises when K contains sharp bound aries and these problems can be remo ved by 
rounding out K. Apossible approac histoset ()= KKB
n 
2 can both be made to work, but consid er K. By assumption Bn 
2
 K and B+ K  K.
equal to the diameter of a cube. Then all cubes contained in K have a neigh bor in all 2n directions that is
n 
2, or to set K = (1+ )K. The approac hes 
Choose  = n 
contained in K, and for all points p in K there is a cube in K that contains p. 
15-2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Convergence analysis of steepest descent and conjugate gradients</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe22/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>/bracketleftBigg /bracketrightBigg /summationdisplay 
/bracketleftBigg /bracketrightBigg /summationdisplay 
/parenleftbig where the last step follows from induction. Since xi = xi1 + ri = /summationtext
ki 
=0 rk, we can now write 
k1 
xk = (1  A)i b 
i=0 
/summationtextk1But the sum i=0 (1  A)i is just the rst k terms of the taylor series 1/y around 1  that is, xk estimates 
1 b using Taylor series! i For  /negationslash= 1 the computation is similar, 
k1 
xk = (1  A)i b 
i=0 
and we get another Taylor series approximation: 1/y around 1/. 
So how well can we choose ? We want the residuals to go to zero quickly. If we had 1  1 matrices, 
we could just set  =  1 and get residual 0 in one step, but in general we need to choose  which works for 
dieren t eigenvalues simultaneously . 
Taylor series only converge well very near where you expand it; this gives some intuition for why the 
condition number should be related to the distance between max and min. If these eigenvalues are far 
apart, then there is no  that works for all the eigenvalues. 
We can bound the L2 norm of the residual by bounding bi by ||b||2 and taking the max of the multipliers 
||rk||2  max i|1  i|k||b||2 
So, we want to minimize 
max i|1  i| 
Since the maxim um will occur at either the largest or the smallest eigenvalue, the best we can do is to 
balance them and have (1  min)= (1  min). This gives that the best  is the recipro cal of the 
midrange of the eigenvalues: 
min+max /parenrightbig1  = 2 
The resulting max i|1  i| is 1  2 where  = max/min, which we call the condition numb er of A.+1 
Note that  is a ratio of eigenvalues, so its unchanged by scaling the matrix. 
From the bound for the L2 norm, we can derive that the number of iterations grows linearly in .Now 
can we do better? 
3 Conjugate Directions 
Curren tly we are going to the minimal of f value along our search direction. As we saw in previous example, 
this can us to take a long zigzag path. What we would really like to do is go the length of the projection 
of x onto our search direction. If we could do that, then after i steps the error would be orthogonal to all 
previous search directions, and wed be done with an n  n matrix after n iterations. 
Suppose we have orthogonal directions d0,...,d n1  the standard basis will do.
We have xi+1 = x + iidi. We want ei+1  di.
dT
i ei+1 = dT
i (ei + idi)=0 
which implies 
dT
i eii =  dT dii 
The good news is we can compute everything except the ei. The bad news is computing ei is equivalent 
to nding x. Fortunately , a mild modication will make the calculation possible. 
22-2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>/parenleftbigg /parenrightbigg/parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg 18.409 An Algorithmists Toolkit Decem ber 1, 2009 
Lecture 22 
Lectur er: Jonathan Kelner 
1 Last time 
Last time, we reduced solving sparse systems of linear equations Ax = b where A is symmetric and positive 
denite to minimizing the quadratic form f(x)= 1 xT Ax  bx + c.2 
The idea of steepest descen t is to pick a point, nd the direction of steepest decrease, step along that 
direction, and iterate until we get close to a zero. The appropriate directions turn out to be the residuals. 
We pick the step length to take us to the minimal f value along the line. 
Each iteration requires only two matrix-v ector multiplications. We can push this down to one by calcu
lating the residuals as 
ri+1 = b  Axi+1 
= b  A(xi + iri) 
=(b  Axi)  iAri 
= ri  iAri 
This can allow oating point error to accum ulate, though that can be xed by occasionally calculating 
the residual using the original formula. 
Today well talk about how to bound the error, and later how to get better performance. 
2 Con vergence Analysis of Steep est Descen t 
We study a very similar metho d that doesnt do the line search  it uses the same step length  at every 
iteration. Asymptotically , the best possible  and usual steepest descen t have the same behavior. 
Steepest descen t doesnt depend on the basis at all, so we might as well pick the eigenbasis for analysis. 
The size of the error at each step certainly wont change if we switch bases, and it will be easier to see whats 
going on. For further cleanliness everything will be stated for 2x2 matrices  everything generalizes. 
If we were trying to solve 
1 0 x1 = b1 
0 2 x2 b2 
the exact solution would be xi = 1 
i b. Keep in mind that were working in the eigenbasis to do the analysis, 
but the actual algorithm does not get to work in the basis, since nding the eigenbasis is as hard as inverting 
the matrx. 
First, lets see what happens if we use  = 1. Obviously this is not general, for example if the s are 
greater than 2, but the algebra is enligh tening. 
Lets start with x0 = 0, so the rst residual r0 = b, and for i&gt; 0 the residual will be 
ri = ri1  Ari1 
=(1  A)ri1 
=(1  A)ib 
22-1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>So far weve been talking about orthogonalit y relativ e to the standard inner product. Theres no real 
reason to do this, and in fact it will be more convenient to work with the inner product ||x||2 = xT Ax,A 
instead of xT Ix as we have been. Geometrically , this unwarps the isolines of the quadratic form into perfect 
circles. 
We can think of this as a change of basis: x/prime = A1/2x, though not for computation  pretty much the 
only way to get the square root of A would be to retriev e the eigenvalues, which would defeat the purpose. 
Suppose we have A-orthogonal search directions (di)  now the unit basis wont do, but suppose for the 
momen t we have magically acquired search directions. 
Again, xi+1 = xi + idi. We want ei+1 A di. 
dT
i Aei+1 = dT
i A(ei + di)=0 
which implies 
dT
i Aeii =  dT
i Adi 
But Aei is just ri, which we do know how to compute. Yay. 
4 Conjugate Gram-Sc hmidt 
Conjugate directions is insucien t for our purposes because we might not have time to do n iterations. Well 
settle for a crude answer, but we need it very fast. 
Also, as mentioned before, we dont have search directions. You may recall the Gram-Sc hmidt process 
for orthogonalizing a set of vectors from a previous class. Does it work for A-orthogonalit y? Certainly; see 
page 5 of slides on Conjugate Gram-Sc hmidt. 
The problem is that Conjugate Gram-Sc hmidt is still too slow. The crucial change we made to the 
algorithm is requiring each direction to be orthogonal to all previous search directions. While this gave us 
good convergence, it means we have to subtract o the projection into each of the previous directions, which 
means that we have to remem ber what the previous directions were. This incurs both time and space cost. 
We need a more sophisticated way to nd the directions. 
5 Conjugate Gradien ts 
The trick is to choose the linearly independen t vectors we feed to Gram-Sc hmidt very carefully . We will 
generate these vectors on the go. Dene Di = span(d0,...,d i1). 
The property that we leverage is that after i steps, Conjugate Directions nds a point in x0 + Di in 
fact, the one that minimizes the size of the error ||ei||A =(eiT Aei)1/2 . 
Let the input to Gram-Sc hmidt be (ui), and dene Ui analogously to Di. By construction, xi  x0 +Di = 
x0 + Ui, and ei will be A-orthogonal to Di = Ui. 
We choose the magic inputs ui = ri. Since ri+1 = Aei+1, by denition ri+1 is plain old orthogonal to 
Di+1 (and Di,Di1,... ). Also, ri+1 = ri  iAdi,so Di+1 = Di  ADi. Putting the two together, ri+1 is 
A-orthogonal to Di. 
Thus, ri+1 only A-projects onto the di component of Di+1. Theres only one thing to subtract o, so 
only one or two A-dot products are needed per iteration again, as in steepest descen t. We no longer need to 
remem ber all the previous search directions, just the very last one, so weve xed the space complexit y as 
well. 
The algorithm is given on a slide on page 6. 
22-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/summationdisplay 6 Con vergence Analysis of Conjugate Gradien ts 
After i iterations, the error is   
i 
 ei = I + j Aj e0 
j=1 
where the s are some mess of s and s. Thus we can think of conjugate gradien ts at the ith step as 
nding these best possible coecien ts for an ith degree polynomial Pi() to make the A-norm of the error 
small. 
||ei||2 
A  min max [Pi()]2||e0||2 
APi (A) 
Any sequence of i-degree polynomials which are 1 at 0 will give bounds on the error; we want ones which are 
small for every eigenvalue   (A). This should remind you of the analysis of steepest descen t, but Taylor 
Series are not the right choice here  theyre designed to work around a point, while we want polynomials 
which will work at every eigenvalue. We can modify the magic polynomials from lecture 6 to work here. 
Recall that Chebyshev polynomials have the property of being 1 at 1, and small for some [0,l] where l&lt; 1 
is a parameter. We want polynomials which are 1 at 0 and small in [min,max]. This allows us to bound 
the error (measured in the A-inner product) at the ith iteration as 
/parenleftbigg /parenrightbiggi2 ||ei||A  21  ||e0||A k+1 
so the number iterations grows with the square root of , which is way better than the linear performance 
of steepest descen t. 
Note that the algorithm isnt actually computing any Chebyshev polynomials  it uses the best poly
nomial, which is at least as good. Also, notice that if we knew the range of the eigenvalues to begin with, 
we could skip to designing a polynomial to estimate A1 . Conjugate gradien ts magically nds the best 
polynomial without explicitly knowing these values. 
22-4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Lattices, fundamental parallelepiped and dual of a lattice, shortest vectors, Blichfield&#8217;s theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>2 1.1 Dual lattices 
Denition. The dual  of lattice  is {x  Rn : v  ,x v  Z}. 
Equiv alently, the dual can be viewed as the set of linear functionals from  to Z. 
Figure 3: Dual lattice 
Denition. For matrix B, its the dual basis B is the unique basis that satises 
1. span(B )= span(B) 
2. BT B = I 
Fact. (L(B)) = L(B). 
Fact. () =. 
Fact. det()= det1
() . 
Shortest vectors and successiv e minima 
One basic parameter of a lattice is the length of the shortest nonzero vector in the lattice, denoted 1. How 
about the second shortest? We are not interested in the second/third/etc shortest vectors which happ en to 
be simply scaler multiples of the shortest vector. Instead, one requires that the next minim um increases 
the dimension of the space spanned: 
Denition. The ith successive minimum of lattice , i(), is dened to be inf{r| dim(span(  B(0,r)) 
i}. 
Figure 4: 1() = 1, 2=2.3 
The following theorem, due to Blicheld , has various importan t consequences, and in particular can be 
used to bound 1. 
Theorem. (Blicheld) For any full-rank lattic e  and (measurable) set S  Rn with vol(S) &gt; det(), there 
exist distinct z1,z2  S such that z1  z2  . 
18-3 Image courtesy of Oded Regev. Used with permission.
Image courtesy of Oded Regev. Used with permission.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 18.409 An Algorithmists Toolkit 2009-11-12 
Lecture 18 
Lecturer: Jonathan Kelner Scrib e: Colin Jia Zheng 
Lattice 
Denition. (Lattice) Given n linearly indep enden t vectors b1, ,bn  Rm, the lattic e generated by them  
is dened as L(b1,b2,  bn)= { xibi|xi  Z}. We refer to b1,  ,bn as a basis of the lattice. Equiv alently, 
if we dene B as the m  n matrix whose columns are b1, ,bn, then the lattice generated by B is  
L(B)= L(b1,b2,  ,bn)= {Bx|x  Zn}. We say that the rank of the lattice is n and its dimension is m. 
If n = m, the lattice is called a full-rank lattice. 
It is easy to see that, L is a lattice if and only if L is a discrete subgroup of (Rn , +). 
Remark. We will mostly consider full-rank lattices, as the more general case is not substan tially dieren t. 
Example. The lattice generated by (1, 0)T and (0, 1)T is Z2, the lattice of all integers points (see Figure 1(a)). 
This basis is not unique: for example, (1, 1)T and (2, 1)T also generate Z2 (see Figure 1 (b)). Yet another 
basis of Z2 is given by (2005, 1)T ; (2006, 1)T . On the other hand, (1, 1)T , (2, 0)T is not a basis of Z2: 
instead, it generates the lattice of all integer points whose coordinates sum to an even number (see Figure 
1(c)). All the examples so far were of full-rank lattices. An example of a lattice that is not full is L((2, 1)T ) 
(see Figure 1(d)). It is of dimension 2 and of rank 1. Finally , the lattice Z = L((1)) is a one-dimensional 
full-rank lattice. 
Figure 1: Lattices of R2 
Denition.  For matrix  B, P (B)= {Bx|x  [0, 1)n} is the fundamental  parallelepip ed of B. 
Examples  of fundamen tal parallelepip eds are the gray areas  in Figure  1. For a full rank lattice  L(B), 
P (B) tiles Rn in the pattern  L(B), in the sense  that Rn = {P (B)+ x : x  L(B)}; see Figure  2. 
18-1 Image courtesy of Oded Regev. Used with permission.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Figure 2: P (B) tiles Rn 
In Figure  1, we saw that not every set of n linearly  indep enden t vectors  B in a rank  n full-rank  lattice  
 is a basis  of . The fundamen tal parallelepip ed characterizes  exactly  when  B is a basis:  
Lemma.  Let  be a rank n full-rank lattic e and B an invertible  n  n matrix.  Then  B is a basis (of ) if 
and only if P (B)  = {0}. 
Proof.   is obvious:   only contains  elemen ts with  integer  coordinates  under  B, and 0 is the only 
elemen t of P (B) with  integer  coordinates.  
For   , need  to show that any lattice point x = By satises  yi  Z. Note  that By with  yi= yi yi
is a lattice  point in P (B). By our assumption  By = 0, ie yi  Z. 
It is natural  to ask when  are two invertible  matices  A, B equivalent  bases,  ie bases  of the same  lattice.  It 
turns  out that this happ ens if and only if A, B are related  by a unimo dular  matrix.  
Denition.  A square  matrix  U is unimo dular  if all entries  are integer  and det(U)= 1. 
Lemma.  U is unimo dular i U1 is unimo dular. 
Proof. Supp ose U is unimo dular . Clearly  U1 has 1 determinan t. To see that U1 has integer  entries,  
note  that they  are simply  signed  minors  of U divided  by det(U). 
Lemma.  Nonsingular  matric es B1,B2 are equivalent  bases if and only if B2 = B1U for some  unimo dular  
matrix  U. 
Proof.  : Since  each column  of B1 has integer  coordinates  under  B2, B1 = B2U for some  integer  matrix  
U. Similarly  B2 = B1V for some  integer  matrix  V . Hence  B1 = B1VU, ie VU = I. Since  V, U are both 
integer  matrices,  this means  that det(U)= 1, as required.  
 : Note  that each column  of B2 is contained  in L(B1) and vice versa.  
Corollary . Nonsingular  matric es B1,B2 are equivalent  if and only if one can be obtaine d from the other  by 
the following  operations  on columns:  
1. bi  bi + kbj for some  k  Z 
2. bi  bj 
3. bi bi 
Now that it is clear  that bases  of a lattice  have the same  absolute  determinan t, we can proceed to dene  the 
determinan t of lattice:  
Denition.  (Determinan t of lattice)  Let L = L(B) be a lattice  of rank n. We dene  the determinant  of L, 
denoted  det(L), as the n-dimensional  volume  of P (B), ie det(L)= det(BT B). In particular  if L is a full 
rank  lattice,  det(L)= |det(B)|. 
18-2 Image courtesy of Oded Regev. Used with permission.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Approximating the volume of a convex body</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>3. Return Vol(B)  i 
The rst step can be done with the separating oracle and the ellipsoid algorithm, or the metho d on the 
problem set. The last step works since K0 = K B = B and KN = K nB = K. For the second step, we 
need to sample. Its easy to sample from highly symmetric objects: the cube is given by n uniforms, U[0, 1], 
the sphere by n gaussians, appropriately rescaled, the ball by picking the direction, then the radius. For 
nonsymmetric bodies, the best bet is a random walk. There are a few ways walk: 
1.	Grid Walk
Intersect a grid with the body; walk on the resulting graph.
2.	Ball Walk
At a point p, pick a random neighbor in a small ball centered at p, and walk there.
3.	Hit and Run
At a point p, draw a random line l through p and walk to a random point l K.
Well use the grid walk. Drop a width  grid on R, the graph H with vertices Zn with edges p p ei, 
and set G = H K. We can walk on G using a membership oracle; walk on H, and if you would go to a 
neighbor not in G, choose again. Note that H has degree 2n, but exponentially many vertices, so we need 
to show that the walk mixes very quickly. This is plausible though, and is easily seen when G is just a 
cube with side length n/. Since the path Pn/ mixes in time polynomial in n/, and the cube is just the 
product P n, its mixing time is n times that of the path, so still polynomial. There are still many problems 
with using a the walk on G to approximate K. 
1. Were only sampling lattice points. After walk mixes, we could take a random vector v from the cube 
of width  centered at p G. But if p + v /K, were in trouble. We could throw it out and re-sample, 
but this would overweight points near the boundary . Alternativ ely, we could start the whole walk over, 
which is acceptable as long as the probabilit y of landing outside is small. 
2. The graph might be (close to) bipartite. Just use the lazy walk. 
3. The graph has nonconstan t degree. Throw in self-lo ops for vertices near boundary .	Equiv alently, our 
walk is: pick a random vector v ei;if p + v K, go there, otherwise, stay put. 
4. The graph G might not be connected! If K has a sharp angle, then the vertex of G closest to the 
corner will not be adjacen t to any other vertices of G. Finer grids dont help, as this is a problem with 
the angle itself. 
Well x the last problem next lecture, by walking on the graph G associated to K/prime =(1+ /epsilon1)K. 
14-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit 10/29/09 
Lecture 14 
Lectur er: Jonathan Kelner 
1 Appro ximating the volume of a con vex body 
Exactly computing the volume a convex body is known to be #P-hard, so the fact that we can approximate 
its volume in P is surprisingthe kind of result you would bet against until you saw it was true. 
Before discussing any algorithms, though, we need to say what it means to be given a convex body K. 
To keep our implemen tation as general as possible, well assume that K is given by some oracle: 
1. Membership oracle
Given a point p, returns yes if p  K and no if p /K.
2. Separation oracle
Given a point p, returns yes if p  K and a separating hyperplane H if p /negationslashK.
Given a reasonable description of a convex body, it is easy to build a separation oracle. For example, 
1. A ball of radius r 
Given p, compute its norm |p|. If less than r, return yes; if greater than r, return the hyperplane 
tangen t to the boundary sphere at rp/|p|. 
2. A cube of side length s 
Given p, compute its l norm. If less than s/2, return yes; if greater, return the face of the cube in 
the violated direction. 
3. A polytop e 
Given p, check each inequalit y. If it satises them all, return yes; if not, return the failed inequalit y. 
In what follows, well assume that our convex body contains a ball of radius 1 centered at the origin, and 
is contained within a ball of radius 2poly(n). These conditions are reasonableafter suitable translation and 
dilation, they hold for any K specied by inequalities of polynomial bit length. 
Given a membership oracle, how could we approximate volume? The naive Monte Carlo algorithm 
pick points from some designated region (say a ball) and check if theyre in Kin general fails. If K is an 
ellipse with major axis of exponential length l and minor axis l1, then the probabilit y of a successful trial 
is exponentially small. No chance of a polynomial time algorithm. But if the body is well-rounded, say 
B2 n  K  nB2 n, then the following algorithm has a chance: 
1. Pick points p1, ,pm 
2. Check if pi  K 
3. Set K/prime := conv{pi|pi  K} 
4. Return the volume of K/prime 
If n = 2, this algorithm works: 
Theorem 1 For any /epsilon1&gt; 0, ther e exists a set P = {p1, ,pm} s.t. m is polynomial in 1//epsilon1 and for any 
well-rounde d 2-dimensional convex body K, Vol(con v(P K))  Vol(K)/(1 + /epsilon1). 
14-1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Multiplicative weights and applications to zero-sum games, linear programming, boosting, and approximation algorithms</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe25/</lecture_pdf_url>
      <lectureno>25</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>xed , &gt; 0, it outputs with probabilit y at least 1   a hypothesis h that achieves error no more than . 
Similarly , it is called -weak, if the error is at most 0.5  . 
Boosting is a very useful, both in theory and in practice, tool of combining weak rules of thumb into 
strong predictors. In particular, the theory of Boosting shows that if there exists a -weak learning algorithm 
for C, then there also exists a strong one. We will show this in case we have a xed training set with N 
points, and where the strong algorithm has a small error with respect to the uniform distribution on the 
training set. 
We use the MW algorithm. In the tth round, we assign a dieren t distribution Dt on the training set, 
and use the weak learning algorithm to retriev e a hypothesis ht, which by assumption has error at most 
0.5  , with respect to Dt . Our nal hypothesis after T rounds, hnal, is obtained by taking majority 
vote among h1,...,hT . The experts in this case are the samples in the training set, and the events are the 
hypotheses produced by the weak learning algorithm. The associated penalt y for expert x on hypothesis ht 
is 1 if ht(x)= c(x), and 0 otherwise. As in the previous exemple, we penalize the experts that are doing 
well, as we want to eventually increase the weight of a point (expert) if our hypothesis got it wrong. We 
can start with D1 being the uniform distribution, and we update according to the MW algor ithm. Finall y, 
after 2 1 T = 2 log 
rounds we get an error rate for hnal on the training set, under the uniform distribution, that is at most , 
as required. 
Appro ximation Algorithms 
We conclude with an application that demonstrates how to use the MW algori thm to get O(log n) appro xi
mation algorithms for many NP-hard problems. The problem that will focus on is the SET COVER problem: 
Given a universe of elemen ts, U = {1,...,n}, and a collection C = {C1,...,Cm} of subsets of U, whose 
union equals U, we want to pick a minim um number of sets from C to cover all of U. An immediate algorithm 
to tackle this problem is the greedy heuristic: at each step, choose the set from C that has not been chosen 
yet and that covers the most out of the yet uncovered elemen ts of U. The MW algorithm will end up takin g 
exaclt y the form of that greedy algorithm, and will further prove the appro ximation bound. 
We associate the elemen ts of the universe with experts, and the sets of C with events. The penalt y for 
expert i under event Cj , M(i, Cj ), will be equal to 1 if i  Cj , and 0 otherwise. In this case, we use the 
following simplied rule for updating the weights, 
w t+1 = w t(1  M(i, Cj )).ii 
The update rule then gives elemen ts that are covered by the newly chosen set a weight of 0, leaving the 
remaining unaltered. Consequen tly, the weight of elemen t i in round t is either 0 or 1, depending on if it has 
being covered already , or not. The distribution we will be using then in round t, 
t 
pit =  wi
t , 
k wk 
is just a uniform distribution over the uncovered elemen ts by round t. We then choose the maximally 
adversarial event (that is, the one that maximizes the penalt y), which coincides with the set Cj that covers 
a maxim um number of uncovered elemen ts, and update our weights. The describ ed MW algorithm coincides 
with the greedy algorithm, in repeatedly picking the set that covers the most uncovered elements. 
For any distribution p1t ,...,pnt on the elemen ts, we have that OPT sets cover everything. That means 
that the total weights of sets involved (accoring to the distribution p) is at least 1, and hence at least one of 
the remaining sets must cover at least 1/OPT fraction. Mathematically , 
max pit  1/OPT. 
j 
iCj 
25-4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Zero-Sum Games 
There are two players, labeled as the row player R and the column player C. Each player has a nite 
set of actions that he can follow. At each round, player R pays player C an amoun t that depends on the 
actions of the two players. In particular, if R plays action i and C plays j, the payo from R to C is M(i, j). 
We assume that the payos are normalized, such that M(i, j)  [0, 1]. Naturally , player R tries to minimize 
the payo, whereas player C tries to maximize it. 
Each player can follow a pure strategy , which dictates a single action to be played repeatedly , or a 
mixed strategy , under which the player has a xed probabilit y distribution over actions, and chooses actions 
randomly according to it. One migh t expect that the order in which players choose their actions migh t play 
a role, since knowledge of your opponents strategy helps you to adop t your strategy appropriately . If we 
let D and P to be the row and column mixed strategies respectively, the von Neumanns minimax Theorem 
says that in this game, the order of choosing actions is actually indieren t for the players. Mathematically , 
 := min max M(D, j) = max min M(i, P ), 
Dj Pi 
where  is the so called value of the game. Our goal is to appro ximate this value, up to some additiv e error 
. 
We deplo y the MW algorithm as follows. Let pure strategies for R corresp ond to experts, and pure 
strategies for C corresp ond to events. Then, the penalt y paid by expert i in case of event j is exactly the 
payo from R to C, if they follow strategies i and j accordingly , that is M(i, j). Assume also that for a 
mixed strategy D, we can ecien tly compute the column strategy j that maximizes M(D, j) (a quan tity 
eventually  ). At step t of the MW algorithm, we choose a distribution Dt over experts, which then 
corresp onds to a mixed strategy for R. Given Dt, we compute the worst possible event, which is the column 
strategy jt that maximizes M(Dt,jt). 
To see why this approac h yields an appro ximation to , rst note that for any distribution D, 
M (D, jt)  min M(i, jt), (1)
i t t 
since a distribution is just a weighted average of pure strategies. Furthermore, as we argued above we have 
M(Dt,jt)   , (2) 
16 log(n)since we pick the payo-maximizing column strategy . According to the MW theory, after T = 2 
rounds and for any distribution D we have 
T T  T 
t=1 M(Dt,jt)   + min t=1 M(i, jt)   + t=1 M(D, jt) .  T i T T 
The rst inequalit y follows from (2) and the second from (1). Since the above is true for any distribution D, 
it is also true for the optimal distribution, and hence 
T 
  t=1 M
T (Dt,jt)   +  . 
This demonstrates that the average penalt y of the algorithm is an appro ximation of the value of the game, 
within and additiv e positive term of . Note that also the average mixed strategy , or the best strategy 
Dt, constitutes an appro ximately optimal strategy as well, since its payo is appro ximately the value of the 
game, against an optimally acting player. 
Linear Programming and the Plotkin-Shmo ys-Tardos framew ork 
There are various ways in which MW theory can used to solve linear programs. Given what we developed 
in the previous section, one immediate way is to cast the LP as a zero-sum game and solve it via MW. 
25-2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Note that there are some interesting trade os between this idea and the traditional ways of solving linear 
programming problems. In particular, ellipsoid and interior point algorithms (IP) achieve an error of  
in O(poly(n) log( 1 
 )) steps. Their dependence on the corresp ondin g notion of the MW penalt y width is 
logarithmic. On the other hand, the MW algorith m achieves an error after O( log(n) ) steps, in case the width 2 
is 1. Otherwise, the dependence on the width is quadratic, as we have shown. To summarize, IP algorithms 
are much better with respect to error and size of numbers (i.e., width), whereas MW are much better with 
respect to the dimension n. 
We now switc h focus to the Plotkin-Shmo ys-Tardos framew ork, which is a more direct way of applying 
MW to linear programming. Our goal is to check to feasibilit y of a set of linear inequalities, 
Ax  b, x  0, 
where A =[a1 ...am]T is an m  n matrix and x an n dimensional vector, or more precisely to nd an 
appro ximately feasible solution x  0, such that for some &gt; 0, 
aiT x   bi  , i. 
The analysis will be based on an oracle that answ ers the following question: Given a vector c and a 
scalar d, does there exist an x  0, such that cT x  d? With this oracle, we will be able to repeatedly check 
whether a convex combination of the initial linear inequalities, aiT x  bi, is infeasible; a condition that is 
sucien t for the infeasibilit y of our origin al problem. Note that the oracle is straightforw ard to construct, 
as it involves a single inequalit y. In particular, it returns a negativ e answer if d&gt; 0 and c&lt; 0. 
The algorithm is as follows. Experts correspond to each of the m constrain ts, and events corresp ond 
to points x  0. The penalt y for the ith expert for the event x will be aiT x  bi, and is assumed to take 
values in [, ]. Although one migh t expect the penalt y to be the violation of the constrain t, it is exactly 
the opposite; the reason is that the algorithm is trying to actually prove infeasibilit y of the problem. In 
the tth round, we use our distribution over experts to generate an inequalit y that would be valid, if the 
problem were feasible: if our distribution is p1t ,...,pt , the inequalit y is tT t The oracle m i piai x  i pibi. 
then either detects infeasibilit y of this constrain t, in which case the original problem is infeasible, or returns 
a point xt  0 that satises the inequalit y. The penalt y we pay then is equal to t(aTt  bi), and the i pii x
weights are updated accordingly . Note that in case infeasibilit y is not detected, the penalt y we pay is always 
nonnegativ e, since xt satises the checked inequalit y. 
162 log(n)If after T = 2 infeasibilit y is not detected, we have the following guaran tee by the MW theory: 
0  
tT 
=1  
i pt
i(aT
i xt  bi)   + 
tT 
=1(aT
i xt  bi) ,T T 
for every i. The rst inequalit y follows by the nonnegativit y of all penalties. If we take x to be the average 
of all visited points xt ,  txx= t ,T 
then this is our appro ximate solution, since from the above inequality we get for all i 
0   + aiT x bi  aiT x bi  . 
Boosting 
We now visit a problem from the area of Machine Learning. Supp ose that we are given a sequence of training 
points, x1,...,xN , which are drawn from a xed but unkno wn to us distribution D. Alongide, we are given 
corresp onding 0  1 labels, c(x1),...,c(xN ), assigned to each point, where c is a function from some concept 
class C that maps points onto 0  1 labels. Our goal is to generate a hypothesis function h that assigns 
labels to points, replicating the function c in the best way possible. This is captured by the average absolute 
error, ED [|h(x)  c(x)|]. We call a learning algorithm to be strong, if for every distrib ution D and any 
25-3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Decem ber 10, 2009 
Lecture 25 
Lecturer: Jonathan Kelner Scrib e: Nikolaos Trichakis 
Multiplicativ e Weights 
In this lecture, we will study various applications of the theory of Multiplicativ e Weights (MW). In this 
section, we briey review the general version of the MW algorithm that we studied in the previous lecture. 
The following sections then show how the theory can be applied to appro ximately solve zero-sum games and 
linear programs, and how it connects with the theory of boosting and appro ximation algorithms. 
We have n experts who predict the outcome of an event in consecutiv e rounds. Supp ose that in each 
round there are P dieren t outcome s for the event. If outcome j realizes, expert i pays a penalt y of M(i, j). 
An important pareme ter will prove to be the maxim um allowable magnitude for the penalt y. For that, let 
M(i, j)  [, ], with 0    , where  is the width. Our goal is to devise a strategy that dictates which 
experts reccomendation to follow, in order to achieve an expected avegare penalt y that is not much worse 
than that of the best expert (in hindsigh t). 
The strategy that we analyzed in the previous lecture is as follows. We main tain for each expert a 
scalar weight, which can be though t of as a qualit y score. Then, at each round we choose to follow the 
recommendation of a specic expert with probabilit y that is proportional to her weight. After the outcome 
is realized, we update the weights of each expert accordingly . In mathematical terms, let wit be the weight 
of the ith epxert at the beginning of round t. Then, the MW algorithm is 
0. Initialize wi 1 = 1, for all i. 
1. At step t, 
a. Follow the recommedation of the ith epxert with probabilit y pit, where 
t 
p t
i =  wi
t . 
j wj 
b. Let jt  P denote the outcome of the event at round t, and Dt 
1t ,...,pnt } the distribution we = {p
used above to select an expert. Our penalt y is denoted by M (Dt,jt), and is equal to M(i, jt), 
where i is the selected expert. 
c. Update the weights as follows: 
t+1 wit(1  )M (i,jt)/ if M(i, jt)  0 w = i wit(1 + )M(i,jt)/ if M(i, jt) &lt; 0. 
1  162 log(n)In the previous lecture we argued that for any &gt; 0, for   min , and after T = 2 rounds24 
and for all i, the average penalt y we get per round obeys: 
T TM(Dt,jt) M(i, jt)t=1 t=1 .   + T T 
In particular our average penalt y per round is at most  bigger than the average penalt y of the best expert. 
25-1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>That shows that after every round, the total penalt y drops signican tly: 
t+1 &lt; t e1/OPT . 
The inequalit y is strict, since the penalt y is always positiv e. Using 1 = n, after OPT log n iterations we get 
 &lt; 1  = 0, which shows that we can cover everything with OPT log n sets  an log n appro ximation.  
25-5</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Brunn-Minkowski inequality (cont.), Brunn&#8217;s theorem, isoperimetric inequality, Grunbaum&#8217;s theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>Dene the volume of the parallel slice Kt, denoted by vK (t), to be its (n  1)-dimensional volume. 
vK (t)=Voln1(Kt). (8) 
We are interested in the behavior of the function vK (t), and in particular, in whether it is concave. 
Consider the Euclidean ball in Rn . The following plots of vK (t) for dieren t n suggest that except for 
n = 2, the function vK (t) is not concave in t. 
As another example, consider a circular cone in R3 . The volume of a parallel slice is proportional to t2,so 
vK (t) is not concave. More generally , vK (t) is proportional to tn1 for a circular cone in Rn . This suggests 
that the (n  1)th root of vK is a concave function. This guess is veried by Brunns theorem. 
Theorem 2 (Brunn s Theorem) Let K be a convex body, and let vK (t) be dened as in (8). Then the 
1 
n1 function vK (t) is concave. 
Pro of Let s,r,t  R with s =(1  )r + t for some   [0, 1]. Dene the (n  1)-dimensional slices 
Kr,K s,K t as in (7). First, we claim that 
(1  )Ar  A t  As. (9) 
We show this by proving that for any x  Ar,y  At, we have z =(1  )x  y  As, as follows. Connect 
the points (r, x) and (t,y) with a straigh t line (see gure 2). By convexity of K, the line lies completely 
in the body. In particular, the point (s,z), which is a convex combination of (r,x) and (t,y), lies in As. 
Therefore, z  As and the claim in (9) is true. Now, by applying the version of Brunn-Mink owski inequalit y 
in (2), we have 
Vol(As) 1 
n1  (1  )Vol(Ar) 1 
n1 + Vol(At) 1 
n1 
 vK (s) 1 
n1  (1  )vK (r) 1 
n1 + v K (t) 1 
n1 (10) 
Figure 2: n-dimensional convex body K in Theorem 2. 
3.2 Isop erimetric Inequalit y 
A few lectures ago, we asked the question of nding the body of a given volume with the smallest surface 
area. The answer, namely the Euclidean ball, is a direct consequence of the Isoperimetric inequalit y. Before 
stating the theorem, let us dene the surface area of a body using the Minkowski sum. 
Denition 3 Let K be a body. The surface area of K is dened as the dierential rate of volume increase 
as we add a small Euclide an ball to the body, i.e., 
Vol(K  /epsilon1B2 n)  Vol(K)S(K) = Vol(K) = lim . (11) 
e0 /epsilon1 
13-3 
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Note that translation of A or B just translates A B, so any statemen t about the translated sets holds 
for the original ones. 
Since A+ and A are strict subsets of A, we know that A+ B+ and A B have fewer boxes than 
A B. Therefore, (1) is true for them by the induction hypothesis. Moreo ver, A+ B+ and A B are 
disjoin t because they dier in sign of the x1 coordinate. Hence, we have 
Vol(A B)	 Vol(A+ B+)+Vol(A B) 
  
Vol(A+)1/n + Vol(B+)1/nn +  
Vol(A)1/n + Vol(B)1/nn 
 n 	 n  1/n  1/n 
= Vol(A+) 1+ Vol(B+) + Vol(A) 1+ Vol(B) 
Vol(A+)	 Vol(A) 
 n   Vol(B) 1/n 
= Vol(A+)+Vol(A) 1+ Vol(A) 
=  
Vol(A)1/n + Vol(B)1/nn , (6) 
where the second inequalit y follows from the induction hypothesis, and the second equalit y is implied by (5). 
Figure 1: A+ and B+ as dened in the proof of Theorem 1. 
3 Applications of Brunn-Mink owski Inequalit y 
In this section, we demonstrate the power of Brunn-Mink owski inequalit y by using it to prove some important 
theorems in convex geometry . 
3.1 Volumes of Parallel Slices 
Let K Rn be a convex body. A parallel slice, denoted by Kt, is dened as an intersection of the body with 
a hyperplane, i.e. 
Kt = K {x Rn|x1 = t}.	 (7) 
13-2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Octob er 27, 2009 
Lecture 13 
Lecturer: Jonathan Kelner Scribe: Jonathan Pines (2009) 
1 Outline 
Last time, we proved the Brunn-Mink owski inequalit y for boxes. Today well go over the general version of 
the Brunn-Mink owski inequalit y and then move on to applications, including the Isoperimetric inequalit y 
and Grunbaums theorem. 
2 The Brunn-Mink owski inequalit y 
Theorem 1 Let A,B Rn be compact measurable sets. Then 
(Vol(A B))1/n (Vol(A))1/n + (Vol(B))1/n. (1) 
The equality holds when A is a translation of a dilation of B (up to zero-measure sets). 
Pro of An equivalent version of Brunn-Mink owski inqualit y is given by 
 1/n
Vol(A (1 )B) (Vol(A))1/n +(1 )(Vol(B))1/n,  [0, 1]. (2) 
The equivalence of (1) and (2) follows from the fact that Vol(A)= nVol(A): 
 1/n   
Vol(A (1 )B)  Vol(A) 1/n + Vol((1 )B) 1/n 
=  
nVol(A) 1/n +  
(1 )nVol(B) 1/n 
=   
Vol(A) 1/n +(1 )  
Vol(B) 1/n . (3) 
The inequalit y (2) implies that the nth root of the volume function is concave with respect to the 
Minkowski sum. 
Here, we sketch the proof for Theorem 1 by proving (1) for any set constructed from a nite collection 
of boxes. The proof can be generalized to any measurable set by approximating the set with a sequence of 
nite collections of boxes and taking the limit. We omit the analysis details here. 
Let A and B be nite collections of boxes in Rn . We prove (1) by induction on the number of boxes in 
A B. Dene the following subsets of Rn: 
A+ = A {x Rn|xn 0} ,A = A {x Rn|xn 0},
B+ = B {x Rn|xn 0} ,B = B {x Rn|xn 0}. (4)
Translate A and B such that the following conditions hold: 
1. A has some pair of boxes separated by the hyperplane {x Rn|x1 =0}. i.e. there exists a box that lies 
completely in the halfspace {x Rn|x1 0} and there is some other box that lies in its complemen t 
half-space (see gure 1). (If theres no such box in that direction we can change coordinates.) 
2. It holds that 
Vol(A+) Vol(B+)= . (5)Vol(A) Vol(B) 
13-1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>4 Lemma 8 We can turn K/prime into a cone while decreasing the ratio.
Pro of Let K/prime = K/prime {x1  0},K/prime = K/prime {x1  0}. Make a cone yQ 0 by picking y having x1 coordinate
+  
positive on the x1-axis, and V (yQ 0)= V (K/prime . Extend the code in the {x1  0} region, so that the volume +
of the extended part equals V (K/prime ); name this code C/prime . Now by Lemma 5, the centroid of C/prime must lie in 
yQ 0.Let H/prime be the translation of H along the x1-axis so that it contains the centroid of C/prime . Then 
r(K,H)= r(C/prime,H)  r(C/prime,H/prime)  1/e. 
This completes the proof of Grunbaums theorem. 
Next Time 
Next time, we will discuss approximating the volume of a convex body. 
13-6</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Let us rst consider the simple example of a circular n-dimensional cone (gure 3). Suppose we cut the 
cone C by the hyperplane {x1 =x1} at its centroid, where 
1  h tR n1 n x1 = t Voln1 dt = h. (16)Vol(C) t=0 h n +1 
Grunbaums theorem states that the circular cone is indeed the worst case if we choose the centroid. 
Figure 3: n-dimensional circular cone. 
First well need the following lemma: 
Lemma 5 Let L = C {x1  x1} by the left side of the cone (which is x1-aligne d with vertex at the origin). 
Then 1 
2  V (L) 
V (C)  1 
e . 
Pro of 
V (L) = V ( n 
n+1 C) 
=  n n 
V (C) V (C) n +1 
1  n n 1 
2  n +1  e 
Theorem 6 (Grunb aums Theorem) Let K be a convex body, and divide it into K1 and K2 using a hyper
plane. If K1 contains the centroid of K, then 
Vol(K1) 
Vol(K)  1 
e . (17) 
In particular, the hyperplane through the centroid divides the volume into almost equal pieces, and the worst 
case ratio is approximately 0.37:0.63. 
Pro of WLOG, change coordinates with an ane transformation so that the centroid is the origin and the 
hyperplane H used to cut is x1 = 0. Then perform the following operations: 
1. Replace every (n 1)-dimensional slice Kt with an (n 1)-dimensional ball with the same volume to 
get K/prime, which is convex per Lemma 7 below. 
2. Turn K into a cone, such that the ratio gets smaller per Lemma 8 below. 
Lemma 7 K is convex. 
Pro of Let K/prime = K/prime {x1 = t} be a parallel slice in the modied body. The radius of K/prime is proportional t t 
1 1 to V (Kt) n1 . By applying Brunn-Mink owski inequalit y, we get that V (Kt)n1 is a concave function in t. 
Thus K/prime is convex. 
13-5</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/epsilon1 
    
 Now we state the theorem: 
Theorem 4 (Isoperimetric inequality) For any convex body K, with n-dimensional volume V (K) and sur
face area S(K), 
1 V (K) 1/n 
 S(K) n1 
(12)V (B2 n S(B2 n) 
Pro of By applying the Brunn-Mink owski inequalit y, we have the following: 
V (K  /epsilon1B2 n)   
V (K)1/n + /epsilon1V (B2 n)1/nn 
 1/nV (B2 n)= V (K) 1+ /epsilon1 V (K) 
V (B2 n) V (K) 1+ n/epsilon1 (13)V (K) 
where the second inequalit y is obtained by keeping the rst two terms of the Taylor expansion of (1 + x)n . 
Now, the denition of surface area in (11) implies: 
V (K)+ n/epsilon1V (K) V
V (
(B
K2 n 
)) 1/n 
 V (K) 
S(K)= V (K)  
 1/n 
= nV (K) V (B2 n) 
V (K) 
n1 
n = nV (K) V (B2 n)1/n. (14) 
For an n-dimensional unit ball, we have S(B2 n)= nV (B2 n). Therefore, 
n1 
n S(K) nV (K) V (B2 n)1/n 
 S(B2 n) 
11 n1 
n S(K) n1 nV (K) V (B2 n)1/n n1 
  S(B2 n) nV (B2 n) 
 1/nV (K)= (15)V (B2 n) 
3.3 Grun baums Theorem 
Given a high-dimensional convex body, we would like to pick a point x such that for any cut of the body by 
a hyperplane, the piece containing x is big. A reasonable choice for x is the centroid, i.e. 
1 x = ydy. Vol(K) yK 
This choice guaran tees to get at least half of the volume for any origin symmetric body, such as a cube 
or a ball. The question is how much we are guaran teed to get for a general convex body, and in particular, 
what body gives the worst case. Do we get a constan t fraction of the body, or does the guaran tee depend 
on dimension? 
13-4</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Johnson-Lindenstrauss theorem (cont.), Dvoretsky&#8217;s theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Minkowski&#8217;s theorem, shortest/closest vector problem, lattice basis reduction, Gauss&#8217; algorithm</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>one can show that every vector with second coordinate greater than |v2| has length at least 3|v|. This 
implies that v is the shortest vector not generated by u. 
A formal description of Gausss algorithm follows. 
While {u,v}, where /bardblu/bardbl/bardblv/bardbl, is not reduced: 
 Set v := vmu, where m Z is chosen to minimize the length of vmu. 
 If /bardblu/bardbl/bardblv/bardbl, break. 
 If /bardblv/bardbl/bardblu/bardbl, then swap u and v, and repeat. 
In the second step, if /bardblu/bardbl/bardblv/bardbleven after the reduction, the basis cannot be further reduced and one can 
prove that 2|uv|/bardblu/bardbl2 . 
The algorithm is like a 2-dimensional discrete version of GramSc hmidt, and is similar to the Euclidean 
GCD algorithm. Can we make it run in polynomial time? It turns out that it actually does run in polynomial 
time, but the proof of this fact is not obvious, and therefore, we do not presen t it here. Instead of this, we 
replace the termination criterion with 
If (1 )/bardblu/bardbl/bardblv/bardbl, break. 
It is easy to prove that the modied algorithm gives an (1 ) approximate answer. Now in each reduction, 
we decrease the length of one of the vectors by at least a constan t factor. Therefore the modied algorithm 
runs in weakly polynomial O(log(/bardblu/bardbl+ /bardblv/bardbl)/) time. 
The proof that Gausss algorithm runs in polynomial time uses the fact that for a sucien tly small , 
after the modied algorithm stops, only one more reduction suces to get a reduced basis. 
4.3 Reduced bases 
We want to extend the notion of reduced bases to higher dimensions. In order to nd a short vector in 
the lattice, we would like to perform a discrete version of the GramSc hmidt. So we need to formalize the 
notion of being orthogonal in lattice problems. One way to do this is to say that the result of our procedure 
is almost orthogonalized so that doing GramSc hmidt does not change much. In this section, we use the 
notation from Section 4.1. 
Denition 6 (Reduced bases) Let {b1,...,b n}be a basis for a lattic e L and let M be its GramSchmidt 
matrix dene d above. Then {b1,...,b n}is a reduced basis if it meets the following two conditions: 
1. All the non-diagonal entries of M satisfy |ik|1/2. 
2. For each i, /bardblSi bi/bardbl2  4
3 /bardblSi bi+1/bardbl2, wher e Si is the subsp ace ortho gonal to span(b1,...,b i1). 
Remark The constan t 4/3 here is somewhat arbitrary . In fact, any number strictly between 1 and 4 will 
do. 
Remark Condition 2 is equivalent to /bardblb
i+1 + i+1,ib
i /bardbl2  43 /bardblbi/bardbl2 and one may think it as requiring 
that the projections of any two successiv e basis vectors bi and bi+1 onto Si satisfy a gapped norm ordering 
condition, analogous to what we did in Gausss algorithm for 2-dimensional case. 
19-4</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit 11/17/2009 
Lecture 19 
Lectur er: Jonathan Kelner Scrib e: Steven Sam 
1 Review of last lecture 
Recall that L  Rn is a lattic e if L is a discrete set and is closed under addition and negation. In this 
lecture, we assume that all lattices are full-rank, i.e., the vectors in the lattice span Rn . 
Given linearly independen t b1,...,b n Rn, one can form a lattice 
L(B)= L(b1,...,b n)= {Bx|x Zn}, 
where B is the set of the bis. We call the bis (and also the set B) a basis for L(B). Recall that a lattice can 
have many dieren t bases. As opposed to linear algebra over a eld, change of bases for lattices is more rigid, 
since the integralit y constrain ts must be preserv ed. Because of this, one cannot usually nd an orthonormal 
basis, and instead one of the most fundamen tal problems becomes nding a nice basis, which consists of 
short and almost orthogonal vectors. 
Recall that for a basis B of L, the fundamental parallelepip ed is 
P(B)= {Bx|x [0,1)n}. 
Furthermore, if L is full rank, then the determinant of L is dened as Vol(P(B)) = |det(B)|, and this is 
independen t of the choice of a basis. The determinan t is inversely proportional to the densit y of a lattice. 
We say that an nn matrix M is unimo dular if all entries of M are integers, and |det(M)|= 1. Last 
time we saw that a matrix U is unimo dular if and only if U1 is unimo dular. This implies that an inverse 
of a unimo dular matrix has integer entries. Unimo dular matrices are interesting for us, because two lattice 
bases B1 and B2 are equivalent if and only if B1 = UB 2, for some unimo dular matrix U. Moreo ver two 
bases are equivalent if and only if one can be obtained from the other by the following operations: 
 bi bi + kbj , for i =/negationslashj and k Z, 
 swapping vectors: bi bj , 
 bi bi. 
Last time, we proved the following theorem, which we will need for the proof of Minkowskis theorem. 
Theorem 1 (Blic held) For any full rank lattic e L and measur able set S  Rn with Vol(S) &gt; det(L), 
ther e exist distinct z1,z2 S such that z1 z2 L. 
2 Mink owskis theorem 
Theorem 2 (Mink owskis Theorem) If L is a full rank lattic e, and S any centr ally-symmetric convex 
set of volume greater than 2n det(L), then K contains a nonzer o point of L. 
Pro of Consider the set S= 1
2 S. Then Vol( S)=2nVol(S) &gt; det(L) by assumption. So we can 
apply Blichelds theorem to conclude that there exist distinct points z1,z2  Ssuch that z1 z2  L. 
In particular, 2z1,2z2  K. Since K is centrally-symmetric, we also have 2z2  K. Hence the point 
z1 z2 = 1
2 (2z1 +(2z2)) is in K since it is convex. 
This theorem is very useful in many settings. For one, many nice number theory theorems follow from 
it. It also guaran tees that the length 1(L) of the shortest vector in the lattice is not too big. 
19-1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>/summationdisplay /summationdisplay 
/summationdisplay 
/summationdisplay /summationdisplay /summationdisplay /summationdisplay /summationdisplay  Let b 
1 := b1. 
 For k =2 to n: b
k := bk [projection of bk onto span(b1,...,b k1)]. 
The projection is computed in the following way: 
projection of bk onto span(b1,...,b k1) = projection of bk onto span(b 
1,...,bk 
1) 
= k1 
projection of bk onto b
i = k1 b
/bardblk 
b 
/bardblb
2 
i b
i 
i=1 i=1 i 
We set coecien ts ki so that kk = 1, and 
k 
bk = kibi . 
i=1 
Therefore, we can write the above as B = MB, where the basis vectors are rows of B and B, and 
   11 0 0  0 1 0 0  0 
 21 22 0  0  21 1 0  0  
M =  . . .. .  =  . . ...  . . . ... . . ...  .  .  . . . . . . . . 
n1 n2 n3  nn n1 n2 n3  1 
Note that det(M) = 1, so for lattices, we have Vol(B) = Vol(B), but since entries of M are not 
necessarily integers, L(B)= L(B) does not have to hold. However, B can be used to bound the length 
1(L(B)) of the shortest vector in L(B). 
Lemma 4 For any nonzer o b  L(B), /bardblb/bardbl mini /bardblb
i /bardbl. 
Pro of Every nonzero b  L(B) can be expressed as b = /summationtext
ik 
=1 ibi, where k /negationslash0 and for each i = is an 
integer. We have 
k k i k1 k 
b= ibi = i ij bj = kb
k + iij bj , 
i=1 i=1 j=1 j=1 i=1 
and therefore, 
/bardblb/bardbl2 /bardblkb
k/bardbl/bardblb
k/bardbl2 , 
which nishes the proof. 
4.2 Gausss Algorithm 
We start by presen ting an algorithm for solving the 2-dimensional SVP exactly . 
We call a basis u,v for a 2-dimensional lattice reduced if /bardblu/bardbl/bardblv/bardbl, and 2|uv|/bardblu/bardbl2 . One can show 
that the following claim holds. 
Prop osition 5 A reduced basis for a 2-dimensional lattic e contains the rst two successive minima of L. 
Sketch of Pro of Rotate the plane, so that u =(u1,0), and v =(v1,v2). We claim that the vector v is a 
vector with the smallest possible nonnegativ e second coordinate. The property 2|uv|/bardblu/bardbl2 implies that 
v1  [ u
2 1 , u
2 1 ], which in turn implies that v is the shortest vector whose second coordinate is v2. Because 
|v2| 3/2|u|, every vector whose second coordinate is greater than |v2| (that is, at least 2|v2|) has length   
at least 3|u| and cannot be shorter than u. Therefore, u is the shortest vector. Also, since |v2| 3/2|v|, 
19-3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Corollary 3 For any full-rank lattic e L, 
 1(L)  n(det L)1/n. 
Pro of We rst bound the volume of the ball B(0,r), for some radius r. This ball contains the hypercube /bracketleftBig /bracketrightBig	 /parenleftBig/parenrightBign n
r
n , r
n . Hence, its volume is greater than 2r
n .
For r = ndet(L)1/n, the volume of B(0,r) is greater than 2n det(L), so the ball contains a nonzero 
lattice vector, and therefore, the length of the shortest vector is at most ndet(L)1/n. 
The above corollary easily generalizes to other minima. For instance, we will see in a problem set that 
/parenleftBigg /parenrightBigg1/nn /productdisplay  i(L)  n(det L)1/n , 
i=1 
where i(L) is the length of the ith shortest vector. 
3 Algorithmic questions 
One could ask, for instance, if the bound given above for 1(L) is tight, and when it holds. Here we will focus 
on the algorithmic aspect of lattices. There are several interesting questions that one can ask for lattices. We 
assume that all lattices have integer coordinates. This is the same as giving them rational coordinates, since 
we can always multiply all coordinates of all vectors by the least common multiple of their denominators. 
	Shortest Vector Problem (SVP) : Find the shortest vector in L. Finding just the length of the 
shortest vector is equivalent. 
	Closest Vector Problem (CVP) : Find the vector in L closest to some given point p. 
Both of the above problems are NP-hard, so one usually focuses on the approximate version of them: 
Find a vector within  of the optim um. Some similar questions, like Does a vector of a given length 
exist? turn out to be non-equiv alent. 
For the approximation versions of SVP and CVP, the gaps between the best known upper and lower 
bounds are very large. For instance, the best polynomial time algorithms for these problems get approxi
mation factors which are essentially exponential in n. The best known factor is roughly 2O(n log log n/ log n). 
The best exact algorithm runs in 2O(n) time. It turns out that one cannot nd the vector guaran teed by 
Minkowskis Theorem. SVP is hard to approximate within any constan t factor unless NP = RP. CVP is hard to approximate within nO(1/ log log n). Appro ximation within the factor n is in NP co-NP. 
4 Lattice basis reduction 
We will show a polynomial time algorithm to approximately solve the SVP within a factor of 2O(n). Because 
of an exponential error this might seem to be a very weak and useless result. Nevertheless, this algorithm is 
good enough to give extremely striking results both in theory and practice. For instance, it can be used to 
show that an integer program with a constan t number of variables can be solved in polynomial time. 
4.1 Review of the GramSc hmidt algorithm 
Since our approac h resembles the GramSc hmidt algorithm, we rst review this metho d for orthogonalizing 
a basis for inner product spaces. 
We are given a basis b1,...,b n for a vector space, and we want to construct an orthogonal basis b1,...,b
n 
such that span(b1,...,b k) = span(b 
1,...,b ), for all k {1,...,k }. In the GramSc hmidt algorithm, the k
vectors b
i are usually normalized, but we will not do it here. 
The process works as follows: 
19-2</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Separating hyperplanes (cont.), Banach-Mazur distance, Fritz John&#8217;s theorem, Brunn-Minkowski inequality</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe12/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Now, switching the order of summation on the left-hand side of (1) gives us 
n m 
2 ci/angbracketleftui,ej /angbracketright2 ,j 
j=1 i=1 
and by the above we know that this is at most n. Further, by condition 2 of Theorem 5, we know that 
m 
i=1 ci/angbracketleftui,ej /angbracketright2 = |ej |2 = 1. Therefore, we get j n 2  n. By the AM-GM inequalit y, we get that j=1 P
=1 2 
 1, which implies that i
n
i
n 1/n2 
i n  1. Equalit y only holds if all the i are equal.  i i=1 i=1 n nThis shows that i &lt; 1 for any such E that is not B2 n, completing the rst part of the proof. i=1 
For the second part, assume that we are given that B2 n is the unique ellipsoid of maximal volume that is 
contained in K. We want to show that for some m, there exist ci and ui for 1  i  m (as in the statemen t 
mof Theorem 5), such that for all vectors x, 
i=1 ci/angbracketleftx,ui/angbracketright2 = |x|2 . Again, this is equivalent to showing that 
m 
ciuiuiT =Idn . 
i=1 
We already observ ed that for origin-symmetric bodies, the condition that m
i=1 ciui = 0 is implied by the 
previous requiremen t. 
Let Ui = uiuT
i . Also, observ e that we can view the space of n  n matrices as a vector space of dimension 
n2 . Hence we can parametrize the space of n  n matrices by Rn 2 . Thus, m
i=1 ciuiuT
i =Idn for ci &gt; 0 
means that Idn /n is in the convex hull of the Ui (if the identity holds, we know that the ci are positive and 
sum to n). 
mIf we cannot nd ci,ui such that 
i=1 ciuiuT
i =Idn, it means that Idn /n is not in the convex hull of 
the Ui. Hence, there must be a hyperplane in the space of matrices that separates Idn /n from the convex 
hull of the Ui. 
2For two nn matrices A and B, let AB denote their dot product in Rn , i.e. AB = i,j Aij Bij . Thus, 
the separating hyperplane gives a matrix H such that A  H  1 for all A  conv(Ui) and (Idn /n)  H&lt; 1. 
Let t =Tr(H)= H  Idn. Let H/prime = H  (t/n)(Idn). Then (Idn /n)  H/prime = (Idn /n)  (H  (t/n)Idn)= 
t/n  ((Idn /n)  (t/n)Idn) = 0. Similarly , since Tr(A) = 1 for all A in conv(Ui), we get that A  H/prime &gt; 0. 
Hence, H/prime is such that: 
1. Tr(H/prime)=0, and 
2. H/prime  (uiuiT ) &gt; 0 for all i. 
Now, let E = x  Rn|xT (Idn +H/prime)x  1 . For all i, we have uT (Idn +H/prime)ui =1+ uiT H/primeui, which i 
is greater than 1 since uT
i H/primeui &gt; 0= H/prime  (uiuT
i ) &gt; 0. Hence ui /negationslashE . Also, since H/prime  (uiuT
i ) &gt; 0 for all i, 
by continuity, there exists /epsilon1&gt; 0 such that for all vectors w in the /epsilon1-neigh borhood of the set of all ui satisfy 
H/prime  (wwT ) &gt; 0. Hence, by the previous argumen t, any such w is not contained in E. 
Note that when  = 0, we get the unit ball B2 n . For every &gt; 0 we have that all w in the /epsilon1-neigh borhood 
of the contact points of B2 n are not contained in E. Hence, as we increase  continuously starting from 0, 
the continuity of the transformation of E implies that for sucien tly small , boundary( K)  E = . 
Hence /epsilon1/prime &gt; 0suchthat(1 + /epsilon1/prime)E  K. Therefore, to conclude the proof, it suces to show that 
Vol(E)  Vol(B2 n), which gives give us a contradiction (as (1 + /epsilon1/prime)E is an ellipse of volume larger than B2 n 
contained in K). 
Let 1,2,..., n be the eigenvalues of Idn +H/prime . Since Vol(E)=( 
in 
=1 i)1, to show that Vol(E)  
Vol(B2 n), we need to show that 
in 
=1 i  1. However we know that 
in 
=1 i = Tr(Idn +H/prime) = Tr(Idn)= n. 
By the AM-GM inequalit y, ( 
in 
=1 i)1/n  ( 
in 
=1 i)/n =1. Hence in 
=1 i  1. This concludes the proof 
of part 2. 
12-4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>m1. i=1 ciui =0, and 
m 2 22. For all vectors x, we have i=1 ci/angbracketleftx,ui/angbracketright= |x|. It is not hard to show that this condition is 
m Tequivalent to the requir ement that i=1 ciuiui =Idn, wher e Idn is the nn identity matrix. 
Since the ui are unit vectors, they are points on the convex body K that also belong to the sphere B2 n . 
mAlso, the rst identity, i.e. i=1 ciui =0, is actually redundan t, since for origin-symmetric bodies it can be 
derived from the second identity. This is because for every ui, its reection in the origin (namely ui)is 
also contained in K B2 n; further we can take the constan ts in the second identity corresp onding to ui and 
ui to be the same, and this establishes the rst equation. 
The second identity says that the contact points of the sphere with K act somewhat like an orthonormal 
basis. They can be weighted so that they are completely isotropic. In other words, the points are not 
concen trated near some proper subspace, but are pretty evenly spread out in all directions. Together they 
mean that the ui can be weighted so that their center of mass is the origin and their inertia tensor is the 
identity. Also, a simple rank argumen t shows that there need to be at least n such contact points, since the 
second identity can only hold for x in the span of the ui. 
Note that Theorem 4 easily follows from Theorem 5. Indeed, assume without loss of generalit y that B2 n 
is the ellipsoid of maximal volume contained in K. We can make this assumption since the particular choice of basis is not important for the proof. We need to show that B2 n K  nB2 n . Now, for all x K,we 
have xui 1 for all i. Hence, |x|2 =  ci(xui)2   ci. In the course of the proof below, we will see that    ci = n. This shows that |x| n, and hence K  nB2 n . 
Thus, once we prove Theorem 5, we will have shown the existence of an ellipsoid E such that 
 E K  nE. 
4.2 Pro of of Johns Theorem 
As part of the proof Theorem 5, we will prove the following things: 
1. If there exist contact points {ui}as required in the statemen t of Theorem 5, then Bn is the unique 2 
ellipsoid of maximal volume that is contained in K. 
2. If Bn is the unique ellipsoid of maximal volume that is contained in K, then there exist points {ui}2 
such that they satisfy the two identities in Theorem 5. 
To prove the rst statemen t, suppose that we are given unit vectors u1,u2,...,u m on the boundary of 
mK and positive real numbers c1,c2,...,c m such that i=1 ciui =0, and for all vectors x, it is the case that 
m 2 2 
i=1 ci/angbracketleftx,ui/angbracketright= |x|. We wish to show that Bn is the unique ellipsoid of maximal volume that is contained 2 
in K. Observ e that it suces to show that among all axis-aligned ellipsoids contained in K, B2 n is the unique 
ellipsoid of maximal volume. This is because what we are trying to prove doesnt mention any basis and is 
only in terms of dot products. Hence, since the statemen t will remain true under rotations, proving it for 
axis-aligned ellipsoids is enough. 
For each ui it is the case that ui k 1 for all k K, Hence ui K . Let E be any axis-aligned ellipsoid 
such that E K. Then K E . Hence {u1,u2,...,u m}E . Since E is axis-aligned, it is of the form 
2 
ix| nx 1.i=1 2 
i 
We also have that Vol(E)/Vol(B2 n)= n i. Therefore, to show that Vol(E) &lt; Vol(B2 n), we must  i=1 
show that n i &lt;1 for any such E that is not Bn 
i=1  2 .  Observ e that E = {y| n 22 1}. Now, since ui ui =1, we have Tr m T = n 
i=1 i yi i=1 ciuiui i=1 ci. 
Since Tr(Idn)= n, this implies that n
i=1 ci = n. 
Let ej denote the vector which hasa1i nthe jth coordinate and 0 in the other coordinates. Clearly 
/angbracketleftui,ej /angbracketrightis the jth coordinate of ui. For 1 im, since ui E, we get that 
jn 
=1 2 
j /angbracketleftui,ej /angbracketright2 1. Summing 
it over all i,weget 
m n n 
ci j 2/angbracketleftui,ej /angbracketright2  ci = n. (1) 
i=1 j=1 i=1 
12-3</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Lemma 9 Let A and B be boxes in Rn . Then
Vol(A B)1/n  Vol(A)1/n + Vol(B)1/n.
Pro of Let A have sides of length a1,...,a n and B have sides of length b1,...,b n. It directly follows from 
the denition of Minkowski sums that A B has sides of length a1 + b1,...,a n + bn. 
We just need to show the following: 
Vol(A)1/n + Vol(B)1/n 
 1. (2)Vol(A B)1/n 
We can rewrite the left-hand side of (2) as 
n n bi)1/n n n bi)1/n( 
i=1 ai)1/n +( 
i=1 = ( 
i=1 ai)1/n 
+ ( 
i=1 
( n
i=1(ai + bi))1/n ( n
i=1(ai + bi))1/n ( n
i=1(ai + bi))1/n 
n 1/n n 1/n  ai  bi = + ai + bi ai + bii=1 i=1 
1 n ai 1 n bi + =1 n ai + bi n ai + bii=1 i=1 
where the inequalit y is just an application of AM-GM. 
Next time, we will prove the Brunn-Mink owski inequalit y for more general bodies, and study some of its 
applications. 
12-6</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Octob er 22, 2009 
Lecture 12 
Lectur er: Jonathan Kelner Scrib es: Alex Levin (2009) 
1 Outline 
Today well go over some of the details from last class and make precise many details that were skipped. Well 
then go on to prove Fritz Johns theorem. Finally , we will start discussing the Brunn-Mink owski inequalit y. 
2 Separating Hyp erplanes 
Given a convex body K  Rn and a point p,a separating hyperplane for K and p is a hyperplane that has 
K on one side of it and p on the other. More formally , for a vector , the hyperplane H = {x|  x =1} is a 
separating hyperplane for K and p if 
1.   x  1 for all x  K, and 
2.   p  1. 
Note that if we replace the right hand side of both the above conditions by 0 or any other constan t, we 
get an equivalent formulation. 
We call a separating hyperplane H a strongly separating hyperplane if the second inequalit y is strict. 
Last time, we sketched a proof of the following theorem: 
Theorem 1 (Separating Hyp erplane Theorem) If K is a convex body and p is a point not containe d 
in K, then ther e exists a hyperplane that strongly separates them. 
Well use the above result to show why the polar of the polar of a convex body is the body itself. Recall 
that for a convex body K, we had dened its polar K to be {p|k  p  1k  K}. 
Theorem 2 Let K be a convex body. Then K = K. 
Pro of We know that K = {p|k  p  1k  K}. Similarly K = {y|p  y  1p  k}. Let y be any 
point in K. Then, by the denition of the polar, for all p  K we have that p  y  1. The denition of the 
polar of K implies that y  K . Since this is true for every y  K, we conclude that K  K . 
The other direction of the proof is the nontrivial one and well have to use the convexity of the body and 
the separating hyperplane theorem. Suppose that we can nd a y  K such that y/ K. Since y  K , 
we have that p  y  1 for all p  K . Since y /negationslashK, there exists a strongly separating hyperplane for y and 
K. Let it be H = {x|v  x =1}. By the denition of separating hyperplane, we have v  k  1 for all k  K. 
Hence, v  K . Also, v  y&gt; 1 (since H is a separating hyperplane), and we just showed that v  K . This 
contradicts our assumption that y  K . Hence K  K. 
3 Banac hMazur Distance 
Recall from last time the denition of the Banac hMazur distance between two convex bodies: 
Denition 3 Let K and L be two convex bodies. The BanachMazur distanc e d(K,L) is the least positive 
d  R for which ther e is a linear image L/prime of L such that L/prime  K  dL/prime, wher e dL/prime is the convex body 
obtaine d by multiplying every vector in L/prime by the scalar d. 
12-1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>5 
6 Sketch of a Simpler Pro of 
If we just wish to prove the existence of an ellipse E that satises the conditions of Fritz Johns Theorem 
without actually characterizing it, then the picture below suggests an alternativ e and possibly simpler proof 
of the result. If any point of K is more than n distance away from the origin, then we can nd an ellipse of larger 
volume than B2 n that is contained in K. 
Figure 2: A simpler proof of the Rounding result. 
The Brunn-Mink owski Inequalit y 
Denition 6 For A,B Rn , the Minkowski sum A B is given by 
A B = {a + b|a A,b B}. 
The Minkowski sum can be dened for any subsets of Rn , but it is nicely behaved if A and B are convex. 
Intuitively, the Minkowski sum is obtained by moving one of the sets around the boundary of the other one. 
The Brunn-Mink owski inequalit y, which relates the volume of A B to the volumes of A and B, implies 
many important theorems in convex geometry . The goal is to bound Vol(A B) in terms of Vol(A) and 
Vol(B). The following are some loose bounds that can be simply veried. 
Fact 7 Vol(A B) max{Vol(A), Vol(B)} 
Pro of Let a A. We have {a}B A B, by denition. Hence, 
Vol(A B) Vol({a}B)=Vol(B). 
Similarly , Vol(A B) Vol(B). 
Fact 8 Vol(A B) Vol(A)+Vol(B) 
Pro of By moving one of the sets around the other one (summing the extreme points), we can get disjoin t 
copies of A and B in A B. 
The bound given by Fact 8 is loose. To see that, consider the case that A = B. In this case, A A =2A 
and hence, Vol(A A)=2n Vol(A). So the volume of A A grows exponentially with n, while the lower 
bound given in the above fact do not. This suggests taking the n-th roots and still get a valid bound. Let 
us rst prove it for boxes. 
12-5 1 m
MC ESL
P
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>4 Figure 1: Dening the distance between K and L. 
Observ e that the above denition takes into consideration only the intrinsic shape of the body, and it is 
independen t of any particular choice of coordinate system. Also observ e that the Banac hMazur distance is 
symmetric in its input argumen ts. If L/prime  K  dL/prime, then by scaling everything by d, we get that dL/prime  dK. 
Hence K  dL/prime  dK, which implies the symmetry property. 
Fritz Johns Theorem 
Let Bn 
2 denote the n-dimensional unit ball. For any two convex bodies K and K/prime, let d(K,K/prime) denote 
the Banac hMazur distance between them. In the rest of this lecture, we will state and prove Fritz Johns 
theorem. 
Theorem 4 For any n-dimensional, origin-symmetric convex body K, we have d(K,B2 n)  n. 
In other words, the theorem states that for every origin-symmetric convex body K, there exists some ellipsoid E such that E  K  nE. We will prove that the ellipsoid of maximal volume that is contained 
in K will satisfy the above containmen t. Informally , the theorem says that up to a factor of n, every convex body looks like a ball. The above bound of n is tight for the cube. If we didnt require the condition that K is origin symmetric, then the 
bound would be n, which would be tight for a simplex. 
The theorem can also be rephrased as the following: there exists a change of the coordinate basis for which Bn 
2  K  nBn 
2 . 
4.1 A sligh tly stronger version of the Fritz John Theorem 
We will actually state and prove a more technical and slightly stronger version of the Fritz John theorem 
that implies our previous formulation. From now on, we assume that all the convex bodies we consider are 
origin-symmetric. 
Theorem 5 Let K be an origin-symmetric convex body. Then K contains a unique ellipsoid of maximal 
volume. Mor eover, this largest ellipsoid is Bn 
2 if and only if the following conditions hold: 
 Bn 
2  K 
Ther e are unit vectors u1,u2,...,u on the boundary of K and positive real numb ers c1,c2,...,c  m m 
such that 
12-2 dL~
K
L~
Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Polar of a convex body, separating hyperplanes, norms and convex bodies, Banach-Mazur distance, Fritz John&#8217;s theorem</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Translation has a very drastic eect on the polar. It can become unbounded just by translating the 
polytop e. 
All these properties can be illustrated using the pictures below. 
3 Polars of General Convex Bodies 
Any convex body can be though t of as the intersection of a (possibly innite) set of half spaces. These are 
called sup orting hyperplanes. Therefore, the polar of a convex body can be seen as the convex hull of a 
(possibly innite) set of points, coming from all of the supp orting hyperplanes. With this intuition one can 
guess about the following : 
	Polar of a sphere is a sphere. 
	Polar of a sphere of radius r is a sphere of radius 1/r. 
	Polar of an ellipse is an ellipse with axes reversed. 
Denition 1 The polar of a convex body C is given by 
C = {x  Rn|x  c  1c  C} 
We observ e that this denition is equiv alent to the previous denition . 
Prop osition 2 For a polytop e C given by C = {x|ai  x  bi,i =1,...,k}, the sets C1 = C2 wher e 
C1 = {x  Rn|x  c  1c  C} and C2 == conv(a1,...,ak). 
We skip the proof as it is easy to verify that if x  C1 then x  C2 and vice versa. 
We will now prove that (C) = C. We would be needing the concept of a separating hyperplane for the 
proof which we introduce now. 
3.1 Separating Hyperplanes 
Given a convex body K  Rn and a point p,a separating hyperplane for K and p is a hyperplan e that has 
K on one side of it and p on the other. More forma lly, for a vector , the hyperplane H = {x|  x =1} is a 
separating hyperplane for K and p if 
1.	x  1 for all x  K, and  
2.	p  1. 
Note that if we replace the right hand side of both the above conditions by 0 or any other constan t, we 
get an equiv alent formulation. 
We call a separating hyperplane H a strongly separating hyperplane if the second inequalit y is strict. 
Theorem 3 Separating Hyperplane Theorem : If K is a convex body and p is a point not containe d in 
K, then there exists a hyperplane that strongly separates them. 
Proof Well sketch an outline of the proof. It can be made rigorous. Consider a point x  K that is 
the closest to p in 2 distance. Consider the plane H that is perpendicular to the line joining x to p and is 
passing through the midp oint of x and p. H must separate K from p because if there is some point of K, 
say y, that is on the same side of H as p, then we can use the convexity of K to conclude that the point 
x which is the intersection of the hyperplane with the line joining x and y is also in K. x is closer to p 
that x since px forms the side of a right angled triangle of which xp is the hypotenuse. This contradicts the 
assumption that x is the point closest to p. 
11-2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>In other words, the theorem states that for every origin-symmetric convex body K, there exists some 
ellipsoid E such that E  K nE. Well prove that the ellipsoid of maximal volume that is contained in 
K will satisfy the above containmen t. 
Informally , the theorem says that up to a factor of n, every convex body looks like a ball. The above 
bound of n is tight for the cube. If we didnt require the condition that K is origin symmetric, then the 
bound would be n, which would be tight for a simplex. 
The theorem can also be rephrase d as the following: There exists a change of the coordinate basis for 
which Bn  K nB2 n .2 
Theorem 8 Let K be an origin-symmetric convex body. Then K contains a unique ellipsoid of maximal 
volume. Moreover, this largest ellipsoid is B2 n if and only if the following conditions hold: 
Bn  2  K 
	Ther e are unit vectors u1,u2,...,um on the boundary of K and positive real numb ers c1,c2,...,cm 
such that 
m1. i=1 ciui =0, and 
m 2 22. for all vectors x, i=1 cix, ui= |x|. 
Since the ui are unit vectors, they are points on the convex body K that also belong to the sphere B2 n . 
mAlso, the rst identity, i.e. i=1 ciui =0, is actually redundan t, since for origin symme tric bodies it can be 
deriv ed from the second identity. This is because for every ui, its reection in the origin is also contained 
in K  B2 n . 
The second identity says that the contact points (of the sphere with K) act somewhat like an orthonormal 
basis. They can be weighted so that they are completely isotropic. In other words, the points are not 
concen trated near some proper subspace, but are prett y evenly spread out in all directions. Together they 
mean that the ui can be weighted so that their center of mass is the origin and their inertia tensor is the 
identity. Also, a simple rank argumen t shows that there need to be at least n such contact points, since the 
second identity can only hold for x in the span of the ui. 
6.1 Proof of Johns Theorem 
Proof As part of the proof of Johns Theorem, well prove the following things: 
1. If there exist contact points {ui} as required in the statemen t of Theorem 8, then Bn is the unique 2 
ellipsoid of maximal volume that is contained in K. 
2. If Bn is the unique ellipsoid of maximal volume that is contained in K, then there exist points {ui}2 
such that they satisfy the two identities in Theorem 8. 
Proof of 1: We are given unit vectors u1,u2,...,um on the boundary of K and positiv e real numbers 
m	 m c1,c2,...,cm such that i=1 ciui =0, and for all vectors x, i=1 cix, ui2 = |x|2 . We wish to show that 
B2 n is the unique ellipsoid of maximal volume that is contained in K. Observ e that it suces to show that 
among all axis-aligned ellipsoids contained in K, B2 n is the unique ellipsoid of maximal volume. This is 
because what we are trying to prove doesnt mention any basis and is only in terms of dot-pro ducts. Hence, 
since the statemen t will remain true under rotati ons, proving it for axis-aligned ellipsoids is enough. 
For each ui we have that for all k  K, ui  k  1. Hence ui  K. Let E be any axis-aligned ellipsoid 
such that E  K. Then K  E. Hence {u1,u2,...,um} E. Since E is axis-aligned, it is of the form  2 nxi{x| i=1 2  1}. n	 n i 
V ol(E)/V ol(B2 n)= i=1 i. Therefore, to show that V ol(E) &lt; Vol(B2 n), we must show that i=1 i &lt; 1 
for any such E which is not B2 n . 
11-7</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Octob er 20, 2009 
Lecture 11 
Lecturer: Jonathan Kelner Scrib e: Chaithanya Bandi 
1 Outline 
Today well introduce and discuss 
Polar of a convex body.  
Corresp ondence between norm functions and origin-symmetric bodies (and see how convex geometry  
can be a powerful tool for functional analysis). 
Fritz-Johns Theorem  
2 The Polar of a Polytop e 
Given a bounded polytop e C  Rn that contains the origin in its interior, we can represe nt C as 
C = {x|x  bi,i =1,...,k}, ai  
where bi &gt; 0. 
Without loss of generalit y, by appropriately scaling each constrain t, we can assume bi =1, i =1,...,k. 
Now the polar of C is given by 
C = conv(a1,...,ak). 
2.1 Examples 
Let C be the square with corners at (1, 1), (1, 1), (1, 1), (1, 1). Then {ai} = {(1, 0), (0, 1), (1, 0), (0, 1)}. 
The polar has corners at (1, 0), (0, 1), (1, 0), (0, 1). Note that the polar is a square rotated and shrunk 
into a diamond. This polytop e is also referred to as the cross polytop e. Note that the facets of C become 
the vertices of C and vice versa. For example, the three dimensional cubes polar is the octahedron. Six 
facets and eight vertice s corresp ond to eight facets and six vertices. 
The size and shape of a polar tends to be the reverse of that of the original set. For example, a short 
bulging rectangle with corners at (100, 3), (100, 3), (100, 3), (100, 3) would have a tall compressed polar 
with corners at (1/100, 0), (0, 1/3). Also note that polars of simplices are simplices. 
2.2 Prop erties of a polar 
Some of the useful properties of a polar is summarised here. The properties will be illustrated using pictures . 
(C) = C (proof later).  
If C is origin-symmetric, then so is C. 
If A  B then B  A.
  
If A is scaled up, then A is scaled down.  
If the polar is low-dimensional, that would mean the original polytop e had to be unbounded in some  
directions. 
11-1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3.2 Polar of a Polar 
Well use the above result to show why the polar of the polar of a convex body is the body itself. Recall 
that for a convex body K, we had dened its polar K to be {p|kp  1k  K}. 
Theorem 4 Let K be a convex body. Then K = K. 
Proof We know that K = {p|k  p  1k  K}. Similarly K = {y|p  y  1p  k}. Let y be any 
point in K. Then, by the denition of the polar, for all p  K we have that py  1. The denition of the  
polar of K implies that y  k. Since this is true for every y  K, we conclude that K  K. 
The other direction of the proof is the nontrivial one and well have to use the convexity of the body and 
the separating hyperplane theorem. If possible, let y be such that y  K and y  K. Since y  K, we 
have that Py  1p  K. Since y  K, there exists a strongly separating hyperplane for y and K. Let it  
be H = {x|v  x =1}. By the denition of separating hyperplane, we have v  k  1k  K. Hence, v  K. 
Also, v y&gt; 1 (since H is a separating hyperplane), and we just showed that v  K. This contradicts our  
assumption that y  K. Hence K  K. 
4 Norms and Symmetric Convex Bodies 
We will show how norms and symmetric convex bodies co-exist. This provides us a way to use the results 
of Convex Geometry in Functional Analysis and vice versa. Recall that a norm on Rn is a map q : Rn R
such that: 
1. q(ax) = aq(x) for a  R (homogeneit y) 
2. q(x + y)  q(x)+ q(y) (triangle inequalit y) 
3. q(x)  0 for all x (nonnegativit y) (actually implied by 1 and 2) 
4. q(x)=0 if and only if x = 0 (positivit y) (without this conditions, q is a seminorm) 
Note that given a norm, one can construct a convex body. The simplest being the unit ball Bq = {x 
Rn|q(x)  1}. It is an easy exercise to verify the convexity of Bq. 
Also as we will show now, given a convex body C, we can come up with a norm under which C is the 
unit ball. Note that C has to be origin symmetric. 
Denition 5 The Minkow ski functional of an origin symmetric convex body C is the map pC : Rn R 
dene d by  
pC (x) = inf 
&gt;0{x  C} 
(We will sometimes denote this by ||x||C , because it is a norm.) 
To prove that this is a norm, one needs to verify the properties of homogeneit y, triangle inequalit y etc. These 
follow from the convexity of the body. 
4.1 Norms, Duals, and the Polar 
For any norm q, we can dene its dual by q(x) = supv=0| qv
(
vx 
) |. It is an exercise to see that the unit ball 
with respect to the dual norm of q is the polar of the unit ball with respect to q. This provides us a direct 
relation between convex geometry and functional analysis. 
11-3</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The following pictures allow us to have a geometric intuition of the norms and their duals. 
11-4</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Observ e that E = in 
=1 i 2yi 2 Also, condition 2 of Theorem 8 is equiv alent to the followmT {Y | 1}. 
ing: i=1 ciuiui = Idn, where Idn is the identity matrix of size n. Now, since ui ui = 1, we have m n n  
Trace( i=1 ciuiuT
i )= i=1 ci. Since Trace(Idn)= n, this implies that i=1 ci = n. 
Let ej denote the vector which has a 1 in the ith coordinate and 0 in the other coordinates. Clearly 
ui,ej  is the jth coordinate of ui. For i  i  m, since ui  E, we get that n i 2ui,ej 2  1. Summing j=1 
it over all i, we get 
mn n
2 
i ui,ej 2  ci = n. 
i=1 j=1 i=1 
m 2 nHowever, since by condition 2 of Theorem 8, i=1ui,ej = ej 2, we get 2 By the AM-GM 
P 
=1 2 ||
i=1 i  n. 
i n
i n 2)1/n 
i ninequalit y, we get that ( i  1. Equalit y only holds if 
, completing the rst part  i=1  1, which implies that n i=1 nall the i are equal. This shows that i &lt; 1 for any such E which is not Bn 
i=1 2 
of the proof.
Proof of 2: Assume that we are give that B2 n is the unique ellipsoid of maximal volume that is contained in
K. We want to show that for some m, there exist ci and ui for 1  i  m (as in the stateme nt of Theorem 
m 2 28), such that for all vectors x, i=1 cix, ui= |x|. This is equiv alent to showing that 
m
ciuiu T
i = Idn. 
i=1 
Also, taking trace of both sides, we get that m = n. We already observ ed that for origin-symmetric  i=1 ci mbodies, the condition that i=1 ciui =0, is implied by the previous condition. 
Let UiT . Also, observ e that we can view the space of nn matrices as a vector of n2 real numbers. = uiui 
2 Hence we can parametrize the space of n  n matrices by Rn . Hence mT = Idn means that Idn/ni=1 ciuiui 
is in the convex hull of the Ui (recall that the ci are positiv e and sum to 1). 
mTIf possible, let there be no ci,ui such that i=1 ciuiui = Idn. This means that Idn/n is not in the convex 
hull of the Ui. Hence, there must be a separating hyperplane H in the space of matrices that separates Idn/n 
from the convex hull of the Ui.  2For two n  n matrices A and B, let A  B denote their dot product in Rn , i.e. A  B = i,j Aij  Bij . 
Thus, the separating hyperplane is a matrix H such that A  conv(Ui),A H  1, and Idn/n H&lt; 1. 
Let t = Trace(H)= H Idn. Let H = H  t/n(Idn). Then Idn/n H = Idn/n (H  t/nIdn)= 
t/n  (Idn/n t/nIdn) = 0. Similarly, since A  conv(Ui), Trace(A) = 1, we get that AH &gt; 0. Hence, 
H is such that: 
1. Trace(H) = 0, and 
2. H  (uiuiT ) &gt; 0 for all i. 
Now, let E = {x  Rn|xT (Idn +H)x  1. For all i, we have uiT (Idn +H)ui = 1+uiT Hui &gt; 1, since 
H  (uiuT ) &gt; 0  uT
i Hui &gt; 0. Hence ui  E. Also, since H  (uiuT
i ) &gt; 0 for all i, by compactness, there i 
exists &gt; 0 such that for all matrices w in the -neigh borhood of the set of all ui satisfy H  (wwT ) &gt; 0. 
Hence, by the previous argume nt, any such w is not contained in E. 
Note that when  = 0, we get the unit ball B2 n . For every &gt; 0 we have that for all w in the 
neigh borhood of the contact points of B2 n , w  E. Hence, as we increase  continuously starting from 0, 
the continuity of the transformation of E implies that for sucien tly small , boundary(K)  E = . 
Hence  &gt; 0 such that (1 + )E  K. Therefore, to conclude the proof, it suces to show that 
V ol(E  V ol(Bn).2 Let 1,2,...,n be the eigen values of Idn + H. Since V ol(E =( n i)1, to show that V ol(E i=1 
V ol(Bn), we need to show that n i  1. However we know that n i = Trace(Idn + H)= 2 i=1   i=1 Trace(Idn)= n. By the AM-GM inequalit y, ( n
i=1 i)1/n  ( n
i=1 i)/n =1. Hence n
i=1 i  1. This 
concludes the proof of part 2. 
To wrap up the proof of Johns Theorem, assume without loss of generalit y that B2 n is the ellipsoid of 
maximal volume contained in K. We can make this assumption since the particular choice of basis is not 
11-8</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>11-5</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>importan t for the proof. We need to show that Bn  K nBn Now, for all x  K, we have xui  1 for 2 2 . 
2 ui)2all i. Henc e, |x|=  ci(x   ci = n. This shows that |x|n, and hence K 
B n 
2 .  
Thus, we have proven the existence of an ellipse E such that 
E  K nE. 
11-9</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>5 Banac hMazur Distance 
Recall from last time the denition of the Banac hMazur distance between two convex bodies: 
Denition 6 Let K and L be two convex bodies. The BanachMazur distanc e d(K, L) is the least positive 
d  R for which theres a linear image L of L such that L  K  dL, wher e dL is the convex body obtaine d 
by multiplying every vector in L by the scalar d. 
Observ e that the above deni tion takes into consideration only the intrinsic shape of the body, and it is 
indep enden t of any particular choice of coordinate system. Also observ e that the Banac hMazur distance is 
symmetric in its input argumen ts. If L  K  dL, then by scaling everyth ing by d, we get that dL  dK. 
Hence K  dL  dK, which implies the symmetry property. 
6 Fritz Johns Theorem 
Let B2 n denote the n-dimensional unit ball. For any two convex bodies K and K, let d(K, K) denote the 
Banac hMazur distance between them. In the rest of this lecture, well state and prove the Fritz Johns 
theorem. 
Theorem 7 For any n-dimensional, origin-sy mmetric convex body K, d(K, Bn) n.2 
11-6</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Diameters and eigenvalues, expander graphs</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe6/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>/radicalBigg 
/summationdisplay 
/summationdisplay 1.1 A First Bound 
Claim 4 For any G, 
ln(n)  (2) 
where  is the diameter of G, n is the number of vertices in the graph, and  =1  2, where 2 is the 
second largest eigenvalue of the M associated with G. 
Claim 4 means that , up to a logarithmic factor, really does provide a direct bound on the diameter of 
the graph. For most graphs, this isnt very tight, but its a good place to start. So, why is it true? 
Pro of We will use random walks to prove claim 4. Let u and v be vertices that are as far as possible from 
one another . Start a random walk at u and let pt(v) be the probabilit y of a walk being at vertex v at time 
t.If pt(v) &gt; 0, then   t. Intuitively, this means that if we start at u and, after t time steps, there is some 
probabilit y of ending up at v (the farthest vertex from u), then there must be a path of length t between 
the two. If there wasnt, pt(v) would be 0. 
Recalling that the stationary distribution is (v)=1/n for regular graphs, we can equivalently state that 
if |pt(v)  (v)| &lt; 1 , then   t. Why? Because if |pt(v)  (v)| &lt; 1 , then pt(v) &gt; 0, implying   t. n n 
Recall from our earlier lecture on random walks that 
|pt(v)  (v)| &lt; (1  )t 
mind
y (v
d)
(y) =(1  )t 
Since G is regular, d(v)= d for all vertices (allowing the last equalit y above). Well now look at what 
happens when we set t = ln 
n : /parenleftbigg/parenrightbiggln n 
ln n 1 1 
 (1  )t =(1  ) &lt; = e n 
With the inequalit y coming from the fact that (1  )1/ &lt; 1 
e for all &gt; 0. Thus, for t = ln 
n , we have that 
|pt(v)  (v)| &lt; 1 , and therefore,   t = ln n . n  
1.2 A better bound 
As stated earlier, bounding  by n 1 is not that great. So, can we do better? Yes. And we do so by using 
an important trick that frequen tly comes up. First, note that if the (u,v)th entry of Ak is non-zero, then 
theres a path of at most length k from u to v. Replacing A with M doesnt change this (it just makes the 
non-zero entries smaller). If eu and ev are basis vectors, then 
11 |pk(v)  (v)| = |e T Mk | &lt;v eu nn 
which would imply   k. Lets then let p(x) be a polynomial of degree k: 
k 
p(x)= cj xj 
j=1 
Note that we can also interpret M as a variable and apply p to it as follows: 
k 
p(M)= cj Mj 
j=1 
If p(M) has no zero entries, then   k. Why? Because all non-zero elemen ts of M indicate all vertices 
that can be reached in one step, M2 is all vertices that can be reached in two steps, and so on. Thus, 
6-2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>/summationdisplay 
/summationdisplay 
/summationdisplay /summationdisplay /summationdisplay /summationdisplay /summationdisplay /summationdisplay 
/parenleftBigg /summationdisplay /parenrightBigg 
/summationdisplay 
/summationdisplay 
/summationdisplay 
/vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /summationdisplay /vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle 
/summationdisplay 
/summationdisplay 
/summationtext for any non-zero entry (u,v)in p(M), there must have been a non-zero elemen t in Mj for 0 &lt;j  k, 
implying the existence of a path from u to v of at most length k. If this is true for all entries in p(M), then 
a path of at most length k exists from any vertex to any other vertex, which means the diameter is at most k. 2 
Claim 5 Suppose p has degree k, p(1)=1, and |p(i)|&lt; 1 for all i 2, then n 
 k (3) 
Pro of For this proof, it is sucien t to show that every entry in p(M) is non-zero. First, recall that we 
can write down any matrix, M, as the following: 
M = iviviT 
i 
where i and vi are the i-th eigenvalue and eigenvector, respectively. Since 
Mk = k
i viviT 
i 
we can write 
  
k k k 
p(M)= cj Mj = cj j 
i viv T 
i =cj j 
i  viv T = T p(i)vivi i 
j=0 j=0 i i j=0 i 
Therefore, we can write the (a,b)th entry of p(M) as follows 
e T
a p(M)eb = e T
a p(i)viviT eb 
i 
= p(i)(e T
a vi)(viT eb) 
i 
= p(i)vi(a)vi(b) 
i 
n1 + p(i)vi(a)vi(b) = n i=2 
n1   n p(i)vi(a)vi(b) 
i=2 
n1 |p(i)||vi(a)||vi(b)|   n i=2 
n 
 1 max |p(i)|n i2 |vi(a)||vi(b)|
i=2 
 1 
n max 
i2 |p(i)| 
&gt; 0 
Where the penultimate step follows from |vi(a)||vi(b)| 1. Let V be the matrix where rows are the i 
eigenvectors vis. Then V V T = I by the orthonormal condition. It follows that V T V = I and the columns 
2Note that were not saying anything about  if p(M) has zero entries. Since there are no restrictions on cj , its possible 
that the summation produces a zero entry for p(M) where for all positiv e cj a non-zero entry would have existed. 
6-3</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>/vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /radicalBig /vextendsingle /vextendsingle /vextendsingle 
/vextendsingle/vextendsingle /vextendsingle /vextendsingle /vextendsingle 
/radicalBig /vextendsingle /vextendsingle /vextendsingle 
/vextendsingle/vextendsingle /vextendsingle /vextendsingle 
/vextendsingle2.2 Do Expanders Exist? 
A natural question is if expanders exist, what are the required parameters of the graphs? It turns out that 
random graphs are expanders, and so, almost all graphs are expanders. But, theres a limit as to how good 
of an expander you can have: 
Claim 9 For any G,  
2  d  2 d  1+ o(1) (5) 
In other words, even though 2 is always larger than a constan t, theres a limit as to how well-connected 
it can be. We wont prove this. But this is not that strong a bound. We already know that 2  d. This is  
just O( d) smaller. Furthermore, this bound is tight since Raman ujan graphs meet it. So thats the limit 
of what an expander can be. 
2.3 Expanders and Randomness 
Expanders are all over the study of randomness, but well just study one interesting property. Well use 
2 = d  2 to simplify formulas, where now, 2 is the second largest eigenvalue of the adjacency matrix. 
Suppose you make a graph by randomly including each edge with probabilit y d/n. In other words, construct 
a graph such that each vertex has an expected number of d edges leaving it. Since the total number of 
possible edges is |S||T | and theres a d/n probabilit y of having each edge, the expected number of edges 
between any two sets S and T will be d|S
n||T | . 
Claim 10 Expander Mixing Lemma: If you choose any two vertex sets, S and T , the dierence in the total 
number of edges between the two and the expected number for a random graph is bounded. Formally, 
e(S,T )  d|S||T | 2    |S| S |T | T n n 
 S  T ) min(|S|, )  min(|T |,  2 
This is surprising because there is no randomness here. This is just a property associated with expanders, 
but it behaves similarly to random graphs. 
Pro of Let  and  be the fraction of total vertices that are in the sets S and T : 
|S| = n |T | = n. 
Let x and y be the characteristic vectors of S and T , respectively. 
Denition 11 A characteristic vector, x, of a set S is a vector of length n that has xi =1 if vi  S, and 
xi =0 otherwise. 
We can now write the number of edges between sets S and T as e(S,T )= xT Ay. Now, as youve probably 
noticed, its benecial to use vectors that are perpendicular to the all-ones vector, 1. So, well rewrite x and 
y as 
v = x  1 w = y  1. 
Clearly , v  1 = w  1 = 0, implying orthogonalit y. Rewriting the number of edges between sets S and T ,we 
get 
e(S,T )= x T Ay 
=(v + 1)T A(w + 1) 
= v T Aw + v T A1 + 1T Aw + 1T A1. 
6-6</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>/vextendsingle /vextendsingle 
/parenleftBig /parenrightBig n n nof V form an orthonormal basis. Hence ( /summationtext|vi(a)||vi(b)|)2  /parenleftbig/summationtext|vi(a)|2 /parenrightbig/parenleftbig/summationtext|vi(b)|2 /parenrightbig 
 1. Thei=2 i=2 i=2
ultimate step follows from our assumption that |p(i)| &lt; 1 for all i  2. Thus, if p(1)=1 and |p(i)| &lt; 1 
n n 
for all i 2, we have that every entry in p(M) is non-zero, implying that   k. 
Claim 6 For any t  (0,1), I assert the existenc e of a magic polynomial, pk (t), with the following properties: 
1. p(
kt) is of degree k 
2. pk (t)(1)=1 
3. /vextendsingle/vextendsingle 
p(
kt)(x)/vextendsingle/vextendsingle 
 2 /parenleftbig 
1+  
2t /parenrightbigk for any x [0,1  t] 
We will provide no proof for this claim here, but the polynomials are derived from Chebyshev polynomials, 
and well use them again later. To provide some intuition, gure 1 shows graphs of these polynomials for 
k = 10 with varying t. Notice that to keep the same bound for smaller values of t, a larger k is required due 
to the oscillations that the polynomial must take on in order to achieve p(1) = 1 while keeping p(x) small 
for x [0,1  t]. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 
0.99 0.991 0.992 0.993 0.994 0.995 0.996 0.997 0.998 0.999 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 
(a) (b) (c) 
Figure 1: (a) t=0.1 (b) t=0.001 (c) t=0.001 zoomed in for x from 0.99 to 1 
If we set t= , we get a degree k polynomial, p, such that 
1. p(1)=1 
/parenleftBig  /parenrightBigk 
2. |p(x)| 21+ 2 for any x [0,2]. 
Additionally , if we set k = 1+ 1 ln(2n), then it is possible to show that p(x) &lt; 1 for all x  [0,2],2 n 
which gives the following bound: /parenleftbigg /parenrightbigg1   1+  ln(2n) (4)
2 
This is much better than our previous bound of   ln(n) . So, strangely , by putting in a particular   
polynomial, we get a bound that grows with 1/ as opposed to just 1/. This foreshado ws our next unit 
on iterativ e linear algebra. 
1.3 Example Application 
Suppose that you have a symmetric matrix M and want the eigenvector associated with the largest eigenvalue. 
For purposes of this example, let them be normalized such that the largest eigenvalue is 1. Then an easy 
way to get an approximate answer for the eigenvector is to compute Mkx for a large k and a random x. 
6-4</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>1 18.409 An Algorithmists Toolkit Septem ber 29, 2009 
Lecture 6 
Lecturer: Jonathan Kelner Scribe: Anthony Kim(2009) 
Topics 
 Diameters and their relationship to 2 
 Expanders 
 Buttery networks 
Diameters and Eigen values 
So far, every time weve dealt with eigenvalues, its had something to do with connectivit y. For example, 
the spectral gap can be used to approximate the qualit y of cuts; it also describ es how well a graph can mix 
under a random walk. They both are saying similar things: the eigenvalue is saying how connected a graph 
is. A walk will mix quickly if theres a lot connected to everything else. The min-cut will likewise be large 
if theres a lot of connectivit y. 
For almost every reasonable property about a graph, theres something you can write down regarding its 
relation to the second eigenvalue of the Laplacian. Today, were going to show the relation between 2 and 
the diameter of a graph. 
Denition 1 The diameter, , is the longest, shortest path between any two vertices of a graph. In other 
words, we can dene the distanc e between two vertices u and v in G as the shortest path connecting the two. 
The diameter of G is the largest distanc e between any two vertices in G. 
Its not immediately clear why the diameter should be related to 2; the following provides intuition1: 
1. Well-connected graphs have big 2 
2. Well-connected graphs have a small  
3. So, graphs with big 2 should have small  
Before proceeding, well be making the following assumption: 
Assumption 2 G is a d-regular graph (this is for simplicity and not really a limiting assumption). 
Well also be utilizing lazy random walks in our investigation. As a reminder, 
Denition 3 A lazy random walk is simply a random walk along a graph with self loops added in: 
A I M = + (1)2d 2 /bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright /bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright 
Random Walk Self-lo ops 
where A is the graphs adjacency matrix and I is the identity matrix. 
Since were using adjacency matrices, the interesting eigenvalues will be close to 1. So, let 2 be the 
second largest eigenvalue of M and  be the gap (i.e.,  =1  2). 
1Note, however, that this is not a prop er syllogism 
6-1</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>/radicalBigg Why does this give the eigenvector associated with 1 = 1? If all other eigenvalues are less than 1, then for 
a large enough k, they will diminish in importance, until all that is left is v1. This is a very intuitive and 
natural algorithm that takes about 1/ steps to get close. 
But we just found a much faster algorithm! Assuming that we know some good bound on  (if we dont, 
we could easily search for it), we can compute pk ()(M)x instead of Mkx to get the dominan t eigenvector. 
This metho d converges much faster, and well get into this more in a few lectures. 
2 Expanders 
If you had to know one set of graphs in your life, these are the ones to know. They often are a counterexample 
to many long-standing conjectures. Also, they turn up literally everywhere. If you didnt know any better, 
you would think that they dont exist from the describ ed properties. But theyre almost every single graph. 
Specically , well be looking at families of d-regular graphs (Gn)n as n goes to innit y: 
Denition 7 (Gn)n is an expander family if 2(Gn) c for some constant c and for all n. 
Most of the graphs weve looked at are not expanders. For example, path graphs have 2 O(1/n2) 
and binary trees have 2 O(1/n). This means that 2 very quickly goes to zero as n  for both cases. 
Expanders dont have this property. Even as n , 2 stays above a constan t. Given this, its not clear 
that they should exist. 
Note: We should think of d as a constan t. In other words, well pick a d and study expanders in that family . 
2.1 Relating Expanders to Cuts 
The rst thing well look at is Cheegers inequalit y for expanders. Recall that 
2 (G)2 
For expanders, this implies c (G)2 
What does this mean? It means that any set, S, of vertices with |S|n/2 has at least (c/2)|S|edges 
leaving it. This is a strong property: for expanders, there are no small cuts that can be made in the graph. 
Every cut that balances the sizes of the sets of vertices cuts a constan t fraction of the edges in the graph. 
The other side of Cheegers inequalit y says 
(G)2 
(1) 2d 
Again, for expanders, this can be rewritten. 
cd (G)  2(1) 
Since d is a constan t, this says that the isoperimetry , , is also bounded above by a constan t. Nor
mally, theres a large gap between the upper-bound and lower-bounds in Cheegers inequalit y. Here weve 
sandwic hed  between two constan ts. Therefore, an equivalent denition of expanders is as follows: 
Denition 8 (Gn)n is expander family if (G) c/prime for some constant c/prime and all n. 
6-5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>We dont prove this, but the high level idea is the following: 
 Select a set of vertices from G. Call this set X. 
 Let Y be the set of vertices that are neither in X nor in N (X). In other words, Y = V \(N(X) X). 
 Now, by construction, we have that e(X,Y )=0. 
Algebra gets a little messy , but you can just plug the above into the expander mixing lemma to show  
this bound. It turns out also that for X/n small and  =2 d 1, we can achieve 
d N(X) |X|.4 
Why is this interesting? What this is saying is that for any set X, there are at least d/4 neighbors not in X. 
Since each vertex has d neighbors total, this bound is quite strong. It turns out that this is about as good 
as you can get with spectral graph theory . To see this, we will generalize the vertex expansion as follows. 
We want to show bounds of the form |N(S)| |S|. In other words, we want to say that the vertex 
expansion of G is greater than or equal to  for any S. Sometimes well only care about expansions of smaller 
sets (e.g., for |S|0.01n). 
Denition 13 G is an (,)-expander if for  &lt; 1 and all sets S with |S|n have |N(S)||S|. 
We showed that Raman ujan graphs are (,d/4) expanders for some constan t . Some applications need 
expansion greater than d/2 but with small (constan t) . These exist, but we cant prove better than d/2 
with spectral techniques.3 
2.6 Bipartite Expanders 
Many of the applications of expanders use bipartite expanders. These are just expanders that are bipartite 
graphs. It is easier to show that these exists (it will be a homew ork problem!). 
Denition 14 A d-regular bipartite graph is an (,)-expander if every set S on the left with |S|n has 
N(S) |S|. 
Whenev er  &lt; 1, there exists some d such that almost all d-regular graphs on n nodes (for n sucien tly 
large), are (,)-expanders. 
3It turns out that random graphs work here. In 2002, Capalb o, Reingold, Vadhan, and Wigderson gave an explicit construc
tion technique with expansion d  o(1). 
6-8</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>/vextendsingle /vextendsingle/vextendsingle /vextendsingle 
/radicalbig 
/radicalBig /vextendsingle /vextendsingle /vextendsingle /vextendsingle/vextendsingle /vextendsingle /vextendsingle 
/vextendsingle
/radicalbig 
/vextendsingle /vextendsingle /vextendsingle /vextendsingle
/radicalbig 
/radicalbig 
/radicalbig 
/radicalbig /radicalBig /vextendsingle /vextendsingle /vextendsingle /vextendsingle/vextendsingle /vextendsingle /vextendsingle 
/vextendsingleUsing the following identities, 
A1 = d1 1T A = d1T , 
we get 
e(S,T )= v T Aw + vT A1 + 1T Aw + 1T A1 
= v T Aw + vT d1 + d1T w + d1T 1 
= v T Aw + dn, 
where we have used the orthogonalit y of v and w with 1 (i.e., vT 1 = 0) to cancel out the middle two terms. 
We now have the following bound: 
vT Aw |e(S,T ) dn| = 
|v||Aw| 
w| |v|2|
2 (n)((1 )n)(n)((1 )n) = n 
2 |S|  S |T |  T , = n 
where the third line follows from the fact that w is orthogonal to 1, and thus, 2 
thatcanaect ,andthefourth linefollowsfromthefactthat (1 ).Toseethis,notethat || n  = w v
1 ||x /radicalBig 
(1 )2 + ||S  is the largest eigenvalue 
|v| = 
 ()2 S = 
= n(1 )2 +(1 )n2 
= n(1 )(1  + ) 
= n(1 ), 
and the same steps show that |w|= n(1 ). Thus, we have shown that |e(S,T ) dn| 2 |S|n  S |T |  T . 
2.4 Some Prop erties We Now Kno w 
 Random walks on expanders mix in a logarithmic number of steps 
 Expanders have logarithmic diameter 
 Expanders have a constan t isoperimetric number 
2.5 Vertex Expansion 
Weve discussed cutting a graph and looking at the number of edges cut. An equivalent way of thinking 
about a cut is to select a set of vertices and then count the number of edges with one vertex in the set and 
one out. Another useful metric can be obtained by counting the number of vertices that are neighbors of a 
set. In other words, for a set of vertices, X, let N(X), be the set of vertices, Y , such that (x,y) E such 
that x X and y X. 
Claim 12 
N(X)  d2|X|
2 +(d2 2)|X|/n 
6-7</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Local and almost-linear time clustering and partitioning (cont.), PageRank, introduction to sparsification</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe8/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>/prime 
/prime Pro of By denition, the vector x  pr(cv + dw) satises the following equation 
x = (cv + dw)+(1  )Wx . 
Let us verify that x/prime  cpr(v)+ dpr(w) satises the same equation: 
(cv + dw)+(1  )Wx/prime = (cv + dw)+(1  )W (cpr(v)+ dpr(w)) 
= cv +(1  )Wc pr(v)+ dw +(1  )Wd pr(w) 
= cpr(v)+ dpr(w) 
= x. 
By Proposition 2, the equation has a unique solution, so that x = x/prime and the result follows. 
Prop osition 4 (Comm utativit y with W ) pr(Ws)= W pr(s). 
Pro of By denition, the vector x  pr(Ws) satises the following equation 
x = (cv + dw)+(1  )Wx . 
Let us verify that x/prime  W pr(s) satises the same equation: 
(cv + dw)+(1  )Wx/prime = Ws +(1  )W 2 pr(s) 
= W (s +(1  )W pr(s)) 
= W pr(s) 
= x. 
By Proposition 2, the equation has a unique solution, so that x = x/prime and the result follows. 
As a corollary of Propositions 2 and 4, we deduce that pr(s) is the unique solution to 
pr(s)= s +(1  )pr(Ws) . (2) 
3.3 Appro ximating PageRank 
We would like to come up with a fast way to nd an approximation to the unique solution pr(s)of 
Equation (1). We now describ e an iterativ e procedure that does that. 
We maintain two vectors p, the appr oximation vector, and r, the error vector, that satisfy the following 
invariant 
p = pr(s  r) . 
Starting with initial values p = 0 and r = s, in each iteration, we pick a vertex u, and update the two vectors 
p and r to the new vectors p/prime and r/prime dened as follows: 
p /prime = p + r(u)u ,
r /prime = r  r(u)u +(1  )r(u)Wu .
The vector u is the char acteristic vector of u, i.e., the vector witha1i nthe coordinate corresp onding to 
vertex u and 0 elsewhere. Given a xed /epsilon1&gt; 0, we keep iterating as long as there exists some vertex u such 
that r(u)  /epsilon1d(u). 
First, we prove that each iteration of the algorithm preserv es the invariant p = pr(s  r). 
Prop osition 5 p/prime = pr(s  r/prime). 
8-4</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Therefore, a good primitiv e to construct an almost linear-time global algorithm is the following. Run a 
random walk starting from v, and, at each step, for every vertex w, approximate the probabilit y that the 
random walk is at w; then take the vertices with the k largest probabilit y masses as a possible cut. Repeat 
this until you get a good cut or you reach a predetermined limit. 
2.3 Obstacles 
We need a bound that says that our general strategy works, and that is why we proved the Lovasz-Simono vits 
theorem. However, the bound we have is global, i.e., it involves the conductance (G) and we do not have 
the time to compute 2 for the whole graph to bound the conductance. Moreo ver, if we exactly compute all 
the probabilities of the random walk, it will take too long. Finally , even if we approximate the probabilities, 
we would need a stronger bound, and the goodness of the approximation depends on the cluster size, which 
we do not know in advance. 
2.4 One Solution 
A reasonable solution goes as follows. We recall that the proof of the Lovasz-Simono vits theorem that we 
discussed last time used cuts on level sets of t . This implies that if a walk does not mix too quickly, 
we know that one of the cuts had bad conductance. Therefore, obtain the following corollary from the 
Lovasz-Simono vits theorem. 
Corollary 1 Let G =(V,E) be a conne cted, undir ected graph with m edges and let (x) be its stationary 
distribution Pdx 
dv . For every subset of vertic es W  V and and every time t,if x  wW dw and (W ) 
vV 
is the conductanc e of the cut (W, W ), then the following inequality holds: 
   1 t 
 p t(w)  (w)  min x, 2m  x 1  (W )2 .   2 wW 
Note that in the last lecture we stated a slightly weaker form of the theorem, where the conductance 
(W ) of the cut (W,W ) was replaced by the conductance (G)of the whole graph. Nevertheless, we did 
actually prove the stronger version stated above. 
The bound above has nothing to do with global properties of the graph. Therefore, we can use Corollary 1 
for local clustering in the following way. If after O  log 
m 2 
steps a set of vertices contains a constan t factor 
more than what it would have under the stationary distribution, then we can get a cut C such that (C)  . 
(The cut can be obtained by mapping the probabilities to the real line and cut like we did with v2 a few 
lectures ago). 
A problem with this approac h is that computing all the probabilities will be too slow. In particular, after 
only a few steps we will have too many nonzero values to keep track of. Lovasz and Simono vits proposed to 
simply zero out the smaller probabilities and then prove that it does not hurt much to do so. However, the 
analysis is really messy . Instead, Andersen, Chung, and Lang [1] propose an approac h that, instead of using 
the probabilit y vector of a lazy random walk, uses a slightly dieren t vector called PageRank; we discuss 
this approac h in the following section. 
(Note that for all of this to work we still need to prove a partial converse. Indeed, one can show that if 
there exists a cut C of conductance 2, then at least |C|/2 of its vertices will give a cut of conductance , 
otherwise the random walk would mix too quickly.) 
8-2</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>4 possibilities, incurring an additional cost that is only a logarithmic multiplicativ e factor. In conclusion, we 
can nd a globally optimal  (up to the usual squaring error times some log factors) by cutting o chunks 
of the graph and repeating. The total running time is almost linear because the running time on each chunk 
is almost linear in its volume. 
Caveat. In a random walk scheme, we need to take 1/ steps in order to get a cut of conductance  1/; hence, that takes time that is about (size of chunk)  poly(1/). Similarly , in a PageRank scheme, 
we need to take 1/ steps in order to get a cut of conductance 1/; again, that takes time that is about 
(size of chunk)  poly(1/). As a consequence, the algorithm will run in time that is almost linear times some 
poly(1/), which is almost linear only if  is at least polylog(n). Impro ving this for smaller conductances is 
still an open problem. 
Intro to Sparsication 
Sparsication is a technique used in dynamic graph algorithms to reduce the dependence of an algorithms 
time on the number of edges in a graph. We briey motivate this technique now, and will discuss it next 
time. 
Suppose that we have a graph G =(V,E) with m =(n2) edges. We would like to solve some cut 
problem (e.g., sparsest cut, min cut, s-t min cut). Most algorithms that solve these kinds of problems 
have running times that typically grow with m, the number of edges in the graph. As a consequence, such 
algorithms are much slower for dense graphs than for sparse graphs. 
It would be really nice if we could someho w throw out a lot of edges from G and still get an approximate 
answer, because the running time of the algorithm for the resulting graph will be close to that for a sparse 
graph. More precisely , is there any way to appro ximate our graph G with a sparse graph G/prime that has the 
property that all of its cuts have more or less the same size as the original graph G? 
To answer this question, next time we will introduce the idea of randomize d sampling . It is not a spectral 
technique, but we will discuss spectral techniques that impro ve it. 
References 
[1]	 Reid Andersen, Fan Chung, and Kevin Lang. Local Graph Partitioning using PageR ank Vectors . In FOCS 06: 
Pro ceedings of the 47th Ann ual IEEE Symp osium on Foundations of Computer Science, pages 475486, Wash
ington, DC, USA, 2006. IEEE Computer Society. Full version available at http://www.math.ucsd.edu/ ~fan/ 
wp/localpartfull.pdf. 8-2, 8-3 
[2] Gershgorin circle theorem. http://en.wikipedia.org/wiki/Gershgorin_circle_theorem 8-3 
[3]	 Nathan Linial and Avi Wigderson. Expander Graphs And Their Applic ations . http://www.math.ias.edu/ ~boaz/ 
ExpanderCourse/ 8-1 
[4]	 Laszl oLovasz and Mikl os Simono vits. The mixing rate of Markov chains, an isop erimetric inequality, and com
puting the volume. In FOCS 90: Pro ceedings of the 31st Ann ual IEEE Symp osium on Foundations of Computer 
Science, pages 346354, Washington, DC, USA, 1990. IEEE Computer Society. 8-1 
8-6</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Pro of By Proposition 3, it suces to show that p/prime + pr(r/prime)= p + pr(r). So let us verify that: 
p + pr(r)= p + pr(r  r(u)u)+ pr(r(u)u) 
= p + pr(r  r(u)u)+ r(u)u +(1  )pr(Wr(u)u) 
=(p + r(u)u)+ pr(r  r(u)u +(1  )r(u)Wu) 
= p /prime + pr(r /prime) . 
where the third equation resulted from an application of Equation (2). 
Next, we prove a bound on the error vector. 
Prop osition 6 ||r/prime||1 ||r||1  r(u). 
Pro of Using the triangle inequalit y, 
||r /prime||1 = ||r  r(u)u +(1  )r(u)Wu||1 ||r  r(u)u||1 +(1  )r(u)||Wu||1 . 
However, ||Wu||1  1. Indeed, the ith elemen t of Wu is 1 when i =/negationslashu and 1 when i = u. Therefore,2d(u) 2 
||r /prime||1 ||r||1  r(u)+(1  )r(u)= ||r||1  r(u) , 
as desired. 
Finally , we prove that the iterativ e procedure works. 
Theorem 7 Fix /epsilon1&gt; 0. Supp ose that in each iteration we pick a vertex u with the property that r(u)  /epsilon1d(u). 
Then the process terminates in O 1 iterations with vectors p and r that satisfy the following properties: /epsilon1 
1. max v dr(
(v
v)
)  /epsilon1. 
2. vol(supp(p))  1 , wher e supp(p) is the set of vertic es for which p is nonzer o and vol(S)   dx./epsilon1 xS 
Pro of Initially , ||r||1 = 1. By Proposition 6, ||r||1 decreases at each iteration by r(u), which by as
sumption is at least /epsilon1d(u). Therefore, since the degree of each vertex is at least 1, ||r||1 decreases at each 
1iteration by at least /epsilon1. We deduce that the algorithm must terminate in at most O /epsilon1 iterations. 
Next, by denition, the process terminates when there are no more vertices u such that r(u)  /epsilon1d(u). 
Therefore, condition (1) is automatically satised. 
Moreo ver, if we let T denote the number of iterations that the algorithm takes to terminate and let 
di denote the degree of the vertex picked in the ith step of the algorithm, then /epsilon1 
iT 
=1 di  1, so that T di  1 . Now note that every vertex in supp(p) must have been picked at least once during the i=1 /epsilon1 
execution of the algorithm, so that 
T  1 vol(supp(p))  di  ,/epsilon1 i=1 
thus showing (2), and completing the proof of the theorem. 
The theorem we just proved gives the approximation to the PageRank vector that we need, and we nally 
get a local clustering algorithm. Note that to nd a cut C we need /epsilon1 = O(1/vol(C)), so that the running 
time of the process is proportional to vol
 (C) . 
In order to obtain from this an almost-linear global partitioning algorithm, we do as follows. Let us 
suppose that (G)is polylog(n). If we pick a random vertex v in a cluster of vertices C with conductance 
2, we will nd with probabilit y at least 0.5 a set with volume at least vol(C)/2. However, this holds only 
if we use appropriate parameters  and /epsilon1, which we do not know! The x is to binary search over the 
8-5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3 PageRank 
3.1 Denition 
Consider an undirected1 connected graph G =(V,E). Recall that a simple random walk on G is a walk that, 
starting at some initial vertex, at each step moves from the curren t vertex to a randomly chosen neighbor 
of the vertex; a lazy random walk on G is a walk that, starting at some initial vertex, at each step with 0.5 
probabilit y stays on the curren t vertex and with 0.5 probabilit y moves from the curren t vertex to a randomly 
chosen neighbor of the vertex. 
We now consider a new Markov process that is a modication of a lazy random walk on a graph. Fix 
some distribution s over the vertices V of G and x a parameter   (0, 1) (called the telep ort probability ). 
Starting from some initial vertex, at each step of the process we do the following: with probabilit y 1   we 
take a step of a lazy random walk on G, and with probabilit y  we telep ort to a vertex drawn from s.For 
simplicit y, we will take s to be a single vertex, i.e., all the probabilit y mass is concen trated on one vertex. 
The process converges to a stationary distribution (because it corresp onds to an aperiodic, irreducible 
Markov chain). For consistency with [1], we denote this stationary distribution (which depends on the 
parameters s and )by pr(s) and call it the PageR ank vector; note that pr(s) is a vector in Rn, where 
n = |V |. Moreo ver, it is easy to see that the stationary distribution pr(s) is the unique solution to the 
following equation: 
pr(s)= s +(1  )W pr(s) , (1) 
where W is the transition matrix corresp onding to a lazy random walk on G. 
The point is that one can show that the Lovasz-Simono vits theorem and its corollary hold for the 
PageRank vector pr(s), where s corresp onds to the starting vertex and  corresp onds to the number 
of time steps. Hence, rephrasing the discussion in Section 2.4, we know that if a subset of vertices S contains 
more than a constan t factor more probabilit y under pr(s) than under the stationary distribution, then we 
can nd a cut with conductance O(  log vSd v ). Moreo ver, approximating the PageRank vector pr(s) 
is robust under smal l errors, because it is the solution of an equation rather than being the result of many 
successiv e computations each with approximations. 
Next, we prove some properties about the PageRank vector and then show how to approximate it. 
(Note that, just like before, we still need to prove a partial converse. Indeed, one can show that if there exists a cut C of conductance , then at least |C|/2 of its vertices will give a cut of conductance O()). 
3.2 Prop erties 
We now prove three properties about the stationary distribution pr. 
Prop osition 2 (Uniqueness) pr(s) is unique. 
Pro of We must show that Equation (1) has a unique solution. Rewrite the equation as (I  (1  
)W )pr(s)= s. The matrix I  (1  )W is strictly diagonally dominan t2 because the o-diagonal 
elemen ts in each column add up to 1/2, while each diagonal elemen t is 1  (1  )(1/2). By the Gershgorin 
circle theorem [2], it must be nonsingular, so that the equation has a unique solution. 
Proposition 2 allows us to extend the denition of PageRank: given any vector s  Rn , not necessarily a 
probabilit y distribution over the vertices of the graph, we dene pr(s) as the unique solution of Equation (1). 
Prop osition 3 (Linearit y) pr(cv + dw)= c  pr(v)+ d  pr(w). 
1Google uses the directed version, because hyperlinks go only one way.
2A matrix is strictly diagonal ly dominant if aii &gt; P|aji| for all i.
j /negationslash=i 
8-3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit Octob er 6, 2009 
Lecture 8 
Lectur er: Jonathan Kelner Scrib e:Alessandr o Chiesa (2009) 
1 Administrivia 
You should probably know that 
	the rst problem set (due Octob er 15) is posted on the class website, and 
	its hints are also posted there. 
Also, today in class there was a majority vote for posting problem sets earlier. Professor Kelner will post 
the problem sets from two years ago, but he reserv es the right to add new problems once a problem set has 
already been posted. 
Questions from last time. 
	What is a level set? The level set of a function corresp onding to a (xed) constan t c is the set of 
points in the functions domain whose image equals c. 
	What is a good reference on applic ations of expander graphs? A course taught by Nathan Linial and 
Avi Wigderson [3]. 
Plan for today. We use what we proved last time to obtain a local clustering algorithm from a random 
walk scheme. Then, noting that similar results to the ones proved last time also hold for PageRank, we 
obtain a second scheme that yields a second, better local clustering algorithm. Finally , we briey motivate 
the technique of sparsication, which we will discuss next time. 
2 Local and Almost Linear-Time Clustering and Partitioning 
2.1 Review of Local Clustering 
Let us briey review local clustering, which we introduced last time. Given a vertex v in some graph G,we 
would like know if v is contained in a cluster, i.e. a subset of vertices that denes a cut with low conductance. 
However, we want the running time of our algorithm to depend on the cluster size, and not on the size of 
the graph. Last time we mentioned that a good example of a problem of this sort is trying to nd a cluster 
of web pages around mit.edu; we surely do not want the running time of this task to depend on the number 
of sites created on the other side of the world. Let us make our goal a little bit more precise: in this lecture 
we will describ e an algorithm that, after running for time almost linear in K, outputs a cluster of size at 
least K/2 around the starting vertex, if such a cluster exists. 
2.2 General Strategy 
We observ e that if we run a random walk starting from some vertex v contained in a cluster, then low-
conductance cuts will be an obstacle to mixing; i.e., the random walk has trouble leaving the cluster. Hence, 
a good guess for the cluster is the set of vertices with the highest probabilit y masses after a given number 
of steps (of a random walk that started at v). Last time we showed that this makes sense by proving the 
Lovasz-Simono vits theorem [4]. 
8-1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Iterative methods to solve linear systems, steepest descent</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>and the matrix AT A is positive denite. 
It is worth noting that while it is clear that the above reduction is theoretically valid, it is less clear 
whether or not such a reduction is practical. While the matrix product AT A has the advantage of positive 
deniteness, it raises several other concerns. For one, matrix multiplication could be as expensive as solving 
the system in the rst place and could destro y sparsit y properties. Additionally , one might worry about the 
eects of replacing A with AT A on convergence speed and condition number. As we shall see, however, the 
trick to getting around these issues is to never actually compute AT A. Instead, since our algorithms will 
only use this matrix in the context of multiplying by a vector, we can perform such multiplications from 
right to left via two matrix-v ector multiplications, thus avoiding the much more expensive matrix-matrix 
multiplication. 
3.2 Converting a Linear Problem to a Quadratic One 
Having assumed now that we are dealing with a symmetric positive denite matrix A, we can recast our 
linear system Ax = b as the condition that the vector x minimizes the quadratic form 
1 f (x)= x T Ax  bx + c.2 
Indeed, the gradien t of f is given by 
1 f (x)= (A + AT )x  b = Ax  b2 
because A is symmetric, and since A is positive denite, the quadratic form f is strictly convex, hence has 
a unique minimizer x given by f(x) = 0. In this case, level (contour) sets of f(x) are ellipsoids with axes 
along the eigenvectors of A and axis lengths inversely proportional to the eigenvalues of A. 
What happens if our assumptions on A are violated? If A is nonsymmetric, vanishing of the gradien t is 
no longer equivalent to the condition Ax = b: instead, we get 21 (A + AT )x = b.If A is negativ e denite, 
f(x) = 0 gives a maxim um rather than a minim um, and if A is symmetric but neither positive nor 
negativ e denite, then vanishing of the gradien t generally gives a saddle point. For more geometric intuition 
and gures (some of which are reproduced in the lecture slides), we refer to [1]. 
4 Steep est Descen t 
4.1 Motiv ation 
We now discuss the technique of steepest descen t, also known as gradient descent, which is a general iterativ e 
metho d for nding local minima of a function f. The idea is that given a curren t estimate xi, the gradien t 
f(xi)or more precisely , its negativ egiv es the direction in which f is decreasing most rapidly . Hence, 
one would expect that taking a step in this direction should bring us closer to the minim um we seek. Keeping 
with our previous notation, we will let x denote the actual minimizer, xi denote our i-th estimate, and 
ei = xi  x, (3) 
ri = b  Ax i = Ae i (4) 
denote the i-th error term and residual, respectively. 
The question now is how to decide what step size to use at each iteration. A logical approac h is to choose 
the step i such that the updated estimate xi+1 = xi  if (xi) minimizes f (xi+1) among all such xi+1.In 
general, the solution to this line search may or may not have a closed form, but in our case of f a quadratic 
form, we can determine the minimizing i explicitly . Indeed, we need only notice that at the minim um along 
a line, the gradien t is orthogonal to the line. Now the negativ e gradien t at the i + 1-st step 
f (xi+1)= b  Ax i+1 = ri+1 
21-4</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Note that x is a xed point of this recurrence because it leaves zero residual: r = b  Ax = 0 by denition 
of x. In other words, x = Mx + z. 
Now dene the error at step k to be ek = xk  x and observ e 
ek+1 = xk+1  x 
= Mx k + z  x 
= M(x + ek)+ z  x 
=(Mx + z  x)+ Me k 
= Me k. 
It follows immediately that ek = M k1e1, and in fact 
ek = Mk x, 
since we could have started our iteration at x0 = 0 in which case e0 = x. Thus, we can think of the error 
growing roughly as a matrix power1 . We pause here to make a denition. 
Denition 1 The spectral radius  of a symmetric matrix M is the absolute value of its largest eigenvalue: 
 = |max|. 
Observ e that it follows from the denition that (in the symmetric case) 
||Mn x||  n||x||, 
so if &lt; 1, then powers of M converge exponentially to zero at a rate given by . The same holds for general 
M if we replace eigen value by singular value. Summarizing, we have the following result. 
Theorem 2 Suppose A is a square matrix admitting a decomposition A = L + S where L is invertible and 
the largest singular value of L1S has absolute value &lt; 1. Then the iteration given by (1), (2) for solving 
Ax = b converges to the correct answer as k . 
2.5 Further Remarks 
As a side note, the two specic examples we began with are cases of Jacobi iteration, in which the matrix A 
is decomp osed as D + S with D diagonal and S small; and Gauss-Seidel iteration, where A = L + S with L 
lower triangular and S small. 
Also, one may wonder why we want to work specically with matrices that look like these. One good 
explanation is that in physics, many natural matrices tend to have larger diagonal values, since we are 
considering the transition matrix of a physical state near equilibrium. 
3 Setup for More Iterativ e Metho ds 
3.1 Assumptions 
For the remainder of this lecture, we will restrict our attention to solving Ax = b for n  n square matrices 
A that are symmetric and positive denite. Note that positive deniteness implies nonsingularit y. These 
conditions may at rst glance appear to be very restrictiv e, but in fact we claim we can reduce any nonde
generate square linear system to such a problem. Indeed, we need only observ e that for an invertible matrix 
A, 
Ax = b i AT Ax = AT b, 
1This is similar to our analysis of stablization in random walks! 
21-3</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>system    
100 1 4 100 
 100 100 3  x =  200  . 
100 100 100 300 
Again, while computing the exact answer would take some work, we can tell at a glance that the solution 
should be close to (1, 1, 1)T . In this case, the above-diagonal entries are all small, and once we ignore these, 
we can easily solve the remaining lower-triangular system. As before, we may now iterativ ely impro ve our 
solution by nding the error and repeating the procedure, converging geometrically to the correct answer. 
2.3 General Idea 
Why do both of these matrices work? One was almost diagonal while the other was almost lower-
triangular. This suggests that the important common attribute of the matrices A is the existence of a 
decomp osition 
A = L + S, 
where L is largeaccoun ting for most of Aand easy to invert, while S is small. We reason that 
L1 would thereb y be a good approximation of A1 . Therefore, we dene 
x1 = L1b, 
r1 = b  Ax 1. 
We can perform iterativ e updates according to 
xk+1 = xk + L1 rk, (1) 
rk+1 = b  Ax k+1. (2) 
In the k-th stage, xk is our curren t approximate solution to Ax = b and rk is called the residual . 
Note that this iterativ e approac h never requires us to invert A: instead, we need only know how to 
multiply vectors by L1 . Aside from this, only the (inexp ensive) operations of matrix-v ector multiplication 
and vector arithmetic are required. Thus, if we know an ecien t way of computing L1y given a vector 
yor alternativ ely, are given a black box that performs this operationthen we may infer a metho d for 
approximately solving Ax = b which may be much faster than the standard techniques for computing the 
exact solution. 
2.4 Analysis 
Of course, for this metho d to be useful, we need to know that our iterations do actually impro ve our estimate. 
We would also like a bound on the impro vement at each stage so that we know when to stop. To obtain 
these results, we need to make precise the notions of L and S being large and small. 
Consider the product 
L1A = L1(L + S)= I + L1S. 
This gives us some intuition that L1 should be a good approximation of A1 when L1S is small 
compared to the identity matrix I. Proceeding with the analysis, let x denote the actual solution to Ax = b. 
Substituting A = L + S,weget Lx = Sx + b, or equivalently, 
x = L1Sx + L1b. 
Dene M = L1S and z = L1b and observ e that we can rewrite our iterativ e step as the recurrence 
xk+1 = xk + L1 rk 
= xk + L1(b  Ax k) 
= xk + L1b  L1Lx k  L1Sx k 
= Mx k + z. 
21-2</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>/parenleftBigg /parenrightBigg Pro of Apply (5)(7) and the denition of the error (3) to nd 
T 
ei+1 = ei + r
Ti ri ri, (8) ri Ar i 
giving the change in the error from step i to step i + 1. In the case that ei is an eigenvector of A, say with 
eigenvalue , we have from (4) that ri = Ae i = e i, and hence (8) reduces to 
1 ei+1 = ei +(e i)=0.  
Remark The above result tells us that steepest descen t works instan tly for error vectors in the eigenspaces 
of A. These spaces have dimensions equal to the multiplicities of the corresp onding eigenvalues, and in 
particular, if A is a multiple of the identity, then steepest descen t converges immediately from any starting 
point. In general, we are not nearly so lucky and the eigenspaces each have dimension 1, but it is worth 
noting that even in this case convergence is qualitativ ely dieren t from that of our rst iterativ e approac h: 
there are particular directions along which steepest descen t works perfectly , whereas our rst approac h only 
gave the correct answer in the trivial case in which the error was already zero. 
In light of the preceding remark, we can expect that convergence should be faster along some directions 
than others, and we will see that this is indeed the case. Before jumping headlong into the convergence 
analysis, however, it is worthwhile to dene a more convenient measure of error. 
Denition 4 The energy norm of a vector e is given by 
||e||A = e T Ae. (9) 
Motiv ation for this denition will be provided in the next lecture; for now, we simply take for granted that 
it obeys the usual properties of a normand hence produces the same qualitativ e notion of convergence 
but lends itself to a cleaner convergence bounds. We will satisfy ourselv es with simply stating the result 
and focus on discussing its consequences, since the proof is just a computation using (8) and (9). A more 
intuitive line of reasoning will also come in the next lecture. 
Theorem 5 Let ei denote the error vector at step i of steepest descent. Let {vj }n be a normalize d eigenj=1 
basis of A with corresponding eigenvalues j , and let ei = /summationtext 
j j vj denote the expansion of ei with respect to 
this eigenbasis. Then 
(/summationtext 
j j 22 
j )2 
||ei+1||2 = ||ei||2 1  (/summationtext 23)(/summationtext 2j ) . (10)A A 
j j j j j 
The general result (10) is quite a mouthful, but fortunately we can understand its avor just by looking 
at the two-dimensional case. In this case we have only two eigenvectors v1 and v2. Assume 1 &gt; 2, so the 
condition number of A is  = 1/2. Dene  = 1/2 to be the ratio of the components of ei along the 
basis vectors. Then (10) simplies to 
||ei+1||2 (2 + 2)2 
A =1  ||ei||2 ( + 2)(3 + 2) . 
A 
Note that the form of the expression on the right corrob orates our preliminary observ ations. If the condition 
number  = 1, convergence occurs instan tly, and if  is close to 1, convergence occurs quickly for all values 
of .If  is large, convergence still occurs instan tly if  =0 or , but now the rate of convergence varies 
substan tially with , with the worst case being when ei is closer to the smaller eigenvector than the larger 
one by a factor of , i.e.,  =  (see the lecture slides or [1] for helpful pictures). 
21-6</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>turns out just to equal the i + 1-st residual, so our orthogonalit y relation reduces to the condition that 
successiv e residuals be orthogonal: 
riT 
+1ri =0. 
Expanding out 
ri+1	= b  Ax i+1 
= b  A(xi + iri) 
= ri  iAr i 
and substituting into the previous equation gives (using A = AT ) 
riT Ar i = (Ar i)T ri = riT ri, 
and thus we have a formula for computing the step size along ri in terms of just ri itself. 
Remark It is important to remem ber that the residuals ri = b  Ax i measure the dierence between 
our objective b and the result Ax i of our approximation in range space, whereas the errors ei = xi  x 
measure the dierence between our approximation and the true solution in domain space. Thus, the 
previous orthogonalit y relation that holds for residual vectors does not mean that successiv e error vectors 
in the domain are orthogonal. It does, however, imply that successiv e dierences between consecutiv e 
approximations are orthogonal because these dierences xi+1  xi = iri are proportional to the residuals. 
4.2	 Algorithm 
To summarize the developmen t thus far, we have obtained an iterativ e algorithm for steepest descen t with 
the following update step: 
ri = b  Ax i (5) 
i = rT 
i ri 
rT 
i Ar i (6) 
xi+1 = xi + iri. (7) 
As an implemen tation note, we point out that the runtime of this algorithm is dominated by the two 
matrix-v ector multiplications: Ax i (used to compute ri) and Ar i (used in nding the step size i). In fact, 
it is enough to do just the latter multiplication because as we saw before, we can alternativ ely write 
ri+1 = ri  iAr i, 
so that after the rst step we can nd residuals by reusing the computation Ar i, which was already done 
in the previous step. In practice, one needs to be careful about accum ulation of roundo errors, but this 
problem may be resolv ed by using (5) every once in a while to recalibrate. 
4.3	 Analysis 
Before dealing with general bounds on the rate of convergence of steepest descen t, we make the preliminary 
observ ation that in certain special cases, steepest descen t converges to the exact solution in just one step. 
More precisely , we make the following claim. 
Claim 3 If the current error vector ei is an eigenve ctor of A, then the subsequent descent step moves directly 
to the correct answer. That is, ei+1 =0. 
21-5</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>4.4 Some Motiv ation 
To summarize, we have seen that the performance of steepest descen t varies depending on the error direction 
and can sometimes be excellen t; however, in the worst case (obtained by maximizing the factor on the right 
side of (10) over all j ) convergence is still only geometric. 
The problem, as can be seen in the lecture gures, is that steepest descen t has the potential to zig-zag 
too much. We will see in the next lecture how the metho d of conjugate gradients overcomes this issue. The 
big idea here is that the so-called zig-zagging comes from situations when the ellipsoidal curves are very 
skew; the disparit y between the magnitudes of the axes of the ellipses causes us to take very tiny steps. Note 
we can then think of the energy norm is really a normalization of the ellipses into spheres, which removes 
this issue. 
References 
[1] Shewchuk, Jonathan. An Introduction to the Conjugate Gradien t Metho d Without the Agonizing Pain. 
August 1994. http://www.cs.cmu.edu/~jrs/jrspapers.html. 
21-7</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit November 21, 2009 
Lecture 21 
Lecturer: Jonathan Kelner Scribe: Yan Zhang 
1 Solving Linear Systems: A Brief Overview 
Given an invertible n  n matrix A and an n-vector b, we would like to solve the matrix equation Ax = b. 
One way to do so is to invert A and multiply both sides by A1 . While this approac h is theoretically valid, 
there are several problems with it in practice. Computing the inverse of a large matrix is expensive and 
susceptible to numerical error due to the nite precision of oating-p oint numbers. Moreo ver, matrices which 
occur in real problems tend to be sparse and one would hope to take advantage of such structure to reduce 
work, but matrix inversion destro ys sparsit y. 
So what would a numerical analyst do? A better metho d is Gaussian elimination, or equivalently, LU 
factorization followed by back-substitution. This technique is competitive when the matrix A is dense and 
unstructured, and it also has the advantage of allowing solution of Ax = b for multiple values of b with 
little additional work. However, it still fails to make use of the fact that systems encoun tered in practice are 
rarely dense and unstructured. As we will see in the next few lectures, iterativ e metho ds are the technique 
of choice for solving such systems. 
2 A First Iterativ e Metho d 
2.1 An Example 
Consider the system  
 100 
1 
4 3 
200 
3 2 
5 
100   x =   800 
1000 
500   . 
While computing the exact solution by hand would be a tedious task, it is a simple matter to nd an 
approximate solution. Roughly speaking, we expect the behavior of our system to be governed by the 
large diagonal entries of the matrix A, so if we just pretend that all o-diagonal entries are zero, the 
solution we obtain should still be reasonably close to the correct answer. Of course, once we ignore the 
o-diagonal entries, solving the system is easy, and we get as a rst approximation x1 =(8, 5, 5)T . 
How close does our approximation come to solving the system? Multiply A by x1 to get (805, 1033, 483)T . 
Subtracting from the desired result b, we nd that we are o by e1 =(5, 33, 17)T . Now this suggests a 
way to impro ve our estimate: since our system is linear, we can adjust our approximation x1 by applying 
the same technique as before with e1 on the right rather than b. Adding this adjustmen t gives an impro ved 
approximation x2 =(7.95, 4.835, 5.17)T , and clearly we can iterate the procedure as many times as we wish 
in hopes of obtaining better and better estimates converging to the true solution. It turns out that in this 
example one more iteration already achieves accuracy of about four signican t gures: our next approxima
tion is (7.9584, 4.8310, 5.1730)T , while the actual answer is (7.9585, 4.8309, 5.1734)T to four decimals. In fact, 
convergence is exponential: the number of correct digits increases linearly with the number of iterations. 
2.2 A Bit Harder 
One might argue that the above example was contrived, since our approximation scheme depended on the 
fact that the diagonal entries of A were much larger than the o-diagonal entries. However, consider the 
21-1</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Properties of the Laplacian, positive semidefinite matricies, spectra of common graphs, connection to the continuous Laplacian</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/resources/mit18_409f09_scribe2/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Proof 
  0 1 2 3 4P5R10
01 2 3
4
5
6 7 89
We will realize Pn as quoti ent of R2n. Supp ose z was an eigen vector of LR2n in which zi = z2n1i for 
0  i&lt;n. Take the rst n comp onents of z and call this vector v. Note that for 0 &lt;i&lt;n: 
[LPn v]i	= 2(vi  neigh bors of i in Pn) 
= 2(zi  neigh bors of i in R2n) 
=(zi  neigh bors of i in R2n)+(z2ni1  neigh bors of (2n  i  1) in R2n) 
1 = 2([LR2n z]i +[LR2n z]2ni1) 
1 = 2(zi + z2ni1) 
= zi
= vi
Now consider the case when i = 0. 
[LPn v]0	= v0  v1 
=2v0  v1 + v0 
=2z0  z1 + z0 
=2z0  z1 + z2n1 
= z0 
= v0 
Hence, v is an eigen vector of LPn . Now we show that such v exists, that is, there exists eigen vector z of 
LR2n in which zi = z2n1i for 0  i&lt;n. Take z, 
zk(u)	= sin(ku/n + /2n) 
= sin(ku/n) cos(/2n) + cos(ku/n) sin(/2n) 
= xk(u) cos(/2n) + yk sin(/2n) 
We see that zk is in the span of xk and yk. Hence, zk is an eigen vector of LR2n with an eigen value 
2  2 cos(2k/n) by lemma 16. Chec k that zk satises zk(i)= zk(2n  1  i) 
4.1	 Graph Products 
The next natural example is the grid graph, which will follow from general theory about product graphs. 
2-6</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>5.1 Discretizing Deriv atives, 1d case 
Consider a 1d function f : R  R, which we wish to discretize at the points (...,k2,k1, k, k+1,k+2,...): 
We appro ximate the rst deriv ative at the line midpoints, df , up to scaling, by taking the dierences dx 
between the values at the adjacen t points: 
The discrete rst deriv ative of f is a function on edges, and is, up to scaling Pn f, the incidence matrix 
d2fof Pn dened earlier. In order to compute the second deriv ative at the original points, dx2 , again up to 
scaling, we take the dierences of the adjacen t midpoints at the vertices: 
The discrete second deriv ative of f is thus, up to scaling, LPn f. 
5.2 Discretizing Deriv atives, 2d case 
Here we discretize f : R2 R on a grid:  
f(k,k)f(k+1,k+1)
f(k+1,k)
f(k+1,k-1) f(k,k-1) f(k-1,k-1)f(k-1,k)f(k-1,k+1) f(k,k+1)
To compute the discrete derivative in the x and y directions, well just look at a little piece . On the 
df df horizon tal edges, we appro ximate dx up to scaling, and do likewise on the vertical edges with dy : 
2-8</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Theorem 4 (Disjoin t Union Spectrum) If LG has eigenve ctors v1,...,vn with eigenvalues 1,...,n, 
and LH has eigenve ctors w1,...,wn with eigenvalues 1,...,n, then LGH has eigenve ctors: 
v1  0,...,vn  0, 0  w1,..., 0  wn 
with corresponding eigenvalues: 
1,...,n,1,...,n. 
Proof By the previous lemma, 
    
LG  H  (v1  0)= LG 0 v1 = 1v1 
0 LH 0 0 
Thus v1  0 is an eigen vector of LGH with eigen value 1. The rest follow by symmetry . 
3.2 The Laplacian of an edge 
Denition 5 Let Le be the Laplacian of the graph on n vertic es consisting of just the edge e. 
Example 1 If e is the edge (v1,v2), then 
  1 10 0
 10 0 
 1  
 0 00 0 Le =   .  . . .   .. ....  
0 00 0 
By additivit y, this lets us write:  
LG = Le (3) 
eE 
This will allow us to prove a number of facts about the Laplacian by proving them for one edge and 
adding them up. The more general technique, which well use more later, is to bound Laplacians by adding 
matrices of substructures. 
So for an edge e,  
Le = 1
1 1  [zeros] .1 
Note that:    
1 
11= 1  1 1  
=2 1
21 
1
2  1
2  
,1 1  
2 
 T 
and so 1
2  1
2 is an eigen vector with eigen value 2. This decomp osition implies: 
x T Lex =  x1 x2  
1
1  1 1  x
x1
2 =(x1  x2)2 . (4) 
Remark The Laplacian is a quadratic form, specically: 
x T LGx = x T ( Le)x = x T Lex = (xi  xj )2 (5) 
eE eE (i,j)E 
This implies that L is positive semidenite . 
2-2</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms. 18.409 Topics in Theoretical Computer Science: An Algorithmist's Toolkit
Fall 2009</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Example 2 The Laplacian and the Incidenc e matrix of the graph G= 
  34
2ab
c1
	  
 3 
11 
01 
01 
 1 10 0  
is LG =  1  =  10 10  	10 1 0  G 
10 0 1 10 0 1 
Lemma 10 LG = T . 
	   
Proof Observ e that T  ij =(ith column of )(jth column of )= e []e,vi []e,vj This gives 
three cases : 
When i = j,	
  2  T	 = [] = 1= deg(i).ij e,vi 
e e inciden t to vi 
When i = j and no edge exists between vi and vj , 
    
T  ij = []e,vi []e,vj =0 
e 
as every edge is non-inciden t to at least one of vi,vj . 
When i = j and exists an edge e between vi and vj , 
    
T  ij = []e,vi []e,vj =[]e,vi []e,vj = 1. 
e 
Corollary 11 Note that	 
x T LGx = ||x||2 = (xi  xj )2 , 
(i,j)E 
This gives another proof that L is PSD. 
3.5 Dimension of the Null Space 
Theorem 12 If G is conne cted, the null space is 1-dimensional and spanned by the vector 1. 
Proof Let x  null(L ), i.e. LGx = 0. This implies 
x T LGx = (xi  xj )2 =0. 
(i,j)E 
Thus, xi = xj for every (i, j)  E. As G is connected, this means that all xi are equal. Thus every member 
of the null space is a multiple of 1. 
Corollary 13 If G is conne cted, 2 &gt; 0. 
Corollary 14 The dimension of the null space of LG is exactly the numb er of connected components of G. 
2-4</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Denition 18 Let G =(V, E), and H =(W, F ). The product graph G  H has vertex set V  W and edge 
set: 
((v1,w), (v2,w)) , (v1,v2)  E, w  W and 
((v, w1), (v, w2)) , (w1,w2)  F, v  V. 
Example 3 Pn  Pm = Gn,m. We see that the vertic es of Pn  Pm are: 

 (v1, w1) (v1, w2)    (v1, wm) 
(v2, w1) (v2, w2)    (v2, wm) 
. . . . . . ... . . . 
(vn, w1) (vn, w2)    (vn, wm) 
 
v(Gn,m)= 
  
The vertic es are written in the above layout because it makes the edges intuitive. The edges are: 
 For a xed w, i.e. a column in the above layout, a copy of Pn. 
 For a xed v, i.e. a row in the above layout, a copy of Pm. 
Thus Pn  Pm = Gn,m, which is to say the product of two path graphs is the grid graph. 
Theorem 19 (Graph Products) If LG has eigenve ctors v1,...,vn with eigenvalues 1,...,n, and LH 
has eigenve ctors w1,...,wk with eigenvalues 1,...,k, then LGH has, for all 1  i  n, 1  j  k, an 
eigenve ctor: 
zij (v, w)= xi(v)yj (w) 
of eigenvalue i + j . 
Note importan tly that eigen values add here, they do not multipl y.
Proof Let Am be the graph with m isolated vertices. We can then decomp ose the product as:
G  H =(G  Ak)  (An  H), 
i.e. the edge union of k disjoin t copies of G and n disjoin t copies of H, exactly as in the denition. By 
Lemmas 1 and 3 we have 
LGH = LGAk + LAnH = LG  Ik + In  LH 
Consider zij = xi  yj as above for a xed i and j, we see that: 
LGH zij =(LG  Ik)(xi  yj )+(In  LH )(xi  yj ) 
=(ixi  yj )+(xi  j yj ) 
=(i + j )(xi  yj )=(i + j )zij . 
Corollary 20 Gn,m has eigenve ctors and eigenvalues completely determi ned by those of Pn and Pm as above. 
Why is this called the Laplacian? 
It turns out that the graph Laplacian is very naturally related to the continuous Laplacian. 
d 	In 1 dimension, the continuous Laplacian is dx .
d2 f + d2f
 In 2 dimensions, the continuous Laplacian is f = dx2 dy2 . 
2-7 5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>3.3 Review of Positiv e Semideniteness 
Denition 6 A symmetric matrix M is positiv e semidenite (PSD) if x  Rn , 
x T Mx  0. 
M is positiv e denite (PD) if the inequality is strict x =0. 
Lemma 7 M is PSD i all eigenvalues i  0. Similarly M is PD i all eigenvalues i &gt; 0. 
Proof Lets consider the matrix M in its eigen basis, that is M = QT Q. Clearly , yT y = i iyi 2  0 
for all y  Rn i i  0 for all i. Similar for PD matrix. 
Lemma 8 (PSD Matrix Decomp osition) M is PSD i there exists a matri x A such that 
M = AT A. (6) 
Note that A can be (n  k) for any k, and that it need not be squar e. Importantly, note that A is not unique. 
Proof 
()If M is positiv e semidenite, recall that M can be diagonalized as 
M = QT Q, 
thus T  
M = QT 1/21/2Q =1/2Q 1/2Q, 
where 1/2 has i on the diagonal. 
()If M = AT A, then 
x T Mx = x T AT Ax =(Ax)T (Ax) 
Letting y =(Ax)  Rk, we see that: 
x T Mx = y T y = ||y||2  0. 
3.4 Factoring the Laplacian 
We know from the previous section that we can factor L as AT A using eigen vectors , but there also exists a 
much nicer factorization which we will show here. 
Denition 9 Let m be the numb er of edges and n be the numb er of vertic es. Then the incidence matrix 
 = G is the m  n matrix given by: 
 
 1 if e =(v, w) and v&lt;w 
e,v =  1 if e =(v, w) and v&gt;w (7) 
0 otherwise. 
2-3</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>7 n  n  i d
1 i	(10) 
Proof By the previous slide and the fact that 1 = 0, we get n
i=2 i = i di. As 2    n, the 
bounds follow immediately . 
Bounding 2 and max 
Theorem 23 (Couran t-Fisc her Formula) For any n  n symmetric matrix A, 
xT Ax 1 = min x T Ax = min 
||x||=1 x=0 xT x 
xT Ax 2 = min x T Ax = min	 (11)x=0 xT x ||x||=1 
xv1 xv1 
xT Ax max = max x T Ax = max 
x=0 xT x ||x||=1 
Proof We consider the diagonalization A = QT Q. As seen earlier, xT Ax =(Qx)T (Qx). As Q is 
orthogonal, we also have ||Qx|| = x. Thus it suce s to consider diagonal matrices. Moreo ver, all of the 
equalities on the right follow immediately from linearit y. Thus we need to consider, for ||x|| = 1: 
  1 x1  
T x =(x1    .  
xix
i 2 
x xn)  ...  ..  = ixi 2 =  2 i . 
n xn 
We compute the gradien t and nd: 
  2ixi 2ix3 
i 3x x T x i =  x2 
i  
(  x2 
i )2 =2i(xi  xi ) 
thus all extremal values occur when one xi = 1 and the rest are 0. The identities follow immediately . 
Corollary 24 (Rayleigh Quotien t) Letting G =(V, E) be a graph with Laplacian LG, 
1 =	0 v1 = 1 
xT LGx (i,j)E (xi  xj )2 
2 =	min xT x = Pmin 2 xv1 x=0 iV xi (12)x=0 x=0 
xT LGx  
(i,j)E (xi  xj )2 
max = max = max  2 x=0xT x x=0iV xi 
The Rayleigh Quotien t is a useful tool for bounding graph spectra. Whereas before we had to consider 
all possible vectors x, now in order to get an upper bound on 2 we need only produce a vector with small 
Rayleigh quotien t. Likewise to get a lower bound on max we need only to nd a vector with large Rayleigh 
quotien t. 
Examples next lecture! 
2-10</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>4 Spectra of Some Common Graphs 
We compute the spectra of some graphs: 
Lemma 15 (Complete graph) The Laplacian for the complete graph Kn on n vertic es has eigenvalue 0 
with multiplicity 1 and eigenvalue n with multiplicity n  1 and associate d eigensp ace {x|x  1 =0}. 
Proof By corollary 13, we conclud e that eigen value 0 has multiplicit y 1. Now, take any vector v which is 
orthogonal to 1 and consider [LKn v]i. Note that this value is equal to 
(n  1)vi  vj = nvi  vj = nvi 
j=i j 
Hence any vector v which is orthogonal to 1 is an eigen vector with an eigen value n. 
Lemma 16 (Ring graph) The Laplacian for the ring graph Rn on n vertic es has eigenve ctors 
xk(u) = sin(2ku/n), and 
yk(u) = cos(2ku/n) 
for 0  k  n/2. Both xk and yk have eigenvalue 2  2 cos(2k/n). Note that, x0 = 0 should be ignor ed and 
y0 is 1, and when n is even xn/2 = 0 should be ignor ed and we only have yn/2. 
Proof . The best way to see is to plot the graph on the circle using these vectors as coordinates. Below is 
the plot for a k = 3. 
Just consider vertex 1. Keep in mind that sin(2x ) = 2 sin(x ) cos(x). Then, 
[Lxk]1	=2xk(1)  xk(0)  xk(2) 
= 2 sin(2k/n)  0  sin(2k2/n) 
= 2 sin(2k/n)  2 sin(2k/n) cos(2k/n) 
= (2  2 cos(2k/n)) sin(2k/n) 
= (2  2 cos(2k/n))xk(1) 
Note that this shows that x(u)= (e2i(ku+c)/n) is an eigen vector for any k  Z,c  C. 
Lemma 17 (Path graph) The Laplacian for the path graph Pn on n vertic es has the same eigenvalues as 
R2n and eigenve ctors 
vk(u) = sin(ku/n + /2n) 
for 0  k&lt;n 
2-5</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>18.409 An Algorithmists Toolkit September 15, 2007 
Lecture 2 
Lecturer: Jonathan Kelner Scribe: Mergen Nachin 2009 
1 Administrative Details 
 Signup online for scribing. 
2 Review of Lecture 1 
All of the following are covered in detail in the notes for Lecture 1: 
	The denition of LG, specically that LG = DG  AG, where DG is a diagonal matrix of degrees and 
AG is the adjacency matrix of graph G. 
 The action of LG on a vector x, namely that 
[LGx]i = deg(i) (xi  average of x on neighbors of i) 
	The eigenvalues of LG are 1    n with corresponding eigenvectors v1,...,vn. The rst and the 
most trivial eigenvector is v1 = 1 with an eigenvalue 1 = 0. We are mostly interested in v2 and 2. 
3 Properties of the Laplacian 
3.1 Some simple properties 
Lemma 1 (Edge Union) If G and H are two graphs on the same vertex set with disjoint edge sets, 
LGH = LG + LH (additivity)	 (1) 
Lemma 2 (Isolated Vertices) If a vertex i  G is isolated, then the corresponding row and column of the 
Laplacian are zero, i.e. [LG]i,j = [LG]j,i = 0 for all j. 
Lemma 3 (Disjoint Union) These together imply that the Laplacian of the disjoint union of G and H is 
direct sum of LG and LH , i.e.: 
 
 LL = L L= 
G 0 
G H G H 0 LH	 
(2) 
Proof Consider the graph G  v(H) = (VG  VH ,EG), namely the graph consisting of G along with the 
vertex set of H as disjoint vertices. Dene v(G)  H similarly. By the second remark, 
 	   LG 0	 0 0 LGv(H) = and L
. 0 0v(G)H = 0 LH 
By denition, G  H = (G  v(H))  (v(G)  H), and so by the rst remark: 
  
	 LG 0  LG H = LG  LH =
. 0 LH 
This implies the Laplacian is the direct sum of the Laplacians of the connected components. Thus, 
2-1</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>f(k+1,k)-f(k,k)
f(k,k)-f(k,k-1)f(k,k)-f(k-1,k)f(k,k+1)-f(k,k)Again, the discrete deriv ative of f is a function on edges. When we consider the concatenation of the 
two discretization of the directional deriv atives, we see that the discretization of the gradien t, up to scaling, 
is Gn,m f. Finally we use this to compute the discretized Laplacian, up to scaling, and get: 
f(k,k+1)+f(k,k-1)+f(k+1, k)+f(k-1, k)
 -4 f(k,k)
Thus the discretized Laplacian in two dimensions of f is LGn,m f. 
5.3 A Note on Fourier Coecien ts 
We observ ed that paths and rings had eigen vectors that looked like Fourier coecients. In the continuous 
case: 
d2 sin(kx + c) = k2 sin(kx + c)dx2 
d2 cos(kx + c) = k2 cos(kx + c)dx2 
Thus sin(kx + c) and cos(kx + c) are eigenfun ctions of the d2 operator, i.e. the 1d Laplacian, both with dx2 
eigen value k2 . 
6 Bounding Laplacian Eigen values 
Lemma 21 (Sum of the eigen values) Given an n-vertex graph G with degrees di, wher e dmax = maxi di, 
and Laplacian LG with eigenvalues i, 
i = di  dmaxn (8) 
ii 
Proof The rst two express ions are both the trace, the upper bound is trivial. 
Lemma 22 (Bounds on 2 and n) Given i and di as above, 
di2  n  i 
1 (9) 
2-9</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
