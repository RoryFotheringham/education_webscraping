<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/</course_url>
    <course_title>Introduction to Statistical Methods in Economics</course_title>
    <course_tags>
      <list>Mathematics </list>
      <list>Probability and Statistics </list>
      <list>Economics </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Conditional expectations and special distributions (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>Cumulative areas under the standard normal distribution 
0 z (Cont.) 
-3 
-2.9 
-2.8 
-2.7 
-2.6 
-2.5 -2.4 
-2.3 -2.2 
-2.1 
-2.0 -1.9 -1.8 -1.7 
-1.6 
-1.5 
-1.4 
-1.3 -1.2 
-1.1 
-1.0 -0.9 -0.8 -0.7 -0.6 
-0.5 
-0.4 
-0.3 
-0.2 -0.1 -0.0 0.0013 
0.0019 
0.0026 
0.0035 
0.0047 
0.0062 0.0082 
0.0107 0.0139 
0.0179 
0.0228 0.0287 0.0359 0.0446 
0.0548 
0.0668 
0.0808 
0.0968 
0.1151 
0.1357 0.1587 0.1841 
0.2119 
0.2420 0.2743 
0.3085 
0.3446 
0.3821 
0.4207 0.4602 0.5000 0.0013 
0.0018 
0.0025 
0.0034 
0.0045 
0.0060 0.0080 
0.0104 0.0136 
0.0174 
0.0222 0.0281 0.0352 0.0436 
0.0537 
0.0655 
0.0793 
0.0951 
0.1131 
0.1335 0.1562 0.1814 
0.2090 
0.2389 0.2709 
0.3050 
0.3409 
0.3783 
0.4168 0.4562 0.4960 0.0013 
0.0017 
0.0024 
0.0033 
0.0044 
0.0059 0.0078 
0.0102 0.0132 
0.0170 
0.0217 0.0274 0.0344 0.0427 
0.0526 
0.0643 
0.0778 
0.0934 
0.1112 
0.1314 0.1539 0.1788 
0.2061 
0.2358 0.2676 
0.3015 
0.3372 
0.3745 
0.4129 0.4522 0.4920 0.0012 
0.0017 
0.0023 
0.0032 
0.0043 
0.0057 0.0075 
0.0099 0.0129 
0.0166 
0.0212 0.0268 0.0336 0.0418 
0.0516 
0.0630 
0.0764 
0.0918 
0.1093 
0.1292 0.1515 0.1762 
0.2033 
0.2327 0.2643 
0.2981 
0.3336 
0.3707 
0.4090 0.4483 0.4880 0.0012 
0.0016 
0.0023 
0.0031 
0.0041 
0.0055 0.0073 
0.0096 0.0126 
0.0162 
0.0207 0.0262 0.0329 0.0409 
0.0505 
0.0618 
0.0749 
0.0901 
0.1075 
0.1271 0.1492 0.1736 
0.2005 
0.2297 0.2611 
0.2946 
0.3300 
0.3669 
0.4052 0.4443 0.4840 0.0011 
0.0016 
0.0022 
0.0030 
0.0040 
0.0054 0.0071 
0.0094 0.0122 
0.0158 
0.0202 0.0256 0.0322 0.0401 
0.0495 
0.0606 
0.0735 
0.0885 
0.1056 
0.1251 0.1469 0.1711 
0.1977 
0.2266 0.2578 
0.2912 
0.3264 
0.3632 
0.4013 0.4404 0.4801 0.0011 
0.0015 
0.0021 
0.0029 
0.0039 
0.0052 0.0069 
0.0091 0.0119 
0.0154 
0.0197 0.0250 0.0314 0.0392 
0.0485 
0.0594 
0.0722 
0.0869 
0.1038 
0.1230 0.1446 0.1685 
0.1949 
0.2236 0.2546 
0.2877 
0.3228 
0.3594 
0.3974 0.4364 0.4761 0.0011 
0.0015 
0.0021 
0.0028 
0.0038 
0.0051 0.0068 
0.0089 0.0116 
0.0150 
0.0192 0.0244 0.0307 0.0384 
0.0475 
0.0582 
0.0708 
0.0853 
0.1020 
0.1210 0.1423 0.1660 
0.1922 
0.2206 0.2514 
0.2843 
0.3192 
0.3557 
0.3936 0.4325 0.4721 0.0010 
0.0014 
0.0020 
0.0027 
0.0037 
0.0049 0.0066 
0.0087 0.0113 
0.0146 
0.0188 0.0238 0.0300 0.0375 
0.0465 
0.0570 
0.0694 
0.0838 
0.1003 
0.1190 0.1401 0.1635 
0.1894 
0.2177 0.2483 
0.2810 
0.3156 
0.3520 
0.3897 0.4286 0.4681 0.0010 
0.0014 
0.0019 
0.0026 
0.0036 
0.0048 0.0064 
0.0084 0.0110 
0.0143 
0.0183 0.0233 0.0294 0.0367 
0.0455 
0.0559 
0.0681 
0.0823 
0.0985 
0.1170 0.1379 0.1611 
0.1867 
0.2148 0.2451 
0.2776 
0.3112 
0.3483 
0.3859 0.4247 0.4641 
0 z 1 2 3 4 5 6 7 8 9 
14
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  This result is also known as the ANOVA identity, where ANOVA stands for Analysis of Variance. In 
particular, since Var(Y|X)0,itfollowsthat 
Var(Y)E[Var(Y|X)] 
which can, loosely speaking, be read as knowing X decreases the variance of Y. 
Proof: We can rewrite 
    
Var(E[Y|X])+E[Var(Y|X)] = E[E[Y|X]2](E[E[Y|X]])2 + EE[Y2|X] EE[Y|X]2 
= E[E[Y|X]2]E[Y]2 + E[Y2]E[E[Y|X]2] 
= E[Y2]E[Y]2 =Var(Y) 
wherethe rstequality usesthepropertyVarX = E[X2]E[X]2,thesecond step usesthelawofiterated 
expectations, and in the last step the rst and last term cancel, which completes the argument  
Example4 Each year, a rms R&amp;D department produces X innovations according to some random 
process, where E[X] =2 and Var(X) =2. Each invention is a commercial success with probability 
p=0.2 (assume independence). The number of commercial successes in a given year are denoted by S. 
(a) Supposewehave5 newinnovationsthisyear. Whatistheprobability that S of them are commercial 
successes? -the conditional p.d.f of S given X =5 is that of a binomial, so e.g. 
P(S =2|X = x)= 5
3(0.2)3(10.2)2 0.05 
(b) Given 5 innovations, what is the expected number of successes? -since S|X =5 B(5,0.2), we 
can use the result from last class 
E[SX =5] = E[B(5,0.2)] =0.25 =1 |  
(c) What is the unconditional expected value of innovations? -By the law of iterated expectations, 
E[S]= E[E[SX]]= E[pX]=0.2E[X]=0.22 =0.4 |  
since we assumed E[X]=2. 
(d) What is the unconditional variance of S? -Recall the law of total variance: 
Var(S) = Var(E[SX])+E[Var(SX)] | |
= Var(Xp)+E[p(1p)X]= p 2Var(X)+p(1p)E[X]=0.042+0.162 =0.4   
This is an example of a mixture of binomials, i.e. conditional on X, we have a binomial distribution for 
S. We can then use the law of iterated expectations to obtain the total number of successes. 
Example5 (IEM Political Markets, continued) If we look at the Republican primaries last year, much 
of the uncertainty already was resolved by Super Tuesday. Say, the conditioning variable Xt isthenumber 
4 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>	  Iowa 
New 
Hampshire Super 
Tuesday 
Ohio, 
Texas0 .2 .4 .6 .8 Clinton/Edwards/Obama 
1/01/08 2/01/08 3/01/08 
Date 
Clinton Edwards 
Obama 
	the Iowa caucus in which Barack Obama won against Hillary Clinton by a signicant margin 
	the New Hampshire primaries which were seen as Hillary Clintons comeback after the defeat in 
Iowa 
	the primaries in Ohio and Texas, two major states which were won by Hillary Clinton 
We can see steep changes in the conditional expectations after each of these events, illustrating how the 
market updates its beliefs about the candidates chances of securing their parties nomination. 
InFinancialEconomics,thistypeof contractswhichpays1dollarif aparticularstate of natureis realized 
are also called Arrow-Debreu securities. 
An important relationship between conditional and unconditional expectation is the Law of Iterated 
Expectations(aclose cousin oftheLawofTotalProbability which wesawearlierinthelecture): 
Proposition1 (Law of Iterated Expectations) 
E [E[YX]]= E[Y] |
Proof: Let g(x)= E[YX = x], which is a function ofx. We can now calculate the expectation |
 	   
E[g(X)] = g(x)fX(x)dx = E[YX = x]fX(x)dx 
	  |
=   
   
 y fXY (x,y) 
fX(x) dy  
fX(x)dx 
    
= yfXY (x,y)dydx 
      
= y fXY (x,y)dx dy 
    
= yfY (y)dy = E[Y] 
 
Proposition2 (Conditional Variance / Law of Total Variance) 
Var(Y)=Var(E[YX])+E[Var(YX)] | |
3 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>  
  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 14 
Konrad Menzel 
April 2, 2009 
Conditional Expectations 
Example1 Each year, a rms R&amp;D department produces X innovations according to some random 
process, where E[X] =2 and Var(X) =2. Each invention is a commercial success with probability 
p =0.2 (assume independence). The number of commercial successes in a given year are denoted by S. 
Since we know that the mean of S B(x,p)= xp, conditional on X = x innovations in a given year, xp 
of them should be successful on average. 
The conditional expectation of X given Y is the expectation of X taken over the conditional p.d.f.: 
Denition1 
E[YX]=   yfY |X(y|X) if Y isdiscrete y 
yfY|X(yX)dy if Y is continuous |
 |
Note that since fY|X(y|X)carries the random variable X as its argument, the conditional expectation is 
also a random variable. However, we can also dene the conditional expectation of Y given a particular 
value of X, 
yfY |X(yx) if Y isdiscrete yE[YX = x]=   yfY|X(y|
|x)dy if Y is continuous |
 
whichisjust a numberfor anygiven value of x as long as the conditional density is dened. 
Sincethecalculationgoesexactlylikebefore,onlythat wenowintegrateoverthe conditional distribution, 
wont do a numerical example (for the problem set, just apply denition). Instead lets discuss more 
qualitative examples to illustrate the dierence between conditional and unconditional examples: 
Example2 (The Market for Lemons) The following is a simplied version of a famous model for 
the market for used cars by the economist George Akerlof. Suppose that there are three types X of used 
cars: cars in an excellent state (melons), average-quality cars (average not in a strict, statistical, 
sense), and carsin apoor condition(lemons). Each type of caris equallyfrequent,i.e. 
1 P(lemon) = P(average) = P(melon) = 3 
Thesellerand abuyerhavethefollowing(dollar) valuations YS and YB, respectively, for each type of cars: 
Type Seller Buyer 
Lemon 5,000$ 6,000$ 
Average 6,000$ 10,000$ 
Melon 10,000$ 11,000$ 
1 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>    
  i.e. most of the mass of the distribution is within one or two standard deviations of the mean. Its useful 
to remember these three quantities in order to get rough estimates of normal probabilities if you dont 
have a tabulation of the c.d.f. at hand. 
    
   
68% of density 
95% of density 
99.7% of density 
Since the standard normal distribution is so commonly used, youll      Image by MIT OpenCourseWare.
nd tabulated values of the c.d.f. 
(z) in any statistics text. 
Often those tables only contain values z 0, but you can obtain the c.d.f. at a value z&gt; 0 using 
symmetry of the distribution: 
(z)=1(z) 
E.g. if we want to know P(Z 1.95), we can look up P(Z 1.95) =0.0256, and calculate P(Z 
1.95) =1P(Z 1.95) =0.9744. 
0 -z z area = (-z)area = 1 - (z)
= (-z)
Ingeneral,if X N(,), we can obtain probabilities of the typeP(a       Image by MIT OpenCourseWare.
Xb)forabusingthe
following steps: 
1. standardize the variable, i.e. rewriting the event as 
P(a X b)= P (a + Z b)= Pa  Z  b 
  
10       </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Values for D ie I 
Values for D ie II 
Values for D ie III 0.03 0.51 1.04 1.78 
2.29 2.51 2.77 3.25 
2.32 2.55 2.83 3.36 
2.35 2.59 2.90 3.49 
2.39 2.64 2.98 3.65 
2.43 2.68 3.06 4.00 
2.47 2.72 3.15 4.55 0.11 0.59 1.14 1.95 
0.19 0.67 1.25 2.15 
0.27 0.76 1.37 (2.40) 
0.35 0.85 1.50 (2.75) 
0.43 0.94 1.63 (3.60) 
+ + + + 
+ + _ + 
+ + _ _ 
+ _ + + 
+ _ + _ + + + _ _ _ + + 
_ _ + _ 
_ _ _ + 
_ _ _ _ 
+ + + 
+ + _ + _ + 
+ _ _ 
_ + + 
_ + _ 
_ _ + 
_ _ _ + _ _ + 
+ _ _ _ 
_ + + + 
_ + + _ 
_ + _ + 
_ + _ _ 
Figure 3: Three types of Galtons dice. They are 1.25 in. cubes, date from 1890,     Image by MIT OpenCourseWare.
and are used for
simulating normally distributed random numbers. Adapted from Stigler, S. (2002): Statistics on the 
Table: The History of Statistical Concepts and Methods 
12 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Cumulative areas under the standard normal distribution (Cont.) 
0.0 
0.1 
0.2 
0.3 
0.4 
0.5 0.6 
0.7 0.8 
0.9 
1.0 1.1 1.2 1.3 
1.4 
1.5 1.6 
1.7 
1.8 
1.9 
2.0 2.1 2.2 2..3 2.4 
2.5 
2.6 
2.7 
2.8 2.9 3.0 0.5000 
0.5398 
0.5793 
0.6179 
0.6554 
0.6915 0.7257 
0.7580 0.7881 
0.8159 
0.8413 0.8643 0.8849 0.9032 
0.9192 
0.9332 0.9452 
0.9554 
0.9641 
0.9713 
0.9772 0.9821 
0.9861 
0.9893 0.9918 
0.9938 
0.9953 
0.9965 
0.9974 0.9981 0.9987 0.5040 
0.5438 
0.5832 
0.6217 
0.6591 
0.6950 0.7291 
0.7611 0.7910 
0.8186 
0.8438 0.8665 0.8869 0.9049 
0.9207 
0.9345 0.9463 
0.9564 
0.9648 
0.9719 
0.9778 0.9826 
0.9864 
0.9896 0.9920 
0.9940 
0.9955 
0.9966 
0.9975 0.9982 0.9987 0.5080 
0.5478 
0.5871 
0.6255 
0.6628 
0.6985 
0.7324 0.7642 
0.7939 
0.8212 0.8461 0.8686 0.8888 
0.9066 
0.9222 0.9357 
0.9474 
0.9573 
0.9656 
0.9726 0.9783 
0.9830 
0.9868 0.9898 
0.9922 
0.9941 
0.9956 
0.9967 
0.9976 0.9982 0.9987 0.5120 
0.5517 
0.5910 
0.6293 
0.6664 
0.7019 0.7357 
0.7673 0.7967 
0.8238 
0.8485 0.8708 0.8907 0.9082 
0.9236 
0.9370 0.9484 
0.9582 
0.9664 
0.9732 
0.9788 0.9834 
0.9871 
0.9901 0.9925 
0.9943 
0.9957 
0.9968 
0.9977 0.9983 0.9988 0.5160 
0.5557 
0.5948 
0.6331 
0.6700 
0.7054 0.7389 
0.7703 0.7995 
0.8264 
0.8508 0.8729 0.8925 0.9099 
0.9251 
0.9382 0.9495 
0.9591 
0.9671 
0.9738 
0.9793 0.9838 
0.9874 
0.9904 0.9927 
0.9945 
0.9959 
0.9969 
0.9977 0.9984 09988 0.5199 
0.5596 
0.5987 
0.6368 
0.6736 
0.7088 0.7422 
0.7734 0.8023 
0.8289 
0.8531 0.8749 0.8944 0.9115 
0.9265 
0.9394 0.9505 
0.9599 
0.9678 
0.9744 
0.9798 0.9842 
0.9878 
0.9906 0.9929 
0.9946 
0.9960 
0.9970 
0.9978 0.9984 0.9989 0.5239 
0.5636 
0.6026 
0.6406 
0.6772 
0.7123 0.7454 
0.7764 0.8051 
0.8315 
0.8554 0.8770 0.8962 0.9131 
0.9278 
0.9406 0.9515 
0.9608 
0.9686 
0.9750 
0.9803 0.9846 
0.9881 
0.9909 0.9931 
0.9948 
0.9961 
0.9971 
0.9979 0.9985 0.9989 0.5279 
0.5675 
0.6064 
0.6443 
0.6808 
0.7157 0.7486 
0.7794 0.8078 
0.8340 
0.8577 0.8790 0.8980 0.9147 
0.9292 
0.9418 0.9525 
0.9616 
0.9693 
0.9756 
0.9808 0.9850 
0.9884 
0.9911 0.9932 
0.9949 
0.9962 
0.9972 
0.9979 0.9985 0.9989 0.5319 
0.5714 
0.6103 
0.6480 
0.6844 
0.7190 0.7517 
0.7823 0.8106 
0.8365 
0.8599 0.8810 0.8997 0.9162 
0.9306 
0.9430 0.9535 
0.9625 
0.9700 
0.9762 
0.9812 0.9854 
0.9887 
0.9913 0.9934 
0.9951 
0.9963 
0.9973 
0.9980 0.9986 0.9990 0.5359 
0.5753 
0.6141 
0.6517 
0.6879 
0.7224 0.7549 
0.7852 0.8133 
0.8389 
0.8621 0.8830 0.9015 0.9177 
0.9319 
0.9441 0.9545 
0.9633 
0.9706 
0.9767 
0.9817 0.9857 
0.9890 
0.9916 0.9936 
0.9952 
0.9964 
0.9974 
0.9981 0.9986 0.9990 
0 z 1 2 3 4 5 6 7 8 9 
Source: B. W. Lindgren, Statistical Theory (New York: Macmillan. 1962), pp. 392-393. 
 15Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text> 
 
 We showed that for X B(n,p), 
E[X]= np 
Var(X)= np(1p) 
Denition3 X is uniformly distributed over the interval [a,b], X U[a,b],ifithasp.d.f. 
if fX(x)= b1 
a a x b 
0 otherwise 
Denition4 X is exponentially distributed with parameter , X E(),ifithasp.d.f. 
ex if x 0 fX(x)= 0 otherwise 
2.2 Standardized Random Variable 
Sometimes, it is useful to look at the following standardization Z of a random variable X 
X E[X]Z :=  
Var(X) 
Using the rules for expectations and variances derived in the last couple of lectures, 
E[X E[X]] E[Z]=  =0 
Var(X) 
and 
Var(X)= Var(X E[X]) = Var(X) =1 Var(X) Var(X) 
If we normalize random variables in this way, its easier to compare shapes of dierent distributions 
independent of their scale and location. 
2.3 The Normal Distribution 
Thenormaldistributioncorrespondstoa continuousrandomvariable, anditturnsoutthatitgivesgood 
approximationsto alarge number of statistical experiments(well see one examplein a second, more on 
this when we discuss the Central Limit Theorem next week). 
Denition5 A random variable Z follows a standard normal distribution -in symbols Z N(0,1) -if 
its p.d.f. is given by 
2 z 1 fZ(z)= (z):= 
2e 2 
for any z R. Its c.d.f. is denoted by 
z 
(z):= P(Z z)= (s)ds 
 
Thec.d.f. of astandard normal randomvariabledoesnthaveaclosed-formexpression(butcanlook up 
valuesintables,alsoprogrammedinany statistical softwarepackage).Thep.d.f. (z)hasacharacteristic 
bell shape and is symmetric around zero: 
6 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>For a large value k for the degrees of freedom, the t-distribution is approximated well by the standard 
normaldistribution. 
Denition8 If Y1 2 and Y2 2 , then k1 k2 
Y1/k1F = Y2/k2 F(k1,k2) 
is said to follow an F-distribution with(k1,k2)degrees of freedom. 
13 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
 The rst thing to notice is that for every type of car, the buyers valuation is higher than the sellers, so 
for each type of car, trade should take place at a price between the buyers and the sellers valuations. 
However, for used cars, quality is typically not evident at rst sight, so if neither the seller nor the buyer 
know the type X of a car in question, their expected valuations are, by the law of iterated expectations 
E[YS]= E[YSlemon]P(lemon)+ E[YSaverage]P(average)+ E[YSmelon]P(melon) | | |
1 = (5,000+6,000+10,000) =7,000 3
E[YS]= E[YBlemon]P(lemon)+ E[YBaverage]P(average)+ E[YBmelon]P(melon) | | |
1 = (6,000+10,000+11,000) =9,000 3
so trade should still take place.
But in a more realistic setting, the seller of the used car knows more about its quality than the buyer
(e.g. history of repairs, accidents etc.) and states a price at which he is willing to sell the car. If the
seller can perfectly distinguish the three types of cars, whereas the buyer cant, the buyer should form
expectations conditional on the seller willing to sell at the quoted price.
If the seller states a price less than 6,000 dollars, the buyer knows for sure that the car is a lemon
because otherwise the seller would demand at least 6,000,i.e.
E[YBYS &lt; 6000] = E[YBlemon] =6000 | |
and trade would take place. However, if the car was in fact a melon, the seller would demand at least 
10,000 dollars, whereas the buyer would pay at most 
E[YB|YS 10,000] = E[YB]=9,000 &lt; 10,000 
so that the seller wont be able to sell the high-quality car at a reasonable price.
The reason why the market for melons breaks down is that in this model, the seller cant credibly
assure the buyer that the car in question is not of lower quality, so that the buyer factors the possibility
of getting the bad deal into his calculation.
Example3 In this example, we are going to look at data on the 2008 presidential nominations from the
IEMPoliticalMarkets, aninternetplatforminwhichpeoplebetonfuturepoliticalevents(thedata can
be obtained from
http://www.biz.uiowa.edu/iem/markets/data nomination08.html).
The market works as follows: for each political candidate i, participants can buy a contract which pays
1 dollar if candidate i wins nomination Yi = 0 dollars otherwise 
At a given point in time t, participants in the market have additional outside information, which well 
call Xt, as e.g. the number of delegates won so far, the momentum of a candidates campaign, or 
statements by sta about the candidates campaign strategy. 
Given that additional information, the expected value of the contract is 
E[Yi|Xt = x]= yifYi|Xt (yi|x)=1 P(Yi =1|Xt = x)+0 P(Yi =0|Xt = x)= P(Yi|Xt) 
yi 
In other words, the dollar amount traders should be willing to pay for the contract for candidate i equals 
the probability that i wins his/her partys nomination given the available information attime t. 
Lets look at the prices of contracts for the main candidates of the Democratic party over the last 3 
months: I put three vertical lines for 3 events which revealed important information about the candidates 
likelihood of winning the Democratic nomination: 
2 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>  What is the p.d.f. of X? By the change of variables formula we saw earlier in this class, 
fX(x)=1 x  1 (x)2 
e  2 = 
2 22 
We can extend the same argument by noting that the linear transformation of a normal random variable 
X1 is again normal. 
Proposition3 If X1 N(,2)and X2 = a + bX1, then 
X2 N(a + b,b22) 
You can check this again using the change of variables formula. 
Wealreadysawthattheexpectationof thesumof n variables X1,...,Xn isthesumoftheirexpectations, 
and that the variance of n independent random variables is the sum of their variances. If the Xis are 
also normal, then their sum is as well: 
Proposition4 If X1,...,Xn are independent normal random variables with Xi N(i,2), then i   n n n    
Y = Xi N i, 2 
i 
i=1 i=1 i=1 
While in the general, wed have to go through the convolution formula we saw a few weeks ago, for the 
sum of normals, we therefore only have to compute the expectation and variance of the sum, and know 
the p.d.f. of the sum right away: 
P
1 (yP i)2 
fY (y)=  e 2 2 
i 
2i 2 Density 
0 .2 .4 .6 .8 
sigma=0.5 
sigma=1 
sigma=2 
5 0 5 
z 
Figure 2: Normal Density for Dierent Values of  
2.3.2 Using Tabulated Values of the Standard Normal 
If X N(,), we can give a roughestimate of the probability with which X is no further than one or 
two standard deviations away from its mean: 
P(1 X +1)=(1)(1)  68% 
P(2 X +2)=(2)(2)  95% 
P(3 X +3)=(3)(3)  99.7% 
9 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>      
 for a standard normal random variable Z 
2. restatetheprobability intermsofthestandard normal c.d.f.,(): 
a  b b a P =  Z      
3. look up the values for the values of the standard normal c.d.f. at the values calculated above in 
order to obtain the probability. 
2.4 Digression: Drawing Standard Normal Random Variables 
We already sawthatitispossibleto convert uniform randomdrawsto any other continuousdistribution 
via theintegral transformation(see thelectures on transformations of random variables). Whatifyou 
dont have a computer? Around 1900, the famous statistician Francis Galton came up with a clever 
mechanical device for simulating normal random variables using dice:1 
The three dierent dice shown in Figure 3 were rolled one after another, while the experimenter lls up 
alistof randomdrawsinthefollowing manner:the rstdiegivestheactual values(youalwaysread o 
what is on the bottom of the side of the die facing you), where the stars are rst left empty, and later 
lled up with rolls of the second die. Finally, the third die gives sequences of pluses and minuses which 
are put in front of the draws put down as we went along with the rst two dice. 
The numbers on the dice were specically chosen as evenly spaced percentiles of the positive half of 
the standard normal distribution in order to ensure that the outcome would in fact resemble a standard 
normal random variable. 
2.5 Functions of Standard Normals: chi-squared, t-and F-distribution 
Due to the importance of the standard normal distribution for estimation and testing, some functions of 
standard normal random variables also play an important role and are frequently tabulated in statistics 
tests. For now welljustgivedenitionsfor completeness,but wellgetback to applicationsin thelast 
third of the class. Im notgoing togivethe correspondingp.d.f.s sincethey are oflimitpractical usefor 
us. 
Denition 6 If Z1,Z2,. . . ,Zk are independent with Zi N(0,1), Y 
chi-squared distribution with k degrees of freedom, in symbols = k 
i=1 Z2 
i is said to follow a 
Y 2 
k 
Here degrees of freedom refers to the number of independent draws that are squared and summed up. 
The expectation of a chi-squared is given by the degrees of freedom, 
k 
Y 2 
k E[Y]= E[Xi 2]= k 
i=1 
Denition7 If X N(0,1) and Y 2 , then k
Z T = 
Y tk 
is said to follow the (student)t-distribution with k degrees of freedom. 
1See Stigler, S. (2002): Statistics on the Table 
11 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>    The standard normal p.d.f. phi(z) 
0 .1 .2 .3 .4 
5 0 5 
z 
2.3.1 Important Properties of the Standard Normal Distribution 
Property1 For a standard normal random variable Z, 
E[Z]=0 
and 
Var(Z)=1 
As a rst illustration why the normal distribution is very useful, it turns out that Binomial random 
variables are approximated by the normal for a large number n oftrials: 
Theorem1 (DeMoivre-Laplace Theorem) If X B(n,p) is a binomial random variable, then for 
any numbers c d, 
X np X E[X]  d 
lim Pc  &lt;d = lim P c&lt;  = Z(z)dz 
n np(1p) n Var(X) d 
c 
Notice that the transformation of the binomial variable to 
X E[X]Z =  
Var(X) 
is in fact a standardization. This result says that for large n, the probability that the standardized 
binomial random variable X fallsinside theinterval(c,d] is approximately the same as for a standard 
normal random variable. As an illustration, I plotted the binomial p.d.f. for increasing values of n, and 
then applied the normalization. 
For n =50,the shape of thebargraphlooksalready relatively similartothebell-shape of the normal 
density. Note that in particular the skewness of the distribution for small n (due to the smallsuccess 
probability p= 1
4)washes out almost entirely. 
Example6 In order to see why this type of approximation is in fact useful for practical purposes, say 
p = 1
5, and we want to calculate the probability that in a sequence of n trials, we have at least 25% 
7 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>    
   n=5 
n=20 
n=50 0 .1 .2 .3 .4 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Binomial p.d.f.s for p=0.25 and different n 
0 .1 .2 .3 .4 
4 2 0 2 4 Densities of Standardized Binomial Random Variables 
Figure 1: Illustration of the DeMoivre-Laplace Theorem 
successes.
If n =5, the probability of having no more than 25% successes can be calculated as
5 5 45 44 1280 P(X 1.25) = P(X =0)+P(X =1) = 0(1p)5 +1 p(1p)4 =55 +5 55 3125 40.96% = 
Whatif XB  
100, 1
5  
, i.e. we increase n to 100? In principle, we could calculate 
P(X25) = P(X =0)+P(X =1)+... + P(X =25) 
So wed have to calculate each summand separately, and since there are pretty many of those, this will be 
very cumbersome. Alternatively, we could limit ourselves to a good approximation using the DeMoivre-
Laplace Theorem. The standardized version of Xisgivenby 
X1001 
Z =   5 = X 20 
10014 4  55 
Therefore, 
5 
4 
P(X25) = P(X20 5) = PZ 45  
 (z)dz 89.44% 
How good is this approximation? I did the calculation, and I obtained P(X25) 91.25%. If we repeat 
 the same exercise for n =200, I obtain the exact binomial probability P(X 50) 96.55% and the 
 normal approximation P(X 50) 96.15%. 
For Z N(0,1), we also call any random variable 
X = + Z 
a normal random variable with mean  and variance 2, or in symbols 
X N(,2) 
8 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
Iowa Florida 
Super 
Tuesday 0 .2 .4 .6 .8 1 Giuliani/Huckabee/McCain/Romney 
12/01/07 1/01/08 2/01/08 3/01/08 
Date 
Giuliani Huckabee 
McCain Romney 
ofpledgeddelegatesbydate t, we can compare the unconditional means before the Iowa primaries to 
the conditional means after Super Tuesday in light of the law of total variance, 
Var(Yi)= E[Var(YiXt)]+Var(E[YiXt]) | |
We can see that while before the Iowa elections, the E[Yi]for the main candidates were in an interme
diate rangefrom10% to40% withlots of variation. However, afterSuperTuesday,prices(i.e. E[YiXt])|
moved close to 0 or 1, and the wiggles have become really tiny. So, in terms of the conditional variance 
formula, the largest part of the ex ante variance Var(Yi)was uncertainty about the conditional mean after 
SuperTuesday, Var(E[YiXt]), whereas the contribution of the conditional variance Var(YiXt) seems to | |
be relatively small. 
If we compare this to the graph for the Democratic race, for the latter, there is still a lot of movement af
ter Super Tuesday, so that conditioning on the number of pledged delegates Xt bySuperTuesdaydoesnt 
take out much of the variance, i.e. Var(YiXt) is still considerably large. As an aside, an often cited |
reason why the Republican race was decided so much earlier is that in the Republican primaries, in each 
state,delegates are allocated notproportionally to each candidates vote share(whichisthe rulefor most 
primaries of the Democratic party), but the winner-takes-all rule, so that even very close victories in the 
rst primaries can get a candidate far enough ahead to make it extremely hard for competitors to catch 
up. 
2 Special Distributions 
In the lecture, we already saw three commonly used distributions, the binomial, the uniform and the 
exponential. Over the next two lectures, we are going to expand this list by a few more important 
examples, and well start with the most frequently used of all, the normal distribution. 
2.1 Recap: Distributions we already saw in Class 
Denition2 X is binomial distributed with parameters (n,p), X B(n,p)if its p.d.f. is given by 
 
 n
px(1p)nx if x {0,1,...,n}fX(x)= x  0 otherwise 
5 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Covariance and conditional expectations (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Property7 
1  (X,Y) 1 
Broadly speaking, we distinguish three cases 
 (X,Y)&gt; 0: X and Y are positively correlated 
 (X,Y)=0: X and Y are uncorrelated 
 (X,Y)&lt; 0: X and Y are negatively correlated 
Property8 
|(X,Y)|=1  Y = aX + b 
for some constants a =0 and b. 
I.e. the absolute value of the correlation coecient equals 1 if there is a deterministic linear relationship 
between the two random variables. In that case we say that X and Y are perfectly correlated. 
Remark1 A very important principle of data analysis is that the statistical relationship between two 
random variables does not necessarily correspond to mechanical or causal statements which we would 
actually wanttomakebased onthedata. E.g. wetypically observeindatasetsthatthetimepeoplespend 
working out in the gym is positively correlated with health, but this does not necessarily mean that sports 
causes health to improve. But it could as well be that some people are so unhealthy that they wouldnt 
even think about going to the gym. 
A more abstract way to see why correlation of X and Y and causation of Y by X areinherentlydier
ent concepts, notice that the covariance is symmetric in X and Y, so we can change their roles. For 
causality however, we think of a specic direction of the relationship, i.e. we could have X  Y or X 
causes/aects Y but simultaneously Y doesnt cause/aect X, so we cant change the roles of X and 
Y. Therefore: 
Correlation does not equal causation! 
(Much)more about that in your Econometrics class. 
1.1 Preview: Regression 
Say, we are interested in the relationship between a workers income Y and her education as measured 
with years of schooling X (for simplicityletsjust assume thatboth are continuous random variables). 
Then we can always rewrite the relationship between X and Y as 
Y = + X + U 
where U is a random variable with E[U] = 0 and Cov(X,U) = 0 (in the regression context it will be 
called the residual). 
One way ofdetermining the value oftheparameters(,)is to solve 
(,)=argmin E[(Y X )2] 
, 
3 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>  
    
  
    Also, by the same steps as above, 
 1  1  1 8 E[Y] = 8x y 2dy dx = x(1x 3)dx 30 x 0   1 8 1 8 x2 x5 83 4 = (x x 4)dx =  =  = 3 0 32 5 0 310 5 
and 
 1 1 1 8 E[X] = 8x 2 ydy dx = x 2(1x 2)dx 
0 x 0 2 
 1 11 8 =4(x 2 x 4)dx =4  = 35 15 0 
Putting all pieces together, and applying property 7, 
4 844 2532 3 4 Cov(X,Y)= E[XY]E[X]E[Y]=   = = 9 15 5 225 225 
We already showed that for two independent random variables X and Y, the variance of the sum equals 
the sum of variances. Heres a generalization to random variables which are not necessariy independent: 
Property4 
Var(X + Y)=Var(X)+Var(Y)+2Cov(X,Y) 
The idea behind the proof is to apply properties 3 and 7 to get 
Var(X + Y)= E[(X + Y)2]E[X + Y]2 
= E[X2 +2XY + Y2]E[X]2 2E[X]E[Y]E[Y]2 
= E[X2]E[X]2 + E[Y2]E[Y]2 +2(E[XY]E[X]E[Y]) 
= Var(X)+Var(Y)+2Cov(X,Y) 
Property5 For random variables X,Y,Z, 
Cov(X,aY + bZ + c)= aCov(X,Y)+bCov(X,Z) 
Property6 
Cov(aX + b,cY + d)= acCov(X,Y) 
Since by the last property, the covariance changes with the scale of X and Y, we would like to have a 
standardized measure which gives us the strength of the relationship between X and Y, and which is 
not aected by changing, say, the units of measurement of the two variables. The most frequently used 
measure of that kind is the correlation coecient: 
Denition2 The correlation coecient of X and Y isgivenby 
Cov(X,Y)(XY)=  
Var(X)Var(Y) 
The correlation coecient is normalized in a way such that 
2 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> 
   
    
  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 13 
Konrad Menzel 
March 31, 2009 
Covariance 
The covariance of X and Y is a measure of the strength of the relationship between the two random 
variables. 
Denition1 For two random variables X and Y, the covariance is dened as 
Cov(X,Y)= E [(X E[X])(Y E[Y])] 
First,byjust applying thedenitions, weget 
Property1 
Cov(X,X)=Var(X) 
Property2 
Cov(X,Y)=Cov(Y,X) 
Furthermore, we have the following result which is very useful to calculate covariances: 
Property3 
Cov(X,Y)= E[XY]E[X]E[Y] 
Thisis ageneralization of the analogousproperty of variances, and theproof uses exactly the samekind 
of argument. Lets do an example to see how this result is useful: 
Example1 Suppose X and Y havejointp.d.f. 
8xy if0  x  y  1 fXY (x,y)= 0 otherwise 
What is the covariance Cov(X,Y)? -Lets calculate the components which enter according to the right-
hand side of the equation in property 7: 
   11 
E[XY]= xyfXY (x,y)dxdy = 8x 2 y 2dydx 
  0 x  1 1 1 8 = 8x 2 y 2dy dx = x 2(1x 3)dx 
0 x 0 3 
  18 1 8 x3 x6 
= (x 2 x 5)dx =  3 0 33 6 0 
811 4 =  = 336 9 
1 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>   
  so that the seller wont be able to sell the high-quality car at a reasonable price.
The reason why the market for melons breaks down is that in this model, the seller cant credibly
assure the buyer that the car in question is not of lower quality, so that the buyer factors the possibility
of getting the bad deal into his calculation.
An important relationship between conditional and unconditional expectation is the Law of Iterated 
Expectations(aclose cousin oftheLawofTotalProbability which wesawearlierinthelecture): 
Proposition1 (Law of Iterated Expectations) 
E [E[Y|X]]= E[Y] 
Proof: Let g(x)= E[Y|X = x], which is a function ofx. We can now calculate the expectation 
    
E[g(X)] = g(x)fX(x)dx = E[Y|X = x]fX(x)dx 
     fXY (x,y) = y dy fX(x)dx 
  fX(x)     
= yfXY (x,y)dydx 
      
= y fXY (x,y)dx dy 
    
= yfY (y)dy = E[Y] 
 
6 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>  Note that since fY|X(y|X)carries the random variable X as its argument, the conditional expectation is 
also a random variable. However, we can also dene the conditional expectation of Y given a particular 
value of X, 
yfY |X(y|x) if Y isdiscrete E[Y|X = x]=  y 
 yfY|X(y|x)dy if Y is continuous 
whichisjust a numberfor anygiven value of x as long as the conditional density is dened. 
Sincethecalculationgoesexactlylikebefore,onlythat wenowintegrateoverthe conditional distribution, 
wont do a numerical example (for the problem set, just apply denition). Instead lets discuss more 
qualitative examples to illustrate the dierence between conditional and unconditional examples: 
Example3 (The Market for Lemons) The following is a simplied version of a famous model for 
the market for used cars by the economist George Akerlof. Suppose that there are three types X of used 
cars: cars in an excellent state (melons), average-quality cars (average not in a strict, statistical, 
sense), and carsin apoor condition(lemons). Each type of caris equallyfrequent,i.e. 
1 P(lemon) = P(average) = P(melon) = 3 
Thesellerand abuyerhavethefollowing(dollar) valuations YS and YB, respectively, for each type of cars: 
Type Seller Buyer 
Lemon 5,000$ 6,000$ 
Average 6,000$ 10,000$ 
Melon 10,000$ 11,000$ 
The rst thing to notice is that for every type of car, the buyers valuation is higher than the sellers, so 
for each type of car, trade should take place at a price between the buyers and the sellers valuations. 
However, for used cars, quality is typically not evident at rst sight, so if neither the seller nor the buyer 
know the type X of a car in question, their expected valuations are, by the law of iterated expectations 
E[YS]=	E[YS|lemon]P(lemon)+ E[YS|average]P(average)+ E[YS|melon]P(melon) 
1 = (5,000+6,000+10,000) =7,000 3
E[YS]= E[YB|lemon]P(lemon)+ E[YB|average]P(average)+ E[YB|melon]P(melon) 
1 = (6,000+10,000+11,000) =9,000 3
so trade should still take place.
But in a more realistic setting, the seller of the used car knows more about its quality than the buyer
(e.g. history of repairs, accidents etc.) and states a price at which he is willing to sell the car. If the
seller can perfectly distinguish the three types of cars, whereas the buyer cant, the buyer should form
expectations conditional on the seller willing to sell at the quoted price.
If the seller states a price less than 6,000 dollars, the buyer knows for sure that the car is a lemon
because otherwise the seller would demand at least 6,000,i.e.
E[YB|YS &lt; 6000] = E[YB|lemon] =6000 
and trade would take place. However, if the car was in fact a melon, the seller would demand at least 
10,000 dollars, whereas the buyer would pay at most 
E[YB|YS  10,000] = E[YB]=9,000 &lt; 10,000 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
  
  Taking rst-order conditions with respect to  (notice that expectations are linear in their argument, so 
we can pass the derivative through the integral), we get 
d 0 = d E[(Y X )2] 
 
= d 
d (yx )2fXY (x,y)dydx 
 
= d 
d  
(yx )2 
fXY (x,y)dydx 
d = E [(Y X )2]d
= E [2X(Y X )] 
Similarly, taking rst-order conditions with respect to , 
d d 0= E[(Y X )2]= E [(Y X )2]= E[2(Y X )] d d
Solving the last expression for , 
 = E[Y]E[X] 
Plugging this into the rst-order condition for , we get 
0= E [X(Y X (E[Y]E[X]))]= E[XY]E[X2] E[X]E[Y]+E[X]2 
so that we can now solve for the parameter 
E[XY]E[X]E[Y] Cov(X,Y) = = E[X2]E[X]2 Var(X) 
We can now verify that indeed E[U]= E[Y X ]= 0 and Cov(X,U)= Cov(X,Y X )=0 
(in fact the rst follows directlyfrom the rst-order condition for , and the second from the rst-order 
conditionfor ). 
Then the regression t  + X is the part of Y whichiscorrelated with(or explained by) X, and 
U is the part of Y which is uncorrelated with X. The parameters  and  are usually called regression 
parameters or least-squares coecients. Linear regression is the workhorse of much of econometrics, 
and youll see this in many variations over the course of 14.32 and other econometrics classes. 
2 Conditional Expectations 
Example2 Each year, a rms R&amp;D department produces X innovations according to some random 
process, where E[X] =2 and Var(X) =2. Each invention is a commercial success with probability 
p =0.2 (assume independence). The number of commercial successes in a given year are denoted by S. 
Since we know that the mean of S  B(x,p)= xp, conditional on X = x innovations in a given year, xp 
of them should be successful on average. 
The conditional expectation of X given Y is the expectation of X taken over the conditional p.d.f.: 
Denition3 
yfY |X(y|X) if Y isdiscrete E[Y|X]=  y 
 yfY|X(y|X)dy if Y is continuous 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Review (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>  25 The probability that a given student scores at least 3 out of 10 is 
1FX(2)=1P(X =0)P(X =1)P(X =2) =10.410100.60.49 450.62 0.48 =1 1876 0.48  
Therefore, the probability that the bottom student scores at least 3 out of ten is 
1FY (2)=[1FX(2)]10 60.39% 
Example 2 Spring 2007 Exam, Problem 3 
If X N(,2), we say that Y = eX has a lognormal distribution, Y L(,2). 
(a) Find the p.d.f. of Y 
(b) Suppose you have $100,000 to invest and you have access to an investment whose return R1 is 
distributed L(,2). Its mean e+2/2 is 1.10, and its variance e2(+2) e2+2 is 0.01. What 
is the probability that your wealth at the end of one period of investment ($100,000R1)isgreater 
than 110,000? 
(c) With the sameparameter values asin(b), whatistheprobability thatyour wealth atthe end of two 
independent periods of investment is greater than $115,000? 
Answer: 
(a) Thistransformationisone-to-one,sowecanusethechangeofvariablesformula. NotethatX canbe 
any real number,sothesupport of Y is(0,). Theinversetransformationis X =ln(Y),whichhas 
dX 1 . Thus, using the change of variables formula, we have fY (y)= 11 exp(1 (ln(y))2)dY = X y 
2 2  
for y&gt; 0 and fY (y)=0 otherwise. 
(b) It is useful to begin by solving for  and 2 . We can factor the expression for the variance as 
e2+2 (e2 1), or[e+ 21 2 2 ]2(e1). Plugging in from the expression for the mean and using the 
fact that the variance is .01, wehave(1.10)2(e2 1) = .01. Solvingfor  yields  .090722098. 
We can then go back and see that  .09119493. 
Now lets nd out the probability that your wealth at the end of one period is greater than 
$110,000. We have P(100000R1 &gt; 110000) = P(R1 &gt; 1.1) = P(ln(R1)&gt; ln(1.1)) = P(ln(R
 1) &gt; 
ln(1.
 1)) =1 (ln(1.
 1)) 1 (.045361051) 1 .5181 = .4819, where you can use the 
normal probability tables to get the value of the standard normal c.d.f. 
(c) P(100000R1R2 &gt; 115000) = P(R1R2 &gt; 1.15) = P(ln(R1)+ln(R2) &gt; ln(1.15)). Note that 
ln(R1) and ln(R2) are independent normals, and so their sum is also normal. The mean is the 
sum of the means and the variance is the sum of the variances. Using tildes to refer to the new 
mean, variance, and standard deviation here, we thus have .18238986, 2 .016460998, and   
.128300421. So continuing along with the earlier calculation, we have P(ln(R1)+ln(R2) &gt; 
ln(1.15)) = P(ln(R1)+ln(R2)&gt; ln(1.15))=1(ln(1.15))1(.3322508) 1.3698 =   
.6302. 
Example 3 Spring 2007 Exam, Problem 4 
Mikael Priks, a Swedish economist, has been studying various economic issues surrounding soccer hooli
ganism using detailed data on hooligan activity, ghts, injuries, etc., collected by the Swedish police and 
self-reportedby oneof thegangs,the FirmanBoys (seewww.lrz-muenchen.de/ces/mikael.htm). Inone 
paper he sought to analyze the determinants of the likelihood and severity of ghts between rival hooligan 
7 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 16 
Konrad Menzel
April 9, 2009
1 General Exam Policies 
	Exam 2 will be in class next Tuesday, April 14, starting at 9:00 sharp 
	relevant material: in rst place topics we covered since last exam, but of course should feel com
fortable with densities, probabilities and other concepts from the rst third of the semester 
	more text problems than on problem sets, but less tedious calculations 
	will hand out normal probability tables with exam, so dont have to bring your own 
	essentially same format as in rst exam 
	bring calculators 
	closed books, closed notes
have about 85 minutes to do exam
  
	Ill give partial credit, so try to get started with all problems 
2 Review 
2.1 Functions of Random Variables 
General set-up: 
	know p.d.f. of X, fX(x)(discrete or continuous) 
	Y is a known function of X, Y = u(X) 
	interested in nding p.d.f. fY (y) 
Thewayhowweobtainthep.d.f. fY (y)dependsonwhetherX iscontinuousordiscrete,and whether 
thefunction u()is one-to-one. Three methods 
1.	if X discrete: 
fY (y)= fX(x) 
{x:u(x)=y} 
1 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>  
  
    3 2.4.3 Central Limit Theorem 
 look at distribution of standardized sample mean 
 Central Limit Theorem: for an i.i.d. sample with Var(Xi)&lt; , 
lim P nXn 
  x =(x) 
n 
where()isthe normal c.d.f. 
 saw graphs illustrating the DeMoivre-Laplace theorem for binomial random variables. 
Sample Problems 
Example 1 Spring 2003 Exam, Problem 3 
Mr Bayson, a third grade teacher in the Baldwin School in Cambridge is up for promotion, and the 
likelihood of it happening will depend in part on his students performance on the MCAS exam. He has 
ten students and the exam will have ten questions on it. Suppose that each student has a 60% chance 
of correctly answering each questions, and that answers on all questions are independent. What is the 
probability that his top scoring student scores at least nine out of ten? What is the probability that his 
bottom-scoring student scores at least three out of ten? 
Answer:
You should notice that this question has two parts: (1) determining the distribution of individual test
scores,and(2) nding thec.d.f.softhemaximumandtheminimum.
Sinceeach studentsexamscore X isthenumberof successesout of10independenttrials,itisabinomial
random variable, X B(10,0.6), for which we know the p.d.f.
 
 10 0.6x0.410x if x {0,1,..., 10}fX(x)= x  0 otherwise 
In general, the maximum Y1 of an i.i.d. sample X1,...,Xn where the c.d.f. of Xi is FX(x)has c.d.f. 
FY1 (y)= FX(y)n 
and the minimum Y2 has c.d.f. 
FY2 (y)=1(1FX(y))n 
The probability that a given student scores less than 9 is 
FX(9) =1P(X =9)P(X =10) =1 10 0.690.4+ 10 0.610 
9 10 
239 310 23 =110 5 
10 +510 =1 5  0.69 
Therefore, the probability that the top student scores at least 9 out of 10 is 
 10 
1FY1 (9)=1[FX(9)]10 =1 1 23 0.69 37.79% 5 
6 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text> 
 
  
 If X and Y areindependent,Cov(X,Y)=0. 
Correlation coecient dened as 
Cov(X,Y)(X,Y)=  
Var(X)Var(Y) 
(X&lt;Y)[1,1], and |(X,Y)|=1 if and only if Y is a deterministic linear function of X. 
2.2.4 Conditional Expectation 
Conditional expectation random variable denedby 
 
E[YX]= yfY|X(yX)dy |
 |
Two important results on conditional expectations: 
	Law of Iterated Expectations
E[E[YX]]= E[Y]
 |
ConditionalVariance  
Var(Y)=Var(E[YX])+E[Var(YX)] | |
2.3 Special Distributions 
2.3.1 Summary 
Lookedfollowing distributions 
 Uniform: X U[a,b]if p.d.f. of X is 
fX(x)=	b1 
a if a x b 
0 otherwise 
 Binomial: X B(n,p)if p.d.f. of X is 
 
fX(x)=  n
x px(1p)nx if x {0,1,...,n} 
 0	 otherwise 
 Exponential: X E()if X hasp.d.f. 
ex if x 0 fX(x)= 0 otherwise 
Normal: X N(,2)if p.d.f. is  
1 (x)2 
fX(x)= e22 
2 
 Poisson:	X P()if p.d.f. is 
  
fX(x)=	x 
xe 
! if x {0,1,2,...}
0 otherwise 
Should know or be able to calculate mean and variance for each distribution. Also showed relationships 
between Binomial and Poisson, and Binomial and Normal. 
4 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>  Sample Problems 
Spring 2003 Exam, Problem 3 Mr Bayson, a third grade teacher in the Baldwin School in Cam
bridge is up for promotion, and the likelihood of it happening will depend in part on his students 
performance on the MCAS exam. He has ten students and the exam will have ten questions on it. Sup
pose that each student has a 60% chance of correctly answering each questions, and that answers on all 
questions are independent. What is the probability that his top scoring student scores at least nine out 
of ten? What is the probability that his bottom-scoring student scores at least three out of ten? 
Spring 2007 Exam, Problem 3 If X N(,2), we saythat Y = eX has a lognormal distribution, 
Y L(,2). 
(a) Find the p.d.f. of Y 
(b) Suppose you have $100,000 to invest and you have access to an investment whose return R1 is 
distributed L(,2). Its mean e+2 /2 is 1.10, and its variance e2(+2) e2+2 is 0.01. What 
istheprobability thatyour wealth atthe end of oneperiod ofinvestment($100,000R1)isgreater 
than110,000? 
(c) Withthe sameparameter values asin(b), whatis theprobability thatyour wealth at the end of 
two independent periods of investment is greater than $115,000? 
Spring 2007 Exam, Problem 4 Mikael Priks, a Swedish economist, has been studying various eco
nomicissuessurrounding soccerhooliganismusingdetaileddataonhooliganactivity, ghts,injuries,etc., 
collectedby theSwedishpoliceand self-reportedby oneofthegangs,the FirmanBoys (seewww.lrz
muenchen.de/ces/mikael.htm). In onepaperhe soughtto analyzethedeterminants of thelikelihood and 
severity of ghts between rival hooligan groups. To do so, he constructed a model of ghts and injuries 
where the number of chance meetings between two rival groups in a season follows a P(5)(Poisson with 
 = 5) distribution. Furthermore, he assumed that at least one injury occurs in every ght and that, in 
fact, any number of injuries up to ten are all equally likely. 
(a) Given those assumptions, what is the expected number of injuries that two rival groups inict on 
each other in a given year? What is the variance of that quantity? 
(b) Suppose instead that a ght only happened with probability one-half when a chance meeting of 
tworivalgroups occurred(you may assumeindependence of chance meetings). How wouldyour 
answerstopart(a) change? 
9 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text> 3. for 2 or more random variables, 
E[a1X1 + ... + anXn + b]= a1E[X1]+... + anE[Xn]+b 
4. if X and Y are independent,then
E[XY]= E[X]E[Y]
 expectation is measure of location of distribution of X. 
 expectation of function Y = u(X)(discrete case: replace integralwith sum) 
 
E[Y]= u(x)fX(x)dx 
 
	Jensens Inequality: if u()is convex,then
E[u(X)]u(E[X])
2.2.2 Variance 
Dened as 
Var(X)= E  
(X E[X])2 
Measure of dispersion of X. 
Important properties of variances 
1. for a constant a,
Var(a)=0
2. can write variance as
Var(X)= E[X2]E[X]2
3. for a linear function of independent random variables X1,...,Xn, 
Var(a1X1 + ... + anXn + b)= a12Var(X1)+... + an2 Var(Xn)+b 
4. more generally for any random variables X1,X2, 
Var(a1X1 + a2X2)= a 2
1Var(X1)+2a1a2Cov(X1,X2)+a 2
2Var(X2) 
2.2.3 Covariance and Correlation 
Covariance dened as 
Cov(X,Y)= E[(Y E[Y])(X E[X])] 
Properties of covariances 
Cov(X,X) = Var(X) 
Cov(X,Y) = Cov(Y,X) 
Cov(X,Y)= E[XY]E[X]E[Y] 
Cov(aX + b,cY + d)= acCov(X,Y) 
3 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>groups. To do so, he constructed a model of ghts and injuries where the number of chance meetings 
between two rival groups in a season follows a P(5)(Poisson with  =5) distribution. Furthermore, he 
assumed that at least one injury occurs in every ght and that, in fact, any number of injuries up to ten 
are all equally likely. 
(a) Given those assumptions, what is the expected number of injuries that two rival groups inict on 
each other in a given year? What is the variance of that quantity? 
(b) Suppose instead that a ght only happened with probability one-half when a chance meeting of two 
rivalgroups occurred(you may assumeindependence of chance meetings). How wouldyour answers 
topart(a) change? 
Answer: 
(a) Let us use X to refer to the number of ghts in a season and Y to refer to the number of in
juries. Here well assume that a chance meeting necessarily results in a ght. We have E(Y)= 
E(E(YX)) = E(5.5X) =5.5E(X) =5.5(5) = 27.5. And we have Var(Y)= E(Var(YX))+ |
E(102 |
Var(E(Y|X))= 1 X)+Var(5.5X). [Note that the variance of the number of injuries in a 12 
ghtis 102 1 , and thus the variance of the number of injuries in X ghtsis 102 1 X if thedistribu12 12 
tionofinjuriesisindependent acrossghts.] Continuing along,wehave E(102 1 X)+Var(5.5X)= 12 99 121 99 121 E(X)+Var(X)= (5)+(5)=192.5.12 4 12 4 
(b) Let us use Z to refer to the number of chance encounters. We have E(Y)= E(E(E(YX)Z))= ||
E(E(5.5XZ))= E(5.5(E(XZ)))= E(5.521 Z)=2.75E(Z)=2.75(5) = 13.75. For the variance, | |
99 E(X)+121 Var(X), but now we can still say that Var(Y)= E(Var(Y|X))+Var(E(Y|X))= 12 4 
E(X) and Var(X) will have changed from part a. It is not hard to see that E(X) is half of 
its previous value (so that now it is 2.5), and we can use the fact that XZ is a binomial with |
p = .5 and Z trials to write the variance of X as Var(X)= E(Var(XZ))+ Var(E(XZ)) = | |
E(Z(.5)(1.5))+Var(.5Z)= .25E(Z)+.25Var(Z)= .25(5)+ .25(5) = 2.5. So going back, we 
99 121 99 121 have Var(Y)= 12 4 12 4E(X)+Var(X)= (2.5)+ (2.5) =96.25. 
8 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 2.3.2 Normal Distribution 
Should know how to standardize random variables: 
X E[X]Z =  
Var(X) 
I will give you a copy of the tabulated c.d.f. of the standard normal, so should know how to read the 
tables. 
Important results on normal distribution: 
1. normal p.d.f. is symmetric about the mean 
2. linear functions of normal random variables are again normally distributed: if X N(,2),then 
Y = aX + b N(a + b,a22). 
3. sums of independent normal random variables are normally distributed 
4. Central Limit Theorem: standardized sample mean for i.i.d. sample X1,...,Xn approximately 
follows a standard normal distribution for large n. 
2.4 Asymptotic Theory 
2.4.1 General Idea 
 always assume i.i.d. sample X1,...,Xn
 only deal with sample mean
n 1Xn = Xi n i=1 
	exact value/distribution often hard or even impossible to derive given our knowledge about distri
bution of Xi 
	thought experiment n  supposed to give approximations forlarge n 
2.4.2 Law of Large Numbers 
	Chebyshevs Inequality: for any &gt; 0,
Var(X)
P(|X E[X]|&gt;) 2 
	Law of Large Numbers: if Xi,...,Xn i.i.d., then for all &gt; 0

 lim P(|Xn E[X]|&gt;)=0 
n 
	independenceassumptionimportant(e.g. correlated eventin wisdomof crowds example. 
	needVar(Xi)&lt; , so LLN doesnt work with very fat tailed distributions. 
5 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
  
  
 
  
 
 2. 2-step method if X continuous: Step 1: obtain c.d.f. FY (y) 
FY (y)= P(u(X)y)= fX(x)dx 
{x:u(x)y} 
Step 2: dierentiate c.d.f. in order to obtain p.d.f.: 
d fY (y)= FY (y)dy 
3. changeof variablesformulaif(a) X continuous, and(b) u()is one-to-one: 
 d  
fY (y)= fX(s(y)) s(y)  dy  
A few important examples which we discussed were: 
	Convolution Formula: if X and Y areindependent,then Z = X + Y hasp.d.f. 
 
fZ(z)= fY (z w)fX(w)dw 
 
Note: if densities of X and/or Y are zero somewhere, be careful with integration limits! 
	Integral Transformation: if X continuous, then the random variable Y = FX(X), where FX()is 
the c.d.f. of X is uniformly distributed. 
	Order Statistics: if X1,...,Xn i.i.d.,then kth lowest value Yk hasp.d.f. 
fYk(y)= k nFX(y)k1 (1FX(y))nk fX(y)k 
2.2 Expectations 
2.2.1 Expectation 
Denition of expectation of X 
	if X discrete, 
E[X]= xfX(x) 
x 
	if X continuous, 
 
E[X]= xfX(x)dx 
 
Important properties of expectations 
1. for constant a,
E[a]= a
2. for linear function of X, Y = aX + b,
E[Y]= aE[X]+b
2 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Review (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec24/</lecture_pdf_url>
      <lectureno>24</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text> 
	
 1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 24 
Konrad Menzel
May 14, 2009
Review 
PointEstimation 
Estimatorfunction (X1,...,Xn)of the sample  
bias of estimator is  
Bias()= E0 []0 
	standard error of estimator given by 
()= Var()
Important criteria for assessing estimators are
Unbiasedness
  
	Eciency 
 Consistency
Methods for constructing Estimators:
1. Method of Moments:
mth population moment is E[Xim]= m()
	mth sample moment is Xm = n 1 
in 
=1 Xim 
compute rst k moments, equate Xm = ! m()for m =1,...,k, and solve for the estimate .  
2.	Maximum Likelihood 
	write down likelihood function for the sample X1,...,Xn, 
n 
L()= f(X1,...,Xn)= f(Xi) |
i=1 |
	nd value of  which maximizes L()or log(L()). 
	usually nd maximum by setting rst derivative to zero, but if support of random variable 
depends on  may not be dierentiable, so you should rather try to see what function looks 
like, and where the maximum should be. 
1 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>    
      Therefore, 
n Yi 1Yi Pn Yi Pn (1Yi)  k k k i=1 k i=1
= 1 L()= 1    i=1 
Taking logs, 
n  n   k  k L()=logL()= Yi log 1 + n  Yi log   i=1 i=1 
Setting the derivative with respect to  to zero, 
( n
i=1 Yi) k (n  n
i=1 Yi) k n n 
0= k 2  k 2  Yi k = n  Yi (k)1  i=1 i=1 
Solving for ,
nk k
ML = n = n  i=1 Yi 1Yn 
Notice that this estimator works even if k&gt;. 
2. Hypothesis Testing Supposethat X1,...,Xn form a random sample from a normal distribution 
with an unknown mean  and a known variance 2 equal to1. 
(a) State the critical region that yields the most powerful test of 
H0 : =0 
HA : =1 
at the 5% signicance level. Calculate the power of this test. 
(b) State the critical region that yields the most powerful test of 
H0 : =1
HA : =0
at the 5% signicance level. 
 (c) Forwhat valuesof n and Xn willyouacceptthehypothesisthat  =0inpart (a) and simultaneously 
acceptthehypothesisthat =1inpart (b)? 
(d) State the critical region that yields the uniformly most powerful test of 
H0 : =0 
HA : &gt; 1 
at the 5% signicance level. State the formula for, and then graph the power function 1() of this 
test. 
(e) How are the critical regions of the tests in parts (a) and (d) related? How are the probabilities of 
committing a type II error related? 
Answers: 
5 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>	testing procedure: reject H0 if T(X)C 
The choice of C determines
 = P(type Ierror) = P(rejectH0)
|
 = P(type IIerror) = P(dont rejectHA)|
alpha is called the size, and 1  the power ofthetest. 
	for two tests with same size , prefer test with greater power 1  
	if  = ()would prefer test with lower ()for all values of , uniformly most powerful test 
	H0 and HA both simple: by Neyman-Pearson Lemma, most powerful test is of form reject if 
f(x|0 ) &lt;k f(X|A) 
	test of the form reject if g(T(X))&lt;g(k) for some monotone function identical to test reject if 
T(X)&lt;k 
Construction of tests depends on form of hypotheses H0 and HA: 
1. both H0 and HA simple: likelihood ratio test
f0(x)
T(x)= fA(x) 
and reject if T(X)&lt;k for some appropriately chosen value k (mostpowerfulbyNeyman-Pearson 
Lemma) 
2. H0 :  = 0 simple, HA :  A composite and 2-sided: construct a 1  condenceinterval 
[A(X),B(X)]and reject if 0 /[A(X),B(X)] 
3. H0 :  = 0 simple, HA : &gt;0 composite and one-sided: construct a 1 2 condenceinterval 
[A(X),B(X)]for  and reject if 0 &lt;A(X). 
4.	general case: Generalized Likelihood Ratio Test statistic 
T(x)= max0 L() = max0 f(x|) 
maxA0 L() maxA0 f(x|) 
and reject if T(X)&lt;k for some appropriately chosen constant k 
Two-SampleTests 
have two independent samples X1,...,Xn1 and Z1,...,Zn2 where Xi N(X,2 ) and Zi  	X
N(Z,2 )i.i.d. Z
2 2 	wanttotest either(i) H0 : X = Z vs. HA : X = Z or (ii)H0 : X 2 = Z vs. HA : X 2 = Z 
	in case(i) form statistic 
  Xn1 Zn2T =  
2 2 
X Z+ n1 n2 
whichis N(0,1) under the null hypothesis. 
3 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>
   
    (a) According to the Neyman-Pearson lemma, the most powerful test is based on the likelihood ratio 
n 1 1 n 2  n  
f(xi=0) 
2 exp 2 i=1 xi n  
r(x)= i
n =1 |= 1 1 
n (xi 1)2  = exp 2  xi 
i=1 f(xi| =1) 
2 exp 2 i=1 i=1 
The most powerful test rejects if the likelihood ratio is less than the critical value, or equivalently,
if Xn &gt;k for some appropriately chosen value of k (onthe examits sucienttopoint outthat we
already derived this in class).
Since under the null hypothesis, Xn N(0,1/n),
PXn &gt;k  =0 =1(nk) 
so choosing k = 1
(1
n ) gives a test of size 5%. The power of this test is given by 
PXn &gt;k =1 =1(n(k1)) 
(b) By similarreasoning asinpart (a),themostpowerful test rejectsif Xn &lt;k,where k=1+
1 
n () . 
(c) Well accept in both tests if 
 kXn k n +1()nXn 1(1) 
 For suciently large n, there will be no values of Xn for which none of the tests rejects. 
(d) Thistestisthe same asinpart (a), because for any value of &gt; 1, the likelihood ratio is a strictly 
decreasing function in the sample mean Xn, and the critical value k for a size  testisdetermined 
only by the distribution under the null hypothesis, which is the same as in part (a). 
(e) The critical regions arethe same,buttheprobability of atypeII errorinpart(d) is smaller, since 
all alternatives are further away from the null than =1. 
6 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>	  	  
	  	  CondenceIntervals 
	nd functions of data A(X1,...,X2)and B(X1,...,Xn)such that
P0 (A(X1,...,Xn)0 B(X1,...,Xn))=1
	then[A(X1,...,Xn),B(X1,...,Xn)]is a1 condenceintervalfor  
 for given signicance level 1 many possible valid condence intervals.
In order to construct condence intervals, usually proceed as follows:
1.	nd a(0)and b(0)such that 
P0 (a(0)T(X1,...,Xn)b(0))=1
for some statistic T(X1,...,Xn)(typicallywill use an estimator here).
2. rewrite the event inside the probability in form
P(A(X1,...,Xn)0 B(X1,...,Xn))=1
3.	evaluate A()and B()at the sample values X1,...,Xn to obtain condence interval  
Some important cases:
unbiased and normally distributed, Var()known:
  
[A(X1,...,Xn),B(X1,...,Xn)]= +1     
Var(),+1 1 Var()2	 2 
unbiased and normally distributed, Var()not known, but have estimator S:  
[A(X1,...,Xn),B(X1,...,Xn)]= + tn1     
Var(),+ tn1 1 Var()2	 2 
not normal, n&gt; 30 or so: most estimators we have seen so far turn out to be asymptotically  
normally distributed, so well use that approximation and calculate condence intervals as in the 
previouscase. Whetherornotweknowthevariance,weusethet-distributionasaway ofpenalizing 
the CI for using approximation. 
not normal, n small:if(a) weknowthep.d.f. of ,canconstructCIsfrom rstprinciples. if(b)  
we dont know the p.d.f., theres nothing we can do. 
HypothesisTesting 
	hypotheses H0 :  0 against HA :  A 
	test statistic T(X)some function of data which takes dierent distributions under the null and the 
alternative 
	critical region C: regions of realizations of T(X)for which we reject the null hypothesis 
2 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
 
  
  
  	in case(ii) form statistic 
2
T =(n1 1)sX
(n2 1)s2 
Z 
whichis F(n11,n21)distributed underthenullhypothesis,and werejectif either T &lt;F1  
	 2 
orif T &gt;F1 1 
2 . 
SampleQuestions(Spring2000Exam) 
1. Method of Moments The random variables X1,...,Xn are independent draws from a continuous 
uniformdistributionwith support[0,]. Youknowfromyoutimein14.30 thatyoucanuseeithermethod 
of moments or maximum likelihood to derive an estimator for  from the sample of Xi. But you want a 
small challenge, so you dene new random variables Y1,...,Yn suchthat 
Yi =0 if Xi k 
1 if Xi &gt;k 
where k is a constant determined by and known to you. you can estimate  only using Y1,...,Yn. 
(a) Assume k (0,). Use thee method of moments to derive an estimator for  as a function of the Yi. 
Also explain why you need k tobeintheinterval(0,). 
(b) Nowassume k (0,)and that k maybegreaterthanorlessthantheunknownparameter .What 
can you say about the relationship between k and  if you observe a random sample with Yn =0? 
(c) Derive the maximum likelihood estimator for  (remember that you can stillonly use Y1,...,Yn for 
the estimation). 
Answers: 
(a) Since  isone-dimensional, onlyhavetouse rstmoment of Yi.Thepopulationexpectationisgiven 
by 
k E[Yi]= P(Xi k)=max 1 ,0 
If k&lt;, the method of moments estimator is obtained by solving 
Yn = E[Yi]=1 k
=1 k
Yn 
(b) If k&gt;, E[Yi]= P(Xi k) =max 1k
,0 = 0 does not depend on  anymore. If we dont 
know whether k is greater or less than , we can use the same reasoning as in the construction of 
the method of moments estimator to bound the parameter  setting 
k k k Yn = max 1
,0 1
 Yn1 
If Yn =0 for a large sample, k islikely tobegreaterthan . 
(c) In order to derive the likelihood function, note that 
k P(Yi =1) = P(Xi k)=1 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Functions of random variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>      
 
 The inverse function will also be strictly monotonic, so that for 0 x 1, the c.d.f. of the random 
variable FX (X)isgivenby 
PFX (X)x = PF 1(FX (X))F 1(x)= PX F 1(x)= FX (F 1(x))= xX X X X 
(the rst equalityuses monotonicity of F 1(), and the third the denition ofa c.d.f.). 
Summarizing, the c.d.f. G()of the random variable F (X)isgivenby 
 
 0 if x&lt; 0 
G(F (x))= x if0 x&lt; 1  1 if x 1 
Wecaneasily check thatthisisalsothec.d.f. of auniformrandomvariableontheinterval[0, 1], so that 
F (X)has the same probability distribution as U[0, 1]  
What is this result useful for? As an example, there are very ecient ways of generating uniform 
random numbers with a computer. If you want to get a sample of n draws from a random variable with 
c.d.f. FX (), you can 
 draw U1,...,Un U[0, 1] 
 transform each uniform draw according to 
Xi = F 1(Ui)X 
By theargumentwemadebefore, X1, ,Xn behavelikearandomvariablewith c.d.f. FX (). Thismethod  
is known as integral(orquantile) transformation. 
Example 7 Say, we have a computer program which allows us to draw a random variable U from a 
uniform distribution, but we actually want to obtain a random draw of X withp.d.f. 
1 e 1
2x if x 0 
otherwise fX (x)= 2 
0 
We can obtain the c.d.f. of X byintegration: 
0 if x&lt; 0 FX (x)= 1e 1
2x if x 0 
so that the inverse of the c.d.f. is given by 
F 1(u)= 2log(1u) for u [0, 1] X 
If we try this using some statistics software or Excel, a histogram of the draws will look like this: 
5 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>y = X2, X ~ U [-1, 1] 
fy (y) 
1 
2 
1 y 
    
  
   
 1.3 Change of Variables Formula for One-to-One Transformations 
It is in general not very convenient to derive the density of Y fromthedensity fX (x)of X throughthe 
c.d.f.s, in particular because this involves one integration and one dierentiation. So one might wonder 
whether there is a more direct connection between the p.d.f.s. 
Before going to the more general case, suppose u(x)= ax for some constant a&gt; 0. Then the c.d.f. of 
Y = u(X)= aX isgivenby 
yFY (y)= fX (x)dx = fX (x)dx = FX 
y aaxy xa 
Using the chain rule, we can derive the p.d.f. of Y 
d d y 1 d 1 fY (y)= FY (y)= = FX (x)= fX (x) FX dy dy a adx a 
What is a good intuition for this? -if a&gt; 1, we could think of the transformation as stretching the axis 
on which the random variable falls. This moves any pair of points on the axis apart by a factor of a, 
butleaves constanttheprobability thatthe variablefallsbetweenthepoints. Therefore,thedistribution 
of Y is thinned out by a factor of a 1 compared to the distribution of X. One could visualize this by 
thinking about a lump of dough containing a number of raisins -the more we spread out the dough, the 
sparser the distribution of the raisins in the dough will be with respect to the surface of the table. 
For dierentiable monotone transformations u()of X we have the following result 
Proposition 1 Let X be a continuous random variable with known density fX (x)suchthat P (a X 
b)=1, and Y = u(X). If u() is strictly increasing and dierentiable on an interval [a,b], and has an 
inverse s(y)= u1(y), then the density of Y isgivenby 
fX (s(y))  d s(y) if u(b)dy u(a)y fY (y)= 
0 otherwise 
Notice that an analogous result is true if u(x)is strictly decreasing on[a,b]. 
Example 6 Let X be uniform on [0, 1], so it has p.d.f. 
1 if0 x 1 fX (x)= 0 otherwise 
3 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 9 
Konrad Menzel 
March 10, 2009 
Functions of Random Variables 
Inthispart of thelecture we aregoing tolook atfunctions of random variables, Y = u(X). Note that Y 
is again a random variable: since X is a mapping from the sample space S into the real numbers, 
X : S R
and u : R R, the composition of u and X is also a mapping from S into the real numbers: 
Y = uX : S R  
Example 1 If X is the life of the rst spark plug in a lawnmower, and Y the life of the second, we may 
be interested in the sum of the two, Z = X + Y . 
Example 2 Beforecoming toMIT,I appliedforseveralGermanfellowships, soI would receiveamonthly 
stipend of X Euros, depending on which fellowship I was going to get, and the exchange rate in, say, 
September 2005 was going to be Y Dollars per Euro. Each quantity was uncertain at the time I was 
applying,but sinceI wasgoing to spend the money intheUS,the mainquantity ofinterest wasthedollar 
amount Z = X Y I was going to receive (at least in terms of the Dollar exchange rate, I could not 
complain). 
Wenowwanttoknowhowtoobtainthedensity and c.d.f. forthetransformed randomvariable u(X), 
so we can treat each problem involving a function of a random variable in the same way as a question 
involving only the random variable itself with a known p.d.f. 
Well consider three cases: 
1. underlying variable X discrete 
2. underlying variable X continuous 
3. X continuous and u(X)strictly increasing 
The last case is of course a special case of the second, but well see that its much easier to work with. 
1 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
 
 
 1.1 Discrete Case -2-Step Method 
If X is adiscrete random variable withp.d.f. fX (x), andY = u(X), whereu()is a known deterministic 
function. Then 
fY (y)= P (Y = y)= P (u(X)= y)= fX (x) 
x:u(x)=y 
Example 3 
if 5 fX (x)= 1 x {2, 1, 0, 1, 2}
0 otherwise 
Thenif Y = g(X)= X,||
 
 fX (0)= 1	if y =0 	 5 
fY (y)=  fX (1)+fX (1)= 52 if y =1 
 fX (2)+fX (2)= 2 if y =2 	 5  0	 otherwise 
Notethatif X isdiscrete,then Y is also discrete. 
1.2 Continuous Case -2-Step Method 
If X is a continuous random variable with p.d.f. fX (x), andY = u(X), then the c.d.f. of Y isgivenby 
FY (y)= P (Y y)= P (u(X)y)= fX (x)dx 
x:u(x)y 
If Y is also continuous, then 
d fY (y)= FY (y)dy 
Note that even if X is continuous, Y need not be continuous. 
Example 4 Y = X, the largest integer smaller than X, is discrete regardless whether X is continuous 
ordiscrete. 
Example 5 
fX (x)=	21 if 1 x 1 
0 otherwise 
Lets look at 
Y = X2 
From X [1, 1], it follows that Y =[0.1]. How do we get the density of Y ? For y [0, 1], the c.d.f. is 
 y 
FY (y)= P (Y y)= P (X2 y)= P (y X y)= 1 dx = y 
y 2 
In sum  
 0 if y&lt; 0 
FY (y)= y if y [0, 1)  1 if y 1 
Since Y is continuous, we can derive the density 
 1d 2y if y [0, 1] fY (y)= FY (y)= dy 0 otherwise 
2 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>XL y1 
y1 1 
Not defined 
Not defined -1Fx (y)Fx(x) 
X2 
X1 0 
1 
XL 
4   
  Whatisthep.d.f. of Y = X2? We can see that on the support of X, u(x)= x2 is strictly increasing and 
dierentiable, so that we can use the inverse of u(), s(y)= y to obtain the p.d.f. of Y , 
 d  1 1 if0 y 1 fY (y)= fX (s(y))  dy s(y)  = fX (y)2y =0 2y 
otherwise 
Thisissimilartooneexamplewedid above, exceptthatinthepreviouscase,thesupportof X was [1, 1], 
so that u(x)= x 2 was not monotone on the support of X. 
Itisvery importanttonotethatthisformula works only for dierentiable one-to-one -i.e. monotone 
-transformations. In other cases, we have to stick to the more cumbersome 2-step methods for the 
discrete and continuous case, respectively. 
1.4 Probability Integral / Quantile Transformation 
For continuous random variables, there is an interesting -and also very useful -result: the c.d.f. of a 
c.d.f. is that of a uniform variable in the following sense: 
Proposition 2 Let X be a continuous random variable with c.d.f. FX (x). Then the c.d.f. evaluated at 
a random draw of X, FX (X)is uniformly distributed, i.e. 
FX (X)U[0, 1] 
You should notice that afunction of a random variableisitself a random variable(welldiscuss thisin 
more detail later on). 
Proof: Since the c.d.f. takes values only between zero and 1, we can already see that the c.d.f. G()of 
F (X)satises 
G(F (X))= P (F (X)x) =0 if x&lt; 0
G(F (X))= P (F (X)x) =1 if x&gt; 1
Withoutlossofgenerality(i.e. only avoiding afewuninteresting extradenitionsorcasedistinctions), 
suppose F ()is strictly monotonic -keeping in mind that all c.d.f.s are nondecreasing. This means that 
there is an inverse function F 1(), i.e. a function such that F 1(F (x))= x. 
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>0 .5 1 1.5 Density 
0 .2 .4 .6 .8 1 
Uniform Draws 
0 .1 .2 .3 .4 .5 Density 
0 5 10 15 
Exponential Draws 
Figure1:Histogramof5,000draws Ui fromaUniform(left),and thetransformation Xi = 2log(1Ui) 
(right) 
Ifyouwanttotry afewexamplesonyourowninExcel,youcancreateuniformrandomdrawsusing the 
RAND() function. You canthenplothistogramsby clickingyourselfthroughthe menus(Tools&gt;Data 
Analysis&gt;AnalysisTools&gt;Histogram). 
6 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Median, quantiles, and variance (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec12/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>  
 
    
 
  2 Variance 
The variance is a measure of the dispersion of a random variable. 
Denition2 The variance of a random variable X isgivenby 
2Var(X)= E (X E[X])
Sometimes we also denote the variance by 2(X)=Var(X). 
Property1 Var(X)=0 if and only if P(X = c)=1 for some constant c. 
Property2 If Y = aX + b, then 
VarY = a 2Var(X) 
Proof: Again, well only look at the continuous case. Using our previous results on expectations 
    
Var(Y) = (aX + bE[aX + b])2fX (x)dx = (ax aE[X])2fX (x)dx 
    
= a 2 (x E[X])2fX (x)dx = a 2Var(X) 
 
It is often convenient for the measure of dispersion to have the same units as the random variable. 
However, this last result implies that the unit of Var(X)will be the square of the unit of X. Therefore, 
we often use the standarddeviation (X)instead, 
(X):= Var(X) 
Property3 
Var(X)= E[X2]E[X]2 
Proof: 
Var(X)= E (X E[X])2 = E X2 2XE[X]+E[X]2 
= E[X2]2E[X]E[X]+E[X]2 = E[X2]E[X]2 
Property4 If Y = a1X1 + a2X2 + ... + anXn + b and X1,...,Xn are independent, then 
Var(Y)= a 2Var(X1)+a 2Var(X2)+... + a 2 Var(Xn)1 2 n
Example3 Suppose X is a discrete random variable with p.d.f. 
1 if x {2,0,1,3,4}fX (x)= 5 
0 otherwise 
If we dene Y =4X 7, what is the variance of Y? 
Var(Y)=42Var(X)=16 E[X2]E[X]2 
We can now calculate 1 6 E[X]= (2+0+1+3+4) = 5 5 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Intuitively,themedianisbased only on ordinal propertiesof therandomvariablewhich arepreserved 
by any strictly increasing transformation. 
We also saw that expectations were linear in the sense that the expectation of a linear function of 
multiple random variables was shown to be equal to that same linear function of the expectations. For 
medians this is no longer true as the following example shows: 
Example1 Suppose X1 and X2 are discrete random variables which are from the same marginal distri
bution  
 0.6 if x =0 
fX (x)= 0.4 if x =1  0 otherwise 
and independent of each other. Then Y = X1 + X2 can take the values 0, 1, and 2, and has p.d.f. 
 
 0.36 if x =0   0.48 if y =1 fY (y)=  0.16 if y =2   0 otherwise 
The median of X1 and X2 is zero, however median(Y)=1 0+0 =median(X1)+median(X2). = 
More generally, the quantiles of averages may also dier from averages of quantiles. The following 
example gives another very practical interpretation of this insight (thanks to Aleksandr Tamarkin for 
providing the following numerical example): 
Example2 Say you are taking the GRE, a standardized test which consists of three components, verbal 
X1, analytic X2, and quantitative X3. Your score is above the 90th percentile for each section of the test. 
Does this mean that you are also above the 90th percentile for the overall score? The general answer is 
no. 
Suppose, including yourself, there are 100 test takers, and the distribution of scores is as follows: 84 
test takers dont get a single point in any section, you got 250 points in each of the three sections, and 
there are three other types of test takers, each with somewhat savant-like insular abilities in only one 
of the three areas. More specically, 5 people are extremely verbally gifted and score 800 on the verbal 
part, but 0 everywhere else, another 5 get 800 on the analytic part, and yet another 5 get 800 on the 
quantitativepart,but zeroin all other sections. In sum,thejointdistribution of scoresis(note thatthis 
is very far from the typical distribution of GRE scores...) 
 
 0.84 (x1,x2,x3)=(0,0,0)     0.01 (x1,x2,x3)=(250,250,250)(you)   0.05 (x1,x2,x3)=(800,0,0) fX1 ,X2,X3 (x1,x2,x3)=  0.05 (x1,x2,x3)=(0,800,0)     0.05 (x1,x2,x3)=(0,0,800)   0 otherwise 
So you are at least at the 95th percentile for each part, but for 15 other test takers, the total score 
is 800, whereas yours is only 750, so you are only at the 85th percentile with respect to the overall score 
across the three sections. 
2 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 12 
Konrad Menzel 
March 19, 2009 
Properties of Medians and Percentiles 
We dened the median of a random variable via 
1 P(X&lt; median(X))= 2 
When X is discrete or has point masses that generate jumps in the c.d.f., this denition may not be 
useful, so in the more general case, we dene the median as 
1 median(X):= min m  R : P(X  m) 2 
The change with respect to the narrower denition is that if the c.d.f. has a discontinuity which makes 
it leap over the value1
2,just locate the median at the point of that discontinuity. We can also dene 
other percentiles of the distribution of X: 
Denition1 For a random variable X, the  quantileisgivenby 
q(X,):= min{q  R : P(X  q  } 
We also call q(X,p/100) the pthpercentile. 
Note that following this denition, the median corresponds to the 50th percentile. Other frequently 
usedquantiles aredeciles(p=10,20,30,..., 90) andquartiles(p=25,50,75). 
Now we wont spend as muchtime onproperties ofquantiles as wedidfor expectations,butIdjust 
liketopoint outtwoimportant waysin which the medianbehaves very dierently fromthe expectation: 
For one, we saw from Jensens Inequality that for a function u(X), the expectationE[u(X)]depends 
a lot on the curvature of u(x)inthe regions wheretheprobability mass of X lies. Generally, the median 
median(u(X))will also be dierent from u(median(X)), but there is a notable exception: 
Proposition1 Suppose u(x)is strictly increasing in the support of X. Then 
median(u(X))= u(median(X)) 
Proof: The median of X satises P(X&lt; median(X)) = 1
2. Since u(x) is strictly increas
ing, the event X&lt;m is identical to the event u(X) &lt;u(m) for any xed number m. Therefore, 
P(u(X) &lt;u(median(X)) = P(X&lt; median(X)) 1
2= , so that u(median(X)) is indeed the median of 
u(X) 
1 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Theexpectationisthereforealsoreferredtoasthe rstmoment of thedistributionof X,and thevariance 
asits second central moment. 
Other frequently used characteristics of a distribution are 
3 = E[(X E[X])3] 
which is called the skewness of the distribution of X, and 
4 = E[(X E[X])4] 
the kurtosis of X. Ahighkurtosis correspondstothedistributionhaving alot ofprobability massinthe 
tails. 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
 
 and 1 30 E[X2]= ((2)2 +02 +12 +32 +42)= =6 5 5 
Therefore, 26 15036 1824 Var(Y)=16 6 =16 =  73 5 25 25 
Example4 Suppose Y  B(n,p). Since Y can be written as the sum of outcomes from n independent 
trials, 
1 with probability pY = X1 + ... + Xn,where Xi = 0 with probability 1p 
we can calculate 
Var(Y)=Var(X1)+... +Var(Xn) 
So what is the variance of Xi? Clearly, 
E[Xi]= p 
also, 
E[Xi 2]=1 p+0  (1p)= p 
Therefore,byproperty3 
Var(Xi)= E[Xi 2]E[Xi]2 = pp 2 = p(1p) 
Therefore 
n 
Var(Y)= Var(Xi)= np(1p) 
i=1 
Since the variance is an expectation, we can directly apply results on expectations of functions of 
random variables directly to the variance of a function of random variables: if Y = r(X), 
    2 
Var(Y)= E[Y2]E[Y]2 = E[r(X)2]E[r(X)]2 = r(t)2fX (t)dt r(t)dt 
  
2.1 Higher-Order Moments 
We saw that the expected value is a measure of the location of a distribution, whereas the variance 
measuresits dispersion. We may look at other moments of the random variable in order to characterize 
its distribution, e.g. whether its symmetric, has fat tails etc. 
Denition3 The r-th moment of X isgivenby 
r  = E[Xr] 
and the r-th central moment is dened as 
r = E[(X E[X])r] 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Constructing estimators (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text> 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 18 
Konrad Menzel 
April 23, 2009 
1 Properties of Estimators (continued) 
1.1 Standard Error 
Often we also want to make statements about the precision of the estimator -we can always state the 
value of the estimate, but how condent are we that it is actually close to the true parameter? 
Denition1 The standard error () of anestimateisthe standarddeviation(or estimated standard 
deviation) of the estimator, 
SE()= Var((X1,...,Xn)) 
Should recall that an estimator is a function of the random variables, and therefore a random variable 
for which we can calculate expectation, variance and other moments. 
Example1 The mean Xn of an i.i.d. sample X1,...,Xn where Var(Xi)= 2 has variance 
n 2 . There
fore, the standard error is SE(Xn)= n 
If we dont know 2, we calculate the estimated standard error 
SE(Xn)= 
n 
The standard error is a way of comparing the precision of estimators, and wed obviously favor the 
estimator which has the smaller variance/standard error. 
Denition2 If A and B are unbiased estimators for ,i.e. E0 [A]= E0 [B]= 0, then we say that 
A is ecient relative to B if 
Var(B)Var(A) 
Sometimes we look at an entire class of estimators  = {1,2,...}, and we say that A is ecient in 
that class if it has the lowest variance of all members of . 
Example2 Supposethat X and Y are scores from two dierent Math tests. You are interested in some 
underlying math ability, and the two scores are noisy (and possibly correlated) measurements with 
E[X]= E[Y]= , Var(X)= X, Var(Y)= Y 2 , and Cov(X,Y)= . Instead of using only one of the 2 XY 
measurements, you decide to combine them into a weighted average pX +(1p)Y instead. What is the 
1 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
 
 

expectation of this weighted average? Which value of p minimizes the variance of the weighted average?
We can interpret this as an estimation problem in which we want to estimate  using a sample of only
two observations. Since all weighted averages of X and Y have mean , well try to nd the ecient
estimator.
From the formula of the variance of a sum of random variables,
2 2Var(pX +(1 p)Y)= p 2
X +2p(1p)XY +(1 p)2
Y 
In order to nd the optimal p, we set the rst derivative equal to zero, i.e. 
0= X +2(1 2p)XY 2(1p)22p2 
Y 
X +2Solvingfor p, we get, assuming that 2 
Y &gt; 2XY (notice that this is also the sucient condition for 
a local minimum) 
 Y 2 XY Var(Y)Cov(X,Y) Cov(Y X,Y) p = = = 2 Var(Y X) Var(Y X)X 2XY + 2 
Y 
2 
Y  Note that if X and Y are uncorrelated, the ecient estimator puts weight p on X whichis = 2 
X2 
Y+
greater the lower the variance of X is relative to that of Y. 
2 Methods for Constructing Estimators 
2.1 Method of Moments 
Thismethod wasproposedby theBritish statisticianKarlPearsonin1894: supposewehavetoestimate 
k parameters of a distribution. then we can look at the rst k sample moments ofthedata, 
n
n i=1 1Xn = Xi 
n
n i=1 
. . . 1 X2 
i X2 = n 
n
n i=1 
and equatethemtothecorresponding population moments foragivenparametervalue, calculated under 
thedistribution, 1 Xk 
i Xk = n 
 
1()= E[Xi] xfX(x;)dx 
 
. . . 
 
k() = E[Xik] kfX(x)dx | x 
 
(1) 
2 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>In order to nd the maximum, we take the derivatives with respect to  and 2, and set them equal to 
zero: n	 n1	 1 
Xi 0= 
2(Xi )=
22 i=1  n i=1
Similarly, 
n	 n nn 2 1	 1  
)2 1  
Xn)2+ 
(Xi )2 2 = 222 2
22 
i=1 
n i=1 (Xi = n i=1 (Xi   0= 
Recall that wealready showed thatthisestimatorisnot unbiasedfor 02,soingeneral,MaximumLikelihood 
Estimators need not be unbiased. 
Example6 Going back to the example with the uniform distribution, suppose X U[0,], and we are 
interested in estimating . For the method of moments estimator, you can see that 
 1()= E[X]= 2 
so equating this with the sample mean, we obtain 
MoM =2Xn 
What is the maximum likelihood estimator? Clearly, we wouldnt pick any max{X1,...,Xn}because 
a sample with realizations greater than has zero probability under . Formally, the likelihood is 
L()= 
 1 n if0 Xi  for all i =1,...,n 
0 otherwise 
We can see that any value of  max{X1,...,Xn}cant be a maximum because L()is zero for all those 
points. Also, for  max{X1,...,Xn}the likelihood function is strictly decreasing in , and therefore, 
it is maximized at 
MLE =max{X1,...,Xn} 
Note that since Xi &lt;0 with probability 1, the Maximum Likelihood estimator is also going to be less 
than 0 with probability one, so its not unbiased. More specically, the p.d.f. of X(n) isgivenby 
 
n 
1 y n1 
if0 y 0fX(n) (y)= n[FX(y)]n1fX(y)=	 0 0 0 
0 otherwise 
so that 
	 0 y n n E[X(n)]= yfX(n) (y)dy = n dy = 00 n +1  0 
We could easily construct an unbiased estimator = n+1 X(n). n 
2.3 Properties of the MLE 
Thefollowing isjust a summary of maintheoretical results onMLE(wontdoproofsfor now) 
 If there is an ecient estimator in the class of consistent estimators, MLE will produce it. 
5 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>	Undercertainregularity conditions,MLEswillhave anasymptotically normaldistribution(this 
comes essentially from an application of the Central Limit Theorem) 
Is Maximum Likelihood always the best thing to do? -not necessarily 
 may bebiased 
 often hard to compute 
	might be sensitive to incorrect assumptions on underlying distribution 
6 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Thenthe method of moments(MoM) estimator can be obtained by solving the equations 
j()= Xnj j=1,...,k 
for . 
Example3 Suppose X1,...,Xn isani.i.d. samplefromaPoissondistributionwith unknownparameter 
,i.e. X P(). The distribution has only one unknown parameter, and the rst population moment is 
givenby 
1()= E[X]=  
Therefore, the MoM estimator is given by 
n
n  
i=1 = 1()Xn =1 Xi 
What if we used more moments than necessary to estimate the parameter? -We also know that for 
A doubleexponential randomvariablehasp.d.f. Example4 
 thePoissondistribution 
E[X2]=Var(X)+E[X]2 = + 2 
1 fY (y)= e|y| 
2 
so we have to estimate two parameters (,). We can look up in a statistics book that 
E[Y]=  E[Y2]=Var(Y)+E[Y]2 =2 2+ 2 
so the method of moments estimator solves 
Y = 
2 2Y2 = + 
2 
so that, solving for (),,
 
 = Y,  = 
2
Y2 (Y)21/2 
2.2 Maximum Likelihood Estimation 
While the method of moments only tries to match a selected number of moments of the population to 
their sample counterparts, we might alternatively construct an estimator which makes the population 
distribution as a whole match the sample distribution as closely as possible. This is what the maximum 
likelihood estimator of aparameter  does,whichisloosely speaking,thevaluewhich mostlikely would 
have generated the observed sample: 
Suppose we have an i.i.d. sample Y1,...,Yn wherethep.d.f. of Y isgivenby fY (y), which is known up |
totheparameter . TheMaximumLikelihood estimator(MLE)is afunction of the data maximizing 
the jointp.d.f. of the data under . 
More specically, we dene the likelihood of the sample as 
n
L()= f(y1,...,yn|)= f(yi)|
i=1 
3 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text> 
 
   
  
 
  
  Usually it is much easier to maximize the logarithm of the likelihood function, 
n
logf(yi) L()=log(L()) | = 
i=1 
Note that since the logarithm is a strictly increasing function, L()and L() will be maximized at the 
same value. 
Proposition1 The expectation of the log-likelihood under the parameter 0, 
E0 [L()]= E[logf(Y)] |
is maximized at the true parameter 0. 
Proof: Since the true density over which we take the expectation is f(y0), we can show that |
E0 [L(Y|)L(Y|0)]0 for all values of  using Jensens Inequality and the fact that log()is concave 
f(Y|) 
0) E0 [L(Y|)L(Y|0)] = E0 [logf(Y|)logf(Y|0)]= E0 log f(Y|
f(y|) 
=log f(y|) 
|0) |0) log f(y0)dy | E0 f(y f(y 
 
= log f(y)dy =log(1) =0 
 |
since f(y|)isadensityand thereforeintegratesto1. Therefore E0 [L(Y|0)]E0 [L(Y|)]forallvalues 
of , so that 0 maximizesthefunction  
Since by the Law of Large Numbers, the log likelihood for and i.i.d. sample 
1 n
n i=1 logf(Yi) p E[logf(Y)] | |
n wed think that maximizing theloglikelihoodforalargei.i.d. sampleshould thereforegiveusaparameter 
close to 0. 
Example5 Suppose X N(0,02), and we want to estimate the parameters  and 2 from an i.i.d. 
sample X1,...,Xn. The likelihood function is 
1 (Xi)2 
L() 22 
2 = e 
i=1 
It turns out that its much easier to maximize the log-likelihood, 
n  1 (Xi)2 
logL() log 22 
2 = e 
i=1 
n21 ( ) Xi   22 2 
 log 
n= 
i=1 
n 1 log(22)2 =  22 (Xi )2 
i=1 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Joint and marginal distributions (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>  
 
 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 7 
Konrad Menzel 
February 26, 2009 
1 Joint Distributions of 2 Random Variables X,Y (ctd.) 
1.1 Continuous Random Variables 
If X and Y are continuous random variables dened over the same sample space S.Thejointp.d.f. of 
(X,Y),fXY (x,y)is a function such that for any subset A ofthe(x,y)plane, 
P((X,Y) A)= fXY (x,y)dxdy 
A 
As in the single-variable case, this density must satisfy 
fXY (x,y) 0 for each(x,y) R2 
and   
fXY (x,y)dxdy =1 
  
Notethat 
 any single point has probability zero 
 any one-dimensional curve on the plane has probability zero 
Example1 A UFO appears at a random location over Wyoming, which -ignoring the curvature of the 
Earth -can be described quite accurately as a rectangle of 276 times 375 miles. The position of the UFO 
is uniformly distributed over the entire state, and can be expressed as a random longitude X (ranging 
from -111 to -104 degrees) and latitude Y (withvalues between 41 and 45 degrees). 
This means thatthejointdensity of the coordinatesisgivenby 
1 
fXY (x,y)= 28 if 111  x 104 and 41  y  45 
0 otherwise 
If the UFO can be seen from a distance of up to 40 miles, what is the probability that it can be seen from 
Casper,WY(whichis roughlyin the middle of the state)? 
Lets look at the problem graphically: This suggests that the set of locations for which the UFO can be 
seen from Casper can be described as a circle with a 40-mile radius around Casper. Also, for the uniform 
density, the probability of the UFO showing up in a region A (i.e. the integral of a constant density over 
1 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
 
4 Example3 Thisexampleisbased onreal-worlddataextra-marital aairscollectedby the Redbook mag
azinein1977.1 Inthe survey,individuals were askedto ratetheir marriage on a scalefrom1(unhappy) 
to3(happy), and to reportthe number of extra-marital aairs,dividedby the number ofyears married. 
Fornowletslook atthejointdistributionof marriagequality, X, with duration of marriage in years, 
Y.Wecanstartfromthe cell probabilitiesgivenby thejointp.d.f.,and thenllinthemarginalp.d.f.s
on the left and at the bottom of the table:
Itisinteresting to notethat, eventhough the marginaldistributions are relatively even,thejointdistribu-
Y 
fXY 1 8 12 fX 
1 4.66% 11.48% 12.98% 29.12% 
X 2 5.16% 14.81% 12.31% 32.28% 
3 13.48% 16.47% 8.65% 38.60% 
fY 23.30% 42.76% 33.94% 100.00% 
tionseemstobeconcentratedalong thebottomleft/top rightdiagonal,withthejointp.d.f. taking much 
lower values in the top-left and bottom-right corners of the table. 
Example4 Recalltheexamplewiththetwosparkplugsfromlasttime. Thejointp.d.f. was 
2e(x+y) if x  0 and y  0 fXY (x,y)= 0 otherwise 
The marginal density of X is 
   
fX(x)= fXY (x,y)dy = 2 e (x+y)dy 
 0   
= ex eydy = ex[10] = ex 
0 
Similarly, 
fY (y)= ey 
Independence 
Recall that we said that two events A and B wereindependentif P(AB)= P(A)P(B). Now well dene 
a similar notion for random variables. 
Denition2 We say that the random variables X and Y are independent iffor any regions A,B  R, 
P(X  A,Y  B)= P(X  A)P(Y  B) 
Note that this requirement is very strict: we are looking at events of the type X  A and Y  B and 
requirethat all pairs of them are mutually independent. 
This denition is not very practical per se, because it may be dicult to check, however if X and Y are 
independent,itfollowsfromthedenitionthatinparticular 
FXY (x,y)= P(X  x,Y  y)= P(X  x)P(Y  y)= FX(x)FY (y) 
From this, it is possible to derive the following condition which is usually much easier to verify 
1Data available at http://pages.stern.nyu.edu/ wgreene/Text/Edition6/tablelist6.htm 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
  
 
  1000   
= ex 1e (1000x) dx 
0  1000 
=   
e x e 1000 
dx 
0 
=1e 1000 1000e1000 =1(1+1000)e 1000 
Again, events over continuous bivariate random variables correspond to areas in the plane, and we nd 
probabilities by integrating the density over those areas. 
2 Joint c.d.f. of 2 Random Variables X,Y 
Illjustgivedenitions. We are notgoing to usethis alotinthis class,butyou shouldhave seenthis. 
Denition1 The joint c.d.f. for random variables (X,Y) is dened as the function FXY (x,y) for 
(x,y) R2 
FXY (x,y)= P(X  x,Y  y) 
We computeprobabilitiesfromjoint c.d.f.s asfollows 
P(a  X  b,c  Y  d)= F(b,d)F(a,d)F(b,c)+F(a,c) 
We have to add in the last term because in a sense, it got subtracted o twice before. 
Joint c.d.f.s are related to p.d.f.s in the following way: for continuous random variables, 
y x 
FXY (x,y)= fXY (u,v)dudv 
  
2 
fXY (x,y)= FXY (x,y)yx 
In the discrete case, 
FXY (x,y)= fXY (u,v) 
uxvy 
3 Marginal p.d.f.s 
If wehave a joint distribution, we may want to recover distribution of one variable X. 
If X and Y are discrete random variables withjointp.d.f. FXY ,then 
fX(x)= fXY (x,y) 
all y 
fY (y)= fXY (x,y) 
all x 
If X and Y are continuous, well essentially replace summation by integration, so that 
  
fX(x)= fXY (x,y)dy 
   
fY (y)= fXY (x,y)dx 
 
(1) 
4 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text> 
 
 Example6 Recalltheexamplewiththetwosparkplugsfromlasttime. Thejointp.d.f. was 
2e(x+y) if x  0 and y  0 fXY (x,y)= 0 otherwise 
and in the last section we derived the marginal p.d.f.s 
fX(x)= ex 
fY (y)= ey 
Therefore,theproductis 
fX(x)fY (y)= 2 e x e y = fXY (x,y) 
so that the lives of spark plug 1 and 2 are independent. 
Remark1 The condition onthejoint and marginaldensitiesforindependence canbe restated asfollows 
for continuous random variables: Whenever we canfactor thejointp.d.f. into 
fXY (x,y)= g(x)h(y) 
where g()depends only on x and h()depends only on y,then X and Y are independent. In particular, 
we dont have to calculate the marginal densities explicitly. 
Example7 Say, wehave ajointp.d.f. 
ce (x+2y) if x  0,y  0 fXY (x,y)= 0 otherwise 
Then we can choose e.g. g(x)= cex and h(y)= e2y, and even though these arent proper densities, 
this is enough to show that X and Y areindependent. 
Example8 Suppose wehavethejointp.d.f. 
cx2y if x2  y  1 fXY (x,y)= 0 otherwise 
Can X and Y beindependent?
Eventhoughineithercase(i.e. whether x 2  y  1 holds or whether it doesnt) the p.d.f. factors into
functions of x and y (forthe zeropart,thatstriviallytrue), we can alsoseethatthesupportof X depends
on the value of Y, and therefore, X and Y cant be independent -e.g. if X  21 , we must have Y  1
4 ,
so that     
1 1 1 1 PX  ,Y  =0 &lt;P X  PY  2 4 2 4 
Notethatthejointsupportof tworandomvariableshastoberectangular(possibly all of R2)inorder 
for X and Y to be independent: if its not, for some realizations of X, certain values of Y would be ruled 
out which could occur otherwise. But if that were true, knowing X doesgive usinformation about Y, so 
they cant be independent. However, this condition on the support alone does not imply independence. 
7 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Fxy 
1 
x y  (-111, 45) (-104, 45) 
(-104, 41) (-111, 41) 
375 mi. 276 mi. 
Casper , WY Wyoming 40 mi. 
(x,y) 
Figure1: TheUFO at(x,y)can be seen    Image by MIT OpenCourseWare.
from Casper, WY
A)of the stateisproportional tothe areaof A. Therefore, we dont have to do any integration, but nding 
the probability reduces to a purely geometric exercise. We can calculate the probability as 
Area(less than 40 miles from Casper) 402 P(less than 40miles from Casper) = =  4.9% Area(All of Wyoming) 375 276 
You should notice that for the uniform distribution, there is often no need to perform complicated inte
gration, but you may be able to treat everything as a purely geometric problem. 
Unlike in the last example, typically, theres no way around integrating the density function in order to 
obtain probabilities, since any nonconstant density re-weights dierent regions in terms of probability 
mass. Well do this in the clean, systematic fashion in the following example: 
Example2 Suppose you have 2 spark plugs in your lawn mower, and let X be the life of spark plug 1, 
and Y the life of spark plug 2. Suppose we can describe the distribution by 
2e(x+y) if x  0 and y  0 fXY (x,y)= 0 otherwise 
Figure2 onpage2 shows whatthejointdensitylookslike. 
2 Figure 2: Joint Density of Lives X and       Image by MIT OpenCourseWare.
Yof Sparkplugs 1 and 2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>    
      Proposition1 X and Y areindependentif and onlyiftheirjoint and marginalp.d.f.s satisfy 
fXY (x,y)= fX(x)fY (y) 
Proof: For discrete random variables, this follows directly from applying the denition to A = {x} 
and B = {y}. For continuous random variables, we can show that if X and Y are independent, we can 
dierentiate the equation 
FXY (x,y)= FX(x)FY (y) 
on both sides in order to obtain 
2 2  fXY (x,y)= FXY (x,y)= [FX(x)FY (y)]= fX(x)FY (y)= fX(x)fY (y)yx yx y 
Conversely,iftheproduct ofthe marginalp.d.f.s equalsthejointp.d.f., we canintegrate 
P(X  A,Y  B)= fXY (x,y)dydx = fX(x)fY (y)dydx 
AB AB 
= fX(x)dx fY (y)dy 
A B 
so that the condition on the marginals implies independence, and weve proven both directions of the 
equivalence  
Example5 Going back to the data on extra-marital aairs, remember that we calculated the marginal 
p.d.f.s of reported marriage quality, X, and years married, Y as 
fX(1)=29.12%,fX(2)=32.28%,fX(3)=38.60% 
and 
fY (1)=23.30%,fY (8)=42.76%,fY (12)= 33.94% 
What should thejointdistributionlooklikeif the two random variables wereinfactindependent? E.g. 
f XY (3,1) = fX(3)fY (1)=38.60% 23.30% =8.99% 
The actual value of the joint p.d.f. at that point was 13.48, so that apparently, the two variables are 
not independent. We can now ll in the remainder of the table under the assumption of independence: 
Comparing this to our last table we see some systematic discrepancies -in particular, the constructed 
Y 
f XY 1 8 12 fX 
1 6.78% 12.45% 9.88% 29.12% 
X 2 7.52% 14.81% 10.96% 32.28% 
3 8.99% 16.50% 13.10% 38.60% 
fY 23.30% 42.76% 33.94% 100.00% 
jointp.d.f. f XY is not as strongly concentrated on the diagonal, which seemed to be a noteworthy feature 
of the actualjointp.d.f.. 
But does this really mean that X and Y are not independent? One caveat is that we calculated the 
probabilities in the joint p.d.f. from a sample of draws from the underlying distribution, so there is 
some uncertainty over how accurately we could measure the true cell probabilities. In the last part of the 
class, we will see a method of testing formally whether the dierences between the constructed and the 
actual p.d.f. are large enough to suggest that the random variables X and Y areinfact not independent. 
6 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>  
  
  In fact, this density can be derived from the assumption that the spark plugs fail independently of one 
another at a xed rate , which doesnt change over their lifetime. 
If the lawn mower is going to work as long as either spark plug works, what is the probability that the 
lawn mower fails within 1000 hours? 
1000 1000 y 
x F 
Figure 3: The Event Lawn Mower Fails before 1000 hrs. in rst situation 
 1000  1000 
P(X  1000,Y  1000) = 2 e (x+y)dydx 
0 0  1000  1000 
= 2 e x e ydydx 
0 0  1000  1000 
= ex eydy dx 
0 0  1000 
= ex  
1e 1000 
dx 
0  2 =1e 1000
Whatisthatprobability if thesecond sparkplug isonly usedif the rstonefails,i.e. howdowecalculate 
P(X + Y  1000)? Note that this only changes the event we care about, i.e. the region of R2 we 
1000 
1000 y f 
x 
Figure 4: The Event Lawn Mower Fails      Image by MIT OpenCourseWare.
before 1000 hrs. in second situation 
integrate over, but we still integrate the same density. 
 1000  1000x 
P(X + Y  1000) = 2 e x e ydy dx 
0 0  1000  1000x 
= ex eydy dx 
0 0 
3 Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Central limit theorem, estimators, bias, and consistency (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text> 
 
   
 
 
 3 General Properties of Estimators 
We will denote the expectation of X under the parameter  -i.e. the expectation of X if the true 
parameter is equal to  -by 
 
E[X]= xfX(x)dx |
 
Similarly, Ill write the variance under the parameter  as 
Var(X)=   
(x E[X])2fX(x|)dx 
 
The bias of an estimator is the dierence between its expectation and the true parameter, 
Bias()= E0 []0 
Of course, wed like an estimator to get the parameter right on average, so that ideally, the bias should 
be zero. 
Denition2 An estimator = (X1,...,Xn)is unbiased for  if 
E0 []= 0 
for all values of 0. 
Example6 Suppose X1,...,Xn is an i.i.d. sample from a N(,2) distribution. We already saw last 
week that the expectation of the sample mean 
 E,2 [Xn]= E,2 [X]=  
 for any value of , so that Xn is an unbiased estimator for the mean  of a normal distribution. 
Example7 Suppose we want to estimate the variance parameter 2 for X N(,2) with unknown 
mean  from an i.i.d. random sample X1,...,Xn. Since 2 = E[(X E[X])2], an intuitively appealing 
estimator would be n 
2 =1 (Xi X)2 
n i=1 
(where we substituted the sample mean for the actual expectation). What is the expectation of this esti
mator if the true parameters of the distribution are (0,02)? 
Recall that E[X2]= E[X]2 +Var(X), so that 
n 
E[2]= E 1(Xi 2 2XiX+ X2) n i=1 
n 1 E[X2 X2] = i  
i=1 
n n 
1 = E[Xi 2]E[X2] n i=1 
n   
2 
n n = 1( 2 + 2)  2 +1 
i=1 
2 n 1 2 =  2 + 2  2  n = n 
6 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  2 While the last example is a little deceptive in that the normal approximation looks quite good for n 
as small as3(atleast optically), with n , we usually mean n 40 or larger for the approximation 
to be reasonably accurate. 
Summarizing, the Central Limit Theorem is particularly useful when we dont want to compute the 
true p.d.f. of the sample mean. There are two situations in which this happens 
	We cant compute the actual p.d.f. because we dont know the exact distribution of the Xis 
	we dont want to compute the actual p.d.f. because the computations are too tedious -which is 
almost invariably true for the general convolution formula (see last example), but also for many 
discrete examples(seeBinomial examplefromlastlecture). 
Estimation 
Sofarinthis class, we startedby assuming that weknewtheparameters of thedistribution of a random
variable -e.g. weknewthat X P()-and thencalculatedprobabilitiesand otherpropertiesof random
samples from that distribution. Now we are going to look at the reverse problem:
Assume that we have an i.i.d. sample of observations from a distribution with unknown parameters ,
how do we get a reasonable answer which value of  in the family of distributions we are looking at
may havegeneratedthedata.
Example2 If for a given coin we dont know the probability for heads in a single toss, we could toss 
 Heads it many times. Then wed think that the fraction of heads, p= may be a good guess for the  Tosses 
probability P(Heads)in a sense to be dened later. 
A parameter is a constantindexing afamily ofdistributionsgivenby thep.d.f.s f(x), where we denote |
parameters generally as 1,...,k. 
Example3 for the binomial distribution,  
 
fX(xn,p)=  n
x px(1p)nx for x =0,1,...,n | 0	 otherwise 
the parameters are the number of trials n and the success rate p. 
	for the normal distribution, 
1 (x)2 
fX(x,)= e| 
2 22
so that parameters are mean  and standard deviation 
	the Poisson distribution has one parameter  
 e x for x =0,1,2,... fX(x|)= 0 x! 
otherwise 
Much of statistics is concerned with determining which member of a known family of distributions gives 
the correct probability distribution of an observed process or phenomenon. In symbols, we want to nd 
the parameter value 0 such that X f(x|0). Thisistheproblemof estimating theparameterswhich 
characterizethedistribution. 
Well always start o a random sample X1,...,Xn, and well always assume that 
X f(x|0)for unknown 0  
4 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
  
  
 Denition1 An estimator of  isastatistic(i.e. afunctionof X1,...,Xn), 
= (X1,...,Xn) 
A realization (x1,...,xn)of the estimator in a sample is called an estimate of . 
Notice that, as a function of the random sample, the estimator is a proper random variable, so we 
will in general be interested in describing its distribution in terms of the p.d.f., and moments of its 
distribution. 
Example4 Suppose, X Bernoulli(0),i.e. X is a zero/one random variable which takes the value 1 
withprobability  andhasp.d.f. 
 
 0 if x =1 
fX(x)= 10 if x =0  0 otherwise 
How do we estimate 0? 
Could use sample mean 
n1 (X1,...,Xn)= Xi n i=1 
e.g. from 5 Bernoulli trials 1,0,0,1,1 
3 (1,0,0,1,1) = 5 
Since the estimator for a sample of 5 observations is a random variable, we can derive its p.d.f.: recall 5thatfor S5  i=1 Xi , S5 B(5,0). Applying the methods for nding p.d.f.s of functions of discrete 
random variables to = S5 , we get 5  
(t)=  
55 
t05t(10)5(1t) if t   
0, 51
52 
53 
54 ,1  
, , ,f 0 otherwise 
Inparticular,thedistribution of the estimatordepends onthetrueprobability 0 -which can be anywhere 
intheinterval[0,1] -butcanonly take6dierentdiscretevalues. 
Example5 If X U[0,]is uniform over an interval depending on the parameter, the p.d.f. is 
 1 if0 x  fX(x)= 0 otherwise 
How could we estimate ? Could use e.g. 
1 = max{X1,...,Xn} 
2 =2Xn 
Say, we sampled three observations from the distribution, 0.2,0.6,0.4. Then 1 =0.6 and 2 =0.8, so 
the two estimators give dierent answers on the same parameter. How should we choose among those 
dierent estimators? -Well get back to this in a moment. 
How do you come up with these functions (X1,...,Xn)?  
How can we determine whether these estimators are reasonable?  
 How should we choose between two or more estimators for the same parameter? 
5 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text> 

  Therefore 2 is not an unbiased estimator for 2, but we can easily construct an unbiased estimator 2 
n 
2 = n 1
1 (Xi Xn)2 
i=1 
Where does this bias come from? Broadly speaking, the reason is that inside the square we are replacing 
  with a (noisy) estimate = Xn. You can check on your own that if 0 was known, the estimator 
2 = 1 n (Xi 0)2 would be unbiased for . n i=1
Having to estimate the mean uses up one degree of freedom in the data -e.g. if we only had a sample 
of one observation, the estimated mean would be equal to that observation, and the naive estimator of 
the variance would give us 2 =0, which is clearly not the right answer. 
Unbiasedness may not be the only thing we care about, since the estimator being equal to the true 
parameter on average doesnt mean that in for a given sample, the estimate is actually going to be close 
tothetrueparameter. 
Denition3 For a sample X1,...,Xn, we say that is a consistent estimatorfor  if as we increase 
n, the estimator convergesinprobability to 0, i.e. for all &gt; 0, 
lim P0 |(X1,...,Xn)0|&lt; =1 
n 
for all values of 0. 
In words, in a suciently large sample, a consistent estimator will be within a small distance from the 
true parameter with high probability. Notice that unbiasedness and consistency are two very dierent 
concepts which overlap, but neither implies the other: 
Example8 Back to one of our estimators for the uniform distribution, X U[0,0]. If we look at 
1 =max{X1,...,Xn} 
we can easily see that 1 is not unbiased for , because due to the nature of the uniform distribution, all 
possiblevaluesofXi arelessthan 0. Therefore,nomatterhowlarge n is, P(max{X1,...,Xn}&lt;0)=1. 
Therefore the expectation E0 [1] &lt;0. However, 1 is consistent for 0: We can easily see that for a 
single observation X from the uniform, the c.d.f. is FX(x)= x 
0 . Since Yn := max{X1,...,Xn}is the 
nth order statistic of the sample, we get from our previous discussion that for 0  1, FYn (y)= n y 
(FX(y))n = y 
0 . Since 1 &lt;0 with probability 1, we can therefore calculate for any sample size n 
and any &gt; 0  n 
P( &gt;)= P(Yn &lt;0 )= 0  n|1 0|0 p 
0where p := 0 &lt; 1 since &gt; 0. Therefore,theprobability of adeviationfrom 0 by morethan  vanishes 
as we increase n, and 1 is therefore consistent. 
Example9 By the Law of Large Numbers, the sample mean converges in probability to E[X]= .
Therefore, for an i.i.d. sample X1,...,Xn of N(,2)random variables, the sample mean is a consistent
estimatorfor .
Alternatively, lets look at an unreasonable estimator (X1,...,Xn)= Xn. Then
E[(X1,...,Xn)]= E[Xn]=  
7 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 17 
Konrad Menzel 
April 16, 2009 
The Central Limit Theorem 
Remember that last week, we saw the DeMoivre-Laplace theorem for Binomial random variables, which 
essentially said that for large values of n, the standardization of the random variable Y B(n,p), 
Z = YE[Y] follows approximately a standard normal distribution. Since a binomial is a sum of i.i.d. 
nVar(Y) 
zero/one random variables Xi (countingthe number of trials resulting in a success), we can think of 
Y as the sample mean of X1,...,Xn. n 
ThereforetheDeMoivre-Laplacetheoremisinfactalsoaresult onthestandardized meanofi.i.d. zero/one 
random variables. The Central Limit Theorem generalizes this to sample means of i.i.d. sequences from 
any other distribution with nite variance. 
Theorem1 (Central Limit Theorem) Suppose X1,...,Xn is a random sample of size n fromagiven 
distribution with mean  and variance 2 &lt; . Then for any xed number x, 
lim P nXn 
  x =(x) 
n 
 We say that nXn convergesindistribution (some people also sayconverges in law) to a normal with 
mean  and variance 2, or in symbols: 
d n(Xn )N(0,2) 
So how does the mean converge both to a constant  (accordingto the Law of Large Numbers), and 
a random variable with variance one (according to the central limit theorem) at the same time? The 
crucial detail here is that for the central limit theorem, we blow the sample mean up by a factor of n 
which turns out tobe exactly the right rate tokeep thedistributionfrom collapsing to apoint(which 
happens for the Law of Large Numbers) or exploding to innity. 
Why isthe standard normaldistribution aplausible candidatefor alimiting distribution of a sample 
meantostart with?Rememberthat weargued thatthesumof twoindependent normal randomvariables 
again follows a normal distribution (though with a dierent variance, but since we only look at the 
standardized mean, this doesnt matter), i.e. the normal family of distributions is stable with respect to 
convolution(i.e. additionofindependentrandomvariablesfromthefamily). Notethatthisisnottrue 
for most otherdistributions(e.g. the uniform orthe exponential). 
Since the sample mean is a weighted sum of the individual observations, increasing the sample from n to 
2n, say, amounts to adding the mean of the sequence Xn+1,...,X2n to the rst mean, and then dividing 
 by 2. Therefore,if wepostulated that evenforlarge n, the distribution of Xn was not such that the sum 
1 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>so this estimator is unbiased. However, for any sample size n, the distribution of the estimator is the 
same as that for Xi N(,), so e.g. for  = 0, the probability 
P(|(X1,...,Xn)0|&lt;0)= P( 0 0 Xn 0 +  0) 
= P 1 &lt;Xn 0 &lt; 1= P(1 &lt;Z&lt; 1) 0 
= (1)(1) 0.6825 &lt;&lt; 1 
for all n, where the standardization Z := Xn

0 0 follows a N(0,1) distribution. By this argument, is 
unbiased,but not consistent. 
8 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text> 
 Also, since X1,X2,...,Xk are independent, we can use the rule on the variance of the sum 
Var(Sk) = Var(X1 + X2 + ... + Xk)=Var(X1)+Var(X2)+... +Var(Xk) 
 1  2   1 111 k = kVar(X1)= k t 2 dt = k 3 2+4 =12 0 
Therefore, the standardization Zk of SK isgivenby 
Sk k 
2Zk =  
k 
12 
We can therefore calculate the densities of the standardizations Z1,Z2,Z3 using the change of variables 
formula(notice that thederivativeisjust equal to 12 )k 
1
fZ1 (z)=	
12 if 
3 z 
3 
0 otherwise  
  1
6 + 6 z if 
6 z 0 
fZ2 (z)= 1 z if0  
6 6 z 
6 
 0 otherwise 
 2  +  13 z if 3 z 1  42 2 
fZ3 (z)=  3
8 z 
8 2
2  if 1 z 1 
  219 
43 z + z 
8 if1 z 3  8  0	 otherwise 
Now lets check how this looks graphically: 
Central Limit Theorem for uniform r.v.s density 
0 .1.2.3.4 
4 2 0 2 4 
z 
Density for Z_1 Density for Z_2 
Density for Z_3 Standard Normal Density 
The p.d.f. for standardized sums of uniform random variables looks very similar to the standard 
normal p.d.f. for a sum over as few as 3 independent draws -which is quite surprising since the uniform 
density itself doesnt look at all like that of a normal random variable. 
3 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>  Xbar_n 
0 .2 .4 .6 .8 1 
Xbar_n 
1.5 1 .5 0 .5 1 
0 100 200 300 400 500 0 100 200 300 400 500 
n n 
  Figure1:Numberofheadsin n coin tosses: sample mean Xn (left)and standardized samplemean nXn 
(right) 
of two independent draws was in the same family of distributions, the distribution of the sample mean 
would still change a lot for arbitrarily large values of n, and can therefore not lead to a stable limit. This 
should motivate why itisplausiblethatthedistribution of the mean approachesthe normaldistribution 
inthelimit. 
Example1 Suppose X1,...,Xn are i.i.d. random variables where Xi U[0,1] is uniform, so the p.d.f. 
is  
fX(x)=1 if0 x 1 
0 otherwise 
We can now use the convolution formula from lecture 10 to compute the p.d.f. for the partial sums 
Sk = X1 + X2 + ... + Sk 
For k =2, weget(needtobe careful aboutintegrationlimits) 
 min{s2,1}
fS2 (s2)= fX(s2 w)fX(w)dw = 1dw
max{s21,0}
   
 s2 if0 s2 1 
= min{s2,1}max{s2 1,0}=2s2 if1 s2 2  0 otherwise 
Now, the next calculations become more tedious because we always have to keep track of the integration 
limits and the kink points in the density. After some calculations, I get for k =3 
 2  if0   s 
2 3 s3 1  3 
fS3 (s3)=  
fS2 (s3 w)fX(w)dw = 2 +3s3 s32 if1 s3 2 
  9
2 3s3 + 1
2 s2
3 if2 s3 3   0 otherwise 
By the rule on expectations of sums of random variables, 
k E[Sk]= kE[X1]= 2 
2 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Hypothesis tests (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>1 
1 n = 4 
n = 25 
n = 100 
      
  1 0 k 2 X1 + X2 
1 0 X n 
4 k Don't reject 
Don't reject Cx : reject 
Cx : reject 
Now we can calculate the probabilities of type I and type II error 
k0 k  = P(X&gt;k| =0) =1 =  2/5 2/5 
k1  = P(X  k| =1) = 2/5 
Therefore, xing any one of ,,k determines the other two, and that choice involves a specic tradeo 
between the probability of type I and type II error -if we increase k, the signicance level  goesdown, 
but so does power 1. Specically, if we choose k = 53 ,   6.7%, and   15.87%. 
For dierent sample sizes, we can graph the trade-o between the probability of type I and type II error 
through the choice of k asfollows: 
A low value of k would give high power, but also a high signicance level, so that increasing k would 
2 move us to the left along the frontier. 
How should we choose k? Recall that in the usual setting, the rst priority is to control the probability 
 of false rejections, so well choose k to keep the probability of a type I error at an acceptably low level, 
usually 5% or 1%. 
Of course, as n  , for xed , the power of the test, 1 goes to 1. As a convention, we usually say 
that a rejection at the signicance level  = 5% is signicant, whereas a rejection at  = 1% is highly 
signicant. Image by MIT OpenCourseWare.
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>   
         
n N 1, 4 under H25 A 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 21 
Konrad Menzel 
May 5, 2009 
Constructing Hypothesis Tests 
If Xi has support SX, then the sample X =(X1,...,Xn)has support Sn The critical region of a test 
is a region CX  Sn of the support of the sample for which we reject the null. X. 
X 
The following example illustrates most of the important issues in a standard setting, so you should 
look at this carefully and make sure that you know how to apply the same steps to similar problems. 
Example 1 Suppose X1,...,Xn are i.i.d. with Xi  N(,4), and weareinterestedintesting H0 :  =0 
against HA : =1. Lets rst look at the case n =2: 
X1 X2 
A 
 o Reject : Cx 
Don't reject 
1 1 
k k 
We could design a test which rejects for values of X1 +X2 which are too large to be compatible with 
=0. We can also represent this rejection region on a line: 
This representation is much easier to use if n is large, so its hard to visualize the rejection region in 
terms of X1,...,Xn directly. However, by condensing the picture from n to a single dimension we may 
loose the ability of specifying really odd-shaped critical regions, but typically those wont be interesting for 
practical purposes anyway. 
 So in this example, we will base our testing procedure on a test statistic Tn(X1,...,Xn)= Xn and reject
for large values of Tn.
How do we choose k? -well have to trade o the two types of error. Suppose now that n =25, and since
Xi  N(,4),
4N0,25 Tn :=X underH0
1 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Example 2 In the previous example, the maintained hypothesis was that {0,1}, but this is a highly 
articial assumption, and usually we have no reason to believe that this is the case. 
Suppose that as before X1,...,X25 is an i.i.d. sample with Xi  N(,4), but now we want to test 
H0 : =0 against HA : =0 
Now HA is a two-sided composite hypothesis(i.e. underthealternative  could take several values, some 
on the left, some on the right of 0). Also well again look at a test thats only based on the sample mean, 
X25 -what should the critical region now look like?
Intuitively, it makes sense to reject H0 for both large and small values of X, i.e. we are unlikely to
see values in either tail if the null hypothesis is true, and the alternative hypothesis states that we are
interested in evidence that  is either greater or smaller than 0.
 o 
C x  
2  
2 f( |  o) = f(Xn |  o) 
Therefore, we are going to choose two values k1 and k2 such  Image by MIT OpenCourseWare.
that
   
 k2 k1 = P(X25 &gt;k2|=0)+P(X25 &lt;k1|=0) =1 + 2/5 2/5 
Whatis ? Sincethe alternativedoes notspecify a singleprobability law,but rather a continuum of them, 
 is not well-dened, i.e. for xed , 
     
 k2  k1 ()= P(X25 &gt;k2|)+P(X25 &lt;k1|)=1 + 2/5 2/5 
Usually for a desired signicance level , we choose k1,k2 symmetrically about the value postulated by 
the null hypothesis (note that since the normal distribution is single-peaked and symmetric, this makes 
the critical region as large as possible. 
The last example should remind you of the way we constructed condence intervals for  from a 
normal population with known variance: the above procedure is in fact identical to the following: 
1. construct a 1  condenceinterval[A(X),B(X)]for  (case 1, see notes for last class) 
2. reject H0 if 0 =0 /[A(X),B(X)] 
Sinceweconstructtheinterval[A(X),B(X)]insuchaway that P(A(X)&lt; &lt;B(X))=1 implicitly 
under the null assumptions, so that 
P(0 /[A(X),B(X)]|H0 :  = 0)=  
3 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  (2) (1) 
2 1 k1 k2 0 f(X1 | (1) f(Xn | (2) 
 
 1 
1 -() 
0 
1 Evaluation and Construction of Tests 
Asinourdiscussionof estimation,we rstintroduced thegeneralideaand sawafewexamples. Nowwe 
will see how to choose among tests and how to construct them from scratch. 
1.1 Properties of Tests 
For any test we care about its level of signicance  = P(type I)and its power 1  =1P(typeII).
If H0 and HA areboth simplehypotheses,  and  arewell-denedforgiven ,and wecansimply choose
the test with the highest 1 ,the most powerful test.
If HA is composite, and H0 is simple, we need a metric for comparing power functions 1  ()=
1  P(typeII|) for a given size . A test is uniformly most powerful (UMP) when it is at least as
powerful at every   HA as any other test of the same size. In general, a UMP test need not exist.
Example 3 Sometimes it is possible to nd a uniformly most powerful test: Suppose Xi  N(,4), and 
we are interested in testing 
H0 : =0 against HA : &gt; 0 
 Recall that the most powerful test for H0 :  =0 against HA :  =1 took the form of reject if X&gt;k, 
the general form of the test does not change no matter what A as long as A &gt;0. Therefore a most 
 powerful test for H0 : =0 against HA : &gt; 0 will also take the form reject if Xn &gt;k. 
For the following important result, denote f0(x)= f0(x1,...,xn) the joint p.d.f. of the sample 
X1,...,Xn under the simple null hypothesis H0 : = 0, and fA(x)thejointp.d.f. of thesampleunder 
HA :  = A. 
Proposition 1 (Neyman-Pearson Lemma) In testing f0 against fA (where both H0 and HA are 
simple hypotheses), the critical region 
f0(x)C(k)= x : &lt;k fA(x) 
is most powerful for any choice of k  0. 
Note that the choice of k depends on the specied signicance level  of the test. This means that the 
most powerful test rejects if for the sample X1,...,Xn,the likelihood ratio 
f0(X1,...,Xn) r(X1,...,Xn)= fA(X1,...,Xn) 
4 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text> 
 
 In the notation of the Neyman-Pearson Lemma, x can be any of the 23 possible combinations ofpieces 
of evidence. Using theassumptionofindependence, we canthereforelistallpossible combinationsof clues 
with their respective likelihood under each hypothesis and the likelihood ratio. I already ordered the list by 
the likelihood ratios in the third column. In the last column, I added 
(k)= f0(x) 
r(x)k 
the cumulative sum over the ordered list of combinations x. 
guilty fA(x) not guilty f0(x) likelihood ratio r(x)= f0(x) (k)fA(x) 
1. all three clues 216/1000 9/1000 0.0417 9/1000 
2. no alibi,found purse 144/1000 21/1000 0.1458 30/1000 
3. ran,no alibi 324/1000 81/1000 0.25 111/1000 
4. no alibi 216/1000 189/1000 0.875 300/1000 
5. ran,found purse 24/1000 21/1000 0.875 321/1000 
6. found purse 16/1000 49/1000 3.0625 370/1000 
7. ran 36/1000 189/1000 5.25 559/1000 
8. none of the clues 24/1000 441/1000 18.375 1 
The jury convicting the defendant only if there is at least 95% condence that the charge is true 
correspondsto aprobability offalse conviction(i.e. ifthedefendantisinfactinnocent) ofless than5%. 
In the terminology of hypothesis test, the sentence corresponds to a rejection of the null hypothesis that 
the defendant is innocent using the most powerful test of size  =5%. 
Looking at the values of (k) in the last column of the table, we can read o that including more than 
the rst two combinations of the evidence raises the probability of a false conviction  to more than 5%. 
Therefore,thejury should convictthedefendantifhedoesnthavean alibi and the emptypurse wasfound 
nearhishome, regardless whetherhe ran whenhe sawthepolice. Inprinciple,thejury couldin addition 
randomize whenthedefendant ran,had no alibi,but nopurse wasfound(thatis case3): ifinthat case, 
thejury convictedthedefendant withprobability 5030  1, the probability of a false conviction would be 81 4
exactly equal to5%,but this wouldprobably notbe considered an acceptablepracticein criminaljustice. 
Example 5 We can now showthat atestbased onthe meanisinfact mostpowerfulinthe normal case.
Suppose Xi  N(,4), and we test H0 :  =0 against HA :  =1, where we observe an i.i.d. sample
X =(X1,...,X25)of 25 observations.
Since the observations are i.i.d. normal, the likelihood ratio evaluated at the observed sample is given by
(Xi0)2 
24f(X|=0) 25 
21 
2 e 
r(X)= = (Xi1)2f(X|=1) 1 24 i=1 
22 e 
25 
12 2 = e8 [(Xi 2Xi+1)(Xi )] 
i=1 
25 1 P25 25 1 = e 8 4 i=1 Xi = e 8 100 X25 
We can see that r(X)depends on the sample only through the sample mean X25 and is strictly decreasing 
in X25. Therefore, the critical region of a most powerful test takes the form 
CX(k)= {x : r(x) k} 
6 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> is low, i.e. the data is much more likely to have been generated under HA. 
Reject Fo(x) 
FA(x) 
Cx(k) r(x)= 
k 
A o 
The most powerful test given in the Neyman-Pearson Lemma explicitly solves the trade-o between 
size  
 = P(reject|H0)= f0(x)dx 
C(k) 
andpower 
1 = P(reject|HA)= fA(x)dx 
C(k) 
at every point x in the sample space (where the integrals are over many dimensions, e.g. typically 
f0(x)x  Rn). Fromtheexpressionsfor  and1 wecanseethatthelikelihood ratio fA(x) gives the price 
ofincluding x with the critical region in terms of how much we pay in terms of size  relativetothe 
gain in power from including the point in the critical region CX. 
Therefore, we should start constructing the critical region by including the cheapest points x -i.e. 
thosewith asmalllikelihood ratio. Thenwecangodownthelistof x ordered according tothelikelihood 
ratio and continue including more points until the size  ofthetestisdowntothedesiredlevel. 
Example 4 A criminal defendant (D) is on trial for a purse snatching. In order to convict, the jury 
must believe that there is a 95% chance that the charge is true. 
There arethreepotentialpieces of evidencetheprosecutor may or may nothavebeen abletoproduce, and 
inagivencasethejury takesadecisiontoconvictbased only onwhich out of thethreecluesitispresented 
with. Below are the potential pieces of evidence, assumed to be mutually independent, the probability of 
observing each piece given the defendant is guilty, and the probability of observing each piece given the 
defendant is not guilty 
guilty not guilty likelihood ratio 
1. D ran when he saw police coming 0.6 0.3 1/2 
2. D has no alibi 0.9 0.3 1/3 
3. Empty purse found near Ds home 0.4 0.1 1/4 
5 Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Confidence intervals (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 19 
Konrad Menzel 
April 28, 2009 
Maximum Likelihood Estimation: Further Examples 
Example1 Suppose X N(0,02), and we want to estimate the parameters  and 2 from an i.i.d. 
sample X1,...,Xn. The likelihood function is 
n 1 (Xi)2 
L()= e 22 
2 i=1 
It turns out that its much easier to maximize the log-likelihood, 
n 
(Xi)2  
logL()=  
log 
21 
 e 22 
i=1 
n
=  
log 1 (Xi )2  

2  22 
i=1 
n
=  n 
2log(22) 1 
(Xi )2 
22 
i=1 
In order to nd the maximum, we take the derivatives with respect to  and 2, and set them equal to 
zero: n n1  1 
0= 
22 i=1 2(Xi )= n i=1 Xi 
Similarly, 
n n nn 2 1  1  1   0= 222 +
2
22 
i=1 (Xi )2 2 = n i=1 (Xi )2 = n i=1 (Xi Xn)2 
Recall that wealready showed thatthisestimatorisnot unbiasedfor 02,soingeneral,MaximumLikelihood 
Estimators need not be unbiased. 
Example2 Going back to the example with the uniform distribution, suppose X U[0,], and we are 
interested in estimating . For the method of moments estimator, you can see that 
 1()= E[X]= 2 
1 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Example7 Suppose X1,...,Xn are i.i.d. with Xi U[0,], and we want to construct a 90% condence 
intervalfor 0. Let 
= max{X1,...,Xn}= X(n) 
the nth order statistic (as we showed last time, this is also the maximum-likelihood estimator). Even 
though, as we saw, is not unbiased for , we can use it to construct a condence interval for . 
From results for order statistics, we saw that the c.d.f. of is given by the c.d.f. of isgivenby 

0n  0  
F()= if0 &lt; 0  1 0 
if &gt;0 
where we plugged in the c.d.f. of a U[0,0]random variable, F(x)= x 
0 .
In order to obtain the functions for A and B, let us rst nd constants a and b such that
P0 (a b)= F(b)F(b)=0.950.05 =0.9 
We can nd a and b by solving 
F(a)=0.05 and F(b)=0.95 
n nso that we obtain a = 
0.050 and b = 
0.950. This doesnt give us a condence interval yet, since 
looking at the denition of a CI, we want the true parameter 0 in the middle of the inequalities, and the 
functions on either side depend only on the data and other known quantities. 
However, we can rewrite 
 
n 
  
0.9= P0  = P0 
0.050 
0.950 = P0 
0.95 0 
0.05 (a  b)   n
n n
Therefore 
 max{X1,...,Xn} max{X1,...,Xn} 
[A,B]=[A(X1,...,Xn),B(X1,...,Xn)]= n, n
0.95 
0.05 
is a 90% condence interval for 0. Notice that in this case, the bounds of the condence intervals depend 
on the data only through the estimator (X1,...,Xn). This need not be true in general. 
Lets recap how we arrived at the condence interval: 
1. rst get estimator (X1,...,Xn)and the distribution of . 
2. nd a(),b()such that
P(a()b())=1
3. rewrite the event by solving for  
P(A(X) B(X))=1 
4. evaluate A(X),B(X)for the observed sample X1,...,Xn 
5. the1  condenceintervalisthengivenby 
CI =[A(X1,...,Xn),B(X1,...,Xn)] 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Example4 Suppose N(0,2), and we want to construct a 1 condence interval. If z1/2 is 
the 1  
2 quantile of the standard normal distribution, i.e. (z1/2)=12 , then we can check that 
CI =[z1/2,+ z1/2] 
covers 0 withprobability 
 
  
0  
P0  z1/2 0 + z1/2 = P0 z1/2   z1/2 
= (z1/2)(z1/2) 
 = = 12 21 
since 0
 is the standardization of , and therefore follows a standard normal distribution.
So if we want a 95% condence interval, z1/2 = z0.975 =1.96, so the condence interval is given by
1.96.
This is the most commonly used way of obtaining condence intervals, so you should make sure that you
understand how this works.
Example5 Poll results are often reported with a margin of error. E.g. the Gallup report for April
18 1 saysthat 46% of voters would vote for Clinton over McCain, 44% would vote for McCain, and 10%
would choose neither or had no opinion. These results were based on 4,385 interviews, and the report
goes on to state that For results based on the total sample of national adults, one can say with 95%
condence that the maximum margin of sampling error is 2 percentage points.
What does this mean? -if the true vote share of a candidate is p, the variance of the average share in a
sampleof n voters would be Var( Xn)= p(1
n p) , andyoucanverify thatthisvarianceishighestfor p=0.5.
 
0.50.5So for a sample of 4,385 interviewees, the maximal standard deviation is 
Var( Xn) 4385 0.76%. 
By theCentralLimitTheorem, Xn is approximately normally distributed, and we already saw that for a 
normaldistribution,95% of theprobability massiswithin1.96 standarddeviationsof themean. Therefore 
  theinterval [Xn 1.960.76%,Xn 1.960.76%] will cover the true vote share with probability greater   
than 95%. For smaller subgroups of voters, the margin of error becomes larger. 
Example6 A lab carries out a chemical analysis on blood to be used as evidence in a trial. To be 
acceptable as evidence, a 90% condence interval for the amount of some substance should have length 
less than 0.001g/ml. The machine used for the analysis gives readings which are normally distributed 
around the true value with standard deviation  =0.005g/ml. How many readings do we need in order 
to make sure that the 90% condence interval is shorter than 0.001g/ml? 
The width of a 90% CIis 
 0.005 0.01645 l =2n 1(0.95) 2 n 1.645 = n 
Therefore, in order for l 0.001, we need n 16.452 =270.6025, so wed need atleast271(independent) 
readings. 
The following example illustrates one way of constructing a condence interval when the distribution 
of the estimator is not normal. 
1http://www.gallup.com/poll/106630/Gallulp-Daily-Clinton-Moves-Within-Points-Obama.aspx 
4 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Example3 Supposethe captain of aNavygunboathasto establish abeachhead on a stretch of shoreline, 
but rst has to make sure that a battery on the beach -which cant be seen directly from the sea -is 
destroyed, or at least severely damaged. 
The boat already took some re from the coast, and based on the direction the projectiles came from, the 
captain has an estimate of the position of the battery, which is normally distributed with variance  2 
around the true position 0. 
The captain can re a volley of missiles on a range of the beach, making sure that everything in that 
range gets destroyed. How can the captain determine what range of the shore to re at so that he can be 
95% sure that the battery will be destroyed so that it will be safe to land troops? 
Shoreline Actual position of battery Estimated position of battery 
CI "estimated f( )"  f()
 () + 1.96  () + 1.96 () 0 
For the normal distribution, we know that 95% of the probability mass     Image by MIT OpenCourseWare.
is within 1.96 standard devia
tions on either side of the mean. So if the captain orders to re at the range CI = [1.96,+1.96], 
the probability that will be such that 0 CI equals95%. 
Sowhileearlieronwewereonlylookingforasinglefunction (X1,...,Xn)whichgivesavaluecloseto 
theactualparametervalue 0,wewill nowtrytoconstructtwofunctions A(X1,...,Xn)&lt;B(X1,...,Xn) 
such that the two functions enclose the true parameter with probability greater or equal to some pre
speciedlevel. 
Denition1 A1 condenceintervalfortheparameter 0 isaninterval [A(X1,...,Xn),B(X1,...,Xn)] 
depending on two data-dependent functions A()and B()such that  
P0 (A(X1,...,Xn)0 B(X1,...,Xn))=1
Typically, these functions are not unique, but by convention, we choose A and B suchthat  
2 proba
bility falls on each side of the interval. 
For a realization of the condence interval, [A(x1,...,xn),B(x1,...,xn)], it doesnt make sense to 
say that P(A(x1,...,xn)0 B(x1,...,xn))=1, since now both the limits of the interval and 
the true parameter are just real numbers, so given a realization of the sample, the estimated interval 
eithercovers 0 (withprobability1,ifyouwill),oritdoesnt. Weshouldbeclearthatitisthecondence 
interval(i.e. thefunctions A()and B())which are random given the true parameter, not 0.  
The following is the most common case for which wed like to construct a condence interval. 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>2.1 Important Cases 
1. is normally distributed, Var()2 is known: can form condence interval 
[A(X),B(X)]= 

 2
1 
1  
,+ 
 2
1 
1  
2 2 
2. is normally distributed, Var()unknown, but have estimator S2 =Var(): condence interval is 
givenby 
[A(X),B(X)]= 

S2tn1 
1  
2  
,+ 
S2tn1 
1  
2  
where tn1(p)isthe pth percentile of a t-distribution with n 1 degrees of freedom. 
3. is not normal, but n&gt; 30 or so: it turns out that all estimators weve seen (except for the 
maximum of the sample for the uniform distribution) will be asymptotically normal by the central 
limittheorem(itisnot always straightforwardhow we applytheCLTin agiven case). So well 
construct condence intervals the same way as in case 2. 
4. not normal, n small:if thep.d.f. of isknown,canformcondenceintervalsfrom rstprinciples 
(as in the last example). If the p.d.f. of is not known, there is nothing we can do. 
The reason for using the t-distribution in the second case is the following: since N  
, 
n 2  
, 
 
/n N(0,1) 
On the other hand, we can check that 
(n 1)S2 
2 n2 
1 
since in this setting, Scan usually be written as a sum of squared normal residuals with mean zero and 
variance 2 . Therefore, 
 /n N(0,1) tn1  
=  
(n1) S2 
2 
S2/n 2 /n 1 n1 
Alsonotethatinthegeneralcase4(andinthelast exampleinvolving auniform), wedid notrequire 
that the statistic (X1,...,Xn)be an unbiased or consistent estimator of anything,butitjusthad tobe 
strictly monotonic in the true parameter. However, the way we constructed condence intervals for the 
normal cases(with or withoutknowledge of the variance of , the estimator has to be unbiased, and in 
case3(n large), it would have to be consistent. 
6 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>so equating this with the sample mean, we obtain 
MoM =2Xn 
What is the maximum likelihood estimator? Clearly, we wouldnt pick any max{X1,...,Xn}because 
a sample with realizations greater than has zero probability under . Formally, the likelihood is 
 1 n if0 Xi  for all i =1,...,n L()=  
0 otherwise 
We can see that any value of  max{X1,...,Xn}cant be a maximum because L()is zero for all those 
points. Also, for  max{X1,...,Xn}the likelihood function is strictly decreasing in , and therefore, 
it is maximized at 
MLE =max{X1,...,Xn} 
Note that since Xi &lt;0 with probability 1, the Maximum Likelihood estimator is also going to be less 
than 0 with probability one, so its not unbiased. More specically, the p.d.f. of X(n) isgivenby 
 
n  
1 y n1 
fX(n) (y)= n[FX(y)]n1fX(y)= 0 0 0 if0 y 0 
0 otherwise 
so that    0  y n n E[X(n)]= 
 yfX(n) (y)dy = 
0 n0 n +1 dy = 0 
We could easily construct an unbiased estimator = n+1 X(n). n 
1.1 Properties of the MLE 
Thefollowing isjust a summary of maintheoretical results onMLE(wontdoproofsfor now) 
	If there is an ecient estimator in the class of consistent estimators, MLE will produce it. 
	Undercertainregularity conditions,MLEswillhave anasymptotically normaldistribution(this 
comes essentially from an application of the Central Limit Theorem) 
Is Maximum Likelihood always the best thing to do? -not necessarily 
 may bebiased
 often hard to compute
	might be sensitive to incorrect assumptions on underlying distribution 
2 Condence Intervals 
In order to combineinformation about the value of the estimate anditsprecision(as e.g. givenbyits 
standard error), what is often done is to report an interval around the estimate which is likely going to 
contain the actual value. 
2 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Probability distribution functions (PDFs), cumulative distribution functions (CDFs), joint distribution of 2 or more random variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text> 3 Joint Distributions of 2 Random Variables X,Y 
Inmanysituations,weareinterested not onlyinasinglerandomvariable,but may careabout relationship 
betweentwoormorevariables,e.g. whethertheoutcomeof someprocessaectstheoutcomeof another. 
E.g. we could look at 
	IQs of identical twins -i.e. X would be one kids IQ, and Y that of her/his sibling 
	educational attainment X andincome Y : while we could look at the distributions of income or 
educationseparately,wecanalsoplotboth variablestogetherforobservationsfromadataset. And 
in the graph it looks like there is in fact a non-trivial relationship between the variables. Income 
Years of schooling 
Figure 6: Schooling   Image by MIT OpenCourseWare.
andIncome
	relapsetimes: sinceitisoftennotpossibletoremoveacancercompletely by surgery,wemay want 
to evaluatethe eectiveness of a medicalprocedure,by looking athowlong ittakes until either(a) 
a newoperationbecomesnecessary(X), or(b) thepatientdies(Y ). While we are interested in 
either outcome, both outcomes are interdependent: if the patient dies before a new operation, we 
simply dont observe when he would have had to undergo surgery otherwise. 
In this part of the class, we will consider the properties of two (or more) random variables simultane
ously, including their relationship. We will also introduce concepts analogous to independence and 
conditional probabilities of events. 
Welet(X,Y )be apair random variablesthat(jointly) takes values(x,y), andeither variable can be 
continuous, discrete, or mixed. 
3.1 Discrete Random Variables 
Inthediscrete case,thejointp.d.f. isgivenby 
fXY (x,y)= P (X = x,Y = y) 
for any(x,y) R2 . If {(x1,y1),..., (xn,yn)}contains allpossible values of(X,Y ),then 
n 
fXY (xi,yi)=1 
i=1 
8    </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> 1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 6 
Konrad Menzel 
February 24, 2009 
Examples 
Supposethatarandom variableissuchthaton someinterval[a,b] on the real axis, the probability of 
X belonging to some subinterval [a,b](where a  a  b  b) is proportional to the length of that 
subinterval. 
Denition1 A random variable X is uniformly distributed on the interval [a,b], a&lt;b, if it has the 
probabilitydensityfunction 
1 if a  x  b fX (x)= ba 
0 otherwise 
In symbols, we then write 
X  U[a,b] 
F(x) 
x y 
a 1 
b-a 
b 
Figure 1: p.d.f for a Uniform Random Variable, X  [a,b] 
For example, if X  U[0, 10],then 
 4  4 1 1 P (3&lt;X&lt; 4) = f(t)dt = dt = 10 10 3 3 
Whatis P (3  X  4)? Since the probability that P (X =3) =0 = P (X = 4), this is the same as 
P (3&lt;X&lt; 4). 
1 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Proof: From properties of probabilities, 
P (X&gt;x)=1P (X  x)=1FX (x) 
Similarly, 
Proposition2 For two real numbers x1 &lt;x2, 
P (x1 &lt;X  x2)= FX (x2)FX (x1) 
Proposition3 For any x, 
P (X&lt;x)= F (x ) 
Proposition4 For any x, 
P (X = x)= F (x +)F (x ) 
This last result means in particular that for continuous variables, P (X = x)=0forall valuesof x. 
Example4 Lets check whether the function GX (x) in the following graph is a c.d.f. The function is 
1 
1 2 3 4 5 6 7 3 
4 
1 
2 
1 
4 G (x) 
x 
between 0 and 1, monotonically increasing, and right-continuous. Lets     Image by MIT OpenCourseWare.
apply the last four propositions 
to this example (just reading numbers o the graph): 
3
4 14  P (X&gt; 4) =1F (4)=1= 
 P (3&lt;X  4) = 3
414 = 12 
12  P (X&lt; 4) = F (4)= 
3
412 = 14  P (X =4) = F (4+)F (4) = 
Example5 Unlike for continuous random variables, where we have a one-line formula linking the p.d.f. 
and the c.d.f., in discrete case, have to use the results on deriving probabilities from c.d.f.s we just 
discussed. Lets look at the relationship in another graphical example 
5 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>  limx F (x)=0 
 limx F (x)=1 
 F ()is nondecreasing(can checkderivativebelow) 
What is the p.d.f. f(x)? 
0 if x&lt; 0 f(x)= F (x)= 1 otherwise (1+x)2 
Is f(x)a p.d.f.? -well, weve essentially already shown that F (x)is a c.d.f. We can see right away that 
f(x) 0 for all x 
and also,   
f(t)dt = lim F (x) lim F (x)=10 =0 
x x  
Example7 If X  U[0, 1], then its c.d.f. is 
  x  0 if x&lt; 0 
FX (x)= fX (t)dt = x if0  x&lt; 1  1 if x  1 
a 1 
x 
xb b Fx(x) 
fx(x) a 
1 
b - a 
Figure 5: p.d.f. and c.d.f. for   Image by MIT OpenCourseWare.
X U[a,b]
7    </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>2 
1.5 
1 
.5 
0 Minimum wage 1979 
1n(2) 1n(5) 1n(10) 1n(25) 
2 Figure 2: Log Wages for Female High-School    Image by MIT OpenCourseWare.
Dropouts in 1979
The Cumulative Distribution Function (c.d.f.) 
Denition2 The cumulativedistributionfunction(c.d.f.) FX of a random variable X isdenedfor 
each real number as 
FX (x)= P (X  x) 
Note that this denition is the same for discrete, continuous or mixed random variables. In particular, 
sinceweallowfor X tobediscrete,notethat P (X  x)maybedierentfrom P (X&lt;x),soitsimportant 
to distinguish the corresponding events. In the denition of the c.d.f., well always use X less or equal 
to x. 
Since the c.d.f. is a probability, it inherits all the properties of probability functions, in particular 
Property1 The c.d.f. only takes values between 0 and 1 
0  FX (x) 1 for all x  R 
Also, since for x1 &lt;x2, the event X  x1 is included in X  x2, we have 
Property2 FX is nondecreasing in x,i.e. 
FX (x1) FX (x2) for x1 &lt;x2 
If we let x , the event(X  x)becomes close (hereImvery sloppy aboutwhatthatmeans) 
to the impossible event in terms of its probability of occurring, whereas if x , the event(X  x) 
becomes almost certain, so that we have 
Property3 
lim F (x) =0 
x 
lim F (x) =1 
x 
3 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Note that a c.d.f. doesnt have to be continuous: if we dene the leftlimit 
F (x )= lim F (x h) 
h&gt;0,h0 
andthe rightlimit 
F (x +)= lim F (x + h) 
h&gt;0,h0 
Recall that in order to be continuous at x, F (x)must satisfy F (x)= F (x+). This need not be true in 
general, as the following example shows: 
Example3 Consider again the experiment of rolling a die, where the random variable X corresponds 
to the number we rolled. Then the c.d.f. of X isgivenby 
 
0 if x &lt; 1    1  if 1  x &lt; 2  6 
FX (x)=        5  if 5  x &lt; 6  6  1 if x  6 
whichhasdiscontinuousjumps at thethe values 1, 2,..., 6. 
0 1 2 3 4 5 6 ---F(x) 
F(x) 
x 1 
1 
2 
1 
6 
Figure 3: c.d.f. of a   Image by MIT OpenCourseWare.
die roll
However, by a general result from real analysis, any monotone function (hence the c.d.f. FX in 
particular) can only have countably many points of discontinuity. 
Furthermore, we always have 
Property4 Any c.d.f. is right-continuous, i.e. 
F (x)= F (x +)
We can now use our knowledge about probabilities to show some more properties of c.d.f.s
Proposition1 For any given x, 
P (X&gt;x)=1FX (x) 
4 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
  Example1 Suppose X hasp.d.f. 
ax2 if0 &lt;x&lt; 3 fX (x)= 0 otherwise 
Whatdoes a have to be? -since P (X  R)=1, the density has to integrate to 1, so a must be such that 
   3
2 ax3 3 27 1= fX (t)dt = atdt = = a 0 =9a 3 3 0 0 
Therefore, a = 91 .
Whatis P (1&lt;X&lt; 2)? -lets calculate the integral
 2 t2 23 13 7 P (1&lt;X&lt; 2) = dt =  = 9 9 39 3 27 1 
Whatis P (1&lt;X)? 
 3 t2 271 26 P (1&lt;X)= fX (t)dt = dt = = 9 27 27 1 1 
1.1 Mixed Random Variables/Distributions 
Many kinds of real-world data exhibit point masses at some values mainly for two dierent reasons: 
	some outcomes are restricted to certain values mechanically, so a lot of probability mass tends to 
cumulate right at the corners of the range of the random variable, e.g. daily rainfall can possibly 
take any positive real value, but there are many days at which rainfall is zero. 
	individuals taking economic decisions may respond to certain institutional rules by positioning 
themselves right at some kind of kinks or discontinuities, e.g. if we look at incomes reported to 
Social Security or the Internal Revenue Service, we observe bunching of individuals at the top 
ends of the tax brackets (since for those individuals, a small increase in income would mean a 
discretejump inthetax rate). 
The corresponding distributions are, strictly speaking, not continuous, because even though realizations 
can be any real-valued numbers, we cant dene a probability density function as we did in the previous 
section,but wellhavetodeal with thepoint masses separately. Some of thisisgoing to come up inyour 
econometrics classes, but we wont spend time on this for now and only look at one example. 
Example2 The following graph is constructed using data from the Current Population Survey (CPS) 
for1979.1 
For the graph, the authors chose a subpopulation with very low income so that the fraction of the sample 
for whom the minimum wage was binding was relatively high. There are some individuals to the left of 
the 1979 value of the minimum wage presumably corresponding to employment in sectors which are in 
part exemptfrom minimum wagelaws(e.g. farming,youth employment). 
1Figure 3b) on p. 1017 in DiNardo, J., N. Fortin and T. Lemieux. Labor Market Institutions and the Distribution of 
Wages, 1973-1992: A Semiparametric Approach. Econometrica 64, no. 5 (1996): 1001-1044. 
2 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text> 
 x 
10 1 
4 
20 30 40 50 3 
4 13 
16 
x 
10 1 
1 
16 
20 30 40 50 1 
4 3 
16 1 
2 Fx(x) 
fx(x) 
Figure 4: c.d.f. and p.d.f. for a discrete   Image by MIT OpenCourseWare.
random variable
2.1 p.d.f. and c.d.f for Continuous Random Variables 
If X has a continuous distribution with p.d.f. f(x) and F (x) (Ill drop the X subscript from now on 
wherever there are no ambiguities), then 
x 
F (x)= P (X  x)= f(t)dt 
 
From the fundamental theorem of calculus, we can in this case write the relationship between c.d.f. and 
p.d.f. as 
d F (x)= F (x)= f(x)dx 
Example6 Let 
0 if x&lt; 0 F (x)= x if x  01+x 
Is F (x)a c.d.f.? -lets check basic properties: 
6 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text> 
  For any subset A  R2 , 
P ((X,Y ) A)= fXY (x,y) 
(x,y)A 
Example8 In a supermarket, let X be the number of people in the regular checkout line, and Y the 
numberofpeopleintheexpressline. Thenthejointp.d.f. of X and Y could look like this: A table of this 
fXY 
0 1 Y 
2 3 4 Total 
X 0 
1 
2 
3 0.1 
0.05 
0 
0 
0.15 0.05 
0.2 
0 
0 
0.25 0.05 
0.2 
0.1 
0 
0.35 0 
0.05 
0.1 
0 
0.15 0 
0 
0.05 
0.05 
0.1 0.2 
0.5 
0.25 
0.05 
1 
form,summarizing thecell-probabilitiesfromthejointp.d.f. of (X,Y ), and the marginal probabilities on 
the sides, is also called a contingency table. As argued before, the probabilities in the table should add up 
to one, and they do. 
We can see from the entries that there seems to be some relationship between the two variables: when the 
number of individuals at the regular checkout is high, then the number of persons in the express line also 
tends to be high. 
We can also calculate probabilities for dierent events based on the p.d.f. as given in the table: 
P (X =2) = 0+0+0.1+0.1+0.05 =0.25 
34 
P (X  2,Y  2) = fXY (x,y)=0.1+0.1+0.05+0+0+0.05 =0.3 
x=2 y=2 
P (|X Y |1) = P (X = Y )+P (|X Y |=1) 
=0.1+0.2+0.1+0+0.05+0.05+0.2+0+0.1+0+0.05 =0.85 
9 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Review (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec08/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text> 
 
  By independence,thejointp.d.f. is 
1 if r  [10, 12] and g  [8.5, 11] fGR(g,r)= fG(g)fR(r)= 5 
0 otherwise 
The probability of the event R  G can be calculated as 
 11  max{g,10} 1  11 max{g 10, 0}  11 g 10 g2 11 21 1 P (R  G)= drdg =	 dg = dg = 2g = 2= 5 5	 5 10 10 10 8.510 8.5 10	 10 
Example4 One of your classmates asked how one can solve the following problem: how many dierent 
ways are there to allocate N indistinguishable blackboardsto k dierent classrooms? This corresponds to 
choosing a partition of blackboards over k classes. We can do the calculation as follows: 
	introduce k 1 separators Z1,Z2 ...,Zk1, which we mix with the blackboards B1,B2,...,BN 
	represent each allocationofblackboardstoroomsasareordering of B1,B2,...,BN ,Z1,Z2,...,Zk1. 
The blackboards before the rst Z to appear in the sequence are those which we are going to put up 
in the rst classroom, the boards up to the second separator go into room 2, etc. If the sequence 
is e.g. Z5,B7,B2,B5,Z4,B9,..., then there is going to be no blackboards in room 1, boards 7, 2, 
and 5 go to room 2 etc. 
	the number of dierent orderings of the sequence is (N +(k 1))! 
	sinceblackboards and separators are equivalent(classrooms arent), wehavetodivideby the number 
ofpermutations of eachtheblackboards(N! permutations), and the separators((k 1)! permuta
tions). 
	putting all pieces together, we have 
(N + k 1)! N + k 1 p = = N!(k 1)! k 1 
possible allocations. 
8 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>2 fZ|X 0 Z 
1 2 
1 
X 2 
3 61.13% 15.42% 23.42% 
75.25% 11.86% 12.88% 
85.36% 8.63% 6.04% 
Putting the conditional c.d.f.s for the values of X =1, 2, 3 together in a table, we get 
Why isthisexerciseinteresting? -whileinthetablewiththejointp.d.f.,theoverallpicturewasnotvery 
clear, we canseethatforlower values of marriagequality X,the conditional p.d.f. putshigherprobability 
mass on higher numbers of aairs. 
Does this mean that dissatisfaction with marriage causes extra-marital aairs? Certainly not: we could 
just do the reverse exercise, and lookat the conditional p.d.f. of reported satisfaction with marriage, X, 
given the number of aairs, Z. E.g. 
fXZ (1, 0) 17.80% fX|Z (1, 0) = = =23.72% fZ (0) 75.04% 
or, summarizing the conditional p.d.f.s in a table:
Weseethattheconditionalp.d.f. of X given a larger number of aairs, Z,puts moreprobability on lower
Z 
fX|Z 0 1 2 
1 23.72% 38.54% 51.24% 
X 2 32.37% 32.88% 31.25% 
3 43.91% 28.58% 17.51% 
satisfaction with the marriage. So we could as well read the numbers as extra-marital aairs ruining the
relationship. This is often referred to as reverse causality: even though we may believe that A causes
B, B may at the same time cause A.
Therefore, even though the conditional distributions shift in a way which is compatible with either story,
wecantinterprettherelationship as causal ineitherdirection,becauseboth storiesareequallyplausible,
and presumably in reality, there is some truth to either of them.
Review 
I dont expect you to memorize any of the examples we did in class, however, especially for text 
problems they can be extremely helpful as models for particular situations/problems. Often you can 
nd a solution strategy to a given question by seeing analogies to examples we discussed in the lecture. 
1. Probability 
Sample Space, Set Theory and Basic Operations 
wontdiscussthis 
2 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
  
 
  probability density function(PDF) fX (x)isdenedby 
P (X = x)= fX (x) if X is discrete 
P (X  A)= fX (t)dt 
A 
 the cumulativedistributionfunction(CDF) FX (x)isdenedby 
FX (x)= P (X  x) 
As an important example for a discrete random variable, we spent some time looking at the Binomial 
distribution, which describes the number X of successes in a sequence of N independent trials, with 
a success probability equal to p foreachtrial. Thep.d.fforthebinomialdistributionwas(youshould 
know this for the exam) 
fX (x)= P (X = x)= Np x(1p)Nx 
x 
Relationship between CDF and PDF 
Getting from the PDF to the CDF 
 if X is discrete, add up 
FX (x)= fX (xi) 
xix 
 if X is continuous, integrate 
x 
FX (x)= P (X  x)= fX (t)dt 
 
Getting from the CDF to the PDF 
	if X isdiscrete,
fX (x)= FX (x +)FX (x )
	if X is continuous
F  d
fX (x)= X (x)= FX (x)dx 
Also, recall main properties of CDF 
 0 FX (x) 1 for all x  R 
 FX (x)nondecreasing in x 
 FX (x)continuous from the right 
 FX (x)is continuous everywhere if and only if X is continuous 
5 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>  &#13;
  Denition of probability 
(P1) P (A) 0 for all A  S 
(P2) P (S)=1 
(P3) if A1,A2,... is a sequence of disjoint sets, 
  
P Ai = P (Ai) 
i=1 i=1 
Special Case: Simple Probabilities 
	S nite 
	P (A)= n(A) where n(B)denotes the number of outcomes in set B n(S) 
Properties of Probability Functions 
	P (AC )=1P (A) 
	P ()=0 
	if A  B,then P (A) P (B) 
	0  P (A) 1 for any event A  S 
	P (A B)= P (A)+P (B)P (A B) 
CalculatingProbabilities 
Try to attack problems in this order 
(i) dene sample space and the event of interest in terms of outcomes 
(ii) for simple probabilities, make sure that you dened the sample space in a way that makes each 
outcome equally likely 
(iii) if you are stuck, start writing out outcomes in sample space explicitly 
CountingRules 
	basic set-up: have set X1,...,XN of N objects 
	multiplication rule: have to be able to factor an experiment into k parts such that the number of 
outcomes mi ineach of themdoesnotdepend ontheoutcomesintheotherparts. Sometimestricky 
(e.g. chess example). 
 several dierent ways of drawing k objectsfromthe set(should rememberthoseforthe exam): 
1.	k draws with replacement, order matters: Nk possibilities 
2. k draws without replacement, order matters (special case: permutation, k = N): (NN
! 
k)! 
possibilities 
3 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>3 JointDistributions 
Looked at 
 joint PDF forX and Y (discrete or continuous) 
 marginal distribution of X with PDF 
  
fX (x)= fXY (x,y)dy 
 
 independence of random variables, most importantly 
fXY (x,y)= fX (x)fY (y) for all(x,y) R2 
if and only if X and Y areindependent 
 conditional distribution of Y given X, 
fXY (x,y)fY |X (y|x)= fY (y) 
Random Problems 
Example2 (Spring 2003 Exam) A Monet expert is given a painting purported to be a lost Monet. 
He is asked to assess the chances that it is genuine and has the following information: 
 In general, only 1% of the found paintings he receives turn out to be genuine, an event well call 
G 
 Found paintings have a dierent frequency of use of certain pigments than genuine Monets do: 
(a) cadmium yellow Y appearsin 20% found paintings, but only 10% genuine ones 
(b) raw umber U appearsin 80% of found paintings, but only 40% of genuine ones 
(c) burnt sienna S appearsin 40% of found, but 60% ofgenuinepaintings 
 This particular painting uses burnt sienna, but not cadmium yellow or raw umber. 
What is the probability that this particular painting is genuine? Do we have to make any additional 
assumptions to answer the question? 
This problem has the following structure: the problem seems to tell us what colors (data SY C UC )
are how likely to appear given that thepaintingisgenuine(state of the world G),i.e. P (B|A). But
we actually want to know how likely the painting is genuine given the colors that were used in it, i.e.
P (A|B). So we are trying to switch the order of conditioning, so well try to use Bayes Theorem.
Lets rst compile the information contained in the problem:
P (Y ) =0.2 
P (Y |G) =0.1 
P (U) =0.8 
P (U|G) =0.4 
P (S) =0.4 
P (S|G) =0.6 
6 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>
and 
P (G)=0.01 
But what do we need to apply Bayes theorem? -the theorem tells us that 
P (G|SY C UC )= P (SY C UC |G)P (G) 
P (SY C UC ) 
However, know only marginal probability of each color, but would need joint probabilities(both condi
tional on G and unconditional). 
Therefore we have to make an additional assumption at this point, and the simplest way to attack this 
is to assume that the use of pigments is independent across the three colors, both unconditionally and 
conditional on G,i.e. 
P (SY C UC |G)= P (S|G)P (Y C |G)P (UC |G)=0.6 0.9 0.6 
and 
P (SY C UC )= P (S)P (Y C )P (UC )=0.4 0.8 0.2 
Using Bayes theorem we get therefore that under the independence assumption 
0.6 0.9 0.6 0.01 81 P (G|SY C UC )= = 0.4 0.8 0.2 1600 
To see how much this assumption mattered, can invent a dierent dependence structure among the 
dierent types of pigments: suppose that for genuine Monets, every painting using sienna S also uses 
umbra U for sure. Then, by the denition of conditional probabilities 
P (SY C UC |G) P (SUC |G)= P (UC |SG)P (S|G)=0 0.6 =0 
so that for a true Monet, it is impossible to nd sienna S, but not umbra, therefore wed know for sure
that thepaintinginquestions cantbe aMonet(note that since ourpaintinghad this combination,it
has to be possible for found paintings in general).
So, summing up, this problem did not give us enough information to answer the question.
Example3 (Exam Fall 1998) Recycling is collected at my house sometime between 10am and noon, 
and any particular minute is as likely as any other. Garbage is collected sometime between 8:30am 
and 11:00am, again with any particular instant as likely as any other. The two collection times are 
independent. 
(a) Whatisthejointp.d.f. ofthetwocollectiontimes, R and G? 
(b) What is the probability that the recycling is collected before garbage? 
The marginal distribution of R is(continuous) uniform withdensity 
fR(r)= 1
2if r  [10, 12] 
0 otherwise 
The marginal distribution of G is discrete with p.d.f. 
2
5 fG(g)= if g  [8.5, 11] 
0 otherwise 
7 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
  3.	k draws without replacement, orderdoesnt matter(combination): N
k = (NN
k!
)!k! possi
bilities(e.g. inbinomialdistribution,countthenumberof alldierent sequencesof successes 
which give the same overall number of successes). 
	partitions: number of ways of allocating N objects across k groups, identities of objects dont 
matter (e.g. number of dierent allocations a ve identical blue balls to four urns): in general 
N + k 1 possibilities, will discuss this below k 1 
	we saw that in one way or another, all these counting rules derive from the multiplication rule, 
where sometimes we had to divide by the number of dierent possibilities of obtaining the same 
event(e.g. dierent orders ofdrawing the same combination). 
Independence,ConditionalProbability,Bayes Theorem 
	A and B areindependentif P (AB)= P (A)P (B) 
	conditionalprobability P (A|B)= P (AB) if P (B)&gt; 0P (B) 
	P (A|B)= P (A)if, and only if, A and B areindependent 
	law of total probability: if B1,...,Bn partition of S, 
P (A)= P (A|B1)P (B1)+... + P (A|Bn)P (Bn) 
The law of total probability links conditional to marginal probabilities, i.e. how to relate P (A) 
to P (A|B1),...,P (A|Bn). Classical application: aggregating over subpopulations/subcases, e.g. 
death rates over dierent types of bypass surgery. 
	Bayes Theorem(simpleformulation): if P (B)&gt; 0,then 
P (B|A)P (A) P (B|A)P (A)P (A|B)= = P (B) P (B|A)P (A)+P (B|AC )P (AC ) 
Bayes Theorem tells us how to switch the order of conditioning, i.e. how to go from P (B|A) 
to P (A|B). Classical application: update beliefs about event A givendata B, e.g. medical test 
example. 
	base rates matter a lot 
For the exam, you should know these relationships by heart. 
2. Random Variables and Distribution Functions 
	random variables give numerical characterization of random events. 
	random variable X is a function from the sample space S to the real numbers R. 
	probability function on S induces a probability distribution of X in R, 
P (X  A)= P {s  S : X(x) A} 
4 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 8 
Konrad Menzel 
March 3, 2009 
Conditional p.d.f.s 
Denition1 The conditionalp.d.f. of Y given X is 
fXY (x,y)fY |X (y|x)= fX (x) 
Notethatif X and Y arediscrete, 
P (Y = y|X = x)fY |X (y|x)= P (X = x) 
whichjust correspondsto the conditionalprobability of the event corresponding to X = x given Y = y 
as dened two weeks ago. 
Notethat 
	for a particular value of the conditioning variable, the conditional p.d.f. has all the properties of a 
usualp.d.f. (i.e. positive,integratesto1) 
	the denition generalizes to any number of random variables on either side 
Example1 Lets go back to the data on extra-marital aairs, and look at the variables we are actually 
most interested in: the number of aairs during the last year, Z, and self-reported quality of the 
marriage, X. The joint p.d.f. is given by Since three quarters of respondents reported not having had 
Z 
fXZ 0 1 2 fX 
1 17.80% 4.49% 6.82% 29.12% 
X 2 24.29% 3.83% 4.16% 32.28% 
3 32.95% 3.33% 2.33% 38.60% 
fZ 75.04% 11.65% 13.31% 100.00% 
an aair, it might be more instructive to look at the p.d.f. of the number of aairs Z conditional on the 
rating of marriage quality. Conditional on the low rating, X =1, we have 
fXZ (1, 0) 17.80% fZ|X (0|1) = = =61.13% fX (1) 29.12% 
1 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>PDF 1</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>  
  
 sets combined(i.e. a set with m +n members), which, according to the formula for combinations, equals 
m + n , which is the right-hand side of the equality we wanted to prove. z 
Putting bits and pieces together, 
P(Z = z)= m + np z(1p)nz 
z 
so that indeed, Z  B(m + n,p). 
As a cautious note, in general the sum Z of two independent random variables X and Y from the same 
family of distributions -in this case the binomial -will not belong to that same family. In that respect, the 
binomial distribution is a very special case, and there are only very few other commonly used distributions 
which have that property. E.g. if X  B(m,pX ) and Y  B(n,pY ) with pX pY , the derivation above = 
is not going to work anymore. 
1.2 Continuous Case 
Suppose X1,...,Xn are continuous withjointp.d.f. fX1,...,Xn (x1,...,xn), andY (lets stickto only one 
variable to keep notation simple) is given by a function 
Y = u(X1,...,Xn) 
If we let 
By := {(x1,...,xn): u(x1,...,xn) y} 
then the p.d.f. of Y isgivenby 
FY (y)= fX1,...,Xn (x1,...,xn)dx1 ...dxn 
(x1 ,...,xn )By 
2 Change of Variables Formula for One-to-One Transformations 
Thisisagainaspecialcase,which worksonlyforcontinuousvariables:Let Abethesupportof X1,...,Xn, 
i.e.	   
P (x1,...,xn) A =1 
and B the induced support of Y1,...,Yn,i.e. 
(Y1,...,Yn) B  (X1,...,Xn) A 
Suppose Y1,...,Yn aregeneratedfrom X1,...,Xn from a dierentiable one-to-one transformation 
Y1 = u1(X1,...,Xn) 
. . . 
Yn = un(X1,...,Xn) 
i.e. every value of(x1,...,xn) A is mapped to a unique element(y1,...,yn) B. We can then dene 
theinverse[s1(x1,...,xn),...,sn(x1,...,xn)] suchthat 
X1 = s1(Y1,...,Yn) 
. . . 
Xn = sn(Y1,...,Yn) 
3 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
 If s1(),...,sn()are dierentiable on B, we dene the matrix 
    
y1 s1 ... yn s1 
 . . J =  .. ..  
  ... y1 sn yn sn 
This matrix of partial derivatives is also called the Jacobian of the inverse transformation. For those of 
you whodidnttakeLinearAlgebra,its sucientifyou can work with the2-by-2 case. You should also 
know that for a two-by-two matrix A,thedeterminantisgivenby 
ab det(A)=det = ad bc cd 
Proposition 1 If the mapping of X1,...,Xn to Y1,...,Yn as outlined above is one-to-one and has a 
dierentiableinverse s1(),...,sn(),thenthejointp.d.f. of Y1,...,Yn isgivenby 
fX1 ,...,Xn (s1(y),...,sn(y))|det(J)| if y  B =support(Y1,...,Yn)fY1,...,Yn (y1,...,yn)= 0 otherwise 
2.1 Linear Transformation 
  X1 
Let X be a vector of random variables, i.e. X =  ... , and 
Xn 
  Y1 
Y =  ...  = AX 
Yn 
for an n  n matrix A withdet(A) = 0. Then the linear mapping Y = AX is one-to-one (since the 
matrixhasaninverse),and wecan nd thejointdistributionof Y using the change of variables formula 
fY1,...,Yn (y1,...,yn)= fX1,...,Xn (x1,...,xn)|det(A1)|=1 fX1,...,Xn (x1,...,xn)|det(A)| 
Example 2 To seehowthis mattersin economics, suppose wehave a simple(partial equilibrium) model 
forthe marketfor orangejuiceinBoston. Firms are willing to supplyquantity qs as a linear function of 
price p with coecients s and s 
qs = s + sp+ us 
where us is a random variable (say, sunshine hours in Florida). Consumers demand quantity qd given 
another random shock ud (sayincome): 
qd = d dp+ ud 
In equilibrium, supply equals demand, i.e. prices are such that qs = qd = q, and price and quantity are 
jointly determined by the following relationship 
        
1 s q s us = + 1 d p d ud 
4 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>  We may know or postulate the joint p.d.f. fU (us,ud) of the shocks (ud,us), from which we can derive 
thejointdistribution ofprice andquantity. Thisjointp.d.f. isgoing todepend crucially ontheJacobian 
(which is the matrix on the left-hand side). In this case, det(J)= d + s, so that if supply and/or 
demand have a nontrivial slope, the transformation from the shocks to price and quantity is one-to-one, 
and the resultingjointp.d.f. is 
fPQ(p,q)= fU (u1(p,q),u2(p,q))|s + d| 
This is a little too far beyond what we are going to cover in this class, but the Jacobian term |s + d| 
captures the interdependence of price and quantity through the market equilibrium. It turns out to be 
the source of what in 14.32 will be called the simultaneity problem which makes it very dicult to 
estimate supply and demand separately from market outcomes. This is one of the fundamental problems 
in econometrics. 
2.2 Distribution of X + Y (Convolution) 
SupposeX and Y areindependent continuousrandomvariableswithp.d.f.s fX (x)and fY (y),respectively, 
so that the joint p.d.f. of the random variables is fXY (x,y)= fX (x)fY (y). What is the p.d.f. of 
Z = X + Y? 
Example 3 Remember that we already did an example like this in class: we were looking at the sum of 
the lives of two spark plugs in a lawnmower, and it turned out that the probability P(X + Y  z) was 
theintegral ofthejointdensity fXY (x,y)over the triangular area dened by {(x,y): y  z x}. So the 
c.d.f. of Z isgivenby 
   zx    zx   
FZ (z)= P(X+Y  z)= fXY (x,y)dydx = fX (x)fY (y)dydx = fX (x)FY (zx)dx 
     
From this, we can derive the density of Z, 
d   
fZ (z)= FZ (z)= fX (x)fY (z x)dx dz  
The random variable Z = X + Y is also called the convolution of X and Y. Note that the last formula 
is valid only for the sum of independent random variables. 
Example 4 The discussion in the previous example was again along the lines of the 2-step method, 
and one might wonder whether it would also be possible to use the shortcut through the formula for 
transformations of variables. 
The mapping from (X,Y)to Z is clearly not one-to-one, so we cant use the formula for transformations 
of variables right away. However, we can do the following trick: dene 
Z = u1(X,Y)= X + Y 
W = u2(X,Y)= Y 
Then, the inverse transformation is dened as 
X = s1(Z,W)= Z W 
Y = s2(Z,W)= W 
Then, 
1 1 detJ =det =1 01 
5 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>  Therefore 
fZW (z,w)= fXY (s1(z,w),s2(z,w))= fX (s1(z,w))fY (s2(z,w))= fX (z w)fY (w) 
We can now obtain the marginal p.d.f. of Z byintegrating thejointp.d.f. over w 
  
fZ (z)= fX (z w)fY (w)dw 
 
which is the same formula as that obtained from the previous derivation. 
Example 5 We already saw several instances of the exponential distribution (e.g. in the lawnmower 
example). Let X and Y be independent exponential random variables with marginal p.d.f.s 
ex if x  0 ey if x  0 fX (x)= fY (y)= 0 otherwise 0 otherwise 
By the last formula, the p.d.f. of Z = X + Y isgivenby 
  
fZ (z)= fX (z w)fY (w)dw 
  z 
= e (zw)e wdw 
0  z 
= e z dw = ze z 
0 
where integration limits in the second step come from the fact that the support of X and Y is restricted 
to the positive real numbers, i.e. for z&lt; 0, fX (z)is zero, whereas for z&gt; w, fY (z w)becomes zero. 
6 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>    1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 10 
Konrad Menzel
March 12, 2009
Functions of 2 or more Random Variables 
Letsrecap whatwehave alreadylearnedaboutjointdistributions of2ormore randomvariables, say 
X1,X2,...,Xn 
 if X1,...,Xn arediscrete,theirjointp.d.f. isgivenby 
fX1 ,...,Xn (x1,...,xn)= P(X1 = x1,...,Xn = xn) 
 if X1,...,Xn are continuous,theirjointp.d.f. is apositivefunction fX1 ,...,Xn (x1,...,xn)such that 
P (X1,...,Xn) D = ... fX1,...,Xn (x1,...,xn)dx1 ...dxn 
D 
for any D  Rn . 
 X1,...,Xn areindependentif 
P(X1  A1,...,Xn  An)= P(X1  A1) ...P (Xn  An) 
recall that this is equivalent to 
fX1 ,...,Xn (x1,...,xn)= fX1 (x1) ...fXn (xn) 
Weare nowgoing tolook athowwe cangeneralizefromtheunivariate casediscussed aboveto2 ormore 
dimensions. 
As for the single-dimensional case well again distinguish three cases: 
1. underlying variables X1,...,Xn discrete 
2. underlying variable X1,...,Xn continuous 
3. X continuous and u(X1,...,Xn)is an n-dimensional one-to-one function 
1 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
  
  
  
 
    1.1 Discrete Case 
Suppose X1,...,Xn arediscrete withjointp.d.f. fX1,...,Xn (x1,...,xn), and Y1,...,Ym aregivenby m 
functions 
Y1 = u1(X1,...,Xn) 
. . . 
Ym = um(X1,...,Xn) 
If we let 
Ay := {(x1,...,xn): r1(x1,...,xn)= y1,...,um(x1,...,xn)= ym} 
thenthejointp.d.f. of Y1,...,Ym isgivenby 
fY1,...,Ym (y1,...,ym)= fX1,...,Xn (x1,...,xn) 
(x1 ,...,xn)Ay 
Example 1 (Sum of Binomial Random Variables) Suppose X  B(m,p) and Y  B(n,p) are 
independent binomial random variables with p.d.f.s 
fX (k)= mp k(1p)mk 
k 
fY (k)= np k(1p)nk 
k 
If we dene Z = X + Y, what is the p.d.f. fZ (z)? Since X is the number of successes in a sequence of 
m independent trials, and Y the number of successes in n trials, both with the same success probability, 
a rst guess would be that then Z should just be the number of successes in m + n trials with success 
probability p,i.e. Z  B(m + n,p). This turns out to be true, but well rst have to check this formally: 
P(Z = z)= P ({X =0,Y = z}or {X =1,Y = z 1}... or {X = z,Y =0}) 
z z 
= P(X = k,Y = z k)= P(X = k)P(Y = z k) 
k=0 k=0 
z     
= mp k(1p)mk np zk(1p)nz+k 
k z k 
k=0 
z    
=  m np z(1p)nz 
k z k 
k=0 
Now, the term pz(1p)nz doesnt depend on k, so we can pull it out of the sum. On the other hand, I 
claim that z      m n m + n = k z k z 
k=0 
Wecaninfact showthisusing counting rules:by themultiplicationruleand theformulaforcombinations, 
m n the term corresponds to the number of dierent sets we can draw which contain k k z k 
elementsfromagroup with m members,and zk elementsfromanothergroup with n members. Summing 
over all values of k, we get the total number of ways in which we can draw a set of z elementsfromboth 
2 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Conditional probability and independence (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text> 
 Ai that candidate i wins thepresidential elections(without conditioning on nomination) 
 Bi that candidate i wins the nomination of her/his party 
 Ck that the nominee of party k wins the election 
We can now usetheprobabilities which areimpliedby the assetforthe corresponding eventto answerthe 
question what the market thinks which candidate of either party has the highest probability of winning 
the presidential elections if nominated P (Ai|Bi)-i.e. nominating which candidate would give each party 
the highest chances of winning the presidency. 
We can(relatively) safely assume that a candidate whois not nominatedby thepartyhas no chances of 
winning the presidency, so that 
Ai  Bi = Ai Bi = Ai 
so that 
P (Ai Bi) P (Ai)P (Ai|Bi)= = P (Bi) P (Bi) 
sothat we only havetoplug inthepricesforthe corresponding assets. Based on assetprices onFebruary 
6 ontheIntradepolitical markets, wegetthefollowing numbers(inthelast columnI reportthe values 
from Mankiws original blog entry, as of November 2006). 
candidate P (Ai) P (Bi) P (Ai|Bi) PNov06(Ai|Bi)
Clinton 28.7% 45.2% 63.5% 51%
Huckabee 0.5% 2.0% 25.0% NA
McCain 34.4% 93.0% 37.0% 63%
Obama 35.0% 53.0% 66.0% 88%
Paul 0.4% 1.2% 33% NA
Romney 1.2% 2.6% 46.2% 50%
In order to distinguish it from the conditional probabilities P (A|Bi),P (A)is also called the marginal 
probability of A. The relationship between marginal and conditionalprobabilitiesisgivenby theLaw of 
Total Probability: 
Theorem1 (Law of Total Probability) Supposethat B1,...,Bn is a partition of the sample space S 
such that P (Bi)&gt; 0 for every i =1,...,n. Then 
n 
P (A)= P (A|Bi)P (Bi) 
i=1 
for any event B. 
Proof: From the denition of a conditional probability, P (A|Bi)P (Bi)= P (A Bi)for any event Bi. 
Since B1,...,Bn isapartitionof thesamplespace,(A B1),..., (A Bn)are disjoint and exhaustive for 
A -i.e. constitute a partition for A. Therefore,by the axiom(P3) onprobabilities of unions ofdisjoint 
sets, n
i=1 P (A Bi)= P (A) 
Example8 Inmedicaldatawecanoften nd thatpatientstreatedby older,and moreexperienced,heart 
surgeons have in fact higher post-operative death rates than those operated by younger surgeons -say 
we observe a death rate of 6.0% for experienced surgeons, and onlyl 5.5% for unexperienced surgeons. 
6 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>2 Many people nd these probabilities very high, but its usually because one is tempted to start thinking 
about the problem by calculating the probability that any of your n friends has the same birthday as you. 
You can convince yourself that this probability is 
 n n 
P (A)=1 1 1 364 =1 365 365 
for which our list looks dierent: The reason for this discrepancy is that in the previous situation, A also 
n P (A) 
5 0.014 
10 0.027 
15 0.040 
20 0.053 
25 0.066 
30 0.079 
50 0.128 
366 0.634 
contained all pairs among your n friends, and that number went up very quickly. 
Independent Events 
Intuitively, we want to dene a notion that for two dierent events A and B the occurrence of A does 
not aect the likelihood of the occurrence of B, e.g. if we toss a coin two times, the outcome of the 
second toss should not be inuenced in any way by the outcome of the rst. In order to keep notation 
simple, we will from now on denote 
P (A B)= P (AB) 
Denition1 The events A and B are said to be independent if 
P (A B)= P (A)P (B) 
From this you can see that independence is merely a property of the probability distribution, not neces
sarily ofthephysical natureoftheevents. Sowhileinsomeexamples(e.g. theseriesof cointosses) we 
have a good intuition for independence, in most cases well have no choice but need to check the formal 
condition. 
Example4 Say we roll a fair die once, what are the probabilities for the events 
A = {2, 4, 6} 
B = {1, 2, 3, 4} 
n(A)and their intersection? Counting outcomes, P (A)= n(S) =3
6 =1
2, and similarly, P (B)=46 =2
3. The 
probability of the intersection of events is 
21 P (AB)= P ({2, 4})= = = P (A)P (B)63 
so the events are independent even though they resulted from the same roll. 
Inordertoseehowindependencedependscrucially ontheunderlyingprobabilitydistribution, nowsuppose 
3 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 3 
Konrad Menzel 
February 10, 2009 
Counting Rules and Probabilities 
Recall thatwith simpleprobabilities,each outcomeisequallylikely,andfora nitesamplespace,wecan 
give the probability of an event A as 
n(A)P (A)= n(S) 
Well now see how to make good use of counting rules to calculate these probabilities. 
Example1 Draw two cards from a deck of 52 cards with replacement, assuming that each cardispicked 
with equal probability. What is the possibility of drawing two dierent cards? 
S = {(A,A), (A,A),...}= n(S)=522 
The event two dierent cards consists of 
52! A = {(A,A), (A,A),...}= n(A)= =52 51 (522)! 
so that 
n(A) 52! 51 P (A)= = =  0.98 n(S) (522)!(52)2 52 
Alternatively, we could have used proposition 1 on probabilities: 
1 51 P (A)=1P (AC )=1P (two cards are same)=1P (2ndcard sameas1st) =1 = 52 52 
In some other examples, computing the probability of an event through its complement may simplify the 
calculation a lot. 
Example2 Suppose Oceania attacks the capital of Eurasia1 with 16 missiles, 8 of which carry a nuclear 
warhead. Suppose the Eurasian army can track all 16 projectiles and has 12 missiles each of which 
can intercept one incoming missile with absolute certainty, but cant tell which of the missiles carry a 
conventional load. What is the combinatorial probability that Eurasia cannot avert disaster and at least 
one of the nuclear warheads reaches its target? What would be your intuitive guess? 
1The names are taken from Orwells novel 1984, so this is not supposed to be a real-world example. 
1 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>thatthediehadbeenmanipulated sothat P (6)= 3
8 1
8 . , whereas for all other numbers n =1,..., 5, P (n)= 
Then,by axiom(P3) on addingprobabilities ofdisjoint events, 
1135 41 P (A)= + + = ,P (B)= = 8888 82 
and 21 5 P (AB)= = &lt;P (A)P (B)= 84 16 
One interpretation of independence is the following: suppose we know that B has occurred, does that 
knowledge change our beliefs about the likelihood of A (andvice versa)? Well formalize this in the next 
section, and it will turn out that if A and B are independent, there is nothing about event A tobelearnt 
fromtheknowledgethat B occurred. 
Proposition1 If A and B are independent, then A and BC are also independent. 
Proof: Since we can partition A into the disjoint events AB and ABC , we can write 
(P 3) Indep. P (ABC )= P (A)P (AB)= P (A)P (A)P (B)= P (A)(1P (B))= P (A)P (BC ) 
proving independence 
We can now extend the denition of independence to more than two events:
Denition2 A collection of events A1,A2,... are independent if for any subset Ai1 ,Ai2 ,... of these
events(allindicesbeingdierent)
P (Ai1 Ai2 ...)= P (Ai1 ) P (Ai2 ) ... 
E.g. for three events A,B,C, 
P (AB)= P (A)P (B),P (AC)= P (A)P (C),P (BC)= P (B)P (C) 
and 
P (ABC)= P (A)P (B)P (C) 
Example5 Let the sample space be S = {s1,s2,s3,s4}, and P (si)= 1
4 for all outcomes. Then each of 
the events 
A = {s1,s2},B = {s1,s3},C = {s1,s4} 
occurs with probability 1
2 . The probability for the event A B is 
1 P (AB)= P ({s1})= = P (A)P (B)4 
andlikewiseforany otherpairof events, sotheeventsare pairwise independent. However, taken together, 
the full collection is not independent since 
11 P (ABC)= P ({s1})= = = P (A)P (B)P (C)48 
Intuitively, once we know that both A and B occurred, we know for sure that C occurred. 
4 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> A B = A ' 
A B = S ' S 3 Conditional Probability 
Suppose the occurrence of A aectsthe occurrence(or non-occurrence) of B and vice versa. How do we 
describe the probability of B given knowledge about A? -We already argued heuristically that if the 
events are independent, then A does not reveal any information about B. But what if it does? How do 
we change probabilities as a result? 
Example6 If we throw a fair die, and I tell you that in fact the outcome was an even number, i.e that 
the event B = {2, 4, 6} occurred. Whats the probability of having rolled a 6? Since there are only three 
equallylikelypossibilitiesin B, 6 being one of them, wed intuitively expect the answer to be 1
3 . Here we 
basically simplied the sample space, to S= B = {2, 4, 6}, and calculated the simple probability for the 
redenedproblem. 
Denition3 Suppose A and B are events dened on S such that P (B)&gt; 0. The conditionalprobability 
of A given that B occurredisgivenby 
P (A B)P (A|B)= P (B) 
Intuitively, the numerator redenes which outcomes in A arepossible once B is known to have occurred, 
the denominator does the same thing for the whole sample space. 
Figure 1: The Event A Conditional on B 
Remark1 Conditional probability and independence: if A and B areindependent, 
P (AB) P (A)P (B)P (A|B)= = = P (A)P (B) P (B) 
so B occurring tells us nothing about A, so the conditional probability is the same as the unconditional 
probability. 
Example7 ThisexampleisadaptedfromGregMankiwsblog:2 OnplatformslikeIntrade,youcantrade 
assets which pay 1$ if agiven event(e.g. Yankees win theWorldSeries) occurs. If the market works as 
it should, the prices of this type of assets at a given time t can be interpreted as probabilities given the 
information traders have at that point in time. On the political market on Intrade, you can trade assets 
for the events 
2http://gregmankiw.blogspot.com/2006/11/bayes-likes-obama 
5 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Since in any event, exactly 4 projectiles reach their target, the sample space S consists of all combinations 
of 4 missiles out of 16. Therefore the number of elements of S is given by the binomial coecient 
  
16 16! n(S)= = 4 12!4! 
Inordertoevaluatetheprobability,oneapproachistousethecomplementrule. Theevent complementary 
to A =Atleast onenuclearwarheadhitstargetis AC =All missileshitting thetarget areconventional, 
and the outcomes in AC aregivenby all combinations of4 missiles out of8(the conventional ones), so 
that  
n(AC )8 8! = = 4 4!4! 
Therefore, 
12!4!8! 12! 8! 1 8 7 6 5 25 P (A)=1P (AC )=1 =1 =1 = 16!4!4! 16! 4! 16 15 14 13 1 26 
So it turns out that this probability is extremely close to one -Im not sure whether you would have 
expected this, but despite being politically incorrect, this example shows that our intuitions may fail easily 
in combinatoric problems, last but not least because of the high numbers of possibilities. 
Example3 The famous birthday paradox is about a (once) popular party game: given you have a 
group of n friends, whatistheprobabilitythat atleast apairofthemhasthesamebirthday(assuming 
that all birthdays are equally likely, which is actually only approximately true empirically)? Again, lets 
look at the complementary event AC that each of your n friends has a dierent birthday: since this 
corresponds to drawing n out of 365 without replacement, we can use the corresponding formula 
365! n(AC )= (365n)! 
so that we can calculate the probability P (A)of at least two of your friends having the same birthday: 
P (A)=1P (AC )=1 n(AC ) =1 365! 
n(S) (365n)!365n 
Thisformulais notyetparticularly easy to read, soletsjust writedowntheprobabilitiesindecimalsfor 
a few values of n: 
n 
5 
10 
15 
20 
25 
30 
50 
366 P (A) 
0.027 
0.117 
0.253 
0.411 
0.569 
0.706 
0.97 
1 
2 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Does this mean that the surgeons skill decreases with age? Probably not -lets suppose there are four 
dierent types of procedures a surgeon may have to perform -single, double, triple, and quadruple bypass 
(the terminology refers to the number of coronary arteries that have to be bypassed articially). The 
complexity of the procedure and the risk to the patient increase in the number of bypasses, and it might 
also be that the patients who are generally sicker may tend to require a more complicated procedure. 
Suppose we are told that for each procedure, patients of the experienced surgeon face signicantly lower 
death rates, but that the overall patient mortality is lower for unexperienced surgeons. In light of the law 
of total probability, how does that t together? Lets look at an example (these numbers are of course 
made up): 
Unexperienced Experienced 
Procedure Death Rate Percentage of Cases Death Rate Percentage of Cases 
Single Bypass 4.0% 50.0% 2.0% 25.0% 
Double Bypass 6.0% 40.0% 4.0% 25.0% 
Triple Bypass 10.0% 9.0% 6.0% 25.0% 
Quadruple Bypass 20.0% 1.0% 12.0% 25.0% 
Total 5.5% 100.0% 6% 100.0% 
In the notation of the Law of Total Probability, the overall death rate P (A) for, say, experienced 
surgeons can be computed from the death rate conditional on procedure Bi, P (A|Bi), and the base 
rates/proportionsP (Bi)of cases corresponding to each procedure. 
We can see that since experienced surgeons are assigned a disproportionately large share of risky cases 
(presumably because you need more experience for these), their average (better: marginal) death rate is 
higherthanthat of unexperienced surgeons, eventhough theyperformbetter conditional oneach treatment 
category. This phenomenon is often referred to as a composition eect. 
So what is the practical importance of each type of probabilities? If you were to choose among surgeons 
for a bypass operation, the type of procedure should only depend on your health status, not whether the 
surgeonis experienced or not, sointhat situationyou should only care aboutthe conditional probabilities. 
Its harder to come up with a good use for the marginal death rates. 
Inmoststatistical analysis,youdinfactbeinterestedin conditional death rates(e.g. ifyouareinterested 
in the eect of experience on mortality), and the variable type of procedure would be treated as what 
statisticians call a confoundingfactor. A classical problem in statistics and econometrics is that often 
many of the relevant confounding factors are not observed, and youll learn about tricks of dealing with 
thatproblem. 
Remark2 Another closely related concept is that of conditionalindependence, whichisgoing tobe very 
important in Econometrics. Two events A and B are said to be independent conditional on event C if 
P (AB|C)= P (A|C)P (B|C) 
It is important to note that 
 unconditionalindependencedoes not imply conditional independence 
 conditionalindependencedoes not imply unconditional independence 
i.e. whether A and B are independent depends crucially on what we are conditioning on. Therell be an 
exercise with a counterexample on the next problem set. 
7 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>4 Conditional Independence (not covered in lecture) 
We can extend the denition of independence to conditional probabilities: 
Denition4 Two events A and B are said to be independent conditional on event C if the conditional 
probabilities satisfy 
P (AB|C)= P (A|C)P (B|C) 
Thisdenitioncorrespondsexactly tothat of unconditionalindependencewhich welooked atbefore,only 
that we restrict ourselves to the new sample space S = C. The notion of conditional independence is 
going toplay animportant rolelater onin econometrics, soitdeserves a special mention. As atechnical 
point,itisimportanttonoticethatconditionalindependencedoesnotimply unconditionalindependence, 
or vice versa. In other words, whether two events are independent depends crucially on what else we 
are conditioning on. I mentioned this in passing in the last lecture, and Im now going to provide the 
following example as an illustration: 
Example9 Lets look again at a roll of a fair die, i.e. S = {1, 2, 3, 4, 5, 6}, where each outcome occurs 
withprobability1
6. 
(1) Making two independent events dependent: Consider the events A = {1, 2, 3, 4} and B = {2, 4, 6}. 
We already saw in a previous example that these events are independent: 
43 1 P (A)P (B)= P ({1, 2, 3, 4})P ({2, 4, 6})=  = = P ({2, 4})= P (AB)66 3 
Now let event C = {3, 6}. Then 
P (AC) P ({3})1 P ({6}) P (BC)P (A|C)= = = = = = P (B|C)P (C) P ({3, 6})2 P (C) P (C) 
However, 
P ()P (AB|C)= =0 = P (A|C)P (B|C)P (C) 
i.e. A and B are not independent conditional on C since their intersections with C aredisjoint. 
(2) Making two dependent events independent: Let D = {2, 3, 4}and E = {2, 4, 6}. We can check that 
D and E are dependent: we can see that P (D)= P (E)=1
2. However, 
11 P (DE)= P ({2, 4})= = = P (D)P (E)34 
But if we condition on F = {3, 4, 5, 6}, 
P ({4}) 1 P (DE|F )= = P ({3, 4, 5, 6})4 
whereas 
P ({3, 4}) P ({4, 6}) 111 P (D|F )P (E|F )=  =  = P ({3, 4, 5, 6}) P ({3, 4, 5, 6}) 22 4 
so that by conditioning on F , D and E becameindependent. 
8 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Confidence intervals (cont.) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Alsonotethatinthegeneralcase4(andinthelast exampleinvolving auniform), wedid notrequire 
that the statistic (X1,...,Xn)be an unbiased or consistent estimator of anything,butitjusthad tobe 
strictly monotonic in the true parameter. However, the way we constructed condence intervals for the 
normal cases(with or withoutknowledge of the variance of , the estimator has to be unbiased, and in 
case3(n large), it would have to be consistent. 
2 Hypothesis Testing 
2.1 Main Idea 
Idea: given a random sample from a population, is there enough evidence to contradict some assertion 
about the population? Lets rst dene a number of important concepts: 
a hypothesis is an assumption about the distribution of a random variable in a population  
the maintainedhypothesis is a hypothesis which cannot be tested, but which we will assume to be  
true no matter what. 
a testablehypothesis is a hypothesis which can and will be tested using evidence from a random  
sample 
the nullhypothesis isthehypothesistobetested  
the alternativehypothesis are otherpossible assumptions aboutthepopulation otherthanthe null  
The testing problem can be stated as whether the parameter 0 corresponding to the density f(x0)|
which our sample X1,...,Xn is drawn from belongs to a certain set of possible parameter values, 0. 
We usually write the null hypothesis as 
H0 :  0 
which we test against the alternative 
HA :  A 
where0 A = . 
If0 = {0}contains only a single parameter value, we say that the hypothesis is simple.A composite 
hypothesis is given by a set  containing multiple points, or an entire range of values. 
Example 2 In the most common setup, H0 is simple, and HA composite, e.g. X N(,02), where 02 
is known, and we want to test whether  =0. In this setting, the maintained hypothesis is that the Xis 
are i.i.d. normal and Var(Xi)= 02 . The null hypothesis is H0 : =0 (simple), andwe test against the 
alternativehypothesis HA : =0 (composite). 
In order to test the hypothesis we have to gather data, and then either accept or reject the null 
hypothesis based on the data. However, since our data will always be only a sample from the entire 
population, there is always a possibility that we will make a mistake in our decision: The probability of 
True State of Nature 
H0 true H0 false 
reject H0 Type I error correct 
Decision 
not reject H0 correct Type II error 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>
Lets recap how we arrived at the condence interval: 
1. rst get estimator/statistic (X1,...,Xn)and the distribution of . 
2. nd a(),b()such that
P(a()b())=1
3. rewrite the event by solving for 
P(A(X) B(X))= P(A() B())=1
4. evaluate A(X),B(X)for the observed sample X1,...,Xn 
5. the1  condenceintervalisthengivenby
CI =[A(X1,...,Xn),B(X1,...,Xn)]
1.1 Important Cases 
1. is normally distributed, Var()2 is known: can form condence interval 
[A(X),B(X)]= 

 21 
1  
2  
,+ 
 2
1 
1  
2  

2. is normally distributed, Var()unknown, but have estimator S2 =Var(): condence interval is 
givenby
[A(X),B(X)]= 
 
S2tn1 
1  
2  
,+ 
S2tn1 
1  
2 
where tn1(p)isthe pth percentile of a t-distribution with n 1 degrees of freedom. 
3. is not normal, but n&gt; 30 or so: it turns out that all estimators weve seen (except for the 
maximum of the sample for the uniform distribution) will be asymptotically normal by the central 
limittheorem(itisnot always straightforwardhow we applytheCLTin agiven case). So well 
construct condence intervals the same way as in case 2. 
4. not normal, n small:if thep.d.f. of isknown,canformcondenceintervalsfrom rstprinciples 
(as in the last example). If the p.d.f. of is not known, there is nothing we can do. 
The reason for using the t-distribution in the second case is the following: since N  
, 2  
,n 
 
/n N(0,1) 
On the other hand, we can check that 
(n 1)S2 
2 
2 n1 
since in this setting, Scan usually be written as a sum of squared normal residuals with mean zero and 
variance 2 . Therefore, 
 = / n N(0,1) tn1 
S2/n  
(n1) S2 
2 
2 /n 1 n1 
2 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>a type I error of a given test, 
 = P(Type Ierror) = P(rejectH0)|
is called the signicancelevel (or also thesize)of the test. If we write 
 = P(Type IIerror) = P(dont rejectHA)|
then1  isthe power ofthetest. 
Usually well x the signicance level of the test, e.g. at 5%, and then try to construct a test that 
has maximal power given that signicance level. So in a sense we prefer to err on not rejecting the null 
hypothesis. 
The logic behind this is a little counter-intuitive at rst, but it comes from the empirical problem 
of generalizing from a few observations to an entire population or a scientic law: even though we may 
only have observed instances which conform with our hypothesis about the population, it is sucient to 
observe one which doesnt in order to disprove it. Therefore we can use empirical evidence only to reject 
a hypothesis, but never to conrm it. The following is a famous example by the philosopher Bertand 
Russell: 
Domestic animals expect food when they see the person who usually feeds them. We know 
that all these rather crude expectations of uniformity are liable to be misleading. The man 
who has fed the chicken every day throughout its life at last wrings its neck instead, showing 
that more rened views as to the uniformity of nature would have been useful to the chicken. 
[..] Themerefactthatsomethinghashappened acertainnumberof timescausesanimalsand 
men to expect that it will happen again. Thus our instincts certainly cause us to believe that 
the sun will rise to-morrow, but we may be in no better a position than the chicken which 
unexpectedly hasitsneck wrung. (Russell,The Problemsof Philosophy) 
Therefore, if we want to present evidence that e.g. a certain drug signicantly improves a patients 
condition, we dene the null hypothesis as H0: the drug has no eect on the patients condition. 
Rejecting this hypothesis means that we have strong evidence for an eect of the drug. I.e. we always 
choose the null hypothesis as the statement we actually want to disprove. 
As another illustration, we could think of the criminal justice system: in a process, both parties 
presentdata(evidence) inordertoproduceadecision guilty or notguilty, andthejury canagain 
maketwoerrors:falsely convicting aninnocentperson(TypeI error),or not convicting acriminal(Type 
II error). Most modern legal systems base criminal trials on the presumption of innocence, i.e. the 
accusedisassumed tobe innocentuntilprovenguilty,orinotherwords,theburdenofproofisonthe 
prosecution whichhastoproduce evidenceto convincethejudge/jury thatthe accusedisinfactguilty. 
Note that decisions taken according to hypothesis tests need not be optimal in the sense that we 
ignore our ex ante probabilities for the null vs. the alternative hypothesis being true, and do not take 
into account the respective costs of making type I or type II errors. For criminal justice, proponents 
of preemption often argue that in many contexts -e.g. terrorism -a type II error may be prohibitively 
costly,sothatthelegal systemshould allowforexceptionsof thepresumptionofinnocenceinsomecases. 
In sum, wed like to formulate a rule which maps each possible outcome of the sample X1,...,Xn to 
a decision reject or do not reject. 
4 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 20 
Konrad Menzel 
April 30, 2009 
CondenceIntervals(continued) 
The following example illustrates one way of constructing a condence interval when the distribution of 
the estimator is not normal. 
Example 1 Suppose X1,...,Xn are i.i.d. with Xi U[0,], and we want to construct a 90% condence 
intervalfor 0. Let 
= max{X1,...,Xn}= X(n) 
the nth order statistic (as we showed last time, this is also the maximum-likelihood estimator). Even 
though, as we saw, is not unbiased for , we can use it to construct a condence interval for . 
From results for order statistics, we saw that the c.d.f. of is given by the c.d.f. of isgivenby 

0  0  
 n 
F()= 0 if0 &lt; 0  1 if &gt;0 
where we plugged in the c.d.f. of a U[0,0]random variable, F(x)= x 
0 .
In order to obtain the functions for A and B, let us rst nd constants a and b such that
P0 (a b)= F(b)F(b)=0.950.05 =0.9 
We can nd a and b by solving 
F(a)=0.05 and F(b)=0.95 
n nso that we obtain a = 
0.050 and b = 
0.950. This doesnt give us a condence interval yet, since 
looking at the denition of a CI, we want the true parameter 0 in the middle of the inequalities, and the 
functions on either side depend only on the data and other known quantities. 
However, we can rewrite 
 
n 
  
0.9= P0 (a b)= 
0.050 
0.950  
= P0 nP0 n  n
0.95 0 
0.05 
Therefore 
 max{X1,...,Xn} max{X1,...,Xn} 
[A,B]=[A(X1,...,Xn),B(X1,...,Xn)]= n, n
0.95 
0.05 
is a 90% condence interval for 0. Notice that in this case, the bounds of the condence intervals depend 
on the data only through the estimator (X1,...,Xn). This need not be true in general. 
1 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Hypothesis tests (cont.) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec23/</lecture_pdf_url>
      <lectureno>23</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>1 
Fn(x) F(x) 
x 
Fn(x) F(x) 
x Dn Largest 
difference 
 We are interested in the inference problem in which we have a random sample X1,...,Xn from 
an unknown family of distributions and want to test whether it has been generated by a particular 
distribution with c.d.f. F(x)(e.g. F(x)=(x) for the standard normal distribution). Since there are 
no specic parameters for which we can do any of the tests outlined in the previous discussion, the idea 
for a test is to check whether Fn(x)does not deviate too much from F(x). 
2.3 The Kolmogorov-Smirnov Test 
In order to test whether an observed sample was generated by a distribution F(x), we reject for large 
values of the Kolmogorov-Smirnov statistic which is dened as 
Dn =sup Fn(x)F(x)
x | | 
wheresupx F(x)denotesthesupremum,i.e. thesmallestupperbound on {F(x): x R}-forcontinuous 
functions on compact sets, this is the same as the maximum, but since the Kolmogorov-Smirnov statistic 
involvesthe sampledistributionfunction whichhasjumps of size n1 , and the supremum is taken over the 
entire real line, it may in fact not be attained at any particular value of x. 
The critical values ofthe statistic canbe obtainedfromits  Image by MIT OpenCourseWare.
asymptotic (i.e. for large n) distribution 
function 
22 xG(Dn)= lim P(Dn &lt;x)=12(1)n1 e2n 
n 
i=1 
The argument leading to this expression is not at all obvious and very technical since it involves distribu
tions overfunctions ratherthan real numbers(randomfunctions are usually called stochasticprocesses). 
6 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>  
  
 1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 23 
Konrad Menzel 
May 12, 2009 
Examples 
Example 1 Assume thatbabies weights(inpounds) atbirth aredistributed according to X N(7,1). 
Now suppose that if an obstetrician gave expecting mothers poor advice on diet, this would cause babies 
to be on average 1 pound lighter (but have same variance). For a sample of 10 live births, we observe 
X10 =6.2. 
	How do we construct a 5% test of the null that the obstetrician is not giving bad advice against the 
alternative that he is? We have 
H0 : =7 against HA : =6 
We showed that for the normal distribution, it is optimal to base this simple test only on the sample 
	   mean, X10, so that T(x)= x10. Under H0, X10 N(7,0.1) and under HA, X10 N(6,0.1). The 
test rejects H0 if X10 &lt;k. We therefore have to pick k in a way that makes sure that the test has 
size5%,i.e. 
0.05 = P(X10 &lt;k|=7) = k
0.17 
where ()is the standard normal c.d.f.. Therefore, we can obtain k by inverting this equation 
1.645 k =7+
0.011(0.05) 7
10 6.48 
Therefore, we reject, since X10 =6.2 &lt; 6.48 = k. 
	What is the power of this test? 
P(X10 &lt; 6.48=6) =6.486 (1.518) 93.55% | 
0.1 
	Suppose we wanted a test with power of at least 99%, what would be the minimum number n of 
newborn babies wed have to observe? The only thing that changes with n is the variance of the 
sample mean, so from the rst part of this example, the critical value is kn = 1.645 7n , whereas the 
 power of a test based on Xn and critical value kn is 
 1 = P(Xn &lt;kn|=6) = n 1.645 
1 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>	  
Example 4 Suppose we have one single observation generated by either 
f0(x)=	2x if0 x 1 or fA(x)= 22x if0 x 1 
0 otherwise 0 otherwise 
	Find thetestingprocedurewhich minimizesthesumof + -dowerejectif X =0.6? Since we only 
have one observation X, its not too complicated to write the critical region directly in terms of X, 
andthereisnothing tobegainedby trying to nd somecleverstatistic(though of courseNeyman-
Pearson would still work here). By looking at a graph of the densities, we can convince ourselves 
that the test should reject for small values of X&lt;k for some critical level k. The probability of 
type I and type II error is, respectively, 
 k 
(k)= P(rejectH0)= 2xdx = k2 |
0 
for 0 k 1, and 
 1 
(k)= P(dont reject|HA)= 
k (22x)dx =2(1k)1+k2 =1k(2k) 
Therefore, minimizing the sum of the error probabilities over k, 
min {(k)+(k)}=min {k2 +1 k(2k)}=min {2k2 +1 2k}
k k	 k 
Setting the rst derivative of the minimand to zero, 
1 0 =4k2 k =2 
Therefore we should reject if X&lt; 21 , and  =  = 41 . Therefore, we would in particular not reject 
H0 for X =0.6. 
	Among all tests such that  0.1, nd the test with the smallest . What is ? Would you reject if 
X =0.4? -rst well solve (k)=0.1 for k. Using the formula from above, k= 
0.1. Therefore, 
(k)=12k+ k2 =1.12
0.1 46.75% 
Since k = 
0.1 0.316 &lt; 0.4, we dont reject H0 for X =0.4. 
Example 5 Suppose we observe an i.i.d. sample X1,...,Xn, where Xi U[0,], and we want to test 
H0 :  = 0 against HA :  = 0, &gt; 0 
There are two options: we can either construct a 1 condenceintervalfor  and reject if it doesnt 
cover 0. Alternatively, we could construct a GLRT test statistic 
L(0)T = maxR+ L() 
Thelikelihoodfunctionisgivenby 
n  n 	 1 for0 Xi ,i =1,...,n L()=	 fX(Xi|)= 0  
otherwise 
i=1 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Since the calculation of the c.d.f. requires that we compute an innite series, using the formula is not 
straightforward. However, most statistics texts tabulate the most common critical values. 
Example 7 Suppose we toss four coins repeatedly, say 160 times and want to test at the signicance 
level  =0.2 whether the sample was generated by a B(4,0.5) distribution. Lets say we observed the 
following sample frequencies: 
Then the Kolmogorov-Smirnov statistic equals 
numberofheads 0 1 2 3 4 
sample frequency 10 33 61 43 13 
cumulative sample frequency Fn() 10 43 104 147 160 
cumulativefrequency under H0 F() 10 50 110 150 160 
dierences 0 7 6 3 0 
1 7 max{0.7,6,3,0}=160 0.044 Dn = 160 
1.07 Using the asymptotic formula from the book, C0.20 = 
160 0.85. Since 0.44 &lt; 0.85, we dont reject the 
null at the 20% level. 
2.4 2-Sample Kolmogorov-Smirnov Test 
Suppose we have two independent random samples, X1,...,Xm and Y1,...,Yn from unknown families of 
distributions, and we want to test whether both samples were generated by the same distribution. The 
idea is that we should test whether Fn(x)and Gn(x)are not too far apart. 
We can construct a test statistic 
D =sup Fn(x)Gn(x)
x | | 
and reject the null for large values of D. A good asymptotic approximation for the critical value for a 
size  testis    1 1 1  reject if D &gt; 2 m + n log 2 
2.5 Pearsons  2 Test 
Suppose each Xi from a sample of n i.i.d. observations is classied into one of k categories, A1,...,Ak. 
Let p1,...,pk be the probabilities of each category, and f1,...,fk be the observed frequencies. Suppose 
we wanttotestthejointhypothesis 
H0 : p1 = 1,p2 = 2,...,pk = k 
against the alternative that at least two or more of these equalities dont hold (note that since the 
probabilities have to add up to one, it cant be that exactly one equality is violated). We can use the 
statistic 
k  (fi ni)2 
T = nii=1 
and reject for large values of T. In order to determine the appropriate critical value, wed have to 
knowhow T is distributed. Unfortunately this distribution depends on the underlying model. However, 
under H0 the distribution is asymptotically independent of model, and for large samples nT 2 
k1 
approximately. As a rule of thumb, the chi-squared approximation works well if n 4k. 
7 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>  Setting 1 0.99, we get the condition 
n 1.645 1(0.99) =2.326 n 3.9712 15.77 
This type of power calculations is frequently done when planning a statistical experiment or survey 
-e.g. in order to determine how many patients to include in a drug test in order to be able to detect 
an eect of a certain size. Often it is very costly to treat or survey a large number of individuals, 
so wed like to know beforehand how large the experiment should be so that we will be able to detect 
any meaningful change with suciently high probability. 
Example 2 Suppose we are still in the same setting as in the previous example, but didnt know the 
variance. Instead, we have an estimate S2 =1.5. How would you perform a test? As we argued earlier, 
the statistic Xn 0T := S/n tn1 
is student-t distributed with n 1 degrees of freedom if the true mean is in fact 0. Therefore we reject 
H0 if 
Xn 7 T = 
S/
10 &lt;t9(5%) 
Plugginginthevaluesfromtheproblem, T = 
10
..
58 
/10 2.066,whichissmallerthan t9(0.05) = 1.83. 
Example 3 Let Xi Bernoulli(p), i =1,2,3. I.e. we are ipping a bent coin three times independently, 
and Xi =1 if it comes up heads, otherwise Xi =0. We want to test H0 : p = 31 
32 . against HA : p = 
Since both hypotheses are simple, can use likelihood ratio test 
3  1 Xi  2 1Xi 23P3 Xi P i=1 3 T = f0(X)= i=1 3 3 = P =232 i=1 Xi 
fA(X) 3 2 Xi 1 1Xi 2 i3
=1 Xi 
i=1 3 3 
Therefore, we reject if 
3 
232 PXi 3 
i=1 k (32 Xi)log2logk 
i=1 
which is equivalent to X3 21  log k . In order to determine k, lets list the possible values of X3 and 6 log 2 
their probabilities under H0 and HA, respectively: 
X3 Prob. under H0 Prob. under HA cumul. prob. under H0 
1 1 8 1 
27 27 27 2 6 12 7 
3 27 27 27 1 12 6 19 
3 27 27 27 
0 8 1 127 27 
So if we want the size of the test equal to  = 1 , we could reject if and only if X3 &gt; 2 , or equivalently 27 3 
we can pick k = 21 . The power of this test is equal to 
8 1 = P(X3 =1|HA)=27 29.63% 
2 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>
The denominator of T is given by the likelihood evaluated at the maximizer, which is the maximum 
likelihood estimator, MLE = X(n) =max{X1,...,Xn}, so that 
 n 
max L()= L(MLE)= 1 
R+ X(n) 
Therefore, 
L(0)  X(n) n 
T = = maxR+ L() 0 
In order to nd the critical value k of the statistic which makes the size of the test equal to the desired 
level, wed have to gure out the distribution under the null  = 0 -could look this up in the section on 
order statistics. 
As an aside, even though we said earlier that for large n, the GLRT statistic is 2-distributed under the 
null, this turns out not to be true for this particular example because the density has a discontinuity at 
the true parameter value. 
2 Other Special Tests 
2.1 Two-Sample Tests 
Suppose we have two i.i.d. samples X1,...,Xn1 and Z1,...,Zn2 ,potentially of dierent sizes n1 and n2, 
and may be from two dierent distributions. 
 N(X,2Xi X) 
Zi  N(Z,2 
Z) 
Two types of hypothesis tests we might want to do are 
1. H0 : X = Z against HA : X = Z, or 
0 : 2 2 
A : 2 22. H
X = Z against H
X = Z 
How should we test these hypotheses? 
1. Here, we will only consider the case in which 2 and 2 areknown(seethebookfor adiscussion X Z
of the other case). Under H0 : X = Z,

T =  X Z N(0,1) 
2 2 
X Z+ n1 n2 
Intuitively, T shouldbelarge(in absolute value) ifthe nullis nottrue. Therefore, a size  test of 
H0 against HA rejects H0 if 
|T|&gt; 1   
2 
2. For the test on the variances, need to recall distributional results: 
(n1 1)s2 
2 (n2 1)s2 
2 X Z 
2 n11, and 2 n21 
X Z 
4 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>x 
321nF 
1
7 
01 
6
7 
5
7
4
7
3
7
2
7 (x) 
-1 0.5independently of another. Also recall that a ratio of independent chi-squares divided by their 
degrees of freedom is distributed F, 
(n1 1)s 2 
2 X 
X /(n1 1)
/(n2 1) Fn11,n21
 S = (n21)s2 
Z 
2 
We clearly dont know 2 and 2 
X = 2, but under H0 : 2 , this expression simplies to X Z Z
2 
XS= s
2sZ 
 Therefore, a size  test rejects if S&gt;Fn11,n21(1/2) or S&lt;Fn11,n21(/2). 
2.2 Nonparametric Inference 
So far, we have mostly considered problems where the data generating process is of form f(x)(family |
of distributions) and known up to a nite dimensional parameter . Testing in that setting is called 
parametricinference. 
As exceptions, we noted in estimation that sample means, variances and other moments had favorable 
properties for estimation of means, variances and higher-order moments of any distribution. 
Since the entire distribution of a random variable can be characterized by its c.d.f., it may seem like a 
goodidea to estimate the c.d.f. from thedata withoutimposing any restrictions(except thatit should 
be a valid c.d.f. of course, i.e. monotone and continuous from the right). 
The sampledistributionfunction Fn(x)isgivenby Z 
jFn(x)= for X(j) x&lt;X(j+1) n 
where X(j) is the jth order statistic(remember that thisis the jth smallest value in the sample), and 
X(0) and X(n+1) . 
Example 6 For a sample {1,3,1,1,.5,2,0}, the ordered sample is {1,0,0.5,1,1,2,3}, and we can 
graph the sample distribution function Fn(x): 
5 Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Bayes theorem and random variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>7</slideno>
          <text>e.g. there is only one way candidate A can get all the votes. We can formulate the probability of each 
outcome in terms of simple probabilities, and from there we can aggregate over equivalent outcomes to 
obtain the probability for a given total number of votes. 
Remark2 Not all random events have a numerical characteristic associated with them in which we are 
interested(e.g. if theeventis itwill raintomorrow,wemightnotcarehowmuch). Thenwedonthave 
to bother with random variables, but can just deal with events as before. Alternatively, we could dene 
a random variable which takes the value 1 if the event occurs, and 0 otherwise (well use that trick 
sometimes in the future). 
7 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Figure1:Stroboscopicimageof acoin ip(Courtesy ofAndrewDavidhazy and theSchool of PhotoArts 
and Sciences of Rochester Institute of Technology. Used with permission. cAndrewDavidhazy,2007.) 
in the initial state s  S (not even taking into account external inuences like, say, the gravitation of a 
car driving by), and no matter how we ip the coin, it is totally impossible to control the initial position, 
velocity, etc. precisely enough to produce the desired outcome with certainty. Also, it will typically be 
impossible to solve the dierential equations describing the system with sucient precision. Therefore, 
we can only give probabilities for being in parts of S, which again maps to probabilities for the outcomes 
H and T . So even though there need not be any genuine randomness in the situation, this is how it 
plays out for us in practice. 
While this denition of the random variable brings out more the philosophical point how we think about 
random variables and probabilities, it is clearly not very operational, and for practical purposes, wed 
rather stick to the rst way of describing the problem. 
Remark1 The probability function on the sample space S induces a probability distribution for X via 
P (X  A)= P ({s  S : X(s) A}) 
for any event A  R in the range of X. 
Eventhoughformally, X is a function from the sample space into the real numbers, we often treat it 
asavariable,i.e. wesay thatit can takeon variousvalueswith thecorrespondingprobabilitieswithout 
specifying its argument. In other words, for most applications, we will only specify P (X  A) without 
any referencetotheunderlying samplespace S and theprobability on S. E.g. in the coin ip example -as 
described above -wewonttrygureouttheexact relationshipbetweencoordinatesin S (initialposition, 
velocity,orientationetc. ofthecoin) and outcomes(whichisnumericallyimpossible) and aprobability 
distribution overthese coordinates,but wejust needtoknowthat P (X =1) = P (X =0) = 1
2 . 
Example8 If we toss 10 coins independently of another, we could dene a random variable X =(Total 
Number of Tails). Well analyze the distribution for this type of random variable in more detail below. 
Example9 We might be interested in the outcome of an election. Say there are 100 million voters 
and 2 candidates. Each voter can only vote for either of the candidates, so there are 2100,000,000 distinct 
outcomes in terms of which voter voted for which candidate. We could now dene the random variable 
X corresponding to the total number of votes for candidate A (and for elections, thats typically all we 
care about). For each value for the number of votes, there is a corresponding number of basic outcomes, 
6 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
 3 Random Variables 
Now lets move on to the second big topic of this class, random variables. 
Example6 Flipping a coin, version I: We could dene a variable X which takes the value 1 if the coin 
comes up Heads H and 0 if we toss Tails T . The sample space for this random experiment is S = {H,T }, 
and the range of the random variable is {0, 1}. 
Denition1 A real-valued random variable X is any function 
S  R X : s  X(s) 
which maps the outcomes of an experiment to the real numbers. 
Asahistorical aside, whentheideaof randomvariableswasdeveloped around1800,therewasnorole 
for genuine randomness in the minds of mathematicians and other scientists. Rather, chance was seen 
as a consequence of us not having full knowledge about all parameters of a situation we are analyzing, 
and ourinability to apply the(supposedlyfullydeterministic) laws of nature topredictthe outcome of 
an experiment. A being able to do all this is known as the Laplacean demon, described by the famous 
mathematician Pierre Simon de Laplace as follows: 
An intellect which at any given moment knew all the forces that animate Nature and the 
mutual positions of the beings that comprise it, if this intellect were vast enough to submit 
itsdatato analysis, could condenseinto a singleformulathe movement of thegreatbodies of 
the universe and that of the lightest atom: for such an intellect nothing could be uncertain, 
andthefuturejustlikethepast wouldbepresentbeforeits eyes.2 
This view ofthe worlddoesntquitehold up to subsequentdevelopmentsinphysics(e.g. genuineinde
terminacy inquantumphysics) or computationaltheory(e.g. G odelstheorem: theintellect wouldhave 
to be more complex than itself since its predictions are part of the universe its trying to predict), but 
its still what underlies our basic concept of probabilities: randomness in the world around us primarily 
reects our lack of information about it. 
Example7 As an illustration, here is version II of the coin ip: in order to illustrate Laplaces idea, we 
could think of a more complicated way of dening the sample space than in the rst go above: in classical 
mechanics, we can (at least in principle) give a full description of the state of the coin (a rigid body) 
at any point in time, and then use the laws of classical mechanics to predict its full trajectory, and in 
particularwhetheritisgoingtoend up withheads(H) or tails(T ) on top. More specically, we could 
describe the sample space as the state of the mechanical system at the point in time the coin is released 
intothe air. Afull(though somewhatidealized) description ofthe state ofthe systemisgivenby(1) the 
position, and(2) the velocity of the center of mass of the coin together with(3) its orientation, and(4) 
its angular momentum at a given time t0 -each of these has 3 coordinates, so S = R12 . Eachpoint s  S 
belongs to one of the two events {H,T }deterministically. If we assign values X =1 to the event H  S 
that heads are up, and X =0 for tails T , this mapping is the random variable 
X : R12 {0, 1}, givenby X : s  1 if s  H 
s  0 otherwise, i.e. if s  T 
Since the problem is -almost literally -very knife-edged, the outcome is highly sensitive to tiny changes 
2Laplace, P. (1814): A Philosophical Essay on Probabilities 
5 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 4 
Konrad Menzel 
February 12, 2009 
Bayes Theorem 
In the last lecture, we introduced conditional probabilities, and we saw the Law of Total Probability 
as a way of relating the unconditional probability P (A) of an event A to the conditional probabilities 
P (A|Bi). Another important relationship between conditional probabilities is Bayes Law which relates 
the conditional probability P (A|B) to the conditional probability P (B|A), i.e. how we can revert the 
order of conditioning. This result plays an important role in many areas of statistics and probability, 
most importantly in situations in which we learn about the state of the world A from observing the 
data B. 
Example1 The ancientGreeks(who apparently didntknow much statisticsyet) noticedthat eachtime 
after a ship had sunk, all surviving seamen reported having prayed to Poseidon, the Greek god of the sea. 
From this observation, they inferred that they were in fact saved from drowning because they hadprayed. 
This example was actually brought up by the English philosopher Francis Bacon in the 16th century. 
In statistical terms, lets dene the events A =survives and B =prayed, so that the question becomes 
whether praying increases the odds of survival, i.e. whether P (A|B)&gt;P (A) p, say. The observation 
that all surviving seaman had been praying translates to P (B|A) =1. Is that information actually 
sucient to answer the question whether praying strictly increases the chances of survival? How do we 
use the information on P (B|A)to learn about P (A|B)? 
From the denition of conditional probabilities, we obtain 
P (AB)= P (A|B)P (B)= P (B|A)P (A) 
Rearranging the second equality, we get 
P (B|A)P (A)P (A|B)= P (B) 
Weve also seen that we can partition the event 
P (B)= P (B|A)P (A)+P (B|AC )P (AC ) 
sothat 
P (B|A)P (A)P (A|B)= P (B|A)P (A)+P (B|AC )P (AC ) 
We can generalize this to any partition of S as summarized in the following theorem: 
1 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Juliet alsoknows thatifRomeolovedher,he wouldgiveherjewelery withprobability P (J|L)=0.80, or 
a serenade with probability 
P (S|L)=0.20 
(this is actually only what Juliet thinks, keeping in mind that Romeo also loves football). If he doesnt 
loveher anymore,hehas noidea whatJuliet wants, andhellgiveher a serenade(or, more realistically, 
the roses she wanted last year, or forget about Valentines Day altogether) with probability 
P (S|LC )=0.80 
(note that, though givinga serenade is still very embarrassing for Romeo, its also much cheaper). 
It turns out that Romeo ends up giving Juliet a serenade. Should she dump him right away? By Bayes 
theorem, Juliets posterior beliefs about Romeos intentions are 
 P (S|L)P (L) (10.8)0.95 2 19 38 P (L|S)= = = 10 20 =  0.826 P (S|L)P (L)+P (S|LC )P (LC ) (10.8)0.95+0.8(10.95) 2 19 81 46  +10 20 10 20 
and well let Juliet decide on her own whether this is still good enough for her. 
Inreal-lifesituations, mostpeoplearent verygood atthistypeofjudgmentsand tend tooverratethe 
reliability of a test like the ones from the last two examples -in the cognitive psychology literature, this 
phenomenon is known as base-rate neglect, where in our example base-rate refers to the proportions 
P (A) and P (AC ) of infected and healthy individuals, or the prior probabilities P (L) and P (LC ) of 
Romeoloving ornotlovingJuliet, respectively. If theseprobabilitiesareverydierent,biasesinintuitive 
reasoning can be quite severe. 
Example5 The Monty Hall paradox:1 There used to be a TV show in which a candidate was asked to 
choose among three doors A,B,C. Behind one ofthedoors,there was aprize(say, abrand-new washing 
machine), and behind each of the other two there was a goat. If the candidate picked the door with the 
prize, he could keep it, if he picked a door with a goat, he wouldnt win anything. In order to make the 
game a little more interesting, after the candidate made an initial choice, the host of the show would 
always open one of the two remaining doors with a goat behind it. Now the candidate was allowed to 
switch to the other remaining door if he wanted. Would it be a good idea to switch? 
Without loss of generality, assume I picked door A initially. The unconditional probability of the prize 
being behind door A is 31 . If the prize was in fact behind door a, the host would open door b or door 
c, both of which have a goat behind them, with equal probability. If the initial guess was wrong, there is 
only one door left which was neither chosen by the candidate nor contains the prize. Therefore, given I 
initiallypicked A and the host then opened C, 
 P (prizebehindA|C opened) = P (C opened|prizebehind A)P (prizebehindA)= 1
2 
1 1
3 = 1 
P (C opened) 2 3 
On the other hand 
P (C opened|prizebehind B)P (prizebehindB)1 31 2 P (prizebehindB|C opened) = = 1 = P (C opened) 2 3 
Therefore, I could increase my chances of winning the prize if I switch doors.
Intuitively, the newly opened door does not convey any information about the likelihood of the prize being
1You can read up on the debate on http://people.csail.mit.edu/carroll/probSem/Documents/Monty-NYTimes.pdf 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Theorem1 (Bayes Theorem) If A1,A2,... is a partition of S, for any event B with P (B)&gt; 0 we 
can write 
P (B|Ai)P (Ai) P (B|Ai)P (Ai)P (Ai|B)= =  P (B) j1 P (B|Aj )P (Aj) 
	P (Ai)isthe prior probability of an event Ai (i.e. probability before experiment is run) 
	P (Ai|B)isthe posterior probability of Ai (i.e. theprobability after we ranthe experiment andgot 
information B -as obtained from Bayes theorem) 
An entire statistical theory of optimal decisions is built on this simple idea: Bayesian Decision Theory. 
Example2 For the previous example of seamen surviving the sinking of a ship, we were able to observe 
P (B|A)=1, and the(unconditional) survival rate of seamen, P (A). However, we can also see that we 
dont have sucient information to answer the question whether praying strictly increases the chances of 
survival since we couldnt observe P (B|AC ), the fraction of seamen who prayed among those who were 
going to drown. It is probably safe to assume that those, fearing for their lives, all of them prayed as well 
(implyingP (B|AC )=1), so that 
1 pP (A|B)=	 = p = P (A)1 p +1  (1p) 
Inasense,thereasoning oftheancientGreeksisaninstanceof survivorbias (well,inaveryliteral 
sense): Bayes theorem shows us that if we can only observe the survivors, we cant make a judgement 
about why they survived unless we know more about the subpopulation which did not survive. 
Example3 An important application of Bayes rule is how we should interpret a medical test. Suppose 
a doctor tests a patient for a very nasty disease, and well call the event that the patient in fact has 
thedisease A. The test can either give a positive result -well call this event B -or a negative result, 
corresponding to event BC . The test is not fully reliable in the sense that we cant determine for sure 
whether the patient has the disease, but the probabilities of a positive test result is 
P (B|A)=99%,P (B|AC )=5% 
Finally, we know that the disease is relatively rare and aects 0.5% of the population which shares the 
patients age, sex, and other characteristics. Lets say the test gives a positive result. What is the 
(conditional)probability that the patient does in fact have the disease? 
Bayes rule gives that 
P (B|A)P (A) P (B|A)P (A)	 0.99 0.005 P (A|B)= =	 =  0.0905 P (B) P (B|A)P (A)+P (B|AC )P (AC )0.99 0.005+0.05 0.995 
Since the overall prevalence of the disease, P (A) is relatively low, even a positive test result gives only 
relatively weak evidence for disease. 
Example4 RomeoandJuliethavebeendatingforsometime,and comeValentinesDay(asareminder: 
thatsSaturday),Romeo cangiveJuliet eitherjewelery, J, or a serenade, S. Juliet wantsjewelery, and 
if Romeo really loved her, he would read her wishes from her eyes, and besides, she had told Romeo about 
the jewelery two weeks earlier, during the nal half hour of the Superbowl. Juliet also has rst doubts 
that Romeo still loves her, an event well call L. To be specic 
P (L)=0.95 
2 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
 behinddoor A, since the host would not have opened it in any case -and in fact, given that we chose A, 
the events A contains the prize and C is opened are independent. However, the fact that he did not 
open B could arise from two scenarios: (1) the prize was behind A, andthehostjust choseto open C 
at random, or(2) theprize wasbehinddoor B, so the host opened door C only because he didnt have a 
choice. Therefore, being able to rule out C only benets event B. 
2 Recap Part 1: Probability 
Before we move onto the second chapter of thelectureletsjust summarize what wehavedone sofar, 
and what you should eventually should feel familiar and comfortable with: 
2.1 Counting Rules 
	drawing n out of N with replacement: Nn possibilities 
	drawing n out of N without replacement: (NN
! 
n)! possibilities 
	permutations: N!possibilities 
N 	combinations of n out of N: possibilities n 
2.2 Probabilities 
	independence: P (AB)= P (A)P (B) 
P (AB)	conditionalprobability P (A|B)= P (B) if P (B)&gt; 0. 
	P (A|B)= P (A)if and only if A and B areindependent 
	Law of Total Probability: for a partition B1,...,Bn of S, where P (Bi)&gt; 0 
n 
P (A)= P (A|Bi)P (Bi) 
i=1 
	BayesTheorem:
P (B|Ai)P (Ai)
P (Ai|B)= P (B) 
There are also a few things we saw about probabilistic reasoning in general: 
	use of set manipulations to reformulate event of interest into something for which its easier to 
calculateprobabilities(e.g. complement,partitionsetc.) 
	the importance of base rates in converting conditional into marginal/unconditional probabilities 
(e.g. in Bayes theorem or composition eects in the heart surgeons example) 
	can sometimes make dependent events A and B independent by conditioning on another event C 
(or make independent events dependent). 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Probabilities and counting rules (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
14.30 Introduction to Statistical Methods in Economics
Spring 2009
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>4 Counting Rules 
The examples we looked at so far were relatively simple in that it was easy to enumerate the outcomes 
in A and S, respectively. If S has many elements, and an event A is suciently complex, it may be 
very tedious and impractical to obtain n(A) and n(S) by going down the full list of outcomes. Today, 
well look at combinatorics part of which gives simple rules for counting the number of combinations or 
permutations ofdiscrete objects(outcomes) corresponding to adierentpattern(event). 
Example2 The famous chess player Bobby Fischer (who died 3 weeks ago) eventually got bored of 
playing classic chess and proposed a variant in which only the 8+8 pawns are set up as usual, but the 
otherpieces(1king,1queen,2bishops,2knights,2 rooks) areputinrandompositionsonthe rstrank, 
where each whitepiecefaces the correspondingblackpiece on the other side. Asfurther restrictions,(1) 
onebishop mustbe on ablack, the other on a white square, and(2) theking must start outbetween the 
two rooks(in order to allowfor castling). 
The idea behind this is that since chess players tend to use standard game openings which work well 
only for the standard starting positions, the new variant forces them to play more creatively if there are 
enoughpossible ways of setting up agameto makeitimpossibleforplayersto memorize openingsfor any 
constellation. But how many possible starting positions are there? 
Well actually do the calculation later on in the lecture today using some of the counting techniques 
introducedinthisclass. Ifyougetbored,youcanalready start guring outa(preferably elegant) way 
of attacking the problem. 
For now, we will not explicitly talk about random experiments or probabilities, but digress on methods 
to enumerate and count outcomes which we will put to use later on in the lecture. 
4.1 Composed Experiments 
Rule1 (Multiplication Rule): If an experiment has 2 parts, where the rst part has m possible 
outcomes, and the second part has n possible outcomes regardless of the outcome in the rst part, then 
the experiment has m  n outcomes. 
Example3 Ifapasswordisrequiredtohave8characters(letters ornumbers),thenthecorresponding 
experiment has 8 parts, each of which has 2 26+10 =62 outcomes(assuming thatthepasswordis case-
sensitive). Therefore, we get a total of 628 (roughly 218 trillion) distinct passwords. Clearly, counting 
those up by hand would not be a good idea. 
Example4 The standardASCII character set used on most computer systemshas127 characters(ex
cluding the space): each character is attributed 1 byte = 8 bit of memory. For historical reasons, the 8th 
bit was used as a parity bit for consistency checks to detect transmission or copying errors in the code. 
Therefore, we have an experiment of 7 parts, each of which has outcomes from {0, 1}, so we have a total 
number of 27 =128 distinct characters. 
Example5 A card deck has 52 cards, so if we draw one card each from a blue and a red deck, we get 
52 52 = 2704 possible combinations of cards (if we cant tell ex post which deck each card came from, 
we get a smaller number of distinguishable outcomes). If, on the other hand, we draw two cards from the 
same deck without putting the rst card back on the stash, regardless of which card we drew rst, only 51 
cards will remain for the second draw. Of course which 51 cards areleftisgoing todepend on which card 
was drawn at rst, but notice that this doesnt matter for the multiplication rule. Therefore, if we draw 
two cards from the same deck, well have 52 51 =2652 possible combinations. 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>   
3 S 
A Bc 
A B B Ac BA 
Figure 1: Partition of A B into disjoint events 
Example: Simple Probabilities 
Suppose we have a nite sample space with outcomes which are ex ante symmetric in the sense that we 
have no reason to believe that either outcome is more likely to occur than another. If we let n(C)denote 
the number of outcomes in an event C, we can dene a probability 
n(A)P (A):= n(S) 
i.e. the probability equals the fraction of all possible outcomes in S that are included in the event 
A. This distribution is called the simple probability distribution or also the logical probability. A 
randomizationdevice(e.g. acoinoradie) forwhich each outcomeisequally likely issaidtobe fair. 
Now lets check whether the three axioms are in fact satised: 
(P1): P (A) 0followsdirectly fromthefactthat n()only takes(weakly) positive values. 
(P2): P (S)= n
n(
(S
S)
) =1. 
(P3): For two disjoint eventsA and B, 
n(A B) n(A)+n(B)+n(A B) n(A) n(B)P (A B)= = = + = P (A)+P (B) n(S) n(S) n(S) n(S) 
For more than two sets, the argument is essentially identical. 
Example1 Suppose a fair die is rolled once. Then the sample space equals S = {1, 2,..., 6}, so n(S)= 
6. What is the probability of rolling a number strictly greater than 4? -since the event is A = {5, 6}, 
n(A) = 2 = 1 n(A)= n({5, 6})=2. Hence P (A)= n(S)63 . 
If a die is rolled twice, what is the probability that the sum of the numbers is less than or equal to 4? 
Lets check: S = {(1, 1), (1, 2),..., (2, 1), (2, 2),..., (6, 6)}, so that n(S)=62 =36. The event 
B = Sum of Dice  4 = {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)} 
so that P (B)= n(B) = 6 = 1 
n(S) 36 6 . 
In a minute, well look at more sophisticated techniques for enumerating outcomes corresponding to 
certain events. 
4 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>   
   Thelast exampleillustratestwotypes of experimentsthat we wanttodescribe moregenerally: sampling 
with replacement versus sampling without replacement, each of which has a dierent counting rule. 
 n draws from a group of size N with replacement: 
N  N  ...  N = Nn 
n times 
possible outcomes. 
 n draws from a group of size N without replacement (whereN  n): 
N(N 1)(N 2)... 3 2 1 N! PN,n := N(N 1)(N 2)... (N (n 1)) = = (N n)(N (n +1))... 3 2 1(N n)! 
possible outcomes, where k!:= 1 2 ... (k 1)k (readas k-factorial),and wedene0! =1. 
In fact, both of these enumeration rules derive from the multiplication rules, but since they are very 
prominent in statistics, we treat them separately. 
4.2 Permutations 
Example6 A shued deck of cards is a permutation of an ordered deck of cards: it contains each card 
exactly once, though the order will in most cases be dierent. 
Denition2 Any ordered rearrangement of objects is called a permutation. 
Note that generating permutations corresponds to drawing N out of a group of N without replacement. 
Example7 Dodecaphonyis a compositionschemein modern classical musicinwhich eachpieceisbased 
on a tone row inwhich each ofthetwelvenotesofthechromaticscale(C,C sharp,D,D sharp,etc. up 
to B) appears exactly once. Therefore, each tone row is a permutation of the chromatic scale, and we 
couldinprinciplecountthenumberof allpossibledistinct melodies (about479million). 
Example8 The famous traveling salesman problem looks at a salesman who has to visit, say, 15 towns 
inanarbitrary order,andgivendistancesbetweentowns,wearesupposed to nd theshortestroutewhich 
passes through each town on thelist(atleast) once. Using ourformulafordrawing15 out of agroup of 
15, we can calculate that there are 15!, which is about 1.3 trillion, dierent paths, so this is a complicated 
problem, and we wont solve it. 
We could imagine that in each town, the salesman has to visit 5 customers. If we consider all possible 
paths from customers to customers, we get (15 5)! permutations(thats alot!). However,it may seem 
sensible to restrict our search to travel plans according to which the salesman meets with all 5 customers 
onceheisintown(in an ordertobedetermined). There are 5! possible orders in which the salesman 
can see customers in each town, and 15! possible orders in which he can visit towns, so we can use the 
multiplication rule to calculate the number of permutations satisfying this additional restriction as 
(5!5!... 5!) 15! =(5!)1515! 
15 times 
whichisstill aninsanelyhigh number,but certainly muchlessthanthe (155)! unrestrictedpermutations.3 
3Few people, if any, have a good intuition for the scale of factorials since k! grows extremely fast in k. Stirlings 
6 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>axioms, it is useful to partition the event A using properties of unions and intersections 
A = A S = A (B BC )=(A B)(A BC )= B (A BC ) 
where the last step uses that B  A impliesthat A B = B. Nowin ordertobe ableto use axiom(P3), 
notethat B and A BC aredisjoint: 
B (A BC )= B (BC A)=(B BC )A = A =  
Therefore, we can conclude 
P (A)= P (B)+P (A BC ) P (B) 
by axiom(P1)  
Proposition4 For any event A, 0 P (A) 1. 
Proof: 0  P (A)is axiom(P1). Forthe secondinequality, note(P1) alsoimpliesthat P (AC )  0. 
Thereforebyproposition1 
P (A)=1P (AC )1 
Proposition5 
P (A B)= P (A)+P (B)P (A B) 
Proof: Note that, as in the proof of proposition 3, we can partition the events A and B into 
A = A S = A (B BC )=(A B)(A BC ) 
and in the same fashion 
B =(B A)(B AC ) 
You can easily check that these are in fact partitions, i.e. each of the two pairs of sets is disjoint. 
Therefore, we can seefrom axiom(P3) that 
P (A)= P ((A B)(A BC ))= P (A B)+P (A BC ) 
and 
P (B)= P (B A)+P (B AC ) 
Therefore 
P (A)+P (B)= P (A B)+
P (A BC )+P (B A)+P (B AC ) 
= P (A B)+P (A B) 
by(P3) since(A B), (A BC ), (B AC )is a partition of A B (gure 6 gives a graphical illustration 
of the idea). Rearranging the last equation gives the desired result  
3 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>ruleforenumerations,thereareatotal of3652 possiblecombinationsofdeathdates. Hencetheprobability 
for this event is 1/3652 which is an extremely small number. 
However,thereisalso alargenumber ofpairs ofpresidents anddaysintheyearwhich couldbepotential 
candidates for a double death date. Now, the probability of the event A that at least two out of the 39 
presidents died on the same day can in principle be calculated as the ratio of the number of all possible 
combinations that have one pair of presidents with the same death date, two pairs, three presidents 
on one death date, and so forth. A more elegant way to attack this problem is by noting that, since 
A AC = S and A AC = , from the axioms P3 and P2 we have P (A)= P (S)P (AC )=1P (AC ). 
The event AC can be formulated as all 39 dead presidents have dierent birthdays. If there were only 
two dead presidents, there are 364 ways in which, given the death date of the rst, the death date of 
the second could fall on a dierent date. You should now note that counting the number of possibilities 
of assigning dierent death dates to each of the n presidents corresponds to the experiment of drawing 
n out of 365 days without replacement, so the number of possibilities is 365! The total number of (365n)! . 
possible assignments of death dates to presidents corresponds to drawing dates without replacement, so 
the number is 365n . 
Hence the probability of having at least one pair of presidents with the same death date is 
365! P (A)=1P (AC )=1 365n(365n)! 
which,for n = 39 is equal to about 87.82%. Using the formula, we can also calculate this probability for 
dierent numbers of presidents: 
n P(A) 
1 0 
2 0.27% 
5 2.71% 
10 11.69% 
20 41.14% 
30 70.63% 
60 99.41% 
366 100.00% 
Aswecanseefromthelastline,oneintuitionfortheprobabilityincreasingtooneisthat weeventually 
exhaust the number of potential separate death dates until eventually we have more dead presidents 
thandeathdates. 
So, to resolve the paradox, while the event that two given presidents died on a xed date would indeed 
be agreat coincidence(i.e. has a verylowprobability), with anincreasing number ofpresidents, there 
is a combinatorial explosion in the number of dierent constellations for such an event. In other words, 
while each individual outcome remains very unlikely, the number of potential coincidences increases 
very steeply, so that with a high probability at least some coincidences must happen. 
Thestorybehindtheotherconspiracytheoriesispresumablythesame:peoplehavebeencombingthrough 
zillions of details trying to nd only a relatively tiny number of stunning parallels between Lincoln and 
Kennedy. In statistics, this search strategy is commonly called data mining, and in this context we 
speak of those rare coincidences which actually occur as false discoveries, which are typically not due 
to any systematic relationships, but simply result from the sheer numbers of potential relationships that 
we might investigate or test simultaneously. 
9 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>  
  
          4.3 Combinations 
Example9 If wewantto counthowmanydierentpokerhands wecandrawfrom asingledeck of cards, 
i.e. 5 cards drawn from a single deck without replacements, we dont care about the order in which cards 
were drawn, but rather whether each of the card was drawn at all. 
Denition3 Any unordered collection of elements is called a combination. 
Acombinationconstitutesadrawwithoutreplacementfromagroup,but sincewenowdonotcareabout 
the order of elements, we dont want to double-count series of draws which consist of the same elements, 
only in dierent orders. For a collection of n elements, there are n! dierent orders in which we could 
havedrawnthem(i.e. the number ofpermutations of the n elements). Therefore the number of dierent 
combinations of n objectsfrom N objectsis 
 outcomesfromdrawing n out of N without replacement N! CN,n = =  orders in which can draw n elements (N n)!n! 
This number is also known as the binomial coecient, and often denoted as 
N N! := n (N n)!n! 
Notethat,eventhough welook at aratiooffactorials,thebinomial coecient alwaystakesintegervalues 
(as it should in order for the number ofcombinations to make sense). 
52 Example10 For poker, we can use this formula to calculate that there are =2598960 possible 5 
hands. 
Example11 A functional study group should not have more than, say, 5 members (there is no peda
gogical justication for this number, but I just want to keep the math from becoming too complicated). 
There are currently 28 students registered for this class. How many possibilities for viable study groups 
(includingstudents working on their own) would be possible? Well have to calculate the number of study 
groups for each group size 1, 2, 3, 4, 5 and addthem up, sothat(ifIdidnt make any mistakes) there are 
28 28 28 28 28 S = + + + + =28+378+3, 276+20, 475+98, 280 =122, 437 1 2 3 4 5 
possible study groups. 
Now back to our challenge problem from the beginning of the class: 
approximation, which works quite well for high values of k is 
k! 
2k k k 
e 
Inthepop-sciliterature a common comparisontoillustrate extremely large numbersinvolvesthe estimated total number of 
atomsinthe observable universe, whichis about1080 (well,Idonthave anintuitionforthateither!). Intermsoffactorials, 
1080 59!. The number 75! can be expressed as roughly 2.51030 (two anda half million trillion trillion) times the number  
of atoms in the universe.
Sinceyoud wantto avoid calculationsinvolving suchhigh numbers, notethatfor mostpurposes, we only havetodeal with
ratios of factorials, so we should rst see which terms cancel, e.g. 98! 9897969594!
= .94! 94! 
7 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>2 Denition1 A probability distribution on a sample space S is a collection of numbers P (A) which 
satises the axioms(P1)-(P3). 
Notethatthe axioms(P1)-(P3) do notpindown a unique assignment ofprobabilitiesto events. Instead, 
these axioms only give minimal requirements which any probability distribution should satisfy in order 
to be consistent with our basic intuitions of what constitutes a probability (well actually check that 
below). Inprinciple anyfunction P ()satisfying thesepropertiesconstitutesavalidprobability,but wed 
haveto see separately whetherits actually agooddescription of the random experiment athand, which 
is always a hard question. In part 5 of this class (Special Distributions), well discuss a number of 
popular choices of P ()for certain standard situations. 
Some Properties of Probabilities 
Now we stillhave to convince ourselves thatthe axioms(P1)-(P3) actually are sucient to ensure that 
ourprobability functionhastheproperties we wouldintuitively expectittohave,i.e. (1)theprobability 
that an eventhappensplustheprobability thatitdoesnthappen should sumto one,(2) theprobability 
that the impossible event, ,happens should equal zero,(3) if an event B is contained in an event A,its 
probability cant be greater than P (A), and(4) theprobabilityfor any event shouldbein theinterval 
[0, 1]. Well now prove these properties from the basic axioms. 
Proposition1 
P (AC )=1P (A) 
Proof: By the denition of the complement AC , 
(P 2) Defn.AC  (P 3) 1= P (S)= P (A AC )= P (A)+P (AC ) 
where the last step uses that A AC = , i.e. that A and its complement are disjoint. Rearranging this, 
weget 
P (AC )=1P (A) 
which is what we wanted to show  
Proposition2 
P ()=0 
Proof: Since C = S, we can use the previous proposition to show that 
Prop.1 (P 2) P ()= P (SC )=1P (S)=11 =0 
Proposition3 If B  A, then P (B)P (A). 
As an aside, cognitive psychologists found out that even though this rule seems very intuitive, people 
often violate it in everyday probabilistic reasoning.2 Proof: In order to be able to use the probability 
2E.g. in a study by the psychologists Daniel Kahneman and Amos Tversky, several people were given the following 
description of Linda: 
Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply 
concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. 
Individuals who were asked to give the probability that Linda was a bank teller tended to state a lower gure than those 
asked about the probability that she was a feminist bankteller. 
2 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>  
5 Example12 Back to Fischer Random Chess: Lets rst ignore the restrictions (1) and (2) about the 
rooks and bishops, i.e. allow for any allocations of the pieces on the bottom rank of the board. We have 
to allocate the 8 white (or black, this doesnt matter) pieces onto the 8 squares in the rst rank of the 
chessboard. Notice that this is a permutation, so that we have 8! possible orderings. However, rooks, 
knights and bishops come in pairs and for the rules of the game the left and the right piece are 
equivalent. Therefore, there are 2 2 2 possible ways of generating each starting position by exchanging 
the two rooks/knights/bishops with each other, respectively. Hence, the number of distinct games is 
8! Games = =7! =5040 8 
As we said earlier,the actual rulesforFischerRandomChessimposefurthermorethat(1) onebishop is 
placed on ablack, andthe other on a white square, and(2) thattheking hasto start outbetweenthetwo 
rooks. For this variant, we can use the multiplication rule if we are a little clever about the order in which 
we ll up the row: I propose that we rst allocate the two bishops, one on a random white, the other on 
a random black square, so there are 4 4 possibilities. Next, the queen takes one out of the remaining 6 
squares(6possibilities,obviously). Nowweputthetwoknightsonany ofthe vesquaresthatareleft. 
5 120 This is a combination, so there are = 62 =10 ways of allocating the knights. Because of the 2 
restriction on the king and the rooks, there is always exactly one way of setting up the three pieces onto 
the three remaining free elds. In sum, we have 
Games =4 4 6 10 1 =960 
potential games to be played. 
The crucial point about the order in which we place the pieces is to make sure that we can apply the 
multiplication rule, i.e. that the way we place the rst pieces does not aect the number ofpossibilities 
we have left for the remaining pieces. As far as I can see, this only matters for the bishops: Say, we 
placed therooksand theking rstand thenputup thebishops. Thenwedhavetodistinguish whether(a) 
allthreepiecesareon eldsofthesamecolor(sowedhave 1 4 =4 possibilities of placing the bishops 
on eldsofdierentcolors),or(b) oneofthethreepiecesstandsona eld of adierentcolorthanthe 
othertwo(leaving us with 2 3 =6 possibilities for the bishops). As long as we place the bishops rst and 
the king before the two rooks, it seems to be irrelevant in which order we proceed thereafter. 
The Presidential Death Date Paradox (Recitation) 
Conspiracy theories about living and dead presidents are typically built around weird coincidences. 
For example, for the two American presidents who were assassinated, i.e. Lincoln and Kennedy, one 
can nd long lists of more or less remarkable commonalities between the two -e.g. Lincoln purportedly 
had a secretary named Kennedy who had warned him not to go the theater where he was shot, whereas 
Kennedy had a secretary named Evelyn Lincoln who had warned him not to go to Dallas before his 
assassination(well,thats atleast whatWikipedia says...). 
One particular coincidence is the fact that several of the 39 presidents who already died share the same 
death dates: Filmore and Taft both died on March 8. John Adams and Thomas Jeerson both died on 
July4thin1826,exactly50yearsafterthesigning of thedeclarationofindependence,andJamesMonroe 
died exactly ve years later, on July 4, 1831. Is this something to be surprised about? 
Lets rst look at the simple probability that two given presidents died on a xed day, say February 6, 
assuming that probabilities equal the proportion of outcomes belonging to that event: we get that there 
isonly onecombinationof thetwopresidents deathdatesfallsonFebruary6th,butby themultiplication 
8 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> 
&#13;
  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 2 
Konrad Menzel 
February 5, 2009 
Probability of Events 
So far, we have only looked at denitions and properties of events -some of them very unlikely to 
happen(e.g. Schwarzeneggerbeing elected44thpresident),othersrelatively certain -butwehaventsaid 
anything about the probability of events, i.e. how likely an event is to occur relative to the rest of the 
sample space. 
Formally, a probability P is dened as a function from a collection of events A = {A1,A2,...}in S1 to 
the real numbers, i.e. 
A  R P : A  P (A) 
In order to get a useful denition of a probability, we require any probability function P to satisfy the 
following axioms: 
(P1) P (A) 0 for any event A A. 
(P2) P (S)=1 -i.e. forsure, something isgoing tohappen 
(P3) For any sequence of disjoint sets A1,A2,..., 
  
P  Ai = P (Ai) 
i1 i1 
As a mathematical aside, in order for these axioms (and our derivations of properties of P (A) next 
lecture) to make sense, it must in fact be true that the collection A in fact contains the event S, and 
the complements and unions of its members, and this is what constitutes a sigma-algebra as dened in 
the footnote on the previous page. But again, for this class, well take this as given without any further 
discussion, so you can ignore this ne print for now. 
1For a consistent denition of a probability, this collection of events must satisfy the following properties 
(S1) S A 
(S2) IfA A, then its complement AC A 
(S3) Anycountable union of events A1,A2,... isin A,i.e. A1 A2 ... A 
Such a collection of events is called a sigma-algebra on S. For the purposes of this class, this is not important, and well 
take it as given that the problem at hand satises these axioms. 
1 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Sets and events (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
14.30 Introduction to Statistical Methods in Economics
Spring 2009
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>as president in November, i.e. as a rst try we could look at the main candidates at the early stages of
the primaries:
S={Clinton, Huckabee, McCain, Obama, Paul, Romney, Schwarzenegger }
Is this a good description? - probably not: even though these are the most likely outcomes, we cant
logically rule out that some other candidate (from either party or independent) enters the race later on,
so a more fool-proof denition of the random experiment would augment the sample space
S={Clinton, Huckabee, McCain, Obama, Paul, Romney, Schwarzenegger,
other Democrat, other Republican, other Independent }
but to keep things simple, lets just ignore this possibility for now.
Some events of interest could be e.g.
The 44th president of the US will be Republican = {Huckabee, McCain, Paul, Romney, Schwarzenegger,
other Republican }
The 44th president is married to the 42nd president = {Clinton }
2.2 More about Sets and Events
2.2.1 Set Inclusion  
The event Biscontained inAif every outcome in Balso belongs to A, or in symbols
BAif (sB=sA)
Clearly, any event Cis contained in the sample space S, i.e.
CSfor any event C
and every event includes the impossible event
 Cfor any event C
ABS
Figure 1: BA-Bimplies A
IfAandBcontain each other, they are equal,
ABandBA=A=B
3Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>14.30 Introduction to Statistical Methods in Economics
Lecture Notes 1
Konrad Menzel
February 3, 2009
1 Introduction and Overview
This class will give you an introduction to Probability Theory and the main tools of Statistics. Probability
is a mathematical formalism to describe and analyze situations in which we do not have perfect knowledge
of all relevant factors. In modern life, we are all routine consumers of statistical studies in elds ranging
from medicine to sociology, and probabilistic reasoning is crucial to follow most of the recent debates in
economics and nance.
In the rst half of this class, well talk about probabilities as a way of describing genuine risk - or our
subjective lack of information - over events.
Example 1 In subprime lending, banks oered mortgages to borrowers who were much less likely to repay
than their usual clientele. In order to manage the risks involved in lending to prospective home-owners
who do not own much that could serve as collateral, thousands of these loans were bundled and resold as
mortgage backed securities, i.e. the bank which made the original loans promised to pay the holder of
that paper whatever repayment it received on the loans. Eventually, there were more complicated nancing
schemes under which the pool was divided into several tranches, where a rst tranche was served rst,
i.e. if the tranche had a nominal value of, say, 10 million dollars, anyone holding a corresponding claim
got repaid whenever the total of repayments in the pool surpassed 10 million dollars. The lower tranches
were paid out according to whatever money was left after serving the high-priority claims.
How could it be that the rst tranche from a pool with many very risky loans was considered to be safe
when each of the underlying mortgages was not? The low-priority tranches were considered riskier - why?
And why did in the end even the safe securities turn out to be much riskier in retrospect than what
everyone in the market anticipated? Well get back to this when we talk about the Law of Large Numbers,
and under which conditions it works, and when it doesnt.
Usually in order to answer this type of question, youll have to know a lot about the distribution (i.e.
the relative likelihood) of outcomes, but in some cases youll actually get by with much less: in some
cases you are only concerned with typical values of the outcome, like expectations or other moments
of a distribution. In other cases you may only be interested in an average over many repetitions of a
random experiment, and in this situation the law of large numbers and the central limit theorem can
sometimes give you good approximations without having to know much about the likelihood of dierent
outcomes in each individual experiment.
The second half of the class deals with the question how we can learn about populations and probability
distributions from data. In any empirical science youll come across the problem of inductive reasoning ,
which means drawing general conclusions from a few (or even very many) observed special cases. In
political polls (who would you vote for in the next presidential election?), a survey institute typically
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>we can see that
A(BC) = {Clinton }  {Clinton,Huckabee,McCain,Paul,Romney,Schwarzenegger }
={Clinton }
(AB)(AC) = {Clinton }  ={Clinton }
as it should be according to the rst distributive property.
2.2.4 Set Complement, Ac
Thecomplement ACofAis the set of outcomes in Swhich do not belong to A, i.e.
AC={sS|s/ A}=S\A
AS
Ac
Figure4:ComplementtoA-notA
Fromthedenition,youcaneasilycheckthatcomplementshavethefollowingproperties
(AC)C=A
AAC=S
AAC=
Fromthelaststatement,itfollowsthat
SC=
andtogetherwiththerstproperty,thisimplies
C=S
Oneusefulsetofrelationshipsbetweenintersectionsandunionsisthefollowing
(AB)C=ACBC
(AB)C=ACBC
6Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>polls at most a few thousand out of over a hundred potential voters. In medical trials, we try to draw
conclusions about the eectiveness of a drug for an entire population from only a few dozen study
participants.
If we only observe a subset of individuals (e.g. a random sample of voters in an exit poll) from a population
of interest (e.g. all voters who turned out for a primary election), there will be some uncertainty over
whether this sample was actually representative for the whole population with respect to the question
were after (e.g. the vote share for a particular candidate). Formalizing this for practical use makes heavy
use of probability theory.
Example 2 The controversial rst Lancet study on civilian casualties in the Iraq war surveyed a random
sample of 1849 households (which had in total 12,801 members) across the country three and a half years
after the invasion in March 2003, and extrapolated the number of deaths reported by the surveyed household
to the total population of the whole country, which is about 29 million. The authors of the study arrived
at an estimate of about 112,000 excess deaths for the rst 18 months after the invasion and stated
that with 95% condence, the actual number was between 69,000 and 155,000. We will see later on in
the course what exactly this statement means. The width of the condence interval around the estimate
gives a measure of the uncertainty inherent in extrapolating from a small sub-population to the entire
country. Since this is a politically and emotionally charged subject, the study triggered an intense debate
in scientic publications and the blogosphere - reading up on the debate will teach you a lot about how
statistics is actually done in practice.
2 Set Theory and Events
2.1 Random Experiments
Denition 1 Arandom experiment is any procedure which can - at least theoretically - (a) be repeated
arbitrarily often and under identical conditions, and (b) has a well-dened set of possible outcomes.
A standard example for this would be a coin ip which can produce two dierent outcomes, heads H
or tails T(well neglect the possibility that the coin ends up standing upright on its edge). Another
important type of experiments in the realm of statistics would be an exit poll after an election: say we
ask 2,000 randomly selected voters as they exit the poll stations to report which candidate they voted
for. We could in principle redraw arbitrarily many further samples of 2,000 individuals from the total
population that turned out to vote on election day, so condition (a) is satised. The set of outcomes
for this experiment is the respective number of interviewed persons which reports to have voted for each
candidate on the ballot.
Denition 2 Thesample space Sis the collection of all possible outcomes of an experiment.
For many purposes, we are not primarily interested in single outcomes, but instead group collections
of outcomes together into events . Therefore we will in the following describe the experiment in terms of
sets.
Denition 3 Anevent Acan be any collection of outcomes (this includes individual outcomes, the empty
set, or the entire sample space).
If the realized outcome is a member of the event A, then Ais said to occur.
Lets look at last years presidential race as an example. At the most elementary level, we could
describe the sample space Sas the collection of individuals who may - as a logical possibility - be elected
2</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>and set inclusion is transitive, i.e.
ABandBC=AC
In our presidential elections example, since (if we can trust Wikipedia, McCain was born on an American
air base in the Panama Canal zone)
The 44th President was born in Panama = {McCain }
The 44th President was born abroad = {McCain, Schwarzenegger }
and
{McCain,Schwarzenegger } The 44th President is a Republican
we can conclude that
The 44th President was born in Panama The 44th President is a Republican
2.2.2 Unions of Sets,  
Theunion ofAandBis the collection of all outcomes that are members of AorB(or both, this is the
logical, inclusive or corresponding to the symbol ). In symbols
AB={sS|sAsB}
The set union is symmetric:
S
A B
A   B
Figure 2: Union of AandB-AorB
AB=BA
Furthermore,
BA=AB=Afor any events A, BS
In particular,
A =A
AA=A
AS=S
It also doesnt matter in which order we take union of sets/events (associative property):
ABC= (AB)C=A(BC)
4Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>ABA   B
AcBc
S
Figure5:Illustrationof(AB)C=ACBC-youcanseetherule(AB)C=ACBCfromthesame
graph.
2.2.5PartitionsofEvents
AandBaredisjoint(ormutuallyexclusive)iftheyhavenooutcomesincommon,i.e.
AB=
AcollectionofeventsA1,A2,...issaidtobeexhaustiveiftheirunionequalsS,i.e.
/uniondisplay
Ai=A1A2...=S
i1
AcollectionofeventsA1,A2,...iscalledapartitionofthesamplespaceif(1)anytwodistinctevents
Ai,Aj(withi=j),AiandAjaredisjointAiAj=,and(2)thecollectionA1,A2,...isexhaustive.
InasimilarfashionwecandenepartitionsofaneventBascollectionsofmutuallyexclusivesubevents
whoseunionequalsB.
/negationslash
A1
A2 A6 A4 A7A3 A5 A8S
Figure6:PartitionofSintoA1,A2,...,A8 
7Image by MIT OpenCourseWare.
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>2.2.3 Intersections of Sets,  
Theintersection ofAandBis the (possibly empty) collection of outcomes that are members of both A
andB, written as
AB={sS|sAsB}
where   denotes the logical and. Some texts use the alternative notation AB=AB. As the set
A   BS
A
B
Figure3:IntersectionofAandB-AandB
union,theintersectionissymmetric,
AB=BA
Also
BA=AB=BforanyeventsA,BS
Fromthisitfollowsthat
A=
AA=A
AS=A
Again,likethesetunion,theintersectionofsetshastheassociativeproperty
ABC=A(BC)=(AB)C
Inaddition,setintersectionandunionhavethedistributiveproperties
A(BC)=(AB)(AC)
and
A(BC)=(AB)(AC)
Asanexample,fortheevents
A=President44isaWoman={Clinton}
B=President44wasborninMidwest={Clinton,Romney}
C=President44isaRepublican={Huckabee,McCain,Paul,Romney,Schwarzenegger}
5Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Discrete and continuous random variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>    Dividing by the length of the interval [xi1,xi) makes sure that the area under the graph for a given 
interval equals the probability of the random variable X taking a value in that interval, i.e. we can 
calculate 
k k  xi  xk 
P (xj  X  xk)= P (xi1  X  xi)= hn(t)dt = hn(t)dt for xj &lt;xk 
i=j+1 i=j+1 xi1 xj 0 .5 1 1.5 2 Density 
.5 1 1.5 2 2.5 
u 
0 .5 1 1.5 2 Density 
.5 1 1.5 2 2.5 
u 
Figure 1: Histograms of the same Distribution for 10 and 30 Bins, respectively 0 .5 1 1.5 2 Density 
.5 1 1.5 2 2.5 
u 
Figure 2: Histogram with 60 Bins and Continuous Density 
Thisisnot completely satisfyingyet, sincethisonly allows ustocalculatetheprobability of X falling 
betweenany twoof thegridpointsfrom x0 &lt;x1 &lt; ... &lt; xn,butnote.g. of asubinterval of,say,[xj ,xk]. 
We can address this by making the grid of points x1,x2,... ner, and therefore the intervals narrower. 
If we shrink the distance between neighboring points xi1,xi to an arbitrarily small dx, well have a 
function h(x)for which the probability that X liesbetween any twopoints a and b can be expressed 
5 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
2 at the same value as before. Then, doing the same steps as before, this time based on the rst posterior 
P (H|B)rather than the prior P (B), the conditional probability would be 
10 48 12 
510  46.21% 8 46.21% P (H|B C)=     = 510  85.51% 10 4812 10 1 46.21%+53.79% 48210 
8 510  46.21%+ 8 210  53.79% 
Alternatively,if wejust aggregate the two series oftrialsinto16heads out of20 trials, weget 
  
20 416 
16 520 
P (A|B)=      85.51% 20 20 416 8+ 16 520 16 220 
so that it doesnt matter whether we update simultaneously or sequentially. This is more generally a 
desirable property of Bayesian updating: the nal result only depends on the overall information we are 
using, not the order in which we update. 
Continuous Random Variables 
Many types of data are results from measurements of some kind: weights, lengths, incomes etc., which 
can -at least conceptually -take any value in some interval (sometimes all) of the real numbers. In 
this case, the denition of a probability density function for discrete random variables is not practical, 
because(a) the number ofpossible outcomesis uncountable, so we cantjust add upprobabilities over 
single values, and(b) theprobability of any particular value on the continuum typically has to be zero. 
This is why we have to deal with this type of random variables separately from the discrete case. 
Denition4 A random variable X has a continuousdistribution if X can take on any values in some 
interval -bounded or unbounded -of the real line. 
For discrete random variables, it was relatively straightforward to dene a probability density function, 
since there was only a nite number of values. A continuous random variable can take more than count
ably many values, and therefore the derivation becomes a little bit more involved. 
The idea goes as follows: we can discretize the distribution by putting the possible values the random 
variable can take into bins, i.e. instead of looking at the probabilities P (X = x), well lookat proba
bilitiesforintervals,i.e. P (x1  X  x2). The graphical representation of this is a histogram: we x a 
set of points x0 &lt;x1 &lt; ... &lt; xn on the real line and calculate the probabilities for X falling into each 
bin, i.e. interval between two subsequent points, so that P (xi1  X  xi). Then for values in the 
interval[x0,xn], we can dene a function 
 
 P (x0Xx1) if x  [x0,x1)  x1x0   .   .  .  
hn(x)= P (xi1Xxi) if x  [xi1,xi)xixi1    .  .  .    P (xn1Xxn) if x  [xn1,xn)xnxn1 
4 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>  
  
  
    
    
  However, since we are only interested in the number X of successes, we have to take into account that 
n there are dierent sequences with x successes. Therefore, the probability of x successesis x 
P (X = x)= np x(1p)nx 
x 
Denition3 A random variable X withprobabilitydensityfunction 
 
 n px(1p)nx if x {1, 2,...,n}fX (x)= P (X = x)= x  0 otherwise 
is said to follow a binomialdistribution withparameters p and n, written as 
X  B(n,p) 
You should notice that previously, we derived the probability distribution for every random experiment 
separately, writing down the number of possible outcomes, outcomes in each event etc. The binomial 
distribution serves as a model for a whole range of random experiments. For any given example which 
fallsintothis category, we canjustlook up theprobabilitiesfor agiven set ofparameters(n,p). 
Example4 In order to make some money o your classmates, you obtained a bent 25 cent coin that 
comes up heads with a probability of pL = 54 . Unfortunately, that coin got mixed up with your regular 
small coins, and only after you inserted 8 out of 9 quarters into the laundromat you notice your mistake. 
Youhastily tossthe coin10 times, andit comes up headsfor atotal of8 times. Woulditbe agoodideato 
continueto rip oyourfriends withthe old cointrick or areyou now stuck with a regular(fair) quarter 
with pF = 1
2 ? I.e. what is P (A|B)for A =remaining coin is bent and B =8 heads out of 10? 
If the coin is fair, 
P (B|AC )= 10 pF 8 (1pF )108 =10 1 
8 8210 
Ifitisbent, 
P (B|A)= 10 pL8 (1pL)108 = 10 48  12 
8 8 510 
Now lets see what Bayes theorem has to say: 
10 48 12 1 
P (B|A)P (A) 8 510 9 48 
P (A|B)= =     = 48 510  46.21% 8 P (B|A)P (A)+P (B|AC )P (AC ) 10 4812 1 + 10 18 510 + 210 
8 510 9 8 210 9 
Therefore, it is more likely that the coin you are left with is in fact a regular quarter -which doesnt mean 
that in total, the probability of heads is still 
4 1 P (H|B)=  46.21%+  53.79%  63.86% 5 2 
However a better idea would of course be to do a few more tosses. If you can repeat the experiment 
arbitrarily often, you will eventually be able to distinguish the two types of coins with an arbitrary degree 
of certainty. As an aside, you can see this exercise as a very basic example for a hypothesis test. 
Sayyougot another8heads out of another10 trials(well call this event C), leaving the share of heads 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>1 
K (X = X) 
2 3 --- K-1 K X 1 
Note that these probabilities sum to 1. 
P(X = X) 
10 
32 
5 
32 
1 
32 
0 1 2 3 4 S X 
Note that in the die roll example, every single outcome corresponded to exactly one value of the 
random variable. In contrast for the ve coin tosses there was a big dierence in the number of outcomes 
corresponding to X = 2 compared to X = 0, say. So mapping outcomes into realizations of a random 
variable may lead to highly skewed distributions even though the underlying outcomes of the random 
experiment may all be equally likely. 
1.1 The Binomial Distribution 
Togeneralizethepreceding example,supposewelook at asequenceof n independentandidentical trials, 
each of which canresultineithera success ora failure (notnecessarilywith equalprobability),and 
we are interested in the total number X of successes. 
Example3 Suppose we sample 100 pieces from a batch of car parts at the producing plant for quality 
control. A piece passing the quality checks would constitute a success, a piece falling short of one or 
more of the criteria would be a failure. We are interested in the distribution of failures as a function 
of the total share of defective parts in the batch in order to infer from the sample whether we have good 
reason to believe that no more than, say, 1% of the pieces dont meet the standards. 
Suppose the probability of a success equals p, and the probability of a failure is therefore q =1p. 
Since the trials are independent by assumption, the probability of any given sequence of x successes and 
n x failures in xed order is 
p x(1p)nx 
2 Images by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>   astheintegral of h(x)from a to b. Thislimitis called theprobability density functionforacontinuous 
random variable: 
Denition5 If random variable X has a continuous distribution, the probability density function 
(p.d.f.) of X is dened a positive function fX (x)such that for any interval A  R 
P (X  A)= fX (t)dt 
A 
X F(x) 
P(a x b) 
a b 
Figure 3: P.D.F. of a Continuous Random  Image by MIT OpenCourseWare.
Variable
From the axioms for a probability function, we can see that any continuous p.d.f. must satisfy 
fX (x) 0 x  R 
and   
fX (x)=1 
 
Hence, if we want to know P (X  A)for A =[a,b] R, we can compute 
 b 
P (X  [a,b])= P (a  X  b)= f(t)dt 
a 
Remark1 Note that for any x  R, 
P (X = x)=0 
if X has a continuous distribution. 
This may seem a little counterintuitive in part also because we use continuous distributions to ap
proximatethings which areinfactdiscrete(e.g. income ortime unemployed). Sofarwehavent seen 
any examples of continuous random variables in any of the probabilities we computed. 
Examples 
Supposethatarandom variableissuchthaton someinterval[a,b] on the real axis, the probability of 
X belonging to some subinterval [a,b](where a  a  b  b) is proportional to the length of that 
subinterval. 
6 3 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text> 
 
  1
fX (x)= ba 
0 otherwise 
F(x) 
x y 
a 1 
b-a 
b 
Figure 4: p.d.f for a Uniform Random Variable, X  [a,b] 
 4  4
1 1
10 10
3 3
ax2 if0 &lt;x&lt; 3
0 otherwise 
a 0 =9a Denition6 A random variable X is uniformly distributed on the interval [a,b], a&lt;b, if it has the 
probabilitydensityfunction 
if a  x  b 
In symbols, we then write 
X  U[a,b] 
For example, if X  U[0, 10],then 
P (3&lt;X&lt; 4) = f(t)dt = dt = 
Whatis P (3  X  4)? Since the probability that P (X =3) =0 = P (X = 4), this is the same as 
P (3&lt;X&lt; 4). 
Example5 Suppose X hasp.d.f. 
fX (x)= 
Whatdoes a have to be? -since P (X  R)=1, the density has to integrate to 1, so a must be such that 
   3  3 3 
1= fX (t)dt = at2dt = ax 27 = 3 3
 0 0 
Therefore, a = 91 .
Whatis P (1&lt;X&lt; 2)? -lets calculate the integral
 2
t2 23 13 7
P (1&lt;X&lt; 2) = dt =  = 9 9 39 3 27
1
Whatis P (1&lt;X)? 
 3 t2 271 26
P (1&lt;X)= fX (t)dt = dt = = 9 27 27
1 1
7 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> 
 
 
  
    
    
    1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 5 
Konrad Menzel 
February 19, 2009 
We distinguish 2 dierent types of random variables: discrete and continuous. 
Discrete Random Variables 
Denition1 Arandomvariable X hasa discrete distributionif X cantakeononly a nite(orcountably 
innite) number of values (x1,x2,...). 
Denition2 If random variable X has a discrete distribution,the probability density function (p.d.f.) 
of X is dened as the function 
fX (x)= P (X = x) 
If {x1,x2,...}is the set of all possible values of X, then for any x/{x1,x2,...}, fX (x)= 0. Also 
 
fX (xi)=1 
i=1 
Theprobability that X  A for A  R is 
P (X  A)= fX (xi) 
xiA 
Example1 If X is the number we rolled with a die, all integers 1, 2,..., 6 are equally likely. More 
generally, we can dene the discrete uniform distribution over the numbers x1,x2,...,xk byitsp.d.f. 
1 
fX (x)= k if x {x1,x2,...,xk} 
0 otherwise 
This corresponds to the simple probabilities for an experiment with sample space S = {x1,x2,...,xk}. 
Example2 Suppose we toss 5 fair coins independently from another and dene a random variable X 
which is equal to the observed number of heads. Then by our counting rules, n(S) =25 = 32, and 
5 n(k heads)= , using the rule on combinations. Therefore k 
51 1 51 5 P (X =0) = = , P (X =1) = = 032 32 132 32, 
5110 5110 P (X =2) = = , P (X =3) = = 232 32 332 32, 
51 5 51 1 P (X =4) = = , and P (X =5) = = . 432 32 532 32 
1 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Law of large numbers (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>9</slideno>
          <text>  
  Again,sincetherolesofthecandidatescanbeexchangedforthenoisevoters,thedistributionissymmetric 
around the mean, and we can use the bound on the probabilities derived above 
1 Var( Xn) PXn &lt; 2 2 
 However, now we can see that Var( Xn) doesnt go to zero anymore as n  since the rst term 
 Var(E[Xn|y]) doesnt depend on n at all.
In numbers, if  =15%(sixtimeslargerthanintheprevious calculation),thebound equals
1 49 560 PXn &lt;  + 2 81 81n 
so that the bound is far above 1
2 no matter how large n is. Note that since this is only an upper bound, 
this doesnt really tell us how likely the event really is, but it is clear that since the variance doesnt 
decrease to zero, the noise voters will always have a strong inuence on the election result. 
HeretheLawofLargenumbersfailsbecausetheyincidentaectsall noisevotersatthesametime, 
so X1,...,Xn are no longer independent. The independence assumption is important because the reason 
why the law of large numbers usually works is that the noise averages out across many observations. 
If onecomponentofthe noise iscommonto(oratleasthighly correlated across) all observations,the 
variance contribution of this component -in our example the 49 term in the bound on the probability 81 
does not disappear even if the sample is very large. 
9 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>  
  
    
    Now note that since noise voters dont favor either candidate, the distribution is symmetric around , so 
that 
  P(Xn &gt;)= P(Xn &lt; ) 
Since  = 1
2 + , the probability of candidate B losing can be obtained as 
Var( Xn) 22P(Xn &lt; ) PXn &lt; 1 Var( Xn)12  = 2 2 4n2 
Lets try a few values: Say, 2 =5%,howlargedoes n have to be in order to keep the probability of 
electing candidate B below 5%? -The bound becomes 
1 90% 90 PXn &lt;  = 2 4(5%)2nn 
so n  1800  95 is sucient to keep the probability of electing for the wrong candidate below 5% even 19 
if 95% of the electorate take their decision at random.
Thisphenomenonisknownasthe wisdomof thecrowds: thestochastic noise intheelectionoutcome
introduced by the uninformed voters averages out in large samples, and in the end only the systematic
signal from the informed voters decides the outcome of the election.
Notice that, like in the Law of Large Numbers, we assumed that individuals votes were independent. 
What happens if we drop independence? 
Suppose that during the televised debate between the candidates, there is a y in the TV studio which 
randomlylandsoneitherA orBsface(with equalprobability 1
2 )and spendssometimecrawling around, 
causing a lot of embarrassment to that candidate. The informed voters dont change their behavior, but 
the uninformed voters vote for the candidate with the y with probability 31 , and with probability 2
3 for 
his opponent. 
 By the law of iterated expectations, the mean of Xn is 
1 1 + E[Xn|y lands on B]  = E[Xn|y lands on A] 2 2 
1 1 1 2 1 = 2+(1 2) + 2+(1 2) = +  2 3 2 3 2 
sothe meanisthe same asbefore. However,the variancedoes change: by theANOVAidentity(condi
tional variance) 
Var( Xn) = Var E[Xn|y] + E Var( Xn|y) 
We can calculate 
 2(12)Var( Xn|y on A) = Var( Xn|y onB) = 9n 
114E[Xn|y onA] = 2+(1 2) = +  333 
222E[Xn|y onB] = 2+(12) = +  333 
sothat 
2 
11 2(12)Var( Xn)=   + 63 9n 
8 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>We say that Xn convergesinprobability to . 
Proof: We use our previous result that 
1 2 Var( Xn)= Var(Xi)= n n 
By ChebyshevsInequality 
  Var( Xn) 2 nP |Xn |&gt;  =  0 2 n2 
This statement says that for large samples, the sample mean is unlikely to be far away from the 
expectationoftherandomvariable. Foragivenn and agivenvariance 2,wecandirectly useChebyshevs 
Inequality to bound the probability that the sample mean is more than a given distance away from . Xbar_n
.5 0 .5 1 1.5
0 50 100 150 200 
n 
Figure 1: 10 sequences of average number of heads in n coin tosses -dashed lines are 1
2n 
Example3 Standardization of Units of Measurement (see Stiglers book): in the middle ages, often 
every city was using a dierent measure of foot, inch, yard, etc. depending on how long the rulers 
corresponding limb was. This means that there is a lot of variation in the units, which complicates trade 
and gives rise to many legal disputes, say whether a given bundle of cloth was really 20 yards long. 
One clever idea people came up with was the following: to determine the length of a rod of 16 feet, you 
takearandomsampleof16individuals(inthiscasetherulewasthe rst16peopletoexitthechurch 
on Sunday morning), and add up the length of their feet to obtain a measurement of 16 feet, then divide 
that length by 16. See Figure 2. According to the formula for the variance of an average for 16 
observations, this should decrease the variance of the new measurement unit by 1 
16 . 
Ifthere were no systematicdierencesinfoot sizes(orthe church-goingpopulation) acrossdierent 
places, this should make it much easier for merchants from dierent places to trade with each other. 
2.1.3 Example: The Wisdom of Crowds 
Suppose that a population of size n chooses among 2 candidates for public oce, where the candidate 
with a simple majority of the vote wins. Well look at the random variable Xi which equals 1 if voter i 
6 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 15 
Konrad Menzel 
April 7, 2009 
1 Special Distributions (continued) 
1.1 The Poisson Distribution 
Often, we may be interested in how often a certain event occurs in a given interval. 
Example1 In airline safety, we may want to have some notion of how safe an airplane model is. The 
following data are from the website www.airsafe.com, and give the total number of ights and number of 
fatal events involving aircraft of a given type up to December 2006. 
Model Flights Events 
Airbus A300 10.35M 9 
Airbus A310 3.94M 6 
Airbus A320/319/321 30.08M 7 
Airbus A340 1.49M 0 
Boeing 727 76.40M 48 
Boeing 737 127.35M 64 
Boeing 747 17.39M 28 
Boeing 757 16.67M 7 
Boeing 767 13.33M 6 
Boeing 777 2.0M 0 
Boeing DC9 61.69M 43 
Boeing DC10 8.75M 15 
Boeing MD11 1.69M 3 
Boeing MD80/MD90 37.27M 14 
Concorde 0.09M 1 
Wecanseeimmediately that sometypesof aircrafthavehadfeweraccidentsthanotherssimplybecause 
they havent been in service for long or were only produced in small numbers. In order to be able to make 
a more meaningful comparison, we need a better way of describing the distribution of the number of fatal 
events. 
Random variables of this type are commonly referred to as countdata, and an often used distribution to 
describethemisthe Poisson distribution 
1 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>  
2 surprisingly good approximation to the observed frequencies of horsekick deaths over the course of a year 
in a given corps of the Prussian army. 
So how do we compare the observed frequencies to a Poisson p.d.f. which, after all, depends on the 
unknown arrival rate ? As a preview to our upcoming discussion of estimation later in this class, 
a plausible candidate for  would be a value of the parameter in the p.d.f. which predicts the same 
expected number of horsekick deaths that we observe in the sample. So for X  P(), what is E[X]? 
Well, as we argued above, a Poisson random variable is the limit of Binomial random variables Xn  
B n,  , where we let the number of trials n go to innity. By our previous discussion of the Binomial n 
distribution, E[Xn]= n n = , regardless of n. Hence, without directly evaluating the innite series 
E[X]=  
x=0 x x 
x!  , we can say that E[X]= . In the data set on horsekick deaths, the sample mean  e 
0 .1 .2 .3 .4 .5 
0 1 2 3 4 5 6 
Source: Andrews, D.F. (1985): Data: a Collection of Problems from Many Fields, Springer by corps and year, 18751894 Deaths from Horsekicks in the Prussian Army 
Frequency Poisson Distribution 
(by year andcorps) is =0.7, and we can now plot the sample frequencies against the theoretical values 
of the Poisson p.d.f. for an arrival rate of  =0.7 in gure 2. The two distributions look strikingly 
similar, and this fact is often referred to as the Law of Small Numbers. 
Asymptotic Theory 
Until now, weve assumed that we know the p.d.f. (or that we can nd it), know parameters (such 
as  and 2 for normal,  for exponential etc.), and then make probability statements based on that 
knowledge. 
In the next part of the class, we wont pretend that we have that information, but well construct 
functions of random variables -which are going to be the estimators -which, along with our knowledge 
of probability, will allow us to infer something about thee underlying distribution. 
A particular estimator which plays an important rule in statistics is the sample mean, which typically 
approximates the expectation of a random variable in a sense well discuss in a few minutes. 
Denition2 A random sample of size n is a sequence X1,...,Xn of n random variables which are 
i.i.d., i.e. the Xis are independent and have same p.d.f. fX(x). 
We also often refer to the realizations of the random variables as the random sample.
The main point of this lecture is going to be that if we have a random sample with n large, we actually
dont need to know a lot about the distribution of Xi (e.g. dont need to know fXi(x)in order to be able
to describe the distribution of the sample mean fairly accurately.
3 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text> 
  h! 
Figure2: Woodcutby Kobel(1535) of16peopledetermining thelegaldenition of a rod of16feet. 
favors candidate A, and 0 otherwise. Candidate A wins if his vote share 
n 1 1Xn =  n 2 i=1 
Candidate A is objectively the better choice, but this is only known to a proportion 2&gt; 0 who 
will vote for candidate A for sure, i.e. P(Xi =1) =1for i =1,..., 2n. The remainder 12 of the 
population doesnt have any substantial information about either candidates and casts votes at random 
with no preference for either candidate, i.e. P(Xi =1) = P(Xi =0) = 21 for i =2n,...,n. The vote 
 share Xn for candidate A is given by 
n n1 1Xn = Xi = + Xi n n i=1 i=2n 
Therefore, the expected vote share for candidate A is 
1 1  =2+ (12)= +  2 2 
and the variance is, by results for the binomial distribution, 
1  1  
1 2 =(12)2 2 =12 
n 4n 
 By the same argument in the proof of Chebyshevs Inequality, we can start from the variance of Xn 
in order to derive bounds on the probabilities 
   +   
Var( Xn) = (t )2dt+ (t )2dt+ (t )2dt 
  + 
 2  
P(Xn &gt;)+P(Xn &lt; )  
7 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>  
    Denition1 X is said to follow Poisson distribution with arrival rate , X  P(),ifithasthep.d.f. 
  
fX(x)=	x 
xe 
! if x {0,1,2,...} 
0 otherwise 
Notethatinparticular, X isdiscrete. 
Property1 For a Poisson random variable X, 
E[X]=  
Var(X)=  
In order to see why the Poisson distribution is a plausible candidate for the distribution of a count 
variable, lets do the following thought experiment: Suppose 
 the probability that the event happens in a time window of length n 1 is pn = n . 
 we also assume that the event happening at any given instant is independent across time. 
We then let the partition of subintervals grow ner by letting n go to innity. If the probability of two 
occurrencesinthesameshrinking subintervalgoestozero,and wethen countthenumber of subintervals 
in which the event occurs at least once we get the total number of occurrences. Notice that this will be 
a binomial random variable with parameters p = n and n. 
Proposition1 For the binomial random variable Xn  B n, n , as n , the p.d.f. converges to 
n  x  nx ex 
lim fXn (x)=lim 1 = 
n n x n n x! 
Proof: Wecantakethelimitof theproductastheproduct oflimits, and evaluateeach termseparately: 
By awell-knownresultfromcalculus(e.g. candoTaylorexpansionsonboth sides), 
n lim 1 = e 
n n 
So we are left with the term 
 x  x x n   n!  Tn = 1 = x n n x!(n x)! n  
for which we can show 
n(n 1)... (n k +1)x x n n 1 n x +1 x 
lim Tn =lim	 = lim  ... =  1 
n n x!(n )x x! n n n  n  x! 
sinceboth x and  are xed and therefore become small compared to n. Putting thepiecestogether,we 
get the expression stated in the proposition  
Example2 A classic example (classic in the history of statistics at least) for count data are records 
on the number of soldiers in the 19th century Prussian cavalry that died after being kicked by a horse. 
As found by the Russian statistician Ladislaus Bortkiewicz in 1898, the Poisson distribution gives a 
2 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
    
    
  
  2 
2 
2 2 
E[x] - E[x] +  E[x] 
2.1.2 Law of Large Numbers 
Denition3 The sample mean is the arithmetic average of n random variables (or realizations) from 
a random sample of size n. We denote it 
n 1 1Xn = (X1 + ... + Xn)= Xi n n i=1 
 Note that since the Xis are random variables, Xn is also a random variable. 
The expectation of the sample mean is 
n n 1 1E[Xn]= E Xi = E[Xi]= E[X1] n n i=1 i=1 
If X1,...,Xn are independent, the variance of the sample mean can be calculated as 
n n  1 1 1Var Xn =Var Xi = Var(Xi)= Var(X1)2 n n n i=1 i=1 
Whatif the Xisarei.i.d. normal, Xi  N(,2)?Weknowthatalinearcombinationofnormalsisagain 
normal with the appropriate variance and mean, so 
1 2Xn  N , n 
Since the variance decreases as we increase n, the mean will eventually be very close to E[Xi]with a 
very high probability. This is essentially what the Law of Large Numbers says: 
Theorem1 (Law of Large Numbers) Suppose X1,...,Xn isasequenceofi.i.d. drawswith E[Xi]=  
and Var(Xi)= 2 for all i. Then for any &gt; 0 (typicallya small value), the sample mean satises 
 lim P |Xn |&gt; =0 
n 
5 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text> The basic idea is that well nd approximations to the p.d.f. which get closer and closer to the truth 
as we increase the size n of the sample. The two main results are: 
1. the Law of Large Numbers: for large n, the sample mean will with all likelihood be close to the 
expectation E[X]of the random variable. 
2. the Central Limit Theorem: the p.d.f. of the standardized sample mean (standardized in the 
senseof thelastlecture: zeromeanand unitvariance) willbecomearbitrarily closetothep.d.f. of 
a standard normal random variable. 
Formally, the asymptotic results state what will happen as n ,butforpracticalpurposes(i.e. for 
nite n), theyalso imply that for n large enough, the approximations will be reasonably accurate. 
2.1 The Law of Large Numbers 
2.1.1 Chebyshevs Inequality 
Chebyshevs Inequality is a formal result which gives a bound on the probability of the realization of a 
random variable being far away from the expectation. 
Proposition2 Let X be a random variable with Var(X)&lt; . Then for any &gt; 0, 
Var(X)P (|X E[X]|) 2 
Proof: Let the p.d.f. of X begivenby fX(x). Well show that 
Var(X) 2P (|X E[X]|) 
By the denition of the variance, 
 
Var(X) = (t E[X])2fX(t)dt 
  E[X]+  E[X]   
= (t E[X])2fX(t)dt+ (t E[X])2fX(t)dt+ (t E[X])2fX(t)dt 
E[X]  E[X]+ 
Each of the three integrals is positive, and in addition, for any t  E[X] or t  E[X]+ 
(t E[X])2  2 
Therefore,wecanjustdrop the rstintegral andget 
 E[X]   
Var(X)  (t E[X])2fX(t)dt+ (t E[X])2fX(t)dt 
 E[X]+ 
 E[X]+   
 2 fX(t)dt+ 2 fX(t)dt 
 E[X]+ 
= 2P (|X E[X]|) 
Therefore, we canjustdividethroughby 2 to get the result  
Remember that we said at an earlier point in this class that the variance of a random variable is a 
measure of its dispersion. Chebyshevs Inequality makes this statement literal by relating the variance 
totheprobability of observing extreme realizations(i.e. valuesthatarefarawayfromthemean) of 
the random variable X. 
4 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Order statistics and expectations (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>11</slideno>
          <text>dividends it pays at any instant in time t develop according to e0.1T , i.e. there is a random growth rate
G1 which takes values 0% withprobability 90%, and 10% withprobability 10%, respectively. Alternatively,
you can hold a government bond which will e0.02t interest at every instant t in the future, i.e. G2 =2%
for sure.
You value one dollar you receive t periods from now as much as e0.15t dollars you can have right now,
but invest in one of the two assets anyway. I.e. in general you value an asset whose returns grow at rate
g for sure as
  
V (g)= e(gr)tdt =01 =1 
0 g rr g 
The expected growth rate of dividends for the risky stock is 
E[G1]=0.90%+0.110% =1% &lt; 2% = E[G2]   
However, your valuation for the stock is 
1 1 90 10 40 E[V (G1)]=0.9 +0.1 = + = =8 0.150 0.150.115 5 5 
whereas you value the bond as 
1 100 200 200 E[V (G2)]= = = &lt; = E[V (G1)] 0.150.02 13 26 25 
Intuitively, uncertainty over the growth rates means that even though 90% of the start-ups dont develop 
(or even fail), the 10% which are successful do so spectacularly as to compensate for the investments 
which turn bad. Formally, the function V (g) is convex in growth rates, so that by Jensens inequality 
investors should value risk in growth rates -though typically not in levels. 
11 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text> 
 Themedianand theexpected valueof arandomvariable X coincideif thedistributionof X issymmetric 
around m(X),i.e. fX (m(X)x)= fX (m(x)+x), but neednot be the same in general. 
Example4 Say X hasp.d.f. 
12 
fX (x)=	9 x if0 x 3 
0 otherwise 
The expected value is 
 3  3  31 1 1 4 81 9 t t2dt = t3dt = t= = =2.25 
0  9 9 0 36 0 36 4 
In order to obtain the median, lets rst calculate the c.d.f. of X 
 x x 31 2 1 3 x FX (x)= tdt = t= 9 27 27 0	 0 
Therefore, solving FX (m)= 21 for m gives 
27	 33 m = 2 m = 
2 2.38 &gt; 2.25 3
Therefore, the median of this distribution is greater than the mean. 
Note that the median may not be unique: 
Example5 Let X be the result of rolling a fair die once. Then for any number m (3, 4], P (X&lt;m)= 
P (X 3) = 1 . Therefore any value in that interval is a median. 2 
3.2 Properties of Expectations 
Property1 If X = c, where c is a constant, then 
E[X]= c 
Property2 If Y = aX + b, then 
E[Y ]= aE[X]+b 
Proof: Lets only look at the continuous case: if X is a continuous random variable with p.d.f. fX (x), 
then the expectation of Y isgivenby 
 	    
E[Y ]= (ax + b)fX (x)dx = a xfX (x)dx + b fX (x)dx = aE[X]+b 1 
	   
so as we can see, the linearity of integrals translates directly to linearity of expectations 
Property3 For Y = a1X1 + a2X2 + ... + anXn + b, 
E[Y ]= a1E[X1]+a2E[X2]+... + anE[Xn]+b 
This is the most general statement of the linearity of expectations, and we will use this property over 
and over in the remainder of this class. 
6 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
 
 
 wont go beyond this example. 
3 Expected Values and Median 
GivenarandomvariableX with ap.d.f. fX (x),wedliketosummarizethemostimportantcharacteristics 
of the entiredistributions withouthaving togivethe entiredensity function. The expected valuetells us 
basically where the distribution of X is centered. 
3.1 Denitions 
Denition1 If X is a discrete random variable, the expected value of X,denoted E[X]isgivenby 
E[X]:= xfX (x) 
x 
if this sum is nite. If X is continuous, the expected value is dened as 
  
E[X]= xfX (x)dx 
 
if the integral is nite. 
Example3 What is the expectation of a binomial random variable, i.e. X B(n,p)? 
n  
E[X]= xnp x(1p)nx 
x 
x=0 
n 
=  
x!(nn!
x
x)!p x(1p)nx 
x=1 
n (n 1)! x1(1p)(n1)(x1) = np p(x 1)!((n 1)(x 1))!x=1 
n1   
n 1 = np x 1 p x(1p)n1x 
x=0 
= np 1 
where in the second row, we can ignore the summand corresponding to x =0 since its zero, in the third 
row, we pull n out of the binomial coecient, and in the following step, we switch the summation index 
fromx to x1. Finally,if wepull np out,thesummandsarethebinomialprobabilitiesfor XB(n1,p), 
and therefore they sum to one. 
Note that a random variable which takes innitely many dierent value may not have a nite ex
pectation, in which case the expectation is not dened. You should also notice that even though the 
expectationgivesasenseof the location of adistribution,itisingeneral nota typical valueforthe 
random variable: e.g. the expected value of a die roll is 1
6 (1+2+3 +4 +5+6) =321 , which is not a 
possible outcome. 
An alternative measure of the position of a distribution is the median: 
Denition2 The median m(X)of a random variable X is a real number such that 
1 P (X&lt;m)= 2 
5 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>For linear functions of a random variable X ofthetype Y = aX +b, we saw above that E[aX +b]= 
aE[X]+ b. This doesnt work for nonlinear functions of a random variable. A particulary important 
result on this is JensensInequality : 
u(X) 
x 
X2 X1 E(X) E[u(X)] 
u(E[X]) 
Figure 3: Example with a discrete distribution   Image by MIT OpenCourseWare.
over{x1,x2}
Proposition2 (Jensens Inequality) Let X be a random variable, and u(x) be a convex function. 
Then 
E[u(X)]u (E[X]) 
The inequality is strict if u() is strictly convex and X takes at least two dierent values with positive 
probability. 
Proof: We can dene a linear function 
r(x)= u(E[X])+u (E[X])(x E[X]) 
whichistangentto u(x)whichpassesthroughthepoint(E[X],u(E[X])). Since u()is convex, 
u(x)r(x) for all x 
Inparticular,     
E[u(X)]= u(t)fX (t)dt  r(t)fX (t)dt = E[r(X)] 
  
Since r(x) is linear by construction, we can use property 2 on expectations of linear functions with 
a = u(E[X])and b = u(E[X])u(E[X])E[X]to obtain 
E[r(X)]= E[aX + b]= aE[X]+b = u (E[X])E[X]+u(E[X])u (E[X])E[X]= u(E[X]) 
Putting this together with the inequality derived before, 
E[u(X)]E[r(X)]= u(E[X]) 
which completes the proof  
Notice that, since for a concave function v(x), its negativev(x)is convex, Jensens Inequality also 
implies that for a concave function v(), 
E[v(X)]v(E[X]) 
8 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>0 .2 .4 .6 
1 2 3 4 5 6 7 8 9 
uniform density old units times 2 
old units times 4 
Figure 1: Eect of a Change of Measurement Units on the Uniform 
This invariance idea may seem like a very articial way of obtaining a distribution, since there is no 
obvious connection to the measurements X nor the measurement units. However, the resulting p.d.f. 
seems to give a good approximation to real-world data which falls into the category -as e.g. numbers 
representing measurements of some kind which appear in the New York Times. The following graph 
shows the theoretical density together with a histogram of the rst digit of GDP measured in local 
currency units(i.e. YenforJapan,C$forCanada,etc.) forthe77 countriesincludedin WorldinFigures 
2007 pocket book from The Economist. Summarizing, this example took a radically dierent approach 0 .1 .2 .3 
1 2 3 4 5 6 7 8 9 
gdp density 
Figure2: Distribution ofFirstDigit ofGDPinLocalCurrency Units andTheoreticalDensity(Numbers 
from The Economist, Pocket World in Figures 2007) 
to determining the distribution of a given function of two random variables X and Y : here we started 
out not knowing the p.d.f.s of X and Y , but then imposed that whatever the resulting distribution was 
going to be, it had to be invariant with respect to a change of units -i.e. the realization of Y . The 
concept of invariance plays a major role in advanced statistics, but for the purpose of this class, we 
4 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>     
  
 
 
 
  Example6 Above, we calculated the expectation of X B(n,p), E[X]= np by summing over the 
possible outcomes of X. But from the last result we can see that there is an easier way of obtaining the 
same result: since X is the number of successes from a sequence of n trials, we can code the outcome of 
each trial as Z1,Z2,...,Zn, where Zi =1 if the ith trial was a success and Zi =0 otherwise. 
E[Zi]=1p +0 (1p)= p   
and therefore, 
n n n 
E[X]= E Zi = E[Zi]= p = np 
i=1 i=1 i=1 
Property4 IF X and Y are independent, then 
E[XY ]= E[X]E[Y ] 
If X and Y are not independent, this will generally not be true. 
3.3 Expectations of Functions of Random Variables 
LetY = r(X). Last week,wesawhowwecouldderivethep.d.f. of Y if weknew fX (x). Forexpectations, 
this problem is much simpler since we are only looking at one single characteristic of the distribution. 
The expectation of Y = r(X)isgivenby 
r(x)fX (x) if X discrete E[Y ]= E[r(X)]=  x 
 r(t)fX (t)dt if X continuous 
Example7 Suppose Y = X1/2 where the p.d.f. of X isgivenby 
2x if0 &lt;x&lt; 1 fX (x)= 0 otherwise 
 1  1  1 
5/2E[Y ]= t1/2fX (t)dt =2 t3/2dt =22 4 x = 5 50 0 0 
The same principle works for functions of 2 or more random variables 
Example8 Suppose we are interested in the function Z = X2 + Y 2 of two random variables X and Y 
withjointp.d.f. 
fXY (x,y)=1 if0 x,y 1 
0 otherwise 
Then     
E[Z] = (x 2 + y 2)fXY (x,y)dxdy 
   11 
= (x 2 + y 2)dxdy 
00  1  11 = x 3 + y 2 x dy 30 0  1 1 = + y 2 dy 30  111 3 112 = y + y = + = 33 3330 
7 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>  
    2 Example2 We could now think of an auction which is dierent from the rst one in that the buyer 
submitting the highest bid still gets to buy the painting, but only has to pay the amount oered by the 
second-highestbidder(thisauctionformatismorecommonthanthe rst,anditsknownasanEnglish, 
or second-price auction). If the submitted bids are random variables C1,...,Cn, the revenue Y of the 
seller now has p.d.f. 
fY (y)=2 nFC (y)[1FC (y)]n2fC (y)2 
Notice that I used dierent letters for the bids, since it is known from economic theory that the same 
bidders should submit dierent bids under the two dierent auction formats. 
Digression: The Distribution of the First Digit of Anything 
(skippedinlecture) 
Heres a somewhat cute but non-standard problem, for which we are not going to use the methods we 
saw earlier. So this is denitely something you can skip for problem sets or exam preparation, but Id 
still like to go over it. 
What is the distribution of the rst digit of a number we basically dont know anything about? To 
be more precise, let X be a measurement of any kind, for which we dont know what it represents, 
nor in what units 1 it is measured -e.g. we could leaf through a newspaper and collect any numbers Y 
representing ameasurementof somekind(incomes,stockindices,populationetc.). Whatisthep.d.f. of 
the rst digit of a number of that kind if we dont know anything else about where it comes from? I.e. 
how do we derive the p.d.f. of the rst decimal of the random variable Z = XY if all we know that X  
and Y are positive, but can be anything? 
A rstguessmightbethatthedistributionof the rstdigitis(discrete) uniform,sincethisdistribution 
intuitively does not seem to contain much information about the numbers nor their units. However, if 
wetakeauniformdistributionand changetheunits(e.g. doubleorquadrupleall numbersintheassumed 
underlying distribution), the distribution of rst digits doesnt remain uniform. E.g. if the true numbers 
are X U[1, 10],4X U[0, 40], so that for the rst digit Y of4X hasp.d.f. 
 1 1  40 + 4 if y {1, 2, 3}
fY (y)= 1 
40 if y {4, 5, 6, 7, 8, 9} 0 otherwise 
Or,inpictures,Soaminimal requirementforthedistributionof the rstdigitsshouldbethatitdoesnt
change if we change the units of measurement.
Whatwearelookingforisinfactarandomvariable X forwhich thedistributionof X doesntchangefor
changes of the scale, i.e. aX for a&gt; 0. This is true if we assume that Z =log(X)U[log(1), log(10)],
since for a scale shift,
  z z+1 log z z +1 log a aP (z aX z +1) = Pa X  a log(10a)log(a) = 
= log(z +1) log(z)= z +1) log(10)log(1) P (z X 
Then the rst digit Y of Z has p.d.f. 
 
fY (y)= log(z+1)log(z) 
log(10) 
0 if y {1, 2, 3, . . . , 9}
otherwise 
3 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>u (x) 
x 
E [x] r (x) 
 
  
  Figure4: r(x)is always less than u(x) 
Example9 (Risk Aversion): Suppose you buy a laptop for 1,200 dollars which comes with a limited 
warranty for the rst year. During that rst year, there is a probability p =10% that you spill a cup of 
coee overthelaptop(orhave a similar accident, whichis allyour ownfault) andhaveto replacethe 
entire motherboard, which will cost 1100 dollars. This repair is not covered by the limited warranty, but 
you may purchase an extended service plan for 115 dollars. Should you buy this additional insurance? 
Withoutthe additionalinsurance, we canthink of thetotal cost ofyourlaptop as a random variable which 
takes values X =1, 200 withprobability 1p, and X =1, 200+1, 100 =2, 300 withprobability p (there 
are dierent ways of setting up this problem, but lets keep things simple for now). With the extended 
service plan, your laptop will cost you Y =1, 200+115 =1, 315 dollars for sure. 
Ifyouonly careabouttheexpected valueof thelaptop,then E[X]=2, 300p+1, 200(1p)=1, 200+1, 100p. 
This is greater than E[Y ] =1, 315 if p 10.45%. But since we said that p =10%, would it still be a 
good idea to buy the service plan? -Economists typically assume that when people take decisions under 
uncertainty, they do not care about the expected amount of money W they can spend, but that they 
experience a utility u(W ) over a dollar amount, for which the additional value of an additional dollar 
increases in the total amount consumed. That means that we assume that u() is a concave function in 
costs, say 
u(c)=4, 800c 
where I assume that our initial wealth is 4, 800, and we can spend 4, 800C, where C is the total cost 
of the laptop. Then the expected utility from not having the service plan is 
E[u(C1)]=0.94, 8001, 200+0.14, 8002, 300 =0.960+0.150 =59   
whereas with the insurance plan, 
E[u(C2)]= 4, 8001, 315 &gt; 3, 481 =59 
In fact, you would be willing to spend up to 4, 8003, 481 = 119 dollars for the insurance, whereas the 
expected additional cost is only 1, 100p =110 dollars. This 9-dollar dierence in what we are willing to 
pay for the insurance is referred to as the risk-premium comes from the fact that u() is concave. By 
Jensensinequality,thisrisk-premiumispositiveif u()is concave, and wesay thatthistypeofpreferences 
exhibits risk aversion. 
Example10 The following example is known as the St. Petersburg Paradox, and it gives an example 
of a random variable which doesnt have a nite expectation. 
We are oered the following gamble: suppose a fair coin is tossed over and over until the rst head 
9 Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>  1 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 11 
Konrad Menzel 
March 17, 2009 
OrderStatistics 
Let X1,...,Xn be independent random variables with identical p.d.f.s fX1 (x)= ... = fXn (x) -well 
generally call such a sequence independent and identically distributed, which is typically abbreviated 
as iid. We are interested in the function 
Yn =max{X1,...,Xn} 
i.e. Yn is the largest value in the sample. 
We can derive the c.d.f. of Yn using independence 
FYn (y)= P (Yn y)= P (X1 y,X2 y,...,Xn y) 
= P (X1 y)P (X2 y)...P (Xn y) 
= FX1 (y)FX2 (y)...FXn (y) 
=[FX (y)]n 
Using the chain rule, we can obtain the p.d.f. of the maximum, 
d n1fYn (y)= FYn (y)= n [FX (y)]fX (y)dy 
Example1 An old painting is sold at an auction. n identical bidders submit their bids B1,...,Bn 
independently, and the marginal c.d.f. of the bids is FB (b). The potential buyer who submitted the 
highestbidgetstobuythepainting andhastopayhisbid(thistypeofauctioniscalledDutch,or rst 
price auction). Then the revenue of the seller of the painting has p.d.f. is 
n1fY (y)= fmax{B1 ,...,Bn}(y)= n [FB (y)]fB (y) 
Now we can generalize this to other ranks in the sample, e.g. 
Yn1 = The 2nd highest value in X1,...,Xn  
This random variableis calledthe(n 1)th order statistic of X1,...,Xn, and we can state its p.d.f. 
Proposition1 Let X1,...,Xn be an iid sequence of random variables with p.d.f. fX (f) and c.d.f. 
FX (x). Then the kth order statistic Yk hasp.d.f. 
fYk (y)= kn [FX (y)]k1[1FX (y)]nkfX (y)k 
1 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> 
  
  
    
 
 
  Proof: Wecansplitthe experimentintotwoparts,(a) oneof the Xishastotakethevalue y according 
to the density fX (y), and (b) given the value y, the other draws in the sequence have to be grouped 
around y in a way that makes y the kth smallest value in the sample. 
Part(b) is abinomial experimentin which the n trials correspond to the n draws of X1,...,Xn, and we 
dene a success in the ith round astheevent(Xi y). Since draws are independent and correspond to 
the same p.d.f. the parameter p in the binomial distribution is equal to FX (y). y being smaller or equal 
tothe kth smallest value corresponds to atleast k successes in the binomial experiment, and therefore 
the corresponding c.d.f. is 
n  
FYk (y)= n [FX (y)]l[1FX (y)]nl 
l 
l=k 
We can now obtain the p.d.f. by dierentiating the c.d.f. with respect to y, using the product and the 
chain rule 
d fYk (y)= FYk (y)dy 
n  n  
= nl[FX (y)]l1[1FX (y)]nlfX (y) n (n l)[FX (y)]l[1FX (y)]nl1fX (y)= T1 T2l l 
l=k l=k 
This expression looks complicated, but it turns out to be essentially a telescopic sum, so that most 
summands will drop out. Noting that for the second term, the summand corresponding to l = n is zero, 
we can rewrite it as 
n1  n   
T2 = n
l (nl)[FX (y)]l[1FX (y)]nl1fX (y)= l n 
1(nl+1)[FX (y)]l1[1FX (y)]nlfX (y) 
l=k l=k+1 
replacing the running index l with l 1. Since for the rst term, 
n n!l n! n l  l = l!(n l)! =(l 1)!(n l +1)!(n l +1) = l 1(n l +1) 
The rst term becomes 
n   
T1 = l n 
1(n l +1)[FX (y)]l1[1FX (y)]nlfX (y) 
l=k 
so that the density equals the l = k term of the sum dening T1 since it is the only one which doesnt 
get canceled out when we subtract T2. Therefore 
k   
fYk (y)= T1 T2 = l n 
1(n l +1)[FX (y)]l1[1FX (y)]nlfX (y) 
l=k 
= k!(n n
k ! 
+1)!(n k +1)[FX (y)]k1[1FX (y)]nkfX (y) 
= kn [FX (y)]k1[1FX (y)]nkfX (y)k 
which is the result we were going to prove  
2 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text> 
  appears. You get 2 dollars when heads appears on the rst ip, 22 dollars if it appears on the second,
and, more generally, 2k dollars if the rst head appears on the xth ip.
How much would you pay to play this game? -in principle, you should be willing to pay your expected
winnings, so lets do the calculation: the probability that exactly x ips are required equals
11 fX (x)= P (x 1tails, 1heads) = =2x 
2x1 2 
Therefore, expected winnings Y can be calculated as 
  x    2  
E[Y ]= 2xfX (x)=2 = 1=  
x=1 x=1 x=1 
Therefore, there is no upper bound on expected winnings. 
Doesthismeanthatwed seepeoplewilling topay aninniteamountforthistypeofbet? -certainly not: 
typically people would oer at most around 25 dollars to play the game. This paradox can be resolved in 
dierent ways: 
	people do not actually care about the expected amount of money, but their valuation of money 
decreases in the total amount they already have, i.e. people maximize some concave function u()
of winnings, as in the previous example. 
	with very smallprobabilities,theamountyouwinisextremelyhigh -i.e. inthetrillions,quadrillions 
etc. ofdollars,and wewould notbelievethatinthiscase,ouropponentcouldliveup tohispromise, 
so in fact there would be some ceiling as to how much we could at best hope to win from this bet. 
In order to make the link back to Jensens inequality, lets also calculate the expected number of coin ips 
in the game for a coin which comes up heads with probability p = a 1 : 
  x1  
G 
a a a a E[X]=  
xa x =1  1 1 1 x =: 
x=1 x=1 
where G(a)is, as you can easily check, the rst derivative with respect to a 1 of 
  x1 1 1 G = = a a x=1 1a 1 
Therefore, by taking the derivative of the new expression for G  
a 1  
, we get that 
1 1 11 E[X]= G =  2a a a 11a 
and since in our example, 1
1 = 21 , the expected number of ips is 2. 
Hence, the 25 dollars many people are still willing to bet are far more than 2E[X] =22 =4 dollarsyoud 
get from the average number of ips. The explanation for this is once more Jensens inequality, and the 
factthat u(x)=2x is an(extremely) convexfunction of x. 
Example11 Supposeyoucanchoosebetweentwoassets:the rstisthestock of anobscureonlinestart
up which makes it possible for people to search web sites for free. It is very risky in that with probability 
90% dividends are constant at e0t =1, and with 10% probability, the companys name is Google, and the 
10 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Hypothesis tests (cont.) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/14-30-introduction-to-statistical-methods-in-economics-spring-2009/resources/mit14_30s09_lec22/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>  
 14.30 Introduction to Statistical Methods in Economics 
Lecture Notes 22 
Konrad Menzel 
May 7, 2009 
Proposition 1 (Neyman-Pearson Lemma) In testing f0 against fA (where both H0 and HA are 
simple hypotheses), the critical region 
f0(x)C(k)= x : &lt;k fA(x) 
is most powerful for any choice of k 0. 
Note that the choice of k depends on the specied signicance level  of the test. This means that the 
most powerful test rejects if for the sample X1,...,Xn,the likelihood ratio 
f0(X1,...,Xn) r(X1,...,Xn)= fA(X1,...,Xn) 
is low, i.e. the data is much more likely to have been generated under HA. 
Reject Fo(x) 
FA(x) 
Cx(k) r(x)= 
k 
A o 
The most powerful test given in the Neyman-Pearson Lemma    Image by MIT OpenCourseWare.
explicitly solves the trade-o between 
size  
 = P(reject H0) = f0(x)dx |
C(k) 
andpower 
1 = P(reject|HA)= fA(x)dx 
C(k) 
1 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
14.30  Introduction to Statistical Methods in Economics 
Spring 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> at every point x in the sample space (where the integrals are over many dimensions, e.g. typically 
f0(x)x Rn). Fromtheexpressionsfor  and1 wecanseethatthelikelihood ratio fA(x) gives the price 
ofincluding x with the critical region in terms of how much we pay in terms of size  relativetothe 
gain in power from including the point in the critical region CX. 
Therefore, we should start constructing the critical region by including the cheapest points x -i.e. 
thosewith asmalllikelihood ratio. Thenwecangodownthelistof x ordered according tothelikelihood 
ratio and continue including more points until the size  ofthetestisdowntothedesiredlevel. 
Example 1 A criminal defendant (D) is on trial for a purse snatching. In order to convict, the jury 
must believe that there is a 95% chance that the charge is true. 
There arethreepotentialpieces of evidencetheprosecutor may or may nothavebeen abletoproduce, and 
inagivencasethejury takesadecisiontoconvictbased only onwhich out of thethreecluesitispresented 
with. Below are the potential pieces of evidence, assumed to be mutually independent, the probability of 
observing each piece given the defendant is guilty, and the probability of observing each piece given the 
defendant is not guilty 
guilty not guilty likelihood ratio 
1. D ran when he saw police coming 0.6 0.3 1/2 
2. D has no alibi 0.9 0.3 1/3 
3. Empty purse found near Ds home 0.4 0.1 1/4 
In the notation of the Neyman-Pearson Lemma, x can be any of the 23 possible combinations ofpieces 
of evidence. Using theassumptionofindependence, we canthereforelistallpossible combinationsof clues 
with their respective likelihood under each hypothesis and the likelihood ratio. I already ordered the list by 
the likelihood ratios in the third column. In the last column, I added 
(k)= f0(x) 
r(x)k 
the cumulative sum over the ordered list of combinations x. 
guilty fA(x) not guilty f0(x) likelihood ratio r(x)= f0(x) (k)fA(x) 
1. all three clues 216/1000 9/1000 0.0417 9/1000 
2. no alibi,found purse 144/1000 21/1000 0.1458 30/1000 
3. ran,no alibi 324/1000 81/1000 0.25 111/1000 
4. no alibi 216/1000 189/1000 0.875 300/1000 
5. ran,found purse 24/1000 21/1000 0.875 321/1000 
6. found purse 16/1000 49/1000 3.0625 370/1000 
7. ran 36/1000 189/1000 5.25 559/1000 
8. none of the clues 24/1000 441/1000 18.375 1 
The jury convicting the defendant only if there is at least 95% condence that the charge is true 
correspondsto aprobability offalse conviction(i.e. ifthedefendantisinfactinnocent) ofless than5%. 
In the terminology of hypothesis test, the sentence corresponds to a rejection of the null hypothesis that 
the defendant is innocent using the most powerful test of size  =5%. 
Looking at the values of (k) in the last column of the table, we can read o that including more than 
2 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>	  is student-t distributed with n 1 degrees of freedom if the true mean is in fact 0. Therefore we reject 
H0 if 

T = Xn 7 &lt;t9(5%) 
S/
10 
Plugginginthevaluesfromtheproblem, T = 
10
..
58 
/10 2.066,whichissmallerthan t9(0.05) = 1.83. 
Example 4 Let Xi Bernoulli(p), i =1,2,3. I.e. we are ipping a bent coin three times independently, 
and Xi =1 if it comes up heads, otherwise Xi =0. We want to test H0 : p = 31 against HA : p = 32 . 
Since both hypotheses are simple, can use likelihood ratio test 
3 Xi 1Xi P3 
3 T = f0(X)= i=1 31 
32 
=23
Pi=1 Xi 
=232P
i=1 Xi 
fA(X) 3  2Xi  11Xi Xi 3 
i=1 2i=1 3	 3 
Therefore, we reject if 
3 
232P
i3
=1 Xi k (32  
Xi)log2logk 
i=1 
which is equivalent to X3 21  logk . In order to determine k, lets list the possible values of X3 and 6log2
their probabilities under H0 and HA, respectively: 
X3 Prob. under H0 Prob. under HA cumul. prob. under H0 
1 1 8 1 
27 27 27 2 6 12 7 
3 27 27 27 1 12 6 19 
3 27 27 27 
0 8	 1 127	 27 
So if we want the size of the test equal to  = 27	 31 , we could reject if and only if X3 &gt; 2, or equivalently 
we can pick k = 21 . The power of this test is equal to 
8 1 = P(X3 =1|HA)=27 29.63% 
Example 5 Suppose we have one single observation generated by either 
f0(x)=	2x if0 x 1 or fA(x)= 22x if0 x 1 
0 otherwise 0 otherwise 
	Find thetestingprocedurewhich minimizesthesumof + -dowerejectif X =0.6? Since we only 
have one observation X, its not too complicated the critical region directly in terms of X, and there 
is nothing to be gained by trying to nd some clever statistic (though of course Neyman-Pearson 
would still work here). By looking at a graph of the densities, we can convince ourselves that the 
test should reject for small values of X. The probability of type I and type II error is, respectively, 
 k 
(k)= P(rejectH0)= 2xdx = k2|
0 
for 0 k 1, and 
 1 
(k)= P(dont reject|HA)= 
k (22x)dx =2(1k)1+k2 =1k(2k) 
5 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>1 the rst two combinations of the evidence raises the probability of a false conviction  to more than 5%. 
Therefore,thejury should convictthedefendantifhedoesnthavean alibi and the emptypurse wasfound 
nearhishome, regardless whetherhe ran whenhe sawthepolice. Inprinciple,thejury couldin addition 
randomize whenthedefendant ran,had no alibi,but nopurse wasfound(thatis case3): ifinthat case, 
thejury convictedthedefendant withprobability 5030 1 
81 4, the probability of a false conviction would be 
exactly equal to5%,but this wouldprobably notbe considered an acceptablepracticein criminaljustice. 
Construction of Tests 
In general there is no straightforward answer to how we should construct an optimal test. The Neyman
PearsonLemmagaveusasimplerecipeforamostpowerful test of onesimplehypothesisagainst another, 
but in most real-world applications, the alternative hypothesis is composite. The following is a list of 
recommendationswhichdo not alwayslead toauniformly mostpowerful test(which sometimesdoesnot 
even exist), but usually yield reasonable procedures: 
1. if both H0 and HA are simple, the Neyman-Pearson Lemma tells us to construct a statistic
f0(x)
T(x)= fA(x) 
and reject if T(X)&gt;k for some appropriately chosen value k (typicallyk is chosen in a way that 
makes sure that the test has size ). This test is also called the likelihood ratio test (LRT). 
2. if H0 :  = 0 is simple and HA :   A is composite and 2-sided, we construct a 1  
condence interval [A(X),B(X)] (usually symmetric) using an estimator . We then reject if 
0 /[A(X),B(X)]. This gives us a test of size  for H0. 
3. if H0 :  = 0 is simple and HA :  A is composite and one-sided, we construct a symmetric 
12 condenceintervalfor  and reject only if the null value is outside the condence interval 
and in the relevant tail in order to obtain a size  test. 
4.	either H0 :  0 or HA :  A composite(orboth): denethe statistic 
T(x)= max0 L() = max0 f(x|) 
maxA0 L() maxA0 f(x|) 
and reject if T(X)&lt;k for some appropriately chosen constant k. This type of test is called the 
generalized likelihood ratio test (GLRT). 
Since we havent discussed the last case yet, some remarks are in order: 
	the test makes sense because T(X)will tend to be small if the data dont support H0 
	densities are alwayspositive, so the statistic willbebetween0 and1(thisisbecause the set over 
which the density is maximized in the denominator contains the set over which we maximize in the 
numerator) 
	we need to know the exact distribution of the test statistic under the null hypothesis, so that we 
can nd an appropriate critical value k. For most distributions we have that in large samples 
2logT(X)2 
p 
where p=dim(0 A)dim(0). 
	the GLRT does not necessarily share the optimality properties of the LRT, in fact in this setting 
with a composite alternative hypothesis a uniformly most powerful test often does not even exist. 
3 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>  
  
 2 Examples 
Example 2 Assume thatbabies weights(inpounds) atbirth aredistributed according to X N(7,1). 
Now suppose that if an obstetrician gave expecting mothers poor advice on diet, this would cause babies 
to be on average 1 pound lighter (but have same variance). For a sample of 10 live births, we observe 
X10 =6.2. 
	How do we construct a 5% test of the null that the obstetrician is not giving bad advice against the 
alternative that he is? We have 
H0 : =7 against HA : =6 
We showed that for the normal distribution, it is optimal to base this simple test only on the sample 
	   mean, X10, so that T(x)= x10. Under H0, X10 N(7,0.1) and under HA, X10 N(6,0.1). The 
test rejects H0 if X10 &lt;k. We therefore have to pick k in a way that makes sure that the test has 
size5%,i.e. 
0.05 = P(X10 &lt;k=7) = k7 | 
0.1 
where ()is the standard normal c.d.f.. Therefore, we can obtain k by inverting this equation 
1.645 k =7+
0.011(0.05) 7
10 6.48 
Therefore, we reject, since X10 =6.2 &lt; 6.48 = k. 
	What is the power of this test? 
P( =6) =6.486 X10 &lt; 6.48| 
0.1 (1.518) 93.55% 
	Suppose we wanted a test with power of at least 99%, what would be the minimum number n of 
newborn babies wed have to observe? The only thing that changes with n is the variance of the 
sample mean, so from the rst part of this example, the critical value is kn = 1.645 7n , whereas the 
 power of a test based on Xn and critical value kn is 
 1 = P(Xn &lt;kn|=6) = n 1.645 
Setting 1 0.99, we get the condition 
n 1.645 1(0.99) =2.326 n 3.9712 15.77 
This type of power calculations is frequently done when planning a statistical experiment or survey 
-e.g. in order to determine how many patients to include in a drug test in order to be able to detect 
an eect of a certain size. Often it is very costly to treat or survey a large number of individuals, 
so wed like to know beforehand how large the experiment should be so that we will be able to detect 
any meaningful change with suciently high probability. 
Example 3 Suppose we are still in the same setting as in the previous example, but didnt know the 
variance. Instead, we have an estimate S2 =1.5. How would you perform a test? As we argued earlier, 
the statistic Xn 0T := S/n tn1 
4 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Therefore, minimizing the sum of the error probabilities over k,
min {(k)+(k)}=min {k2 +1 k(2k)}=min {2k2 +1 2k}
k k	 k 
Setting the rst derivative of the minimand to zero, 
1 0 =4k2 k =2 
Therefore we should reject if X&lt; 21, and  =  = 41 . Therefore, we would in particular not reject 
H0 for X =0.6. 
	Among all tests such that  0.1, nd the test with the smallest . What is ? Would you reject if 
X =0.4? -rst well solve (k)=0.1 for k. Using the formula from above, k= 
0.1. Therefore, 
 (k)=12k+ k2 =1.12
0.1 46.75% 
Since k = 
0.1 0.316 &lt; 0.4, we dont reject H0 for X =0.4. 
Example 6 Suppose we observe an i.i.d. sample X1,...,Xn, where Xi U[0,], and we want to test 
H0 :  = 0 against HA :  = 0, &gt; 0 
There are two options: we can either construct a 1 condenceintervalfor  and reject if it doesnt 
cover 0. Alternatively, we could construct a GLRT test statistic 
L(0)T = maxR+ L() 
Thelikelihoodfunctionisgivenby 
n  n 	 1 for0 Xi ,i =1,...,n L()= fX(Xi|)= 0  
otherwise 
i=1 
The denominator of T is given by the likelihood evaluated at the maximizer, which is the maximum 
likelihood estimator, MLE = X(n) =max{X1,...,Xn}, so that 
 n 
max L()= L(MLE)= 1 
R+	 X(n) 
Therefore, 
L(0)  X(n) n 
T = = maxR+ L() 0 
In order to nd the critical value k of the statistic which makes the size of the test equal to the desired 
level, wed have to gure out the distribution under the null  = 0 -could look this up in the section on 
order statistics. 
As an aside, even though we said earlier that for large n, the GLRT statistic is 2-distributed under the 
null, this turns out not to be true for this particular example because the density has a discontinuity at 
the true parameter value. 
6 </text>
        </slide>
      </slides>
    </lecture>
    <videos>
      <video>
        <video_url/>
        <video_title/>
        <transcript>
          <slice>
            <text_slice/>
            <time_slice/>
          </slice>
        </transcript>
      </video>
    </videos>
  </lectures>
</doc>
