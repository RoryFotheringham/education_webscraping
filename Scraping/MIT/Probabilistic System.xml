<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/</course_url>
    <course_title>Probabilistic Systems Analysis and Applied Probability</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Mathematics </list>
      <list>Systems Engineering </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Weak Law of Large Numbers (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 19 Chebyshevs inequality
Limit theorems  I
Random variable X
Readings: Sections 5.1-5.3; (with nite mean and variance2)
start Section 5.42=/integraldisplay
()2x  f X(x)dx
c
2 2X1, . . . , X ni.i.d. /integraldisplay
(x)fX(x)dx+/integraldisplay
(x)fX(x)dx
c
X1++
XnMn=n
What happens as ?2c P(|X|c)
n
Why bother? 2P(|X|c)c2
A tool: Chebyshevs inequality
Convergence in probability1P(X 
Convergence of||k)k2 Mn
(weak law of large numbers)
Deterministic limits Convergence in probability
Sequence an Sequence of random variables Yn
Number a
converges in probability to a number a:
(almost all) of the PMF/PDF of Yn,
anconverges to aeventually gets concentrated
(arbitrarily) close to aliman=an
aneventually gets and stays
(arbitrarily) close to a For every /epsilon1&gt;0,
limP(|Yna|/epsilon1)=0n
For every /epsilon1&gt;0,
there exists n0,
such that for every nn0,
we have |ana|/epsilon1. 1 - 1/n pmf of Yn
1/n 
0 n 
Does Ynconverge?
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Convergence of the sample mean The pollsters problem
(Weak law of large numbers)
f: fraction of population that . . . 
X1,X2,...i.i.d.
2ith (randomly selected) person polled:nite mean and variance 
X1++ 1 if yes, X ,ni= M= Xn
0,if no. n
Mn=(X
1++Xn)/n
E[Mn]=fraction of yes in our sample
Goal: 95% condence of 1% error
Var( Mn)=P(|Mnf|.01).05
Use Chebyshevs
Var(2 inequality:
M (|n) 2PMn|/epsilon1) =2 P(|Mnf|M2 .01) /epsilon1 n/epsilon1 n
(0.01)2
2 1Mnconverges in probability to  =x
n(0.01)24n(0.01)2
Ifn= 50, 000,
thenP(|Mnf|.01).05
(conservative)
Dierent scalings of Mn The central limit theorem
X1, . . . , X ni.i.d. Standa n=X1+
2 rdized S +Xn:
nite variance SnE[Sn]SnZn= nE[X]=
nLook at three variants of their sum:Sn
zero mean
Sn= + + Xn variance2X1  n
unit variance
Sn Let be a standard normal r.v.Mn= variance2/nn Z
(zero mean, unit variance)converges in probability to E[X] (WLLN)
Sn every c:
 constant variance2Theorem: For
nP(Znc)P(Zc)
Asymptotic shape?
P(Zc) is the standard normal CDF,
(c), available from the normal tables
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-1-probability-models-and-axioms/</video_url>
          <video_title>Lecture 1: Probability Models and Axioms</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:17</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: OK, so welcome to
6.041/6.431, the class on</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>probability models
and the like.</text_slice>
            </slice>
            <slice>
              <time_slice>0:31</time_slice>
              <text_slice>I'm John Tsitsiklis.</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>I will be teaching this class,
and I'm looking forward to</text_slice>
            </slice>
            <slice>
              <time_slice>0:36</time_slice>
              <text_slice>this being an enjoyable and
also useful experience.</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>We have a fair amount of staff
involved in this course, your</text_slice>
            </slice>
            <slice>
              <time_slice>0:44</time_slice>
              <text_slice>recitation instructors and also
a bunch of TAs, but I</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>want to single out our head
TA, Uzoma, who is the key</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>person in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>0:54</time_slice>
              <text_slice>Everything has to
go through him.</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>If he doesn't know in which
recitation section you are,</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>then simply you do not exist,
so keep that in mind.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>1:04</time_slice>
              <text_slice>So we want to jump right into
the subject, but I'm going to</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>take just a few minutes
to talk about a few</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>administrative details and
how the course is run.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>So we're going to have lectures
twice a week and I'm</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>going to use old fashioned
transparencies.</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>Now, you get copies of these
slides with plenty of space</text_slice>
            </slice>
            <slice>
              <time_slice>1:23</time_slice>
              <text_slice>for you to keep notes on them.</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>A useful way of making good use
of the slides is to use</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>them as a sort of mnemonic
summary of</text_slice>
            </slice>
            <slice>
              <time_slice>1:33</time_slice>
              <text_slice>what happens in lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>Not everything that I'm going
to say is, of course, on the</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>slides, but by looking them you
get the sense of what's</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>happening right now.</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>And it may be a good idea to
review them before you go to</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:47</time_slice>
              <text_slice>So what happens in recitation?</text_slice>
            </slice>
            <slice>
              <time_slice>1:48</time_slice>
              <text_slice>In recitation, your recitation
instructor is going to maybe</text_slice>
            </slice>
            <slice>
              <time_slice>1:52</time_slice>
              <text_slice>review some of the theory
and then solve some</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>problems for you.</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>And then you have tutorials
where you meet in very small</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>groups together with your TA.</text_slice>
            </slice>
            <slice>
              <time_slice>2:02</time_slice>
              <text_slice>And what happens in tutorials
is that you actually do the</text_slice>
            </slice>
            <slice>
              <time_slice>2:05</time_slice>
              <text_slice>problem solving with the help
of your TA and the help of</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>your classmates in your
tutorial section.</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>Now probability is
a tricky subject.</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>You may be reading the text,
listening to lectures,</text_slice>
            </slice>
            <slice>
              <time_slice>2:16</time_slice>
              <text_slice>everything makes perfect sense,
and so on, but until</text_slice>
            </slice>
            <slice>
              <time_slice>2:20</time_slice>
              <text_slice>you actually sit down and try
to solve problems, you don't</text_slice>
            </slice>
            <slice>
              <time_slice>2:23</time_slice>
              <text_slice>quite appreciate the
subtleties and the</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>difficulties that
are involved.</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>So problem solving is a key
part of this class.</text_slice>
            </slice>
            <slice>
              <time_slice>2:30</time_slice>
              <text_slice>And tutorials are extremely
useful just for this reason</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>because that's where you
actually get the practice of</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>solving problems on your own,
as opposed to seeing someone</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>else who's solving
them for you.</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>OK but, mechanics, a key part
of what's going to happen</text_slice>
            </slice>
            <slice>
              <time_slice>2:46</time_slice>
              <text_slice>today is that you will turn in
your schedule forms that are</text_slice>
            </slice>
            <slice>
              <time_slice>2:51</time_slice>
              <text_slice>at the end of the handout that
you have in your hands.</text_slice>
            </slice>
            <slice>
              <time_slice>2:55</time_slice>
              <text_slice>Then, the TAs will be working
frantically through the night,</text_slice>
            </slice>
            <slice>
              <time_slice>2:59</time_slice>
              <text_slice>and they're going to be
producing a list of who goes</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>into what section.</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>And when that happens, any
person in this class, with</text_slice>
            </slice>
            <slice>
              <time_slice>3:09</time_slice>
              <text_slice>probability 90%, is going to be
happy with their assignment</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>and, with probability 10%,
they're going to be unhappy.</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>Now, unhappy people have
an option, though.</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>You can resubmit your form
together with your full</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>schedule and constraints, give
it back to the head TA, who</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>will then do some further
juggling and reassign people,</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>and after that happens, 90% of
those unhappy people will</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>become happy.</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>And 10% of them will
be less unhappy.</text_slice>
            </slice>
            <slice>
              <time_slice>3:42</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>3:42</time_slice>
              <text_slice>So what's the probability that a
random person is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>unhappy at the end
of this process?</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>It's 1%.</text_slice>
            </slice>
            <slice>
              <time_slice>3:50</time_slice>
              <text_slice>Excellent.</text_slice>
            </slice>
            <slice>
              <time_slice>3:51</time_slice>
              <text_slice>Good.</text_slice>
            </slice>
            <slice>
              <time_slice>3:51</time_slice>
              <text_slice>Maybe you don't need
this class.</text_slice>
            </slice>
            <slice>
              <time_slice>3:53</time_slice>
              <text_slice>OK, so 1%.</text_slice>
            </slice>
            <slice>
              <time_slice>3:54</time_slice>
              <text_slice>We have about 100 people in this
class, so there's going</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>to be about one unhappy
person.</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>I mean, anywhere you look in
life, in any group you look</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>at, there's always one unhappy
person, right?</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>So, what can we do about it?</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>Another important part about
mechanics is to read carefully</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>the statement that we have about
collaboration, academic</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>honesty, and all that.</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>You're encouraged, it's
a very good idea to</text_slice>
            </slice>
            <slice>
              <time_slice>4:19</time_slice>
              <text_slice>work with other students.</text_slice>
            </slice>
            <slice>
              <time_slice>4:21</time_slice>
              <text_slice>You can consult sources that
are out there, but when you</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>sit down and write your
solutions you have to do that</text_slice>
            </slice>
            <slice>
              <time_slice>4:28</time_slice>
              <text_slice>by setting things aside and just
write them on your own.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>You cannot copy something
that somebody else</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>has given to you.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>One reason is that we're not
going to like it when it</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>happens, and then another reason
is that you're not</text_slice>
            </slice>
            <slice>
              <time_slice>4:44</time_slice>
              <text_slice>going to do yourself
any favor.</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>Really the only way to do well
in this class is to get a lot</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>of practice by solving
problems yourselves.</text_slice>
            </slice>
            <slice>
              <time_slice>4:51</time_slice>
              <text_slice>So if you don't do that on your
own, then when quiz and</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>exam time comes, things are
going to be difficult.</text_slice>
            </slice>
            <slice>
              <time_slice>4:59</time_slice>
              <text_slice>So, as I mentioned here, we're
going to have recitation</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>sections, that some of them are
for 6.041 students, some</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>are for 6.431 students, the
graduate section of the class.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>Now undergraduates
can sit in the</text_slice>
            </slice>
            <slice>
              <time_slice>5:12</time_slice>
              <text_slice>graduate recitation sections.</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>What's going to happen there is
that things may be just a</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>little faster and you may be
covering a problem that's a</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>little more advanced and
is not covered in</text_slice>
            </slice>
            <slice>
              <time_slice>5:23</time_slice>
              <text_slice>the undergrad sections.</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>But if you sit in the graduate
section, and you're an</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>undergraduate, you're still
just responsible for the</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>undergraduate material.</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>That is, you can just do the
undergraduate work in the</text_slice>
            </slice>
            <slice>
              <time_slice>5:35</time_slice>
              <text_slice>class, but maybe be exposed
at the different section.</text_slice>
            </slice>
            <slice>
              <time_slice>5:41</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>5:43</time_slice>
              <text_slice>A few words about the
style of this class.</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>We want to focus on basic
ideas and concepts.</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>There's going to be lots of
formulas, but what we try to</text_slice>
            </slice>
            <slice>
              <time_slice>5:53</time_slice>
              <text_slice>do in this class is to actually
have you understand</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>what those formulas mean.</text_slice>
            </slice>
            <slice>
              <time_slice>5:58</time_slice>
              <text_slice>And, in a year from now when
almost all of the formulas</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>have been wiped out from your
memory, you still have the</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>basic concepts.</text_slice>
            </slice>
            <slice>
              <time_slice>6:05</time_slice>
              <text_slice>You can understand them, so when
you look things up again,</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>they will still make sense.</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>It's not the plug and chug kind
of class where you're</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>given a list of formulas, you're
given numbers, and you</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>plug in and you get answers.</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>The really hard part is usually
to choose which</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>formulas you're going to use.</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>You need judgment, you
need intuition.</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>Lots of probability problems, at
least the interesting ones,</text_slice>
            </slice>
            <slice>
              <time_slice>6:32</time_slice>
              <text_slice>often have lots of different
solutions.</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>Some are extremely long, some
are extremely short.</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>The extremely short ones usually
involve some kind of</text_slice>
            </slice>
            <slice>
              <time_slice>6:40</time_slice>
              <text_slice>deeper understanding of what's
going on so that you can pick</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>a shortcut and use it.</text_slice>
            </slice>
            <slice>
              <time_slice>6:46</time_slice>
              <text_slice>And hopefully you are going
to develop this</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>skill during this class.</text_slice>
            </slice>
            <slice>
              <time_slice>6:51</time_slice>
              <text_slice>Now, I could spend a lot of time
in this lecture talking</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>about why the subject
is important.</text_slice>
            </slice>
            <slice>
              <time_slice>6:58</time_slice>
              <text_slice>I'll keep it short because I
think it's almost obvious.</text_slice>
            </slice>
            <slice>
              <time_slice>7:02</time_slice>
              <text_slice>Anything that happens in
life is uncertain.</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>There's uncertainty anywhere, so
whatever you try to do, you</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>need to have some way of dealing
or thinking about this</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>uncertainty.</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>And the way to do that in a
systematic way is by using the</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>models that are given to us
by probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>So if you're an engineer and
you're dealing with a</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>communication system or signal
processing, basically you're</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>facing a fight against noise.</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>Noise is random, is uncertain.</text_slice>
            </slice>
            <slice>
              <time_slice>7:30</time_slice>
              <text_slice>How do you model it?</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>How do you deal with it?</text_slice>
            </slice>
            <slice>
              <time_slice>7:33</time_slice>
              <text_slice>If you're a manager, I guess
you're dealing with customer</text_slice>
            </slice>
            <slice>
              <time_slice>7:36</time_slice>
              <text_slice>demand, which is, of
course, random.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>Or you're dealing with the
stock market, which is</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>definitely random.</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>Or you play the casino, which
is, again, random, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>7:48</time_slice>
              <text_slice>And the same goes for pretty
much any other field that you</text_slice>
            </slice>
            <slice>
              <time_slice>7:51</time_slice>
              <text_slice>can think of.</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>But, independent of which field
you're coming from, the</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>basic concepts and tools are
really all the same.</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>So you may see in bookstores
that there are books,</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>probability for scientists,
probability for engineers,</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>probability for social
scientists, probability for</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>astrologists.</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>Well, what all those books have
inside them is exactly</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>the same models, the same
equations, the same problems.</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>They just make them somewhat
different word problems.</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>The basic concepts are just one
and the same, and we'll</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>take this as an excuse for not
going too much into specific</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>domain applications.</text_slice>
            </slice>
            <slice>
              <time_slice>8:31</time_slice>
              <text_slice>We will have problems and
examples that are motivated,</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>in some loose sense, from
real world situations.</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>But we're not really trying in
this class to develop the</text_slice>
            </slice>
            <slice>
              <time_slice>8:42</time_slice>
              <text_slice>skills for domain-specific
problems.</text_slice>
            </slice>
            <slice>
              <time_slice>8:46</time_slice>
              <text_slice>Rather, we're going to try to
stick to general understanding</text_slice>
            </slice>
            <slice>
              <time_slice>8:49</time_slice>
              <text_slice>of the subject.</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>So the next slide, of which you
do have in your handout,</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>gives you a few more details
about the class.</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>Maybe one thing to comment here
is that you do need to</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>read the text.</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>And with calculus books, perhaps
you can live with a</text_slice>
            </slice>
            <slice>
              <time_slice>9:09</time_slice>
              <text_slice>just a two page summary of all
of the interesting formulas in</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>calculus, and you can get by
just with those formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>But here, because we want
to develop concepts and</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>intuition, actually reading
words, as opposed to just</text_slice>
            </slice>
            <slice>
              <time_slice>9:24</time_slice>
              <text_slice>browsing through equations,
does make a difference.</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>In the beginning, the class
is kind of easy.</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>When we deal with discrete
probability, that's the</text_slice>
            </slice>
            <slice>
              <time_slice>9:32</time_slice>
              <text_slice>material until our first quiz,
and some of you may get by</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>without being too systematic
about following the material.</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>But it does get substantially
harder afterwards.</text_slice>
            </slice>
            <slice>
              <time_slice>9:43</time_slice>
              <text_slice>And I would keep restating that
you do have to read the</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>text to really understand
the material.</text_slice>
            </slice>
            <slice>
              <time_slice>9:52</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>9:52</time_slice>
              <text_slice>So now we can start with the
real part of the lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>Let us set the goals
for today.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>So probability, or probability
theory, is a framework for</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>dealing with uncertainty, for
dealing with situations in</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>which we have some kind
of randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>So what we want to do is, by the
end of today's lecture, to</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>give you anything that you need
to know how to set up</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>what does it take to set up
a probabilistic model.</text_slice>
            </slice>
            <slice>
              <time_slice>10:23</time_slice>
              <text_slice>And what are the basic rules of
the game for dealing with</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>probabilistic models?</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>So, by the end of this lecture,
you will have</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>essentially recovered
half of this</text_slice>
            </slice>
            <slice>
              <time_slice>10:34</time_slice>
              <text_slice>semester's tuition, right?</text_slice>
            </slice>
            <slice>
              <time_slice>10:36</time_slice>
              <text_slice>So we're going to talk
about probabilistic</text_slice>
            </slice>
            <slice>
              <time_slice>10:39</time_slice>
              <text_slice>models in more detail--</text_slice>
            </slice>
            <slice>
              <time_slice>10:40</time_slice>
              <text_slice>the sample space, which is
basically a description of all</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>the things that may happen
during a random experiment,</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>and the probability law, which
describes our beliefs about</text_slice>
            </slice>
            <slice>
              <time_slice>10:50</time_slice>
              <text_slice>which outcomes are more
likely to occur</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>compared to other outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>10:56</time_slice>
              <text_slice>Probability laws have to obey
certain properties that we</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>call the axioms of
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>11:00</time_slice>
              <text_slice>So the main part of today's
lecture is to describe those</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>axioms, which are the rules of
the game, and consider a few</text_slice>
            </slice>
            <slice>
              <time_slice>11:09</time_slice>
              <text_slice>really trivial examples.</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>OK, so let's start
with our agenda.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>The first piece in a
probabilistic model is a</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>description of the sample
space of an experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>So we do an experiment, and by
experiment we just mean that</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>just something happens
out there.</text_slice>
            </slice>
            <slice>
              <time_slice>11:30</time_slice>
              <text_slice>And that something that happens,
it could be flipping</text_slice>
            </slice>
            <slice>
              <time_slice>11:33</time_slice>
              <text_slice>a coin, or it could be rolling
a dice, or it could be doing</text_slice>
            </slice>
            <slice>
              <time_slice>11:39</time_slice>
              <text_slice>something in a card game.</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>So we fix a particular
experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>11:44</time_slice>
              <text_slice>And we come up with a list of
all the possible things that</text_slice>
            </slice>
            <slice>
              <time_slice>11:48</time_slice>
              <text_slice>may happen during
this experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>So we write down a list of all
the possible outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>So here's a list of all the
possible outcomes of the</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>I use the word "list," but, if
you want to be a little more</text_slice>
            </slice>
            <slice>
              <time_slice>12:02</time_slice>
              <text_slice>formal, it's better to think
of that list as a set.</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>So we have a set.</text_slice>
            </slice>
            <slice>
              <time_slice>12:08</time_slice>
              <text_slice>That set is our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>And it's a set whose elements
are the possible outcomes of</text_slice>
            </slice>
            <slice>
              <time_slice>12:14</time_slice>
              <text_slice>the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>So, for example, if you're
dealing with flipping a coin,</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>your sample space would be
heads, this is one outcome,</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>tails is one outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>12:24</time_slice>
              <text_slice>And this set, which has two
elements, is the sample space</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>of the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>What do we need to think about
when we're setting up the</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>First, the list should be
mutually exclusive,</text_slice>
            </slice>
            <slice>
              <time_slice>12:36</time_slice>
              <text_slice>collectively exhaustive.</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>What does that mean?</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>Collectively exhaustive means
that, no matter what happens</text_slice>
            </slice>
            <slice>
              <time_slice>12:42</time_slice>
              <text_slice>in the experiment, you're
going to get one of the</text_slice>
            </slice>
            <slice>
              <time_slice>12:45</time_slice>
              <text_slice>outcomes inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>So you have not forgotten any
of the possibilities of what</text_slice>
            </slice>
            <slice>
              <time_slice>12:51</time_slice>
              <text_slice>may happen in the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>12:53</time_slice>
              <text_slice>Mutually exclusive means that
if this happens, then that</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>cannot happen.</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>So at the end of the experiment,
you should be able</text_slice>
            </slice>
            <slice>
              <time_slice>13:01</time_slice>
              <text_slice>to point out to me just one,
exactly one, of these outcomes</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>and say, this is the outcome
that happened.</text_slice>
            </slice>
            <slice>
              <time_slice>13:10</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>13:11</time_slice>
              <text_slice>So these are sort of
basic requirements.</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>There's another requirement
which is a little more loose.</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>When you set up your sample
space, sometimes you do have</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>some freedom about the details
of how you're going to</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>describe it.</text_slice>
            </slice>
            <slice>
              <time_slice>13:24</time_slice>
              <text_slice>And the question is,
how much detail are</text_slice>
            </slice>
            <slice>
              <time_slice>13:27</time_slice>
              <text_slice>you going to include?</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>So let's take this coin flipping
experiment and think</text_slice>
            </slice>
            <slice>
              <time_slice>13:31</time_slice>
              <text_slice>of the following sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>One possible outcome is heads,
a second possible outcome is</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>tails and it's raining, and the
third possible outcome is</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>tails and it's not raining.</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>So this is another possible
sample space for the</text_slice>
            </slice>
            <slice>
              <time_slice>13:52</time_slice>
              <text_slice>experiment where I flip
a coin just once.</text_slice>
            </slice>
            <slice>
              <time_slice>13:56</time_slice>
              <text_slice>It's a legitimate one.</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>These three possibilities are
mutually exclusive and</text_slice>
            </slice>
            <slice>
              <time_slice>14:01</time_slice>
              <text_slice>collectively exhaustive.</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>Which one is the right
sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>14:05</time_slice>
              <text_slice>Is it this one or that one?</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>Well, if you think that my coin
flipping inside this room</text_slice>
            </slice>
            <slice>
              <time_slice>14:12</time_slice>
              <text_slice>is completely unrelated to the
weather outside, then you're</text_slice>
            </slice>
            <slice>
              <time_slice>14:15</time_slice>
              <text_slice>going to stick with
this sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>14:18</time_slice>
              <text_slice>If, on the other hand, you have
some superstitious belief</text_slice>
            </slice>
            <slice>
              <time_slice>14:22</time_slice>
              <text_slice>that maybe rain has an effect
on my coins, you might work</text_slice>
            </slice>
            <slice>
              <time_slice>14:27</time_slice>
              <text_slice>with the sample space
of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>14:29</time_slice>
              <text_slice>So you probably wouldn't do
that, but it's a legitimate</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>option, strictly speaking.</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>Now this example is a little bit
on the frivolous side, but</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>the issue that comes up here is
a basic one that shows up</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>anywhere in science
and engineering.</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>Whenever you're dealing with a
model or with a situation,</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>there are zillions of details
in that situation.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>And when you come up with a
model, you choose some of</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>those details that you keep in
your model, and some that you</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>say, well, these
are irrelevant.</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>Or maybe there are small
effects, I can neglect them,</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>and you keep them outside
your model.</text_slice>
            </slice>
            <slice>
              <time_slice>15:05</time_slice>
              <text_slice>So when you go to the real
world, there's definitely an</text_slice>
            </slice>
            <slice>
              <time_slice>15:09</time_slice>
              <text_slice>element of art and some judgment
that you need to do</text_slice>
            </slice>
            <slice>
              <time_slice>15:12</time_slice>
              <text_slice>in order to set up an
appropriate sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>So, an easy example now.</text_slice>
            </slice>
            <slice>
              <time_slice>15:23</time_slice>
              <text_slice>So of course, the elementary
examples are</text_slice>
            </slice>
            <slice>
              <time_slice>15:26</time_slice>
              <text_slice>coins, cards, and dice.</text_slice>
            </slice>
            <slice>
              <time_slice>15:29</time_slice>
              <text_slice>So let's deal with dice.</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>But to keep the diagram small,
instead of a six-sided die,</text_slice>
            </slice>
            <slice>
              <time_slice>15:34</time_slice>
              <text_slice>we're going to think about the
die that only has four faces.</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>So you can do that with
a tetrahedron,</text_slice>
            </slice>
            <slice>
              <time_slice>15:40</time_slice>
              <text_slice>doesn't really matter.</text_slice>
            </slice>
            <slice>
              <time_slice>15:41</time_slice>
              <text_slice>Basically, it's a die that when
you roll it, you get a</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>result which is one,
two, three or four.</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>However, the experiment that I'm
going to think about will</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>consist of two rolls
of a dice.</text_slice>
            </slice>
            <slice>
              <time_slice>15:55</time_slice>
              <text_slice>A crucial point here--</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>I'm rolling the die twice, but
I'm thinking of this as just</text_slice>
            </slice>
            <slice>
              <time_slice>16:01</time_slice>
              <text_slice>one experiment, not two
different experiments, not a</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>repetition twice of the
same experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>So it's one big experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>16:12</time_slice>
              <text_slice>During that big experiment
various things could happen,</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>such as I'm rolling the
die once, and then I'm</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>rolling the die twice.</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>16:22</time_slice>
              <text_slice>So what's the sample space
for that experiment?</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>Well, the sample space
consists of</text_slice>
            </slice>
            <slice>
              <time_slice>16:27</time_slice>
              <text_slice>the possible outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>16:28</time_slice>
              <text_slice>One possible outcome is that
your first roll resulted in</text_slice>
            </slice>
            <slice>
              <time_slice>16:33</time_slice>
              <text_slice>two and the second roll
resulted in three.</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>In which case, the outcome that
you get is this one, a</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>two followed by three.</text_slice>
            </slice>
            <slice>
              <time_slice>16:42</time_slice>
              <text_slice>This is one possible outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>The way I'm describing things,
this outcome is to be</text_slice>
            </slice>
            <slice>
              <time_slice>16:49</time_slice>
              <text_slice>distinguished from this outcome
here, where a three is</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>followed by two.</text_slice>
            </slice>
            <slice>
              <time_slice>16:56</time_slice>
              <text_slice>If you're playing backgammon, it
doesn't matter which one of</text_slice>
            </slice>
            <slice>
              <time_slice>17:00</time_slice>
              <text_slice>the two happened.</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>But if you're dealing with a
probabilistic model that you</text_slice>
            </slice>
            <slice>
              <time_slice>17:05</time_slice>
              <text_slice>want to keep track of everything
that happens in</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>this composite experiment, there
are good reasons for</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>distinguishing between
these two outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>17:15</time_slice>
              <text_slice>I mean, when this happens,
it's definitely something</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>different from that happening.</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>A two followed by a three is
different from a three</text_slice>
            </slice>
            <slice>
              <time_slice>17:22</time_slice>
              <text_slice>followed by a two.</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>So this is the correct sample
space for this experiment</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>where we roll the die twice.</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>It has a total of 16 elements
and it's, of</text_slice>
            </slice>
            <slice>
              <time_slice>17:32</time_slice>
              <text_slice>course, a finite set.</text_slice>
            </slice>
            <slice>
              <time_slice>17:35</time_slice>
              <text_slice>Sometimes, instead of describing
sample spaces in</text_slice>
            </slice>
            <slice>
              <time_slice>17:39</time_slice>
              <text_slice>terms of lists, or sets, or
diagrams of this kind, it's</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>useful to describe
the experiment in</text_slice>
            </slice>
            <slice>
              <time_slice>17:46</time_slice>
              <text_slice>some sequential way.</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>Whenever you have an experiment
that consists of</text_slice>
            </slice>
            <slice>
              <time_slice>17:50</time_slice>
              <text_slice>multiple stages, it might be
useful, at least visually, to</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>give a diagram that shows you
how those stages evolve.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>And that's what we do by using
a sequential description or a</text_slice>
            </slice>
            <slice>
              <time_slice>18:04</time_slice>
              <text_slice>tree-based description by
drawing a tree of the possible</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>evolutions during
our experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>So in this tree, I'm thinking
of a first stage in which I</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>roll the first die, and there
are four possible results,</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>one, two, three and
four.and 4.</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>And, given what happened, let's
say in the first roll,</text_slice>
            </slice>
            <slice>
              <time_slice>18:24</time_slice>
              <text_slice>suppose I got a one.</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>Then I'm rolling the second
dice, and there are four</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>possibilities for what may
happen to the second die.</text_slice>
            </slice>
            <slice>
              <time_slice>18:32</time_slice>
              <text_slice>And the possible results
are one, tow,</text_slice>
            </slice>
            <slice>
              <time_slice>18:33</time_slice>
              <text_slice>three and four again.</text_slice>
            </slice>
            <slice>
              <time_slice>18:36</time_slice>
              <text_slice>So what's the relation between
the two diagrams?</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>Well, for example, the outcome
two followed by three</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>corresponds to this
path on the tree.</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>So this path corresponds to
two followed by a three.</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>Any path is associated to a
particular outcome, any</text_slice>
            </slice>
            <slice>
              <time_slice>18:54</time_slice>
              <text_slice>outcome is associated to
a particular path.</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>And, instead of paths, you may
want to think in terms of the</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>leaves of this diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>Same thing, think of each one
of the leaves as being one</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>possible outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>19:07</time_slice>
              <text_slice>And of course we have 16
outcomes here, we have 16</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>outcomes here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>Maybe you noticed the subtlety
that I used in my language.</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>I said I rolled the first
dice and the result</text_slice>
            </slice>
            <slice>
              <time_slice>19:18</time_slice>
              <text_slice>that I get is a two.</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>I didn't use the word "outcome."
I want to reserve</text_slice>
            </slice>
            <slice>
              <time_slice>19:23</time_slice>
              <text_slice>the word "outcome" to mean the
overall outcome at the end of</text_slice>
            </slice>
            <slice>
              <time_slice>19:28</time_slice>
              <text_slice>the overall experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>So "2, 3" is the outcome
of the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>The experiment consisted
of stages.</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>Two was the result in the first
stage, three was the</text_slice>
            </slice>
            <slice>
              <time_slice>19:41</time_slice>
              <text_slice>result in the second stage.</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>You put all those results
together, and</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>you get your outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>OK, perhaps we are splitting
hairs here, but it's useful to</text_slice>
            </slice>
            <slice>
              <time_slice>19:53</time_slice>
              <text_slice>keep the concepts right.</text_slice>
            </slice>
            <slice>
              <time_slice>19:56</time_slice>
              <text_slice>What's special about this
example is that, besides being</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>trivial, it has a sample
space which is finite.</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>There's 16 possible
total outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>Not every experiment has
a finite sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>Here's an experiment in which
the sample space is infinite.</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>So you are playing darts and
the target is this square.</text_slice>
            </slice>
            <slice>
              <time_slice>20:17</time_slice>
              <text_slice>And you're perfect at that game,
so you're sure that your</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>darts will always fall
inside the square.</text_slice>
            </slice>
            <slice>
              <time_slice>20:26</time_slice>
              <text_slice>So, but where exactly your dart
would fall inside that</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>square, that itself is random.</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>We don't know what
it's going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>20:32</time_slice>
              <text_slice>It's uncertain.</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>So all the possible points
inside the square are possible</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>outcomes of the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>So a typical outcome of the
experiment is going to a pair</text_slice>
            </slice>
            <slice>
              <time_slice>20:43</time_slice>
              <text_slice>of numbers, x,y, where x
and y are real numbers</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>between zero and one.</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>Now there's infinitely many
real numbers, there's</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>infinitely many points in the
square, so this is an example</text_slice>
            </slice>
            <slice>
              <time_slice>20:55</time_slice>
              <text_slice>in which our sample space
is an infinite set.</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>OK, so we're going to revisit
this example a little later.</text_slice>
            </slice>
            <slice>
              <time_slice>21:06</time_slice>
              <text_slice>So these are two examples of
what the sample space might be</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>in simple experiments.</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>Now, the more important order of
business is now to look at</text_slice>
            </slice>
            <slice>
              <time_slice>21:18</time_slice>
              <text_slice>those possible outcomes and to
make some statements about</text_slice>
            </slice>
            <slice>
              <time_slice>21:21</time_slice>
              <text_slice>their relative likelihoods.</text_slice>
            </slice>
            <slice>
              <time_slice>21:23</time_slice>
              <text_slice>Which outcome is more
likely to occur</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>compared to the others?</text_slice>
            </slice>
            <slice>
              <time_slice>21:29</time_slice>
              <text_slice>And the way we do this
is by assigning</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>probabilities to the outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>21:36</time_slice>
              <text_slice>Well, not exactly.</text_slice>
            </slice>
            <slice>
              <time_slice>21:38</time_slice>
              <text_slice>Suppose that all you were to do
was to assign probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>to individual outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>21:44</time_slice>
              <text_slice>If you go back to this example,
and you consider one</text_slice>
            </slice>
            <slice>
              <time_slice>21:49</time_slice>
              <text_slice>particular outcome-- let's
say this point--</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>what would be the probability
that you hit exactly this</text_slice>
            </slice>
            <slice>
              <time_slice>21:55</time_slice>
              <text_slice>point to infinite precision?</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>Intuitively, that probability
would be zero.</text_slice>
            </slice>
            <slice>
              <time_slice>22:01</time_slice>
              <text_slice>So any individual point in this
diagram in any reasonable</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>model should have zero
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>So if you just tell me that
any individual outcome has</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>zero probability, you're
not really telling me</text_slice>
            </slice>
            <slice>
              <time_slice>22:14</time_slice>
              <text_slice>much to work with.</text_slice>
            </slice>
            <slice>
              <time_slice>22:17</time_slice>
              <text_slice>For that reason, what instead
we're going to do is to assign</text_slice>
            </slice>
            <slice>
              <time_slice>22:20</time_slice>
              <text_slice>probabilities to subsets of the
sample space, as opposed</text_slice>
            </slice>
            <slice>
              <time_slice>22:25</time_slice>
              <text_slice>to assigning probabilities
to individual outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>22:29</time_slice>
              <text_slice>So here's the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>22:32</time_slice>
              <text_slice>We have our sample space,
which is omega, and we</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>consider some subset of
the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>Call it A. And I want to assign
a number, a numerical</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>probability, to this particular
subset which</text_slice>
            </slice>
            <slice>
              <time_slice>22:50</time_slice>
              <text_slice>represents my belief about how
likely this set is to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>22:57</time_slice>
              <text_slice>What do we mean "to occur?"
And I'm introducing here a</text_slice>
            </slice>
            <slice>
              <time_slice>23:01</time_slice>
              <text_slice>language that's being used
in probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>When we talk about subsets of
the sample space, we usually</text_slice>
            </slice>
            <slice>
              <time_slice>23:07</time_slice>
              <text_slice>call them events, as
opposed to subsets.</text_slice>
            </slice>
            <slice>
              <time_slice>23:10</time_slice>
              <text_slice>And the reason is because it
works nicely with the language</text_slice>
            </slice>
            <slice>
              <time_slice>23:14</time_slice>
              <text_slice>that describes what's
going on.</text_slice>
            </slice>
            <slice>
              <time_slice>23:16</time_slice>
              <text_slice>So the outcome is a point.</text_slice>
            </slice>
            <slice>
              <time_slice>23:19</time_slice>
              <text_slice>The outcome is random.</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>The outcome may be inside this
set, in which case we say that</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>event A occurred, if we get
an outcome inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>Or the outcome may fall outside
the set, in which case</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>we say that event
A did not occur.</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>So we're going to assign
probabilities to events.</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>And now, how should we
do this assignment?</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>Well, probabilities are meant to
describe your beliefs about</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>which sets are more likely to
occur versus other sets.</text_slice>
            </slice>
            <slice>
              <time_slice>23:52</time_slice>
              <text_slice>So there's many ways that
you can assign those</text_slice>
            </slice>
            <slice>
              <time_slice>23:55</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>But there are some ground
rules for this game.</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>First, we want probabilities to
be numbers between zero and</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>one because that's the
usual convention.</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>So a probability of zero means
we're certain that something</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>is not going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>Probability of one means that
we're essentially certain that</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>something's going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>24:14</time_slice>
              <text_slice>So we want numbers between
zero and one.</text_slice>
            </slice>
            <slice>
              <time_slice>24:17</time_slice>
              <text_slice>We also want a few
other things.</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>And those few other things are
going to be encapsulated in a</text_slice>
            </slice>
            <slice>
              <time_slice>24:23</time_slice>
              <text_slice>set of axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>What "axioms" means in this
context, it's the ground rules</text_slice>
            </slice>
            <slice>
              <time_slice>24:29</time_slice>
              <text_slice>that any legitimate
probabilistic</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>model should obey.</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>You have a choice of what kind
of probabilities you use.</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>But, no matter what you use,
they should obey certain</text_slice>
            </slice>
            <slice>
              <time_slice>24:40</time_slice>
              <text_slice>consistency properties because
if they obey those properties,</text_slice>
            </slice>
            <slice>
              <time_slice>24:44</time_slice>
              <text_slice>then you can go ahead and do
useful calculations and do</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>some useful reasoning.</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>So what are these properties?</text_slice>
            </slice>
            <slice>
              <time_slice>24:51</time_slice>
              <text_slice>First, probabilities should
be non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>OK?</text_slice>
            </slice>
            <slice>
              <time_slice>24:56</time_slice>
              <text_slice>That's our convention.</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>We want probabilities to be
numbers between zero and one.</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>So they should certainly
be non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>25:02</time_slice>
              <text_slice>The probability that event
A occurs should be a</text_slice>
            </slice>
            <slice>
              <time_slice>25:04</time_slice>
              <text_slice>non-negative number.</text_slice>
            </slice>
            <slice>
              <time_slice>25:06</time_slice>
              <text_slice>What's the second axiom?</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>The probability of the entire
sample space is equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>Why does this make sense?</text_slice>
            </slice>
            <slice>
              <time_slice>25:15</time_slice>
              <text_slice>Well, the outcome is certain to
be an element of the sample</text_slice>
            </slice>
            <slice>
              <time_slice>25:20</time_slice>
              <text_slice>space because we set up a
sample space, which is</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>collectively exhaustive.</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>No matter what the outcome is,
it's going to be an element of</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>25:29</time_slice>
              <text_slice>We're certain that event omega
is going to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>Therefore, we represent this
certainty by saying that the</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>probability of omega
is equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>Pretty straightforward so far.</text_slice>
            </slice>
            <slice>
              <time_slice>25:47</time_slice>
              <text_slice>The more interesting axiom
is the third rule.</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>Before getting into it,
just a quick reminder.</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>If you have two sets, A and B,
the intersection of A and B</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>consists of those elements that
belong both to A and B.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>And we denote it this way.</text_slice>
            </slice>
            <slice>
              <time_slice>26:09</time_slice>
              <text_slice>When you think
probabilistically, the way to</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>think of intersection is by
using the word "and." This</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>event, this intersection, is the
event that A occurred and</text_slice>
            </slice>
            <slice>
              <time_slice>26:21</time_slice>
              <text_slice>B occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>If I get an outcome inside here,
A has occurred and B has</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>occurred at the same time.</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>So you may find the word "and"
to be a little more convenient</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>than the word "intersection."</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>And similarly, we have some
notation for the union of two</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>events, which we
write this way.</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>The union of two sets, or two
events, is the collection of</text_slice>
            </slice>
            <slice>
              <time_slice>26:46</time_slice>
              <text_slice>all the elements that belong
either to the first set, or to</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>the second, or to both.</text_slice>
            </slice>
            <slice>
              <time_slice>26:51</time_slice>
              <text_slice>When you talk about events, you
can use the word "or." So</text_slice>
            </slice>
            <slice>
              <time_slice>26:55</time_slice>
              <text_slice>this is the event that A
occurred or B occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>And this "or" means that it
could also be that both of</text_slice>
            </slice>
            <slice>
              <time_slice>27:03</time_slice>
              <text_slice>them occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>27:08</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>So now that we have this
notation, what does</text_slice>
            </slice>
            <slice>
              <time_slice>27:11</time_slice>
              <text_slice>the third axiom say?</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>The third axiom says that if we
have two events, A and B,</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>that have no common elements--</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>so here's A, here's B,
and perhaps this is</text_slice>
            </slice>
            <slice>
              <time_slice>27:29</time_slice>
              <text_slice>our big sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>The two events have no
common elements.</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>So the intersection of the two
events is the empty set.</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>There's nothing in their
intersection.</text_slice>
            </slice>
            <slice>
              <time_slice>27:38</time_slice>
              <text_slice>Then, the total probability of
A together with B has to be</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>equal to the sum of the
individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>27:46</time_slice>
              <text_slice>So the probability that A occurs
or B occurs is equal to</text_slice>
            </slice>
            <slice>
              <time_slice>27:50</time_slice>
              <text_slice>the probability that
A occurs plus the</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>probability that B occurs.</text_slice>
            </slice>
            <slice>
              <time_slice>27:55</time_slice>
              <text_slice>So think of probability
as being cream cheese.</text_slice>
            </slice>
            <slice>
              <time_slice>27:58</time_slice>
              <text_slice>You have one pound of cream
cheese, the total probability</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>assigned to the entire
sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>And that cream cheese is spread
out over this set.</text_slice>
            </slice>
            <slice>
              <time_slice>28:12</time_slice>
              <text_slice>The probability of A is how much
cream cheese sits on top</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>of A. Probability of B is how
much sits on top of B. The</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>probability of A union B is
the total amount of cream</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>cheese sitting on top of this
and that, which is obviously</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>the sum of how much is
sitting here and how</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>much is sitting there.</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>So probabilities behave
like cream cheese, or</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>they behave like mass.</text_slice>
            </slice>
            <slice>
              <time_slice>28:38</time_slice>
              <text_slice>For example, if you think of
some material object, the mass</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>of this set consisting of two
pieces is obviously the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>the two masses.</text_slice>
            </slice>
            <slice>
              <time_slice>28:53</time_slice>
              <text_slice>So this property is a
very intuitive one.</text_slice>
            </slice>
            <slice>
              <time_slice>28:55</time_slice>
              <text_slice>It's a pretty natural
one to have.</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>29:00</time_slice>
              <text_slice>Are these axioms enough for
what we want to do?</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>I mentioned a while ago that
we want probabilities to be</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>numbers between zero and one.</text_slice>
            </slice>
            <slice>
              <time_slice>29:10</time_slice>
              <text_slice>Here's an axiom that tells you
that probabilities are</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>29:13</time_slice>
              <text_slice>Should we have another axiom
that tells us that</text_slice>
            </slice>
            <slice>
              <time_slice>29:17</time_slice>
              <text_slice>probabilities are less
than or equal to one?</text_slice>
            </slice>
            <slice>
              <time_slice>29:21</time_slice>
              <text_slice>It's a desirable property.</text_slice>
            </slice>
            <slice>
              <time_slice>29:23</time_slice>
              <text_slice>We would like to have
it in our hands.</text_slice>
            </slice>
            <slice>
              <time_slice>29:26</time_slice>
              <text_slice>OK, why is it not
in that list?</text_slice>
            </slice>
            <slice>
              <time_slice>29:29</time_slice>
              <text_slice>Well, the people who are in the
axiom making business are</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>mathematicians and
mathematicians tend to be</text_slice>
            </slice>
            <slice>
              <time_slice>29:35</time_slice>
              <text_slice>pretty laconic.</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>You don't say something if
you don't have to say it.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>And this is the case here.</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>We don't need that extra axiom
because we can derive it from</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>the existing axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>Here's how it goes.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>One is the probability over
the entire sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>29:55</time_slice>
              <text_slice>Here we're using the
second axiom.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>Now the sample space consists
of A together with the</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>complement of A. OK?</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>When I write the complement of
A, I mean the complement of A</text_slice>
            </slice>
            <slice>
              <time_slice>30:14</time_slice>
              <text_slice>inside of the set omega.</text_slice>
            </slice>
            <slice>
              <time_slice>30:16</time_slice>
              <text_slice>So we have omega, here's A,
here's the complement of A,</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>and the overall set is omega.</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:25</time_slice>
              <text_slice>Now, what's the next step?</text_slice>
            </slice>
            <slice>
              <time_slice>30:27</time_slice>
              <text_slice>What should I do next?</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>Which axiom should I use?</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>We use axiom three because a set
and the complement of that</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>set are disjoint.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>They don't have any
common elements.</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>So axiom three applies and
tells me that this is the</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>probability of A plus the
probability of A complement.</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>In particular, the probability
of A is equal to one minus the</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>probability of A complement,
and this is less</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>than or equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>Because probabilities
are non-negative,</text_slice>
            </slice>
            <slice>
              <time_slice>31:06</time_slice>
              <text_slice>by the first axiom.</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>So we got the conclusion
that we wanted.</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>Probabilities are always less
than or equal to one, and this</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>is a simple consequence of the
three axioms that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>31:20</time_slice>
              <text_slice>This is a really nice argument
because it actually uses each</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>one of those axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>31:26</time_slice>
              <text_slice>The argument is simple, but you
have to use all of these</text_slice>
            </slice>
            <slice>
              <time_slice>31:29</time_slice>
              <text_slice>three properties to get the
conclusion that you want.</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>So we can get interesting things
out of our axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>Can we get some more
interesting ones?</text_slice>
            </slice>
            <slice>
              <time_slice>31:40</time_slice>
              <text_slice>How about the union
of three sets?</text_slice>
            </slice>
            <slice>
              <time_slice>31:44</time_slice>
              <text_slice>What kind of probability
should it have?</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>So here's an event consisting
of three pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>31:52</time_slice>
              <text_slice>And I want to say something
about the probability of A</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>union B union C. What I would
like to say is that this</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>probability is equal to the sum
of the three individual</text_slice>
            </slice>
            <slice>
              <time_slice>32:05</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>How can I do it?</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>I have an axiom that
tells me that I can</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>do it for two events.</text_slice>
            </slice>
            <slice>
              <time_slice>32:12</time_slice>
              <text_slice>I don't have an axiom
for three events.</text_slice>
            </slice>
            <slice>
              <time_slice>32:15</time_slice>
              <text_slice>Well, maybe I can manage things
and still be able to</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>use that axiom.</text_slice>
            </slice>
            <slice>
              <time_slice>32:20</time_slice>
              <text_slice>And here's the trick.</text_slice>
            </slice>
            <slice>
              <time_slice>32:22</time_slice>
              <text_slice>The union of three sets, you can
think of it as forming the</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>union of the first two sets and
then taking the union with</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>the third set.</text_slice>
            </slice>
            <slice>
              <time_slice>32:35</time_slice>
              <text_slice>OK?</text_slice>
            </slice>
            <slice>
              <time_slice>32:36</time_slice>
              <text_slice>So taking unions, you can
take the unions in any</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>order that you want.</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>So here we have the
union of two sets.</text_slice>
            </slice>
            <slice>
              <time_slice>32:44</time_slice>
              <text_slice>Now, ABC are disjoint,
by assumption or</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>that's how I drew it.</text_slice>
            </slice>
            <slice>
              <time_slice>32:51</time_slice>
              <text_slice>So if A, B, and C are disjoint,
then A union B is</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>disjoint from C. So here
we have the union of</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>two disjoint sets.</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>So by the additivity axiom, the
probability of that the</text_slice>
            </slice>
            <slice>
              <time_slice>33:05</time_slice>
              <text_slice>union is going to be the
probability of the first set</text_slice>
            </slice>
            <slice>
              <time_slice>33:08</time_slice>
              <text_slice>plus the probability
of the second set.</text_slice>
            </slice>
            <slice>
              <time_slice>33:12</time_slice>
              <text_slice>And now I can use the additivity
axiom once more to</text_slice>
            </slice>
            <slice>
              <time_slice>33:15</time_slice>
              <text_slice>write that this is probability
of A plus probability of B</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>plus probability of C. So by
using this axiom which was</text_slice>
            </slice>
            <slice>
              <time_slice>33:25</time_slice>
              <text_slice>stated for two sets, we can
actually derive a similar</text_slice>
            </slice>
            <slice>
              <time_slice>33:28</time_slice>
              <text_slice>property for the union of
three disjoint sets.</text_slice>
            </slice>
            <slice>
              <time_slice>33:32</time_slice>
              <text_slice>And then you can repeat
this argument as many</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>times as you want.</text_slice>
            </slice>
            <slice>
              <time_slice>33:35</time_slice>
              <text_slice>It's valid for the union of
ten disjoint sets, for the</text_slice>
            </slice>
            <slice>
              <time_slice>33:39</time_slice>
              <text_slice>union of a hundred disjoint
sets, for the union of any</text_slice>
            </slice>
            <slice>
              <time_slice>33:42</time_slice>
              <text_slice>finite number of sets.</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>So if A1 up to An are disjoint,
then the probability</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>of A1 union An is equal to the
sum of the probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>33:59</time_slice>
              <text_slice>the individual sets.</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>Special case of this
is when we're</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>dealing with finite sets.</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>Suppose I have just a finite
set of outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>I put them together in a set
and I'm interested in the</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>probability of that set.</text_slice>
            </slice>
            <slice>
              <time_slice>34:19</time_slice>
              <text_slice>So here's our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>There's lots of outcomes, but
I'm taking a few of these and</text_slice>
            </slice>
            <slice>
              <time_slice>34:26</time_slice>
              <text_slice>I form a set out of them.</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>This is a set consisting
of, in this</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>picture, three elements.</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>In general, it consists
of k elements.</text_slice>
            </slice>
            <slice>
              <time_slice>34:38</time_slice>
              <text_slice>Now, a finite set, I can write
it as a union of single</text_slice>
            </slice>
            <slice>
              <time_slice>34:43</time_slice>
              <text_slice>element sets.</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>So this set here is the union
of this one element set,</text_slice>
            </slice>
            <slice>
              <time_slice>34:49</time_slice>
              <text_slice>together with this one element
set together with that one</text_slice>
            </slice>
            <slice>
              <time_slice>34:52</time_slice>
              <text_slice>element set.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>So the total probability of this
set is going to be the</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>sum of the probabilities of
the one element sets.</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>Now, probability of a one
element set, you need to use</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>the brackets here because
probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>are assigned to sets.</text_slice>
            </slice>
            <slice>
              <time_slice>35:12</time_slice>
              <text_slice>But this gets kind of tedious,
so here one abuses notation a</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>little bit and we get rid of
those brackets and just write</text_slice>
            </slice>
            <slice>
              <time_slice>35:19</time_slice>
              <text_slice>probability of this single,
individual outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>In any case, conclusion from
this exercise is that the</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>total probability of a finite
collection of possible</text_slice>
            </slice>
            <slice>
              <time_slice>35:33</time_slice>
              <text_slice>outcomes, the total probability
is equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>sum of the probabilities
of individual elements.</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>So these are basically the
axioms of probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>Or, well, they're almost
the axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>35:49</time_slice>
              <text_slice>There are some subtleties
that are involved here.</text_slice>
            </slice>
            <slice>
              <time_slice>35:53</time_slice>
              <text_slice>One subtlety is that this axiom
here doesn't quite do</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>the job for everything
we would like to do.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>And we're going to come
back to this at</text_slice>
            </slice>
            <slice>
              <time_slice>36:03</time_slice>
              <text_slice>the end of the lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>36:05</time_slice>
              <text_slice>A second subtlety has to
do with weird sets.</text_slice>
            </slice>
            <slice>
              <time_slice>36:10</time_slice>
              <text_slice>We said that an event is a
subset of the sample space and</text_slice>
            </slice>
            <slice>
              <time_slice>36:13</time_slice>
              <text_slice>we assign probabilities
to events.</text_slice>
            </slice>
            <slice>
              <time_slice>36:16</time_slice>
              <text_slice>Does this mean that we are going
to assign probability to</text_slice>
            </slice>
            <slice>
              <time_slice>36:19</time_slice>
              <text_slice>every possible subset
of the sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>36:23</time_slice>
              <text_slice>Ideally, we would
wish to do that.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>Unfortunately, this is
not always possible.</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>If you take a sample space, such
as the square, the square</text_slice>
            </slice>
            <slice>
              <time_slice>36:35</time_slice>
              <text_slice>has nice subsets, those that you
can describe by cutting it</text_slice>
            </slice>
            <slice>
              <time_slice>36:38</time_slice>
              <text_slice>with lines and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>But it does have some very ugly
subsets, as well, that</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>are impossible to visualize,
impossible to imagine, but</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>they do exist.</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>And those very weird sets are
such that there's no way to</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>assign probabilities to them
in a way that's consistent</text_slice>
            </slice>
            <slice>
              <time_slice>36:56</time_slice>
              <text_slice>with the axioms of
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>36:58</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:59</time_slice>
              <text_slice>So this is a very, very fine
point that you can immediately</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>forget for the rest
of this class.</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>You will only encounter these
sets if you end up doing</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>doctoral work on the theoretical
aspects of</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>So it's just a mathematical
subtlety that some very weird</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>sets do not have probabilities
assigned to them.</text_slice>
            </slice>
            <slice>
              <time_slice>37:22</time_slice>
              <text_slice>But we're not going to encounter
these sets and they</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>do not show up in any
applications.</text_slice>
            </slice>
            <slice>
              <time_slice>37:29</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:29</time_slice>
              <text_slice>So now let's revisit
our examples.</text_slice>
            </slice>
            <slice>
              <time_slice>37:32</time_slice>
              <text_slice>Let's go back to the
die example.</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>We have our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>Now we need to assign
a probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>There's lots of possible
probability laws</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>that you can assign.</text_slice>
            </slice>
            <slice>
              <time_slice>37:44</time_slice>
              <text_slice>I'm picking one here,
arbitrarily, in which I say</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>that every possible outcome
has the same</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>probability of 1/16.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>Why do I make this model?</text_slice>
            </slice>
            <slice>
              <time_slice>37:58</time_slice>
              <text_slice>Well, empirically, if you have
well-manufactured dice, they</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>tend to behave that way.</text_slice>
            </slice>
            <slice>
              <time_slice>38:04</time_slice>
              <text_slice>We will be coming back
to this kind of story</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>later in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>But I'm not saying that this
is the only probability law</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>that there can be.</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>You might have weird dice in
which certain outcomes are</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>more likely than others.</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>But to keep things simple, let's
take every outcome to</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>have the same probability
of 1/16.</text_slice>
            </slice>
            <slice>
              <time_slice>38:24</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>Now that we have in our hands
a sample space and the</text_slice>
            </slice>
            <slice>
              <time_slice>38:29</time_slice>
              <text_slice>probability law, we can
actually solve any</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>problem there is.</text_slice>
            </slice>
            <slice>
              <time_slice>38:33</time_slice>
              <text_slice>We can answer any question that
could be posed to us.</text_slice>
            </slice>
            <slice>
              <time_slice>38:36</time_slice>
              <text_slice>For example, what's the
probability that the outcome,</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>which is this pair, is
either 1,1 or 1,2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>We're talking here about this
particular event, 1,1 or 1,2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:50</time_slice>
              <text_slice>So it's an event consisting
of these two items.</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>According to what we were just
discussing, the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>a finite collection of outcomes
is the sum of their</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>Each one of them has probability
of 1/16, so the</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>probability of this is 2/16.</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>How about the probability of the
event that x is equal to</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>one. x is the first roll, so
that's the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>the first roll is
equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>39:18</time_slice>
              <text_slice>Notice the syntax that's
being used here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>Probabilities are assigned to
subsets, to sets, so we think</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>of this as meaning the set of
all outcomes such that x is</text_slice>
            </slice>
            <slice>
              <time_slice>39:32</time_slice>
              <text_slice>equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>39:33</time_slice>
              <text_slice>How do you answer
this question?</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>You go back to the picture and
you try to visualize or</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>identify this event
of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>39:40</time_slice>
              <text_slice>x is equal to one corresponds
to this event here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:45</time_slice>
              <text_slice>These are all the outcomes at
which x is equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>39:48</time_slice>
              <text_slice>There's four outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>Each one has probability 1/16,
so the answer is 4/16.</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>39:57</time_slice>
              <text_slice>How about the probability
that x plus y is odd?</text_slice>
            </slice>
            <slice>
              <time_slice>40:06</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>That will take a little
bit more work.</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>But you go to the sample space
and you identify all the</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>outcomes at which the sum
is an odd number.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>So that's a place where the sum
is odd, these are other</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>places, and I guess that
exhausts all the possible</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>outcomes at which we
have an odd sum.</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>We count them.</text_slice>
            </slice>
            <slice>
              <time_slice>40:32</time_slice>
              <text_slice>How many are there?</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>There's a total of
eight of them.</text_slice>
            </slice>
            <slice>
              <time_slice>40:35</time_slice>
              <text_slice>Each one has probability 1/16,
total probability is 8/16.</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>And harder question.</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>What is the probability that the
minimum of the two rolls</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>is equal to 2?</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>This is something that you
probably couldn't do in your</text_slice>
            </slice>
            <slice>
              <time_slice>40:48</time_slice>
              <text_slice>head without the help
of a diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>But once you have a diagram,
things are simple.</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>You ask the question.</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>OK, this is an event, that the
minimum of the two rolls is</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>equal to two.</text_slice>
            </slice>
            <slice>
              <time_slice>41:01</time_slice>
              <text_slice>This can happen in
several ways.</text_slice>
            </slice>
            <slice>
              <time_slice>41:03</time_slice>
              <text_slice>What are the several ways
that it can happen?</text_slice>
            </slice>
            <slice>
              <time_slice>41:05</time_slice>
              <text_slice>Go to the diagram and try
to identify them.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>So the minimum is equal to two
if both of them are two's.</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>Or it could be that x is two and
y is bigger, or y is two</text_slice>
            </slice>
            <slice>
              <time_slice>41:18</time_slice>
              <text_slice>and x is bigger.</text_slice>
            </slice>
            <slice>
              <time_slice>41:21</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>I guess we rediscover that
yellow and blue make green, so</text_slice>
            </slice>
            <slice>
              <time_slice>41:29</time_slice>
              <text_slice>we see here that there's
a total of</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>five possible outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>41:34</time_slice>
              <text_slice>The probability of this
event is 5/16.</text_slice>
            </slice>
            <slice>
              <time_slice>41:41</time_slice>
              <text_slice>Simple example, but the
procedure that we followed in</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>this example actually applies
to any probability model you</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>might ever encounter.</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>You set up your sample space,
you make a statement that</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>describes the probability law
over that sample space, then</text_slice>
            </slice>
            <slice>
              <time_slice>42:00</time_slice>
              <text_slice>somebody asks you questions
about various events.</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>You go to your pictures,
identify those events, pin</text_slice>
            </slice>
            <slice>
              <time_slice>42:07</time_slice>
              <text_slice>them down, and then start kind
of counting and calculating</text_slice>
            </slice>
            <slice>
              <time_slice>42:11</time_slice>
              <text_slice>the total probability for those
outcomes that you're</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>considering.</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>This example is a special case
of what is called the discrete</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>uniform law.</text_slice>
            </slice>
            <slice>
              <time_slice>42:22</time_slice>
              <text_slice>The model obeys the discrete
uniform law if all outcomes</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>42:28</time_slice>
              <text_slice>It doesn't have to
be that way.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>That's just one example
of a probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>42:33</time_slice>
              <text_slice>But when things are that way,
if all outcomes are equally</text_slice>
            </slice>
            <slice>
              <time_slice>42:36</time_slice>
              <text_slice>likely and we have N of them,
and you have a set A that has</text_slice>
            </slice>
            <slice>
              <time_slice>42:45</time_slice>
              <text_slice>little n elements, then each
one of those elements has</text_slice>
            </slice>
            <slice>
              <time_slice>42:51</time_slice>
              <text_slice>probability one over
capital N since all</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>outcomes are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>And for our probabilities to add
up to one, each one must</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>have this much probability, and
there's little n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>That gives you the probability
of the event of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>So problems like the one in the
previous slide and more</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>generally of the type described
here under discrete</text_slice>
            </slice>
            <slice>
              <time_slice>43:11</time_slice>
              <text_slice>uniform law, these problems
reduce to just counting.</text_slice>
            </slice>
            <slice>
              <time_slice>43:15</time_slice>
              <text_slice>How many elements are there
in my sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>How many elements are there
inside the event of interest?</text_slice>
            </slice>
            <slice>
              <time_slice>43:21</time_slice>
              <text_slice>Counting is generally simple,
but for some problems it gets</text_slice>
            </slice>
            <slice>
              <time_slice>43:24</time_slice>
              <text_slice>pretty complicated.</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>And in a couple of weeks, we're
going to have to spend</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>the whole lecture just on the
subject of how to count</text_slice>
            </slice>
            <slice>
              <time_slice>43:31</time_slice>
              <text_slice>systematically.</text_slice>
            </slice>
            <slice>
              <time_slice>43:33</time_slice>
              <text_slice>Now the procedure we followed in
the previous example is the</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>same as the procedure you would
follow in continuous</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>probability problems.</text_slice>
            </slice>
            <slice>
              <time_slice>43:41</time_slice>
              <text_slice>So, going back to our dart
problem, we get the random</text_slice>
            </slice>
            <slice>
              <time_slice>43:44</time_slice>
              <text_slice>point inside the square.</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>That's our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>We need to assign a
probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>For lack of imagination, I'm
taking the probability law to</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>be the area of a subset.</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>So if we have two subsets of
the sample space that have</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>equal areas, then I'm
postulating that they are</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>equally likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>44:06</time_slice>
              <text_slice>The probably that they fall
here is the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>probability that they
fall there.</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>The model doesn't have
to be that way.</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>But if I have sort of complete
ignorance of which points are</text_slice>
            </slice>
            <slice>
              <time_slice>44:16</time_slice>
              <text_slice>more likely than others,
that might be the</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>reasonable model to use.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>So equal areas mean equal
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>If the area is twice as large,
the probability is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>44:27</time_slice>
              <text_slice>twice as big.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>So this is our model.</text_slice>
            </slice>
            <slice>
              <time_slice>44:32</time_slice>
              <text_slice>We can now answer questions.</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>Let's answer the easy one.</text_slice>
            </slice>
            <slice>
              <time_slice>44:35</time_slice>
              <text_slice>What's the probability
that the outcome is</text_slice>
            </slice>
            <slice>
              <time_slice>44:38</time_slice>
              <text_slice>exactly this point?</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>That of course is zero because
a single point has zero area.</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>And since this probability is
equal to area, that's zero</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>44:51</time_slice>
              <text_slice>How about the probability that
the sum of the coordinates of</text_slice>
            </slice>
            <slice>
              <time_slice>44:55</time_slice>
              <text_slice>the point that we got is less
than or equal to 1/2?</text_slice>
            </slice>
            <slice>
              <time_slice>45:00</time_slice>
              <text_slice>How do you deal with it?</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>Well, you look at the picture
again, at your sample space,</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>and try to describe the event
that you're talking about.</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>The sum being less than 1/2
corresponds to getting an</text_slice>
            </slice>
            <slice>
              <time_slice>45:12</time_slice>
              <text_slice>outcome that's below this line,
where this line is the</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>line where x plus
y equals to 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>So the intercepts of that line
with the axis are 1/2 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>So you describe the event
visually and then you use your</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>45:30</time_slice>
              <text_slice>The probability law that we have
is that the probability</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>of a set is equal to the
area of that set.</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>So all we need to find is the
area of this triangle, which</text_slice>
            </slice>
            <slice>
              <time_slice>45:39</time_slice>
              <text_slice>is 1/2 times 1/2 times 1/2,
half, equals to 1/8.</text_slice>
            </slice>
            <slice>
              <time_slice>45:48</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>45:49</time_slice>
              <text_slice>Moral from these two examples is
that it's always useful to</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>have a picture and work with
a picture to visualize the</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>events that you're
talking about.</text_slice>
            </slice>
            <slice>
              <time_slice>45:58</time_slice>
              <text_slice>And once you have a probability
law in your hands,</text_slice>
            </slice>
            <slice>
              <time_slice>46:01</time_slice>
              <text_slice>then it's a matter of
calculation to find the</text_slice>
            </slice>
            <slice>
              <time_slice>46:04</time_slice>
              <text_slice>probabilities of an
event of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>46:06</time_slice>
              <text_slice>The calculations we did in these
two examples, of course,</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>were very simple.</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>Sometimes calculations may be
a lot harder, but it's a</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>different business.</text_slice>
            </slice>
            <slice>
              <time_slice>46:15</time_slice>
              <text_slice>It's a business of calculus, for
example, or being good in</text_slice>
            </slice>
            <slice>
              <time_slice>46:19</time_slice>
              <text_slice>algebra and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>As far as probability is
concerned, it's clear what you</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>will be doing, and then maybe
you're faced with a harder</text_slice>
            </slice>
            <slice>
              <time_slice>46:27</time_slice>
              <text_slice>algebraic part to actually carry
out the calculations.</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>The area of a triangle
is easy to compute.</text_slice>
            </slice>
            <slice>
              <time_slice>46:32</time_slice>
              <text_slice>If I had put down a very
complicated shape, then you</text_slice>
            </slice>
            <slice>
              <time_slice>46:36</time_slice>
              <text_slice>might need to solve a hard
integration problem to find</text_slice>
            </slice>
            <slice>
              <time_slice>46:39</time_slice>
              <text_slice>the area of that shape, but
that's stuff that belongs to</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>another class that you have
presumably mastered by now.</text_slice>
            </slice>
            <slice>
              <time_slice>46:46</time_slice>
              <text_slice>Good, OK.</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>So now let me spend just a
couple of minutes to return to</text_slice>
            </slice>
            <slice>
              <time_slice>46:49</time_slice>
              <text_slice>a point that I raised before.</text_slice>
            </slice>
            <slice>
              <time_slice>46:52</time_slice>
              <text_slice>I was saying that the axiom that
we had about additivity</text_slice>
            </slice>
            <slice>
              <time_slice>46:56</time_slice>
              <text_slice>might not quite be enough.</text_slice>
            </slice>
            <slice>
              <time_slice>46:58</time_slice>
              <text_slice>Let's illustrate what I mean
by the following example.</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>Think of the experiment where
you keep flipping a coin and</text_slice>
            </slice>
            <slice>
              <time_slice>47:04</time_slice>
              <text_slice>you wait until you obtain heads
for the first time.</text_slice>
            </slice>
            <slice>
              <time_slice>47:08</time_slice>
              <text_slice>What's the sample space
of this experiment?</text_slice>
            </slice>
            <slice>
              <time_slice>47:11</time_slice>
              <text_slice>It might happen the first flip,
it might happen in the</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>tenth flip.</text_slice>
            </slice>
            <slice>
              <time_slice>47:14</time_slice>
              <text_slice>Heads for the first time might
occur in the millionth flip.</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>So the outcome of this
experiment is going to be an</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>integer and there's no bound
to that integer.</text_slice>
            </slice>
            <slice>
              <time_slice>47:23</time_slice>
              <text_slice>You might have to wait very
much until that happens.</text_slice>
            </slice>
            <slice>
              <time_slice>47:26</time_slice>
              <text_slice>So the natural sample
space is the set of</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>all possible integers.</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>Somebody tells you some
information about the</text_slice>
            </slice>
            <slice>
              <time_slice>47:35</time_slice>
              <text_slice>probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>The probability that you have
to wait for n flips is equal</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>to two to the minus n.</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>Where did this come from?</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>That's a separate story.</text_slice>
            </slice>
            <slice>
              <time_slice>47:44</time_slice>
              <text_slice>Where did it come from?</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>Somebody tells this to us, and
those probabilities are</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>plotted here as a
function of n.</text_slice>
            </slice>
            <slice>
              <time_slice>47:52</time_slice>
              <text_slice>And you're asked to find the
probability that the outcome</text_slice>
            </slice>
            <slice>
              <time_slice>47:54</time_slice>
              <text_slice>is an even number.</text_slice>
            </slice>
            <slice>
              <time_slice>47:56</time_slice>
              <text_slice>How do you go about calculating
that probability?</text_slice>
            </slice>
            <slice>
              <time_slice>47:59</time_slice>
              <text_slice>So the probability of being an
even number is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>48:02</time_slice>
              <text_slice>of the subset that consists
of just the even numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>So it would be a subset of this
kind, that includes two,</text_slice>
            </slice>
            <slice>
              <time_slice>48:11</time_slice>
              <text_slice>four, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>48:13</time_slice>
              <text_slice>So any reasonable person would
say, well the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>48:18</time_slice>
              <text_slice>obtaining an outcome that's
either two or four or six and</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>so on is equal to the
probability of obtaining a</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>two, plus the probability of
obtaining a four, plus the</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>probability of obtaining
a six, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>These probabilities
are given to us.</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>So here I have to
do my algebra.</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>I add this geometric series and
I get an answer of 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>That's what any reasonable
person would do.</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>But the person who only knows
the axioms that they posted</text_slice>
            </slice>
            <slice>
              <time_slice>48:48</time_slice>
              <text_slice>just a little earlier
may get stuck.</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>They would get stuck
at this point.</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>How do we justify this?</text_slice>
            </slice>
            <slice>
              <time_slice>48:59</time_slice>
              <text_slice>We had this property for the
union of disjoint sets and the</text_slice>
            </slice>
            <slice>
              <time_slice>49:04</time_slice>
              <text_slice>corresponding property that
tells us that the total</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>probability of finitely many
things, outcomes, is the sum</text_slice>
            </slice>
            <slice>
              <time_slice>49:11</time_slice>
              <text_slice>of their individual
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>49:13</time_slice>
              <text_slice>But here we're using it on
an infinite collection.</text_slice>
            </slice>
            <slice>
              <time_slice>49:17</time_slice>
              <text_slice>The probability of infinitely
many points is equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>49:23</time_slice>
              <text_slice>sum of the probabilities
of each one of these.</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>To justify this step we need
to introduce one additional</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>rule, an additional axiom, that
tells us that this step</text_slice>
            </slice>
            <slice>
              <time_slice>49:34</time_slice>
              <text_slice>is actually legitimate.</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>And this is the countable
additivity axiom, which is a</text_slice>
            </slice>
            <slice>
              <time_slice>49:39</time_slice>
              <text_slice>little stronger, or quite
a bit stronger, than the</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>additivity axiom
we had before.</text_slice>
            </slice>
            <slice>
              <time_slice>49:45</time_slice>
              <text_slice>It tells us that if we have a
sequence of sets that are</text_slice>
            </slice>
            <slice>
              <time_slice>49:49</time_slice>
              <text_slice>disjoint and we want to find
their total probability, then</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>we are allowed to add their
individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>49:58</time_slice>
              <text_slice>So the picture might
be such as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>50:01</time_slice>
              <text_slice>We have a sequence of sets,
A1, A2, A3, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>50:07</time_slice>
              <text_slice>I guess in order to fit them
inside the sample space, the</text_slice>
            </slice>
            <slice>
              <time_slice>50:10</time_slice>
              <text_slice>sets need to get smaller
and smaller perhaps.</text_slice>
            </slice>
            <slice>
              <time_slice>50:13</time_slice>
              <text_slice>They are disjoint.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>We have a sequence
of such sets.</text_slice>
            </slice>
            <slice>
              <time_slice>50:17</time_slice>
              <text_slice>The total probability of falling
anywhere inside one of</text_slice>
            </slice>
            <slice>
              <time_slice>50:21</time_slice>
              <text_slice>those sets is the sum of their
individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>50:25</time_slice>
              <text_slice>A key subtlety that's involved
here is that we're talking</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>about a sequence of events.</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>By "sequence" we mean that
these events can</text_slice>
            </slice>
            <slice>
              <time_slice>50:36</time_slice>
              <text_slice>be arranged in order.</text_slice>
            </slice>
            <slice>
              <time_slice>50:38</time_slice>
              <text_slice>I can tell you the first event,
the second event, the</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>third event, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>50:43</time_slice>
              <text_slice>So if you have such a collection
of events that can</text_slice>
            </slice>
            <slice>
              <time_slice>50:46</time_slice>
              <text_slice>be ordered as first, second,
third, and so on, then you can</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>add their probabilities
to find the</text_slice>
            </slice>
            <slice>
              <time_slice>50:54</time_slice>
              <text_slice>probability of their union.</text_slice>
            </slice>
            <slice>
              <time_slice>50:55</time_slice>
              <text_slice>So this point is actually a
little more subtle than you</text_slice>
            </slice>
            <slice>
              <time_slice>50:58</time_slice>
              <text_slice>might appreciate at this point,
and I'm going to return</text_slice>
            </slice>
            <slice>
              <time_slice>51:00</time_slice>
              <text_slice>to it at the beginning
of the next lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>51:04</time_slice>
              <text_slice>For now, enjoy the first
week of classes</text_slice>
            </slice>
            <slice>
              <time_slice>51:07</time_slice>
              <text_slice>and have a good weekend.</text_slice>
            </slice>
            <slice>
              <time_slice>51:09</time_slice>
              <text_slice>Thank you.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Iterated Expectations; Sum of a Random Number of Random Variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l12/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Section means and variances (ctd.) Example
1/summationdisplay10
2 1/summationdisplay30
2 var(X)=E[var( X|Y)] + var( E[X ])Y(xi90) = 10 (xi60) = 20|
10 20i=1 i=11f (x)X
2/3
var(X|Y= 1) = 10 var( X|Y= 2) = 20
1/3
1 2 x
Y =1 Y =2  
10, w.p.1/3var(X|Y)=20, w.p.2/3
E[X |Y= 1] = E[X |Y= 2] =
[var( X| = 20 =50Y)]110 +2E333
var(X|Y= 1) = var( X|Y= 2) =
var(X)= E[var( X|Y)] + var( E[X |Y])E[X]=50= + 2003
= (average variability within sections)var(E[X |Y]) =
+ (variability between sections)
Sum of a random number of Variance of sum of a random number
independent r.v.s of independent r.v.s
N: number of stores visited var(Y)=E[var( Y|N)] + var( E[Y
(Nis a nonnegative integer r.v.)|N])
E[Y N ]=NE[X]
Xi: money spent in store i |
var(E[Y|N]) = ( E[X])2var(N)
Xiassumed i.i.d.
var(Y N =n)=n var(X)independent of N |
var(Y|N)=N var(X)
Let Y=X1++X E[var( Y|N)] = E[N] var( X)N
E[Y|N=n]= E[X1+X2++Xn|N=n]
=E[X1+X2++Xn]
=E[X]+E[X]+ +E[X] var(Y)=E[var( Y|N)] + var( E[Y1 2|N])n
=nE[X] = [ ] var( )+(2EN X E[X]) var( N)
E[Y |N]=NE[X]
E[Y]=E[E[Y |N]]
=E[NE[X]]
=E[N]E[X]
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 12 Conditional expectations
Readings: Section 4.3; Given the value yof a r.v. Y:
parts of Section 4.5E[X |Y=y]= xp
no/summationdisplay
X|Y(x y )
(mean and variance only; transforms) x|
(integral in continuous case)
Lecture outline Stick example: stick of length /lscript
break at uniformly chosen point Y
Conditional expectation break again at uniformly chosen point X
Law of iterated expectations yE[X |Y=y]= (number)2Law of total variance
Sum of a random numberY
of independent r.v.s E[X |Y]= (r.v.)2
mean, variance
Law of iterated expectations:
E[E[X |Y]]=/summationdisplay
E[X |Y=y]pY(y)=E[X]
y
In stick example:
E[X]=E[E[X |Y]] =E[Y/2] = /lscript/4
var(X|Y)and its expectation Section means and variances
Two sections:var(X|Y=)=E/bracketleftBig
(2y X E[X |Y=y]) |Y=y/bracketrightBig
y= 1 (10 students); y= 2 (20 students)
var(X|Y): a r.v.
1with
10/summationdisplay10130
value var( X|Y=y) when Y=y y=1: xi= 90 y=2: xi= 6020i=1 i=11
Law of total variance:/summationdisplay
var(X)=E[var( X|Y)] + var( E[X |Y])130+E[X]= =30i/summationdisplay 90 10 60x20
i= 7030=1
Proof:E[X |Y= 1] = 90, E[X |Y= 2] = 60
(a) Recall: var( X)=E[2X](E[X])2
90, w.p.1/3E[X Y ]=
2 2|

60, w.p.2/3(b) var( X|Y)=E[X |Y](E[X |Y])
[ [ |]] =190 +2E E X Y 60 = 70 = E[X]3 3
(c)E[var( X|Y)] =E[2X]E[(E[X |Y])2]
(d) var( E[X |Y]) = E[X |Y])2 1 2E[( ](E[X])2var(E[X |Y]) = (90 70)2+ (60 70)2
3 3
600= = 200Sum of right-hand sides of (c), (d):3
[2] ])2EX (E[X = var( X)
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-2-conditioning-and-bayes-rule/</video_url>
          <video_title>Lecture 2: Conditioning and Bayes' Rule</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Markov Chains - II (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Steady-State Probabilities Visit frequency interpretation
Do the rij(n) converge to some j?
(independent of the initial state i)
j= kpkj
Yes, if:/summationdisplay
k
recurrent states are all in a single class,
and (Long run) frequency of being in j:j
single recurrent class is not periodic
Frequency of transitions kj:kpkj
Assuming yes, start from key recursion
Frequency of transitions into j:
rij(n)= rik(n1)pkj
k/summationdisplay
/summationdisplay kpkj
k
1 jpjj  
take the limit as n 1p1j 
j=/summationdisplay
kpkj, for all j
k 2
2p2j jAdditional equation:
.  .  ..  .  . /summationdisplay
j=1
j mpmj m
Example Birth-death processes
1- p -q 1- p 1 1 1- q0 m
0.5 0.8p p0 1
0 1 2 3...m0.5q1 q q2 m
2 1 pi
i+10.2i ipi=i+1qi+1
qi+1
Special case: pi=pand qi=qfor all i
=p/q=load factor
pi+1=i=iq
i=i0,i =0,1, . . . , m
Assume p&lt;q and m
0=1
E[Xn]= (in steady-state)1358 Markov Chains Chap. 7
be viewed as the long-term expected fraction of transitions that move the state
fromjtok.
Expected Frequency of a Particular Transition
Consider ntransitions of a Markov chain with a single class which is aperi-
odic, starting from a given initial state. Let qjk(n) be the expected number
of such transitions that take the state from jtok. Then, regardless of the
initial state, we have
lim
nqjk(n)
n=jpjk.
Given the frequency interpretation of jandkpkj, the balance equation
j=m/summationdisplay
k=1kpkj
has an intuitive meaning. It expresses the fact that the expected frequency j
of visits to jis equal to the sum of the expected frequencies kpkjof transitions
that lead to j; see Fig. 7.13.
Figure 7.13: Interpretation of the balance equations in terms of frequencies. In
a very large number of transitions, we expect a fraction kpkjthat bring the state
from ktoj. (This also applies to transitions from jto itself, which occur with
frequency jpjj.) The sum of the expected frequencies of such transitions is the
expected frequency jof being at state j.
In fact, some stronger statements are also true, such as the following. Whenever
we carry out a probabilistic experiment and generate a trajectory of the Markov chain
over an innite time horizon, the observed long-term frequency with which state jis
visited will be exactly equal to j, and the observed long-term frequency of transitions
from jtokwill be exactly equal to jpjk. Even though the trajectory is random, these
equalities hold with essential certainty, that is, with probability 1.
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 17 Review
Markov Processes  IIDiscrete state, discrete time, time-homogeneous
Readings: Section 7.3 Transition probabilities pij
Markov property
Lecture outlinerij(n)= P(Xn=j|X0=i)
Review
Key recursion:
Steady-State behaviorrij(n)=
Steady-state convergence theorem/summationdisplay
rik(n
k1)pkj
Balance equations
Birth-death processes
Warmup Periodic states
9 3 4 The states in a recurrent class are
6 7periodic if they can be grouped into
1 groups so that
5 1 2d&gt; all transitions from
8one group lead to the next group
P(X1=2,X2=6,X3=7| X0= 1) =
P(X4=7| X0= 2) =
Recurrent and transient states
State iisrecurrent if: 9
starting from i,5
and from wherever you can go,4
there is a way of returning to i 8 6
3If not recurrent, called transient
Recurrent class : 7 2 1
collection of recurrent states that
communicate to each otherand to no other state
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-3-independence/</video_url>
          <video_title>Lecture 3: Independence</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:23</time_slice>
              <text_slice>PROFESSOR: Let us start.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>So as always, we're to have
a quick review of what we</text_slice>
            </slice>
            <slice>
              <time_slice>0:30</time_slice>
              <text_slice>discussed last time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:31</time_slice>
              <text_slice>And then today we're going
to introduce just one new</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>concept, the notion of
independence of two events.</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>And we will play with
that concept.</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>So what did we talk
about last time?</text_slice>
            </slice>
            <slice>
              <time_slice>0:43</time_slice>
              <text_slice>The idea is that we have an
experiment, and the experiment</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>has a sample space omega.</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>And then somebody comes and
tells us you know the outcome</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>of the experiments happens to
lie inside this particular</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>event B. Given this information,
it kind of</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>changes what we know about
the situation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>It tells us that the outcome
is going to be somewhere</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>So this is essentially
our new sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>And now we need to we reassign
probabilities to the various</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>possible outcomes, because, for
example, these outcomes,</text_slice>
            </slice>
            <slice>
              <time_slice>1:16</time_slice>
              <text_slice>even if they had positive
probability beforehand, now</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>that we're told that B occurred,
those outcomes out</text_slice>
            </slice>
            <slice>
              <time_slice>1:22</time_slice>
              <text_slice>there are going to have
zero probability.</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>So we need to revise
our probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>The new probabilities are
called conditional</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>probabilities, and they're
defined this way.</text_slice>
            </slice>
            <slice>
              <time_slice>1:33</time_slice>
              <text_slice>The conditional probability that
A occurs given that we're</text_slice>
            </slice>
            <slice>
              <time_slice>1:37</time_slice>
              <text_slice>told that B occurred is
calculated by this formula,</text_slice>
            </slice>
            <slice>
              <time_slice>1:40</time_slice>
              <text_slice>which tells us the following--</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>out of the total probability
that was initially assigned to</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>the event B, what fraction of
that probability is assigned</text_slice>
            </slice>
            <slice>
              <time_slice>1:49</time_slice>
              <text_slice>to outcomes that also
make A to happen?</text_slice>
            </slice>
            <slice>
              <time_slice>1:54</time_slice>
              <text_slice>So out of the total probability
assigned to B, we</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>see what fraction of that total
probability is assigned</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>to those elements here that
will also make A happen.</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>Conditional probabilities are
left undefined if the</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>denominator here is zero.</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>An easy consequence of the
definition is if we bring that</text_slice>
            </slice>
            <slice>
              <time_slice>2:16</time_slice>
              <text_slice>term to the other side, then we
can find the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>two things happening by taking
the probability that the first</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>thing happens, and then, given
that the first thing happened,</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>the conditional probability that
the second one happens.</text_slice>
            </slice>
            <slice>
              <time_slice>2:28</time_slice>
              <text_slice>Then we saw last time that we
can divide and conquer in</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>calculating probabilities of
mildly complicated events by</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>breaking it down into
different scenarios.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>So event B can happen
in two ways.</text_slice>
            </slice>
            <slice>
              <time_slice>2:41</time_slice>
              <text_slice>It can happen either together
with A, which is this</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>probability, or it can happen
together with A complement,</text_slice>
            </slice>
            <slice>
              <time_slice>2:48</time_slice>
              <text_slice>which is this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>So basically what we're saying
that the total probability of</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>B is the probability of this,
which is A intersection B,</text_slice>
            </slice>
            <slice>
              <time_slice>2:57</time_slice>
              <text_slice>plus the probability of that,
which is A complement</text_slice>
            </slice>
            <slice>
              <time_slice>3:02</time_slice>
              <text_slice>intersection B.</text_slice>
            </slice>
            <slice>
              <time_slice>3:07</time_slice>
              <text_slice>So these two facts here,
multiplication rule and the</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>total probability theorem, are
basic tools that one uses to</text_slice>
            </slice>
            <slice>
              <time_slice>3:14</time_slice>
              <text_slice>break down probability
calculations</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>into a simpler parts.</text_slice>
            </slice>
            <slice>
              <time_slice>3:18</time_slice>
              <text_slice>So we find probabilities of
two things happening by</text_slice>
            </slice>
            <slice>
              <time_slice>3:21</time_slice>
              <text_slice>looking at each one at a time.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>And this is what we do to break
up a situation with two</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>different possible scenarios.</text_slice>
            </slice>
            <slice>
              <time_slice>3:29</time_slice>
              <text_slice>Then we also have
the Bayes rule,</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>which does the following.</text_slice>
            </slice>
            <slice>
              <time_slice>3:33</time_slice>
              <text_slice>Given a model that has
conditional probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>this kind, the Bayes rule
allows us to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>conditional probabilities in
which the events appear in</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>different order.</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>You can think of these
probabilities as describing a</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>causal model of a certain
situation, whereas these are</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>the probabilities that you get
after you do some inference</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>based on the information that
you have available.</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>Now the Bayes rule, we derived
it, and it's a trivial</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>half-line calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>4:01</time_slice>
              <text_slice>But it underlies lots
and lots of useful</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>things in the real world.</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>We had the radar example
last time.</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>You can think of more
complicated situations in</text_slice>
            </slice>
            <slice>
              <time_slice>4:10</time_slice>
              <text_slice>which there's a bunch or lots of
different hypotheses about</text_slice>
            </slice>
            <slice>
              <time_slice>4:14</time_slice>
              <text_slice>the environment.</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>Given any particular setting in
the environment, you have a</text_slice>
            </slice>
            <slice>
              <time_slice>4:18</time_slice>
              <text_slice>measuring device that
can produce</text_slice>
            </slice>
            <slice>
              <time_slice>4:21</time_slice>
              <text_slice>many different outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>And you observe the final
outcome out of your measuring</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>device, and you're trying
to guess which</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>particular branch occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>That is, you're trying to guess
the state of the world</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>based on a particular
measurement.</text_slice>
            </slice>
            <slice>
              <time_slice>4:38</time_slice>
              <text_slice>That's what inference
is all about.</text_slice>
            </slice>
            <slice>
              <time_slice>4:40</time_slice>
              <text_slice>So real world problems only
differ from the simple example</text_slice>
            </slice>
            <slice>
              <time_slice>4:44</time_slice>
              <text_slice>that we saw last time in that
this kind of tree is a little</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>more complicated.</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>You might have infinitely
many possible</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>outcomes here and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>So setting up the model may be
more elaborate, but the basic</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>calculation that's done based
on the Bayes rule is</text_slice>
            </slice>
            <slice>
              <time_slice>5:01</time_slice>
              <text_slice>essentially the same as
the one that we saw.</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>Now something that we discuss
is that sometimes we use</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>conditional probabilities to
describe models, and let's do</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>this by looking at a
model where we toss</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>a coin three times.</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>And how do we use conditional
probabilities to</text_slice>
            </slice>
            <slice>
              <time_slice>5:19</time_slice>
              <text_slice>describe the situation?</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>So we have one experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>But that one experiment consists
of three consecutive</text_slice>
            </slice>
            <slice>
              <time_slice>5:26</time_slice>
              <text_slice>coin tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>So the possible outcomes, our
sample space, consists of</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>strings of length 3 that tell
us whether we had heads,</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>tails, and in what sequence.</text_slice>
            </slice>
            <slice>
              <time_slice>5:39</time_slice>
              <text_slice>So three heads in a row is
one particular outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>5:43</time_slice>
              <text_slice>So what is the meaning
of those labels in</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>front of the branches?</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>So this P here, of course,
stands for the probability</text_slice>
            </slice>
            <slice>
              <time_slice>5:51</time_slice>
              <text_slice>that the first toss
resulted in heads.</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>And let me use this notation
to denote that</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>the first was heads.</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>I put an H in toss one.</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>How about the meaning of
this probability here?</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>Well the meaning of this
probability is</text_slice>
            </slice>
            <slice>
              <time_slice>6:10</time_slice>
              <text_slice>a conditional one.</text_slice>
            </slice>
            <slice>
              <time_slice>6:11</time_slice>
              <text_slice>It's the conditional probability
that the second</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>toss resulted in heads,
given that the first</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>one resulted in heads.</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>And similarly this label here
corresponds to the probability</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>that the third toss resulted in
heads, given that the first</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>one and the second one
resulted in heads.</text_slice>
            </slice>
            <slice>
              <time_slice>6:35</time_slice>
              <text_slice>So in this particular model that
I wrote down here, those</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>probabilities, P, of obtaining
heads remain the same no</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>matter what happened in
the previous toss.</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>For example, even if the first
toss was tails, we still have</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>the same probability, P, that
the second one is heads, given</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>that the first one was tails.</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>So we're assuming that no matter
what happened in the</text_slice>
            </slice>
            <slice>
              <time_slice>7:01</time_slice>
              <text_slice>first toss, the second toss will
still have a conditional</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>probability equal to P. So that
conditional probability</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>does not depend on what happened
in the first toss.</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>And we will see that this is a
very special situation, and</text_slice>
            </slice>
            <slice>
              <time_slice>7:16</time_slice>
              <text_slice>that's really the concept of
independence that we are going</text_slice>
            </slice>
            <slice>
              <time_slice>7:19</time_slice>
              <text_slice>to introduce shortly.</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>But before we get to
independence, let's practice</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>once more the three skills that
we covered last time in</text_slice>
            </slice>
            <slice>
              <time_slice>7:29</time_slice>
              <text_slice>this example.</text_slice>
            </slice>
            <slice>
              <time_slice>7:30</time_slice>
              <text_slice>So first skill was
multiplication rule.</text_slice>
            </slice>
            <slice>
              <time_slice>7:33</time_slice>
              <text_slice>How do you find the
probability of</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>several things happening?</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>That is the probability that
we have tails followed by</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>heads followed by tails.</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>So here we're talking about this
particular outcome here,</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>tails followed by heads
followed by tails.</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>And the way we calculate such
a probability is by</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>multiplying conditional
probabilities along the path</text_slice>
            </slice>
            <slice>
              <time_slice>8:01</time_slice>
              <text_slice>that takes us to this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>And so these conditional
probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>8:05</time_slice>
              <text_slice>are recorded here.</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>So it's going to be (1 minus P)
times P times (1 minus P).</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>So this is the multiplication
rule.</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>Second question is how do we
find the probability of a</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>mildly complicated event?</text_slice>
            </slice>
            <slice>
              <time_slice>8:20</time_slice>
              <text_slice>So the event of interest here
that I wrote down is the</text_slice>
            </slice>
            <slice>
              <time_slice>8:23</time_slice>
              <text_slice>probability that in the
three tosses, we had a</text_slice>
            </slice>
            <slice>
              <time_slice>8:25</time_slice>
              <text_slice>total of one head.</text_slice>
            </slice>
            <slice>
              <time_slice>8:28</time_slice>
              <text_slice>Exactly one head.</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>This is an event that can
happen in multiple ways.</text_slice>
            </slice>
            <slice>
              <time_slice>8:33</time_slice>
              <text_slice>It happens here.</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>It happens here.</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>And it also happens here.</text_slice>
            </slice>
            <slice>
              <time_slice>8:41</time_slice>
              <text_slice>So we want to find the total
probability of the event</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>consisting of these
three outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>8:46</time_slice>
              <text_slice>What do we do?</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>We just add the probabilities
of each individual outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>How do we find the probability
of an individual outcome?</text_slice>
            </slice>
            <slice>
              <time_slice>8:53</time_slice>
              <text_slice>Well, that's what we just did.</text_slice>
            </slice>
            <slice>
              <time_slice>8:56</time_slice>
              <text_slice>Now notice that this outcome
has probability P times (1</text_slice>
            </slice>
            <slice>
              <time_slice>9:00</time_slice>
              <text_slice>minus P) squared.</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>That one should not be there.</text_slice>
            </slice>
            <slice>
              <time_slice>9:07</time_slice>
              <text_slice>So where is it?</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>Ah.</text_slice>
            </slice>
            <slice>
              <time_slice>9:09</time_slice>
              <text_slice>It's this one.</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>OK, so the probability of this
outcome is (1 minus P times P)</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>times (1 minus P), the
same probability.</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>And finally, this one is again
(1 minus P) squared times P.</text_slice>
            </slice>
            <slice>
              <time_slice>9:25</time_slice>
              <text_slice>So this event of one head can
happen in three ways.</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>And each one of those three ways
has the same probability</text_slice>
            </slice>
            <slice>
              <time_slice>9:32</time_slice>
              <text_slice>of occurring.</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>And this is the answer.</text_slice>
            </slice>
            <slice>
              <time_slice>9:36</time_slice>
              <text_slice>And finally, the last thing that
we learned how to do is</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>to use the Bayes rule to</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>calculate and make an inference.</text_slice>
            </slice>
            <slice>
              <time_slice>9:44</time_slice>
              <text_slice>So somebody tells you that there
was exactly one head in</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>your three tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>9:49</time_slice>
              <text_slice>What is the probability
that the first</text_slice>
            </slice>
            <slice>
              <time_slice>9:52</time_slice>
              <text_slice>toss resulted in heads?</text_slice>
            </slice>
            <slice>
              <time_slice>9:55</time_slice>
              <text_slice>OK, I guess you can guess the
answer here if I tell you that</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>there were three tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>One of them was heads.</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>Where was that head
in the first, the</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>second, or the third?</text_slice>
            </slice>
            <slice>
              <time_slice>10:07</time_slice>
              <text_slice>Well, by symmetry, they should
all be equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>10:10</time_slice>
              <text_slice>So there should be probably
just 1/3 that that head</text_slice>
            </slice>
            <slice>
              <time_slice>10:13</time_slice>
              <text_slice>occurred in the first toss.</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>Let's check our intuition
using the definitions.</text_slice>
            </slice>
            <slice>
              <time_slice>10:19</time_slice>
              <text_slice>So the definition of conditional
probability tells</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>us the conditional probability
is the probability of both</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>things happening.</text_slice>
            </slice>
            <slice>
              <time_slice>10:27</time_slice>
              <text_slice>First toss is heads, and we have
exactly one head divided</text_slice>
            </slice>
            <slice>
              <time_slice>10:33</time_slice>
              <text_slice>by the probability
of one head.</text_slice>
            </slice>
            <slice>
              <time_slice>10:40</time_slice>
              <text_slice>What is the probability that the
first toss is heads, and</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>we have exactly one head?</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>This is the same as the event
heads, tails, tails.</text_slice>
            </slice>
            <slice>
              <time_slice>10:51</time_slice>
              <text_slice>If I tell you that the first is
heads, and there's only one</text_slice>
            </slice>
            <slice>
              <time_slice>10:54</time_slice>
              <text_slice>head, it means that the
others are tails.</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>So this is the probability of
heads, tails, tails divided by</text_slice>
            </slice>
            <slice>
              <time_slice>11:03</time_slice>
              <text_slice>the probability of one head.</text_slice>
            </slice>
            <slice>
              <time_slice>11:06</time_slice>
              <text_slice>And we know all of these
quantities probability of</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>heads, tails, tails is P times
(1 minus P) squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>Probability of one
head is 3 times P</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>times (1 minus P) squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:17</time_slice>
              <text_slice>So the final answer is 1/3,
which is what you should have</text_slice>
            </slice>
            <slice>
              <time_slice>11:22</time_slice>
              <text_slice>a guessed on intuitive
grounds.</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>Very good.</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>So we got our practice on
the material that we</text_slice>
            </slice>
            <slice>
              <time_slice>11:31</time_slice>
              <text_slice>did cover last time.</text_slice>
            </slice>
            <slice>
              <time_slice>11:33</time_slice>
              <text_slice>Again, think.</text_slice>
            </slice>
            <slice>
              <time_slice>11:33</time_slice>
              <text_slice>There's basically three basic
skills that we are practicing</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>and exercising here.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>In the problems, quizzes, and in
the real life, you may have</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>to apply those three skills in
somewhat more complicated</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>settings, but in the
end that's what it</text_slice>
            </slice>
            <slice>
              <time_slice>11:49</time_slice>
              <text_slice>boils down to usually.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>Now let's focus on this special
feature of this</text_slice>
            </slice>
            <slice>
              <time_slice>11:55</time_slice>
              <text_slice>particular model that I
discussed a little earlier.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>Think of the event heads
in the second toss.</text_slice>
            </slice>
            <slice>
              <time_slice>12:05</time_slice>
              <text_slice>Initially, the probability of
heads in the second toss, you</text_slice>
            </slice>
            <slice>
              <time_slice>12:09</time_slice>
              <text_slice>know, that it's P, the
probability of</text_slice>
            </slice>
            <slice>
              <time_slice>12:12</time_slice>
              <text_slice>success of your coin.</text_slice>
            </slice>
            <slice>
              <time_slice>12:14</time_slice>
              <text_slice>If I tell you that the first
toss resulted in heads, what's</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>the probability that the
second toss is heads?</text_slice>
            </slice>
            <slice>
              <time_slice>12:21</time_slice>
              <text_slice>It's again P. If I tell you that
the first toss was tails,</text_slice>
            </slice>
            <slice>
              <time_slice>12:24</time_slice>
              <text_slice>what's the probability that
the second toss is heads?</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>It's again P. So whether I tell
you the result of the</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>first toss, or I don't tell
you, it doesn't make any</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>difference to you.</text_slice>
            </slice>
            <slice>
              <time_slice>12:38</time_slice>
              <text_slice>You would always say the
probability of heads in the</text_slice>
            </slice>
            <slice>
              <time_slice>12:40</time_slice>
              <text_slice>second toss is going to P, no
matter what happened in the</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>first toss.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>This is a special situation to
which we're going to give a</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>name, and we're going to call
that property independence.</text_slice>
            </slice>
            <slice>
              <time_slice>12:53</time_slice>
              <text_slice>Basically independence between
two things stands for the fact</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>that the first thing, whether
it occurred or not, doesn't</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>give you any information, does
not cause you to change your</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>beliefs about the
second event.</text_slice>
            </slice>
            <slice>
              <time_slice>13:08</time_slice>
              <text_slice>This is the intuition.</text_slice>
            </slice>
            <slice>
              <time_slice>13:11</time_slice>
              <text_slice>Let's try to translate this
into mathematics.</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>We have two events, and we're
going to say that they're</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>independent if your initial
beliefs about B are not going</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>to change if I tell you
that A occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>So you believe something
how likely B is.</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>Then somebody comes and tells
you, you know, A has happened.</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>Are you going to change
your beliefs?</text_slice>
            </slice>
            <slice>
              <time_slice>13:39</time_slice>
              <text_slice>No, I'm not going
to change them.</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>Whenever you are in such a
situation, then you say that</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>the two events are
independent.</text_slice>
            </slice>
            <slice>
              <time_slice>13:47</time_slice>
              <text_slice>Intuitively, the fact that A
occurred does not convey any</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>information to you about the
likelihood of event B. The</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>information that A provides
is not so</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>useful, is not relevant.</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>A has to do with
something else.</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>It's not useful for your
guessing whether B is going to</text_slice>
            </slice>
            <slice>
              <time_slice>14:06</time_slice>
              <text_slice>occur or not.</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>So we can take this as a first
attempt into a definition of</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>independence.</text_slice>
            </slice>
            <slice>
              <time_slice>14:15</time_slice>
              <text_slice>Now remember that we have this
property, the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>two things happening is the
probability of the first times</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>the conditional probability
of the second.</text_slice>
            </slice>
            <slice>
              <time_slice>14:27</time_slice>
              <text_slice>If we have independence, this
conditional probability is the</text_slice>
            </slice>
            <slice>
              <time_slice>14:31</time_slice>
              <text_slice>same as the unconditional
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>So if we have independence
according to that definition,</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>we get this property that you
can find the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>14:41</time_slice>
              <text_slice>two things happening by just
multiplying their individual</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>14:45</time_slice>
              <text_slice>Probability of heads in
the first toss is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>Probability of heads in the
second toss is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>Probability of heads
heads is 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>That's what happens if your two
tosses are independent of</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>each other.</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>So this property here is
a consequence of this</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>definition, but it's actually
nicer, better, simpler,</text_slice>
            </slice>
            <slice>
              <time_slice>15:08</time_slice>
              <text_slice>cleaner, more beautiful to take
this as our definition</text_slice>
            </slice>
            <slice>
              <time_slice>15:12</time_slice>
              <text_slice>instead of that one.</text_slice>
            </slice>
            <slice>
              <time_slice>15:14</time_slice>
              <text_slice>Are the two definitions
equivalent?</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>Well, they're are almost the
same, except for one thing.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>Conditional probabilities are
only defined if you condition</text_slice>
            </slice>
            <slice>
              <time_slice>15:24</time_slice>
              <text_slice>on an event that has positive
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>15:26</time_slice>
              <text_slice>So this definition would be
limited to cases where event A</text_slice>
            </slice>
            <slice>
              <time_slice>15:31</time_slice>
              <text_slice>has positive probability,
whereas this definition is</text_slice>
            </slice>
            <slice>
              <time_slice>15:34</time_slice>
              <text_slice>something that you can
write down always.</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>We will say that two events are
independent if and only if</text_slice>
            </slice>
            <slice>
              <time_slice>15:43</time_slice>
              <text_slice>their probability of happening
simultaneously is equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>15:46</time_slice>
              <text_slice>product of their two individual
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>And in particular, we can have
events of zero probability.</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>There's nothing wrong
with that.</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>If A has 0 probability, then A
intersection B will also have</text_slice>
            </slice>
            <slice>
              <time_slice>16:01</time_slice>
              <text_slice>zero probability, because it's
an even smaller event.</text_slice>
            </slice>
            <slice>
              <time_slice>16:04</time_slice>
              <text_slice>And so we're going to get
zero is equal to zero.</text_slice>
            </slice>
            <slice>
              <time_slice>16:09</time_slice>
              <text_slice>A corollary of what I just said,
if an event A has zero</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>probability, it's actually
independent of any other event</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>in our model, because
we're going to get</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>zero is equal to zero.</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>And the definition is going
to be satisfied.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>This is a little bit harder to
reconcile with the intuition</text_slice>
            </slice>
            <slice>
              <time_slice>16:27</time_slice>
              <text_slice>we have about independence, but
then again, it's part of</text_slice>
            </slice>
            <slice>
              <time_slice>16:32</time_slice>
              <text_slice>the mathematical definition.</text_slice>
            </slice>
            <slice>
              <time_slice>16:35</time_slice>
              <text_slice>So what I want you to retain
is this notion that the</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>independence is something that
you can check formally using</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>this definition, but also you
can check intuitively by if,</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>in some cases, you can reason
that whatever happens and</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>determines whether A is going
to occur or not, has nothing</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>absolutely to do with whatever
happens and determines whether</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>B is going to occur or not.</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>So if I'm doing a science
experiment in this room, and</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>it gets hit by some noise that's
causes randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>And then five years later,
somebody somewhere else does</text_slice>
            </slice>
            <slice>
              <time_slice>17:16</time_slice>
              <text_slice>the same science experiment
somewhere else, it gets hit by</text_slice>
            </slice>
            <slice>
              <time_slice>17:19</time_slice>
              <text_slice>other noise, you would usually
say that these experiments are</text_slice>
            </slice>
            <slice>
              <time_slice>17:23</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>17:23</time_slice>
              <text_slice>So what events happen in one
experiment are not going to</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>change your beliefs about what
might be happening in the</text_slice>
            </slice>
            <slice>
              <time_slice>17:33</time_slice>
              <text_slice>other, because the sources of
noise in these two experiments</text_slice>
            </slice>
            <slice>
              <time_slice>17:36</time_slice>
              <text_slice>are completely unrelated.</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>They have nothing to
do with each other.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>So if I flip a coin here today,
and I flip a coin in my</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>office tomorrow, one shouldn't
affect the other.</text_slice>
            </slice>
            <slice>
              <time_slice>17:47</time_slice>
              <text_slice>So the events that I get from
these should be independent.</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>So that's usually how
independence arises.</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>By having distinct physical</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>phenomena that do not interact.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>Sometimes you also get
independence even though there</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>is a physical interaction, but
you just happen to have a</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>numerical accident.</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>A and B might be physically
related very tightly, but a</text_slice>
            </slice>
            <slice>
              <time_slice>18:13</time_slice>
              <text_slice>numerical accident happens and
you get equality here, that's</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>another case where we
do get independence.</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>Now suppose that we have
two events that are</text_slice>
            </slice>
            <slice>
              <time_slice>18:24</time_slice>
              <text_slice>laid out like this.</text_slice>
            </slice>
            <slice>
              <time_slice>18:27</time_slice>
              <text_slice>Are these two events
independent or not?</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>The picture kind of tells
you that one is</text_slice>
            </slice>
            <slice>
              <time_slice>18:36</time_slice>
              <text_slice>separate from the other.</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>But separate has nothing
to do with independent.</text_slice>
            </slice>
            <slice>
              <time_slice>18:41</time_slice>
              <text_slice>In fact, these two events are as
dependent as Siamese twins.</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>Why is that?</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>If I tell you that A occurred,
then you are certain that B</text_slice>
            </slice>
            <slice>
              <time_slice>18:51</time_slice>
              <text_slice>did not occur.</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>So information about the
occurrence of A definitely</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>affects your beliefs about the
possible occurrence or</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>non-occurrence of B. When the
picture is like that, knowing</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>that A occurred will change
drastically my beliefs about</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>B, because now I suddenly
become certain</text_slice>
            </slice>
            <slice>
              <time_slice>19:13</time_slice>
              <text_slice>that B did not occur.</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>So a picture like this is a
case actually of extreme</text_slice>
            </slice>
            <slice>
              <time_slice>19:18</time_slice>
              <text_slice>dependence.</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>So don't confuse independence
with disjointness.</text_slice>
            </slice>
            <slice>
              <time_slice>19:23</time_slice>
              <text_slice>They're very different
types of properties.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>AUDIENCE: Question.</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>PROFESSOR: Yes?</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>AUDIENCE: So I understand
the explanation, but the</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>probability of A intersect B
[INAUDIBLE] to zero, because</text_slice>
            </slice>
            <slice>
              <time_slice>19:31</time_slice>
              <text_slice>they're disjoint.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>PROFESSOR: Yes.</text_slice>
            </slice>
            <slice>
              <time_slice>19:33</time_slice>
              <text_slice>AUDIENCE: But then the product
of probability A and</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>probability B, one of them
is going to be 1.</text_slice>
            </slice>
            <slice>
              <time_slice>19:37</time_slice>
              <text_slice>[INAUDIBLE]</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>PROFESSOR: No, suppose that
the probabilities are 1/3,</text_slice>
            </slice>
            <slice>
              <time_slice>19:42</time_slice>
              <text_slice>1/4, and the rest
is out there.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>You check the definition
of independence.</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>Probability of A intersection
B is zero.</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>Probability of A times the
probability of B is 1/12.</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>The two are not equal.</text_slice>
            </slice>
            <slice>
              <time_slice>20:00</time_slice>
              <text_slice>Therefore we do not
have independence.</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>AUDIENCE: Right.</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>So what's wrong with the
intuition of the probability</text_slice>
            </slice>
            <slice>
              <time_slice>20:05</time_slice>
              <text_slice>of A being 1, and the
other one being 0?</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>[INAUDIBLE].</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>PROFESSOR: No.</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>The probability of A given
B is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>Probability of A is
equal to 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>So again, these two
are different.</text_slice>
            </slice>
            <slice>
              <time_slice>20:26</time_slice>
              <text_slice>So we had some initial beliefs
about A, but as soon as we are</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>told that B occurred, our
beliefs about A changed.</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>And so since our beliefs
changed, that means that B</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>conveys information about A.</text_slice>
            </slice>
            <slice>
              <time_slice>20:40</time_slice>
              <text_slice>AUDIENCE: So can you not draw
independent [INAUDIBLE] on a</text_slice>
            </slice>
            <slice>
              <time_slice>20:42</time_slice>
              <text_slice>Venn diagram?</text_slice>
            </slice>
            <slice>
              <time_slice>20:43</time_slice>
              <text_slice>PROFESSOR: I can't hear you.</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>AUDIENCE: Can you draw</text_slice>
            </slice>
            <slice>
              <time_slice>20:45</time_slice>
              <text_slice>independence on a Venn diagram?</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>PROFESSOR: No, the Venn diagram
is never enough to</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>decide independence.</text_slice>
            </slice>
            <slice>
              <time_slice>20:53</time_slice>
              <text_slice>So the typical picture in which
you're going to have</text_slice>
            </slice>
            <slice>
              <time_slice>20:56</time_slice>
              <text_slice>independence would be one event
this way, and another</text_slice>
            </slice>
            <slice>
              <time_slice>21:00</time_slice>
              <text_slice>event this way.</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>You need to take the probability
of this times the</text_slice>
            </slice>
            <slice>
              <time_slice>21:03</time_slice>
              <text_slice>probability of that, and check
that, numerically, it's equal</text_slice>
            </slice>
            <slice>
              <time_slice>21:07</time_slice>
              <text_slice>to the probability of
this intersection.</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>So it's more than
a Venn diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>21:14</time_slice>
              <text_slice>Numbers need to come
out right.</text_slice>
            </slice>
            <slice>
              <time_slice>21:19</time_slice>
              <text_slice>Now we did say some time ago
that conditional probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>21:23</time_slice>
              <text_slice>are just like ordinary
probabilities, and whatever we</text_slice>
            </slice>
            <slice>
              <time_slice>21:27</time_slice>
              <text_slice>do in probability theory
can also be done</text_slice>
            </slice>
            <slice>
              <time_slice>21:31</time_slice>
              <text_slice>in conditional universes.</text_slice>
            </slice>
            <slice>
              <time_slice>21:34</time_slice>
              <text_slice>Talking about conditional
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>So since we have a notion of
independence, then there</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>should be also a notion of
conditional independence.</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>So independence was defined
by the probability that A</text_slice>
            </slice>
            <slice>
              <time_slice>21:55</time_slice>
              <text_slice>intersection B is equal to the
probability of A times the</text_slice>
            </slice>
            <slice>
              <time_slice>21:59</time_slice>
              <text_slice>probability of B.</text_slice>
            </slice>
            <slice>
              <time_slice>22:01</time_slice>
              <text_slice>What would be a reasonable
definition of conditional</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>independence?</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>Conditional independence would
mean that this same property</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>could be true, but in a
conditional universe where we</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>are told that the certain
event happens.</text_slice>
            </slice>
            <slice>
              <time_slice>22:15</time_slice>
              <text_slice>So if we're told that the event
C has happened, then</text_slice>
            </slice>
            <slice>
              <time_slice>22:19</time_slice>
              <text_slice>were transported in a
conditional universe where the</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>only thing that matters are
conditional probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>And this is just the same plain,
previous definition of</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>independence, but applied in
a conditional universe.</text_slice>
            </slice>
            <slice>
              <time_slice>22:35</time_slice>
              <text_slice>So this is the definition of
conditional independence.</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>So it's independence, but with
reference to the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>22:46</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>22:48</time_slice>
              <text_slice>And intuitively it has, again,
the same meaning, that in the</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>conditional world, if I tell you
that A occurred, then that</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>doesn't change your
beliefs about B.</text_slice>
            </slice>
            <slice>
              <time_slice>22:58</time_slice>
              <text_slice>So suppose you had a
picture like this.</text_slice>
            </slice>
            <slice>
              <time_slice>23:01</time_slice>
              <text_slice>And somebody told you that
events A and B are independent</text_slice>
            </slice>
            <slice>
              <time_slice>23:06</time_slice>
              <text_slice>unconditionally.</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>Then somebody comes and tells
you that event C actually has</text_slice>
            </slice>
            <slice>
              <time_slice>23:14</time_slice>
              <text_slice>occurred, so we now live
in this new universe.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>In this new universe, is the
independence of A and B going</text_slice>
            </slice>
            <slice>
              <time_slice>23:22</time_slice>
              <text_slice>to be preserved or not?</text_slice>
            </slice>
            <slice>
              <time_slice>23:25</time_slice>
              <text_slice>Are A and B independent
in this new universe?</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>The answer is no, because in the
new universe, whatever is</text_slice>
            </slice>
            <slice>
              <time_slice>23:34</time_slice>
              <text_slice>left of event A is this piece.</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>Whatever is left of event
B is this piece.</text_slice>
            </slice>
            <slice>
              <time_slice>23:39</time_slice>
              <text_slice>And these two pieces
are disjoint.</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>So we are back in a situation
of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>So in the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>universe, A and B are disjoint.</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>And therefore, generically,
they're not going to be</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>23:54</time_slice>
              <text_slice>What's the moral of
this example?</text_slice>
            </slice>
            <slice>
              <time_slice>23:58</time_slice>
              <text_slice>Having independence in the
original model does not imply</text_slice>
            </slice>
            <slice>
              <time_slice>24:01</time_slice>
              <text_slice>independence in a conditional
model.</text_slice>
            </slice>
            <slice>
              <time_slice>24:05</time_slice>
              <text_slice>The opposite is also possible.</text_slice>
            </slice>
            <slice>
              <time_slice>24:08</time_slice>
              <text_slice>And let's illustrate
by another example.</text_slice>
            </slice>
            <slice>
              <time_slice>24:12</time_slice>
              <text_slice>So I have two coins, and both
of them are badly biased.</text_slice>
            </slice>
            <slice>
              <time_slice>24:17</time_slice>
              <text_slice>One coin is much biased
in favor of heads.</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>The other coin is much biased
in favor of tails.</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>So the probabilities
being 90%.</text_slice>
            </slice>
            <slice>
              <time_slice>24:28</time_slice>
              <text_slice>Let's consider independent flips
of coin A. This is the</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>relevant model.</text_slice>
            </slice>
            <slice>
              <time_slice>24:34</time_slice>
              <text_slice>This is a model of two
independent flips</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>of the first coin.</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>There's going to be two flips,
and each one has probability</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>0.9 of being heads.</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>So that's a model that describes
coin A. You can</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>think of this as a conditional
model which is a model of the</text_slice>
            </slice>
            <slice>
              <time_slice>24:52</time_slice>
              <text_slice>coin flips conditioned on the
fact that they have chosen</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>coin A.</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>Alternatively we could be
dealing with coin B In a</text_slice>
            </slice>
            <slice>
              <time_slice>25:01</time_slice>
              <text_slice>conditional world where we
chose coin B and flip it</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>twice, this is the
relevant model.</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>The probability of two heads,
for example, is the</text_slice>
            </slice>
            <slice>
              <time_slice>25:10</time_slice>
              <text_slice>probability of heads the first
time, heads the second time,</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>and each one is 0.1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>Now I'm building this into a
bigger experiment in which I</text_slice>
            </slice>
            <slice>
              <time_slice>25:19</time_slice>
              <text_slice>first start by choosing one of
the two coins at random.</text_slice>
            </slice>
            <slice>
              <time_slice>25:25</time_slice>
              <text_slice>So I have these two coins.</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>I blindly pick one of them.</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>And then I start
flipping them.</text_slice>
            </slice>
            <slice>
              <time_slice>25:32</time_slice>
              <text_slice>So the question now is, are the
coin flips, or the coin</text_slice>
            </slice>
            <slice>
              <time_slice>25:36</time_slice>
              <text_slice>tosses, are they independent
of each other?</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>If we just stay inside this
sub-model here, are the coin</text_slice>
            </slice>
            <slice>
              <time_slice>25:46</time_slice>
              <text_slice>flips independent?</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>They are independent, because
the probability of heads in</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>the second toss is the same,
0.9, no matter what happened</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>in the first toss.</text_slice>
            </slice>
            <slice>
              <time_slice>26:03</time_slice>
              <text_slice>So the conditional probabilities
of what happens</text_slice>
            </slice>
            <slice>
              <time_slice>26:06</time_slice>
              <text_slice>in the second toss are not
affected by the outcome of the</text_slice>
            </slice>
            <slice>
              <time_slice>26:10</time_slice>
              <text_slice>first toss.</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>So the second toss and the first
toss are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>So here we're just dealing
with plain,</text_slice>
            </slice>
            <slice>
              <time_slice>26:17</time_slice>
              <text_slice>independent coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>Similarity the coin flips within
this sub-model are also</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>26:28</time_slice>
              <text_slice>Now the question is, if we look
at the big model as just</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>one probability model, instead
of looking at the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>26:38</time_slice>
              <text_slice>sub-models, are the coin flips
independent of each other?</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>Does the outcome of a few coin
flips give you information</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>about subsequent coin flips?</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>Well if I observe ten
heads in a row--</text_slice>
            </slice>
            <slice>
              <time_slice>27:02</time_slice>
              <text_slice>So instead of two coin flips,
now let's think of doing more</text_slice>
            </slice>
            <slice>
              <time_slice>27:05</time_slice>
              <text_slice>of them so that the tree
gets expanded.</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>So let's start with this.</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>I don't know which coin it is.</text_slice>
            </slice>
            <slice>
              <time_slice>27:16</time_slice>
              <text_slice>What's the probability that
the 11th coin toss</text_slice>
            </slice>
            <slice>
              <time_slice>27:18</time_slice>
              <text_slice>is going to be heads?</text_slice>
            </slice>
            <slice>
              <time_slice>27:25</time_slice>
              <text_slice>There's complete symmetry here,
so the answer could not</text_slice>
            </slice>
            <slice>
              <time_slice>27:29</time_slice>
              <text_slice>be anything other than 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>So let's justify it,
why is it 1/2?</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>Well, the probability that the
11th toss is heads, how can</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>that outcome happen?</text_slice>
            </slice>
            <slice>
              <time_slice>27:42</time_slice>
              <text_slice>It can happen in two ways.</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>You can choose coin A, which
happens with probability 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>27:50</time_slice>
              <text_slice>And having chosen coin A,
there's probability 0.9 that</text_slice>
            </slice>
            <slice>
              <time_slice>27:54</time_slice>
              <text_slice>it results in that you get
heads in the 11th toss.</text_slice>
            </slice>
            <slice>
              <time_slice>27:58</time_slice>
              <text_slice>Or you can choose coin B. And
if it's coin B when you flip</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>it, there's probably 0.1
that you have heads.</text_slice>
            </slice>
            <slice>
              <time_slice>28:06</time_slice>
              <text_slice>So the final answer is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>So each one of the coins is
biased, but they're biased in</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>different ways.</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>If I don't know which coin it
is, their two biases kind of</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>cancel out, and the probability
of obtaining heads</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>is just in the middle,
then it's 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>28:27</time_slice>
              <text_slice>Now if someone tells you that
the first ten tosses were</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>heads, is that going to
change your beliefs</text_slice>
            </slice>
            <slice>
              <time_slice>28:34</time_slice>
              <text_slice>about the 11th toss?</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>Here's how a reasonable person
would think about it.</text_slice>
            </slice>
            <slice>
              <time_slice>28:41</time_slice>
              <text_slice>If it's coin B the probability
of obtaining 10 heads in a row</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>is negligible.</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>It's going to be 0.1
to the 10th.</text_slice>
            </slice>
            <slice>
              <time_slice>28:55</time_slice>
              <text_slice>If it's coin A. The probability
of 10 heads in a</text_slice>
            </slice>
            <slice>
              <time_slice>28:59</time_slice>
              <text_slice>row is a more reasonable
number.</text_slice>
            </slice>
            <slice>
              <time_slice>29:01</time_slice>
              <text_slice>It's 0.9 to the 10th.</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>So this event is a lot more
likely to occur with coin A,</text_slice>
            </slice>
            <slice>
              <time_slice>29:10</time_slice>
              <text_slice>rather than coin B.</text_slice>
            </slice>
            <slice>
              <time_slice>29:13</time_slice>
              <text_slice>The plausible explanation of
having seen ten heads in a row</text_slice>
            </slice>
            <slice>
              <time_slice>29:18</time_slice>
              <text_slice>is that I actually chose coin A.
When you see ten heads in a</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>row, you are pretty certain that
it's coin A that we're</text_slice>
            </slice>
            <slice>
              <time_slice>29:29</time_slice>
              <text_slice>dealing with.</text_slice>
            </slice>
            <slice>
              <time_slice>29:30</time_slice>
              <text_slice>And once you're pretty certain
that it's coin A that we're</text_slice>
            </slice>
            <slice>
              <time_slice>29:33</time_slice>
              <text_slice>dealing with, what's the
probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>next toss is heads?</text_slice>
            </slice>
            <slice>
              <time_slice>29:38</time_slice>
              <text_slice>It's going to be 0.9.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>So essentially here I'm doing
an inference calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>Given this information, I'm
making an inference about</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>which coin I'm dealing with.</text_slice>
            </slice>
            <slice>
              <time_slice>29:53</time_slice>
              <text_slice>I become pretty certain that
it's coin A, and given that</text_slice>
            </slice>
            <slice>
              <time_slice>29:57</time_slice>
              <text_slice>it's coin A, this probability
is going to be 0.9.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>And I'm putting an approximate
sign here, because the</text_slice>
            </slice>
            <slice>
              <time_slice>30:04</time_slice>
              <text_slice>inference that I did
is approximate.</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>I'm pretty certain it's coin A.
I'm not 100% certain that</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>it's coin A.</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>But in any case what happens
here is that the unconditional</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>probability is different from
the conditional probability.</text_slice>
            </slice>
            <slice>
              <time_slice>30:19</time_slice>
              <text_slice>This information here makes
me change my beliefs</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>about the 11th toss.</text_slice>
            </slice>
            <slice>
              <time_slice>30:25</time_slice>
              <text_slice>And this means that the 11th
toss is dependent on the</text_slice>
            </slice>
            <slice>
              <time_slice>30:30</time_slice>
              <text_slice>previous tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>So the coin tosses have
now become dependent.</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>What is the physical link that
causes this dependence?</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>Well, the physical link is
the choice of the coin.</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>By choosing a particular coin,
I'm introducing a pattern in</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>the future coin tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>And that pattern is what
causes dependence.</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>OK, so I've been playing a
little bit too loose with the</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>language here, because we
defined the concept of</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>independence of two events.</text_slice>
            </slice>
            <slice>
              <time_slice>31:01</time_slice>
              <text_slice>But here I have been referring
to independent coin tosses,</text_slice>
            </slice>
            <slice>
              <time_slice>31:06</time_slice>
              <text_slice>where I'm thinking about
many coin tosses,</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>like 10 or 11 of them.</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>So to be proper, I should have
defined for you also the</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>notion of independence of
multiple events, not just two.</text_slice>
            </slice>
            <slice>
              <time_slice>31:18</time_slice>
              <text_slice>We don't want to just say coin
toss one is independent from</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>coin toss two.</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>We want to be able to say
something like, these 10 then</text_slice>
            </slice>
            <slice>
              <time_slice>31:26</time_slice>
              <text_slice>coin tosses are all independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>31:29</time_slice>
              <text_slice>Intuitively what that means
should be the same thing--</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>that information about some of
the coin tosses doesn't change</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>your beliefs about the remaining
coin tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>31:40</time_slice>
              <text_slice>How do we translate that into
a mathematical definition?</text_slice>
            </slice>
            <slice>
              <time_slice>31:43</time_slice>
              <text_slice>Well, an ugly attempt
would be to impose</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>requirements such as this.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>Think of A1 being the event that
the first flip was heads.</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>A2 is the event of that the
second flip was heads.</text_slice>
            </slice>
            <slice>
              <time_slice>32:00</time_slice>
              <text_slice>A3, the third flip, was
heads, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>32:04</time_slice>
              <text_slice>Here is an event whose
occurrence is not determined</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>by the first three coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>And here's an event whose
occurrence or not is</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>determined by the fifth
and sixth coin flip.</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>If we think physically that
all those coin flips have</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>nothing to do with each other,
information about the fifth</text_slice>
            </slice>
            <slice>
              <time_slice>32:22</time_slice>
              <text_slice>and sixth coin flip are not
going to change what we expect</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>from the first three.</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>So the probability of this
event, the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>32:30</time_slice>
              <text_slice>probability, should be the
same as the unconditional</text_slice>
            </slice>
            <slice>
              <time_slice>32:33</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>And we would like a relation
of this kind to be true, no</text_slice>
            </slice>
            <slice>
              <time_slice>32:38</time_slice>
              <text_slice>matter what kind of formula you
write down, as long as the</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>events that show up here are
different from the events that</text_slice>
            </slice>
            <slice>
              <time_slice>32:47</time_slice>
              <text_slice>show up there.</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>That's sort of an
ugly definition.</text_slice>
            </slice>
            <slice>
              <time_slice>32:52</time_slice>
              <text_slice>The mathematical definition that
actually does the job,</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>and leads to all the
formulas of this</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>kind, is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>We're going to say that the
collection of events are</text_slice>
            </slice>
            <slice>
              <time_slice>33:03</time_slice>
              <text_slice>independent if we can find the
probability of their joint</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>occurrence by just multiplying
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>And that will be true even if
you look at sub-collections of</text_slice>
            </slice>
            <slice>
              <time_slice>33:17</time_slice>
              <text_slice>these events.</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>Let's make that more precise.</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>If we have three events, the
definition tells us that the</text_slice>
            </slice>
            <slice>
              <time_slice>33:24</time_slice>
              <text_slice>three events are independent
if the following are true.</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>Probability A1 and A2 and A3,
you can calculate this</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>probability by multiplying
individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>But the same is true even if
you take fewer events.</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>Just a few indices out
of the indices</text_slice>
            </slice>
            <slice>
              <time_slice>33:46</time_slice>
              <text_slice>that we have available.</text_slice>
            </slice>
            <slice>
              <time_slice>33:48</time_slice>
              <text_slice>So we also require P(A1
intersection A2) is P(A1)</text_slice>
            </slice>
            <slice>
              <time_slice>33:54</time_slice>
              <text_slice>times P(A2).</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>And similarly for the other
possibilities of</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>choosing the indices.</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>OK, so independence,
mathematical definition,</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>requires that calculating
probabilities of any</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>intersection of the events we
have in our hands, that</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>calculation can be done by just
multiplying individual</text_slice>
            </slice>
            <slice>
              <time_slice>34:25</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>And this has to apply to the
case where we consider all of</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>the events in our hands or just</text_slice>
            </slice>
            <slice>
              <time_slice>34:33</time_slice>
              <text_slice>sub-collections of those events.</text_slice>
            </slice>
            <slice>
              <time_slice>34:36</time_slice>
              <text_slice>Now these relations just by
themselves are called pairwise</text_slice>
            </slice>
            <slice>
              <time_slice>34:42</time_slice>
              <text_slice>independence.</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>So this relation, for example,
tells us that A1 is</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>independent from A2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:48</time_slice>
              <text_slice>This tells us that A2 is
independent from A3.</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>This will tell us that A1
is independent from A3.</text_slice>
            </slice>
            <slice>
              <time_slice>34:54</time_slice>
              <text_slice>But independence of all the
events together actually</text_slice>
            </slice>
            <slice>
              <time_slice>34:58</time_slice>
              <text_slice>requires a little more.</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>One more equality that has to do
with all three events being</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>considered at the same time.</text_slice>
            </slice>
            <slice>
              <time_slice>35:07</time_slice>
              <text_slice>And this extra equality
is not redundant.</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>It actually does make
a difference.</text_slice>
            </slice>
            <slice>
              <time_slice>35:13</time_slice>
              <text_slice>Independence and pairwise
independence</text_slice>
            </slice>
            <slice>
              <time_slice>35:15</time_slice>
              <text_slice>are different things.</text_slice>
            </slice>
            <slice>
              <time_slice>35:17</time_slice>
              <text_slice>So let's illustrate the
situation with an example.</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>Suppose we have two
coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>The coin tosses are independent,
so the bias is</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>1/2, so all possible outcomes
have a probability of 1/2</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>times 1/2, which is 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>35:36</time_slice>
              <text_slice>And let's consider now a bunch
of different events.</text_slice>
            </slice>
            <slice>
              <time_slice>35:40</time_slice>
              <text_slice>One event is that the
first toss is heads.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>This is this blue set here.</text_slice>
            </slice>
            <slice>
              <time_slice>35:48</time_slice>
              <text_slice>Another event is the second
toss is heads.</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>And this is this black
event here.</text_slice>
            </slice>
            <slice>
              <time_slice>36:00</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>Are these two events
independent?</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>If you check it mathematically,
yes.</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>Probability of A is probability
of B is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>Probability of A times
probability of B is 1/4, which</text_slice>
            </slice>
            <slice>
              <time_slice>36:13</time_slice>
              <text_slice>is the same as the probability
of A intersection B,</text_slice>
            </slice>
            <slice>
              <time_slice>36:16</time_slice>
              <text_slice>which is this set.</text_slice>
            </slice>
            <slice>
              <time_slice>36:18</time_slice>
              <text_slice>So we have just checked
mathematically that A and B</text_slice>
            </slice>
            <slice>
              <time_slice>36:20</time_slice>
              <text_slice>are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>36:22</time_slice>
              <text_slice>Now lets consider a third event
which is that the first</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>and second toss give
the same result.</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>I'll use a different color.</text_slice>
            </slice>
            <slice>
              <time_slice>36:32</time_slice>
              <text_slice>First and second toss to
give the same result.</text_slice>
            </slice>
            <slice>
              <time_slice>36:35</time_slice>
              <text_slice>This is the event that
we obtain heads,</text_slice>
            </slice>
            <slice>
              <time_slice>36:38</time_slice>
              <text_slice>heads or tails, tails.</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>So this is the probability
of C. What's the</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>probability of C?</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>Well, C is made up of two
outcomes, each one of which</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>has probability 1/4, so the
probability of C is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>36:55</time_slice>
              <text_slice>What is the probability
of C intersection A?</text_slice>
            </slice>
            <slice>
              <time_slice>36:58</time_slice>
              <text_slice>C intersection A is just this
one outcome, and has</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>probability 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>What's the probability of A
intersection B intersection C?</text_slice>
            </slice>
            <slice>
              <time_slice>37:10</time_slice>
              <text_slice>The three events intersect just
this outcome, so this</text_slice>
            </slice>
            <slice>
              <time_slice>37:13</time_slice>
              <text_slice>probability is also 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>37:18</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:24</time_slice>
              <text_slice>What's the probability
of C given A and B?</text_slice>
            </slice>
            <slice>
              <time_slice>37:29</time_slice>
              <text_slice>If A has occurred, and B has
occurred, you are certain that</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>this outcome here happened.</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>If the first toss is H and the
second toss is H, then you're</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>certain of the first
and second toss</text_slice>
            </slice>
            <slice>
              <time_slice>37:41</time_slice>
              <text_slice>gave the same result.</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>So the conditional probability
of C given A and</text_slice>
            </slice>
            <slice>
              <time_slice>37:46</time_slice>
              <text_slice>B is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>So do we have independence
in this example?</text_slice>
            </slice>
            <slice>
              <time_slice>37:54</time_slice>
              <text_slice>We don't.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>C, that we obtain the same
result in the first and the</text_slice>
            </slice>
            <slice>
              <time_slice>38:00</time_slice>
              <text_slice>second toss, has probability
1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:04</time_slice>
              <text_slice>Half of the possible outcomes
give us two coin flips with</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>the same result-- heads,
heads or tails, tails.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>So the probability
of C is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:12</time_slice>
              <text_slice>But if I tell you that the
events A and B both occurred,</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>then you're certain
that C occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>38:20</time_slice>
              <text_slice>If I tell you that we had heads
and heads, then you're</text_slice>
            </slice>
            <slice>
              <time_slice>38:23</time_slice>
              <text_slice>certain the outcomes
were the same.</text_slice>
            </slice>
            <slice>
              <time_slice>38:25</time_slice>
              <text_slice>So the conditional probability
is different from the</text_slice>
            </slice>
            <slice>
              <time_slice>38:28</time_slice>
              <text_slice>unconditional probability.</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>So by combining these two
relations together, we get</text_slice>
            </slice>
            <slice>
              <time_slice>38:37</time_slice>
              <text_slice>that the three events
are not independent.</text_slice>
            </slice>
            <slice>
              <time_slice>38:42</time_slice>
              <text_slice>But are they pairwise
independent?</text_slice>
            </slice>
            <slice>
              <time_slice>38:45</time_slice>
              <text_slice>Is A independent from B?</text_slice>
            </slice>
            <slice>
              <time_slice>38:49</time_slice>
              <text_slice>Yes, because probability of A
times probability of B is 1/4,</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>which is probability of
A intersection B. Is C</text_slice>
            </slice>
            <slice>
              <time_slice>38:58</time_slice>
              <text_slice>independent from A?</text_slice>
            </slice>
            <slice>
              <time_slice>39:02</time_slice>
              <text_slice>Well, the probability
of C and A is 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>The probability of C is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>The probability of A is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>So it checks.</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>1/4 is equal to 1/2 and 1/2,
so event C and event A are</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>Knowing that the first toss was
heads does not change your</text_slice>
            </slice>
            <slice>
              <time_slice>39:24</time_slice>
              <text_slice>beliefs about whether the two
tosses are going to have the</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>same outcome or not.</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>Knowing that the first was
heads, well, the second is</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>equally likely to be
heads or tails.</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>So event C has just the
same probability,</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>again, 1/2, to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>To put it the opposite way,
if I tell you that the two</text_slice>
            </slice>
            <slice>
              <time_slice>39:46</time_slice>
              <text_slice>results were the same--</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>so it's either heads, heads
or tails, tails--</text_slice>
            </slice>
            <slice>
              <time_slice>39:51</time_slice>
              <text_slice>what does that tell you
about the first toss?</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>Is it heads, or is it tails?</text_slice>
            </slice>
            <slice>
              <time_slice>39:54</time_slice>
              <text_slice>Well, it doesn't tell
you anything.</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>It could be either over the
two, so the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>heads in the first toss is equal
to 1/2, and telling you</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>C occurred does not
change anything.</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>So this is an example that
illustrates the case where we</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>have three events in which
we check that pairwise</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>independence holds for
any combination of</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>two of these events.</text_slice>
            </slice>
            <slice>
              <time_slice>40:19</time_slice>
              <text_slice>We have the probability of their
intersection is equal to</text_slice>
            </slice>
            <slice>
              <time_slice>40:21</time_slice>
              <text_slice>the product of their
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>On the other hand, the three
events taken all together are</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>not independent.</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>A doesn't tell me anything
useful, whether C is going to</text_slice>
            </slice>
            <slice>
              <time_slice>40:32</time_slice>
              <text_slice>occur or not.</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>B doesn't tell me
anything useful.</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>But if I tell you that both A
and B occurred, the two of</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>them together tell me something
useful about C.</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>Namely, they tell me that C
certainly has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>Very good.</text_slice>
            </slice>
            <slice>
              <time_slice>40:53</time_slice>
              <text_slice>So independence is this somewhat
subtle concept.</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>Once you grasp the intuition of
what it really means, then</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>things perhaps fall in place.</text_slice>
            </slice>
            <slice>
              <time_slice>41:02</time_slice>
              <text_slice>But it's a concept where
it's easy to get some</text_slice>
            </slice>
            <slice>
              <time_slice>41:06</time_slice>
              <text_slice>misunderstanding.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>So just take some
time to digest.</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>So to lighten things up, I'm
going to spend the remaining</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>four minutes talking about the
very nice, simple problem that</text_slice>
            </slice>
            <slice>
              <time_slice>41:18</time_slice>
              <text_slice>involves conditional
probabilities and the like.</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>So here's the problem,
formulated exactly as it shows</text_slice>
            </slice>
            <slice>
              <time_slice>41:28</time_slice>
              <text_slice>up in various textbooks.</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>And the formulation says
the following.</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>Well, consider one of those
anachronistic places where</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>they still have kings or queens,
and where actually</text_slice>
            </slice>
            <slice>
              <time_slice>41:40</time_slice>
              <text_slice>boys take precedence
over girls.</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>So if there is a boy--</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>if the royal family has a boy,
then he will become the king</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>even if he has an older sister
who might be the queen.</text_slice>
            </slice>
            <slice>
              <time_slice>41:58</time_slice>
              <text_slice>So we have one of those
royal families.</text_slice>
            </slice>
            <slice>
              <time_slice>42:02</time_slice>
              <text_slice>That royal family had two
children, and we know that</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>there is a king.</text_slice>
            </slice>
            <slice>
              <time_slice>42:11</time_slice>
              <text_slice>There is a king, which means
that at least one of the two</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>children was a boy.</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>Otherwise we wouldn't
have a king.</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>What is the probability that the
king's sibling is female?</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>42:28</time_slice>
              <text_slice>I guess we need to make some
assumptions about genetics.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>Let's assume that every child
is a boy or a girl with</text_slice>
            </slice>
            <slice>
              <time_slice>42:33</time_slice>
              <text_slice>probability 1/2, and that
different children, what they</text_slice>
            </slice>
            <slice>
              <time_slice>42:39</time_slice>
              <text_slice>are is independent from what
the other children were.</text_slice>
            </slice>
            <slice>
              <time_slice>42:42</time_slice>
              <text_slice>So every childbirth is basically
a coin flip.</text_slice>
            </slice>
            <slice>
              <time_slice>42:47</time_slice>
              <text_slice>OK, so if you take that,
you say, well,</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>the king is a child.</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>His sibling is another child.</text_slice>
            </slice>
            <slice>
              <time_slice>42:55</time_slice>
              <text_slice>Children are independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>So the probability that the
sibling is a girl is 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>That's the naive answer.</text_slice>
            </slice>
            <slice>
              <time_slice>43:07</time_slice>
              <text_slice>Now let's try to
do it formally.</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>Let's set up a model
of the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>The royal family had two
children, as we we're told, so</text_slice>
            </slice>
            <slice>
              <time_slice>43:15</time_slice>
              <text_slice>there's four outcomes--</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>boy boy, boy girl, girl
boy, and girl girl.</text_slice>
            </slice>
            <slice>
              <time_slice>43:22</time_slice>
              <text_slice>Now, we are told that there is
a king, which means what?</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>This outcome here
did not happen.</text_slice>
            </slice>
            <slice>
              <time_slice>43:29</time_slice>
              <text_slice>It is not possible.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>There are three outcomes
that remain possible.</text_slice>
            </slice>
            <slice>
              <time_slice>43:33</time_slice>
              <text_slice>So this is our conditional
sample space given</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>that there is king.</text_slice>
            </slice>
            <slice>
              <time_slice>43:40</time_slice>
              <text_slice>What are the probabilities
for the original model?</text_slice>
            </slice>
            <slice>
              <time_slice>43:43</time_slice>
              <text_slice>Well with the model that we
assume that every child is a</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>boy or a girl independently with
probability 1/2, then the</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>four outcomes would be equally
likely, and they're like this.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>These are the original
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>43:57</time_slice>
              <text_slice>But once we are told that this
outcome did not happen,</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>because we have a king, then
we are transported to the</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>smaller sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>In this sample space, what's
the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>sibling is a girl?</text_slice>
            </slice>
            <slice>
              <time_slice>44:10</time_slice>
              <text_slice>Well the sibling is a girl in
two out of the three outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>So the probability that
the sibling is a</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>girl is actually 2/3.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>So that's supposed to
be the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>44:25</time_slice>
              <text_slice>Maybe a little
counter-intuitive.</text_slice>
            </slice>
            <slice>
              <time_slice>44:29</time_slice>
              <text_slice>So you can play smart and say,
oh I understand such problems</text_slice>
            </slice>
            <slice>
              <time_slice>44:32</time_slice>
              <text_slice>better than you, here is a trick
problem and here's why</text_slice>
            </slice>
            <slice>
              <time_slice>44:35</time_slice>
              <text_slice>the answer is 2/3.</text_slice>
            </slice>
            <slice>
              <time_slice>44:37</time_slice>
              <text_slice>But actually I'm not fully
justified in saying that the</text_slice>
            </slice>
            <slice>
              <time_slice>44:41</time_slice>
              <text_slice>answer is 2/3.</text_slice>
            </slice>
            <slice>
              <time_slice>44:42</time_slice>
              <text_slice>I made lots of hidden
assumptions when I put this</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>model down, which I
didn't yet state.</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>So to reverse engineer this
answer, let's actually think</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>what's the probability model for
which this would have been</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>And here's the probability
model.</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>The royal family--</text_slice>
            </slice>
            <slice>
              <time_slice>45:02</time_slice>
              <text_slice>the royal parents decided to
have exactly two children.</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>They went and had them.</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>It turned out that at
least one was a boy</text_slice>
            </slice>
            <slice>
              <time_slice>45:11</time_slice>
              <text_slice>and became a king.</text_slice>
            </slice>
            <slice>
              <time_slice>45:13</time_slice>
              <text_slice>Under this scenario--</text_slice>
            </slice>
            <slice>
              <time_slice>45:15</time_slice>
              <text_slice>that they decide to have
exactly two children--</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>then this is the big
sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>It turned out that
one was a boy.</text_slice>
            </slice>
            <slice>
              <time_slice>45:23</time_slice>
              <text_slice>That eliminates this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>And then this picture
is correct and this</text_slice>
            </slice>
            <slice>
              <time_slice>45:27</time_slice>
              <text_slice>is the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>But there's hidden assumptions
being there.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>How about if the royal
family had followed</text_slice>
            </slice>
            <slice>
              <time_slice>45:35</time_slice>
              <text_slice>the following strategy?</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>We're going to have children
until we get a boy, so that we</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>get a king, and then
we'll stop.</text_slice>
            </slice>
            <slice>
              <time_slice>45:45</time_slice>
              <text_slice>OK, given they have two
children, what's the</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>probability that the
sibling is a girl?</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>It's 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:51</time_slice>
              <text_slice>The reason that they had two
children was because the first</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>was a girl, so they had
to have a second.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>So assumptions about
reproductive practices</text_slice>
            </slice>
            <slice>
              <time_slice>46:00</time_slice>
              <text_slice>actually need to come in,
and they're going</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>to affect the decisions.</text_slice>
            </slice>
            <slice>
              <time_slice>46:04</time_slice>
              <text_slice>Or, if it's one of those ancient
kingdoms where a king</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>would always make sure too
strangle any of his brothers,</text_slice>
            </slice>
            <slice>
              <time_slice>46:11</time_slice>
              <text_slice>then the probability that the
sibling is a girl is actually</text_slice>
            </slice>
            <slice>
              <time_slice>46:15</time_slice>
              <text_slice>1 again, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>46:17</time_slice>
              <text_slice>So it means that one needs to be
careful when you start with</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>loosely worded problems to
make sure exactly what it</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>means and what assumptions
you're making.</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>All right, see you next week.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Counting (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Combinations Binomial probabilities
n
: number of k-element subsetsk nindependent coin tosses
of a given n-element setP(H)=p
Two ways of constructing an ordered
sequence of kdistinct items:
P(HTTHHH)=
Choose the kitems one at a time:
n!n(n1)(nk+1) = choices()! (sequence) =# heads(1)#t a i l sn k P p p
Choose kitems, then order them
(k! possible orders)
P(kheads) =
P(seq.)
Hence:
kad seq.
nhe
n!k!=k (nk)! = (# of khead seqs )kp(1kp)n.
n
kn!= =k(1 )nkpk!(nk)!n
k
p
kn
=
=0n
k
Coin tossing problem Partitions
event B: 3 out of 10 tosses were heads. 52-card deck, dealt to 4 players
Given that Boccurred, FindP(each gets an ace)
what is the (conditional) probability
Outcome: a partition of the 52 cardsthat the rst 2 tosses were heads?
number of outcomes:
All outcomes in set Bare equally likely:52!probability3p(1p)7
13! 13! 13! 13!
Conditional probability law is uniform Count number of ways of distributing the
four aces: 4 3 2
Number of outcomes in B: 
Count number of ways of dealing the
Out of the outcomes in B, remaining 48 cards
how many start with HH? 48!
12! 12! 12! 12!
Answer:
48!43212! 12! 12! 12!
52!
13! 13! 13! 13!
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 4 Discrete uniform law
Readings: Section 1.6 Let all sample points be equally likely
Then,
Lecture outlinenumber of elements of A AP(A)= =| |
total number of sample points Principles of counting| |
Just count...Many examples
permutations
k-permutations
combinations
partitions
Binomial probabilities
Basic counting principle Example
rstagesProbability that six rolls of a six-sided die
nichoices at stage iall give dierent numbers?
Number of outcomes that
make the event happen:
Number of elementsin the sample space:
Number of choices is: n
1n2n r
Answer:
Number of license plates
with 3 letters and 4 digits =
...if repetition is prohibited =
Permutations: Number of ways
of ordering nelements is:
Number of subsets of {1,...,n }=
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-4-counting/</video_url>
          <video_title>Lecture 4: Counting</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation, or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:21</time_slice>
              <text_slice>PROFESSOR: OK.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>So today's lecture will be on
the subject of counting.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>So counting, I guess, is
a pretty simple affair</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>conceptually, but it's a
topic that can also get</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>to be pretty tricky.</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>The reason we're going to talk
about counting is that there's</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>a lot of probability problems
whose solution actually</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>reduces to successfully counting
the cardinalities of</text_slice>
            </slice>
            <slice>
              <time_slice>0:45</time_slice>
              <text_slice>various sets.</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>So we're going to see the basic,
simplest methods that</text_slice>
            </slice>
            <slice>
              <time_slice>0:49</time_slice>
              <text_slice>one can use to count
systematically in various</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>situations.</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>So in contrast to previous
lectures, we're not going to</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>introduce any significant
new concepts of a</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>probabilistic nature.</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>We're just going to use the
probability tools that we</text_slice>
            </slice>
            <slice>
              <time_slice>1:04</time_slice>
              <text_slice>already know.</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>And we're going to apply them
in situations where there's</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>also some counting involved.</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>Now, today we're going
to just touch the</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>surface of this subject.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>There's a whole field of
mathematics called</text_slice>
            </slice>
            <slice>
              <time_slice>1:16</time_slice>
              <text_slice>combinatorics who are people who
actually spend their whole</text_slice>
            </slice>
            <slice>
              <time_slice>1:19</time_slice>
              <text_slice>lives counting more and
more complicated sets.</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>We were not going to get
anywhere close to the full</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>complexity of the field, but
we'll get just enough tools</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>that allow us to address
problems of the type that one</text_slice>
            </slice>
            <slice>
              <time_slice>1:36</time_slice>
              <text_slice>encounters in most common
situations.</text_slice>
            </slice>
            <slice>
              <time_slice>1:39</time_slice>
              <text_slice>So the basic idea, the basic
principle is something that</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>we've already discussed.</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>So counting methods apply in
situations where we have</text_slice>
            </slice>
            <slice>
              <time_slice>1:49</time_slice>
              <text_slice>probabilistic experiments with
a finite number of outcomes</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>and where every outcome--</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>every possible outcome--</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>has the same probability
of occurring.</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>So we have our sample space,
omega, and it's got a bunch of</text_slice>
            </slice>
            <slice>
              <time_slice>2:04</time_slice>
              <text_slice>discrete points in there.</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>And the cardinality of the set
omega is some capital N. So,</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>in particular, we assume that
the sample points are equally</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>likely, which means that every
element of the sample space</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>has the same probability
equal to 1 over N.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>And then we are interested in a
subset of the sample space,</text_slice>
            </slice>
            <slice>
              <time_slice>2:26</time_slice>
              <text_slice>call it A. And that
subset consists</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>of a number of elements.</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>Let the cardinality of that
subset be equal to little n.</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>And then to find the probability
of that set, all</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>we need to do is to add the
probabilities of the</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>individual elements.</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>There's little n elements, and
each one has probability one</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>over capital N. And
that's the answer.</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>So this means that to solve
problems in this context, all</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>that we need to be able to do
is to figure out the number</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>capital N and to figure out
the number little n.</text_slice>
            </slice>
            <slice>
              <time_slice>3:00</time_slice>
              <text_slice>Now, if somebody gives you a set
by just giving you a list</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>and gives you another set,
again, giving you a list, it's</text_slice>
            </slice>
            <slice>
              <time_slice>3:07</time_slice>
              <text_slice>easy to count there element.</text_slice>
            </slice>
            <slice>
              <time_slice>3:09</time_slice>
              <text_slice>You just count how much
there is on the list.</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>But sometimes the sets are
described in some more</text_slice>
            </slice>
            <slice>
              <time_slice>3:15</time_slice>
              <text_slice>implicit way, and we may have to
do a little bit more work.</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>There's various tricks that are</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>involved in counting properly.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>And the most common
one is to--</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>when you consider a set of
possible outcomes, to describe</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>the construction of those
possible outcomes through a</text_slice>
            </slice>
            <slice>
              <time_slice>3:33</time_slice>
              <text_slice>sequential process.</text_slice>
            </slice>
            <slice>
              <time_slice>3:35</time_slice>
              <text_slice>So think of a probabilistic
experiment that involves a</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>number of stages, and in each
one of the stages there's a</text_slice>
            </slice>
            <slice>
              <time_slice>3:42</time_slice>
              <text_slice>number of possible choices
that there may be.</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>The overall experiment consists
of carrying out all</text_slice>
            </slice>
            <slice>
              <time_slice>3:48</time_slice>
              <text_slice>the stages to the end.</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>And the number of points in the
sample space is how many</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>final outcomes there can be in
this multi-stage experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>So in this picture we have an
experiment in which of the</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>first stage we have
four choices.</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>In the second stage, no matter
what happened in the first</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>stage, the way this is drawn
we have three choices.</text_slice>
            </slice>
            <slice>
              <time_slice>4:16</time_slice>
              <text_slice>No matter whether we ended up
here, there, or there, we have</text_slice>
            </slice>
            <slice>
              <time_slice>4:19</time_slice>
              <text_slice>three choices in the
second stage.</text_slice>
            </slice>
            <slice>
              <time_slice>4:22</time_slice>
              <text_slice>And then there's a third stage
and at least in this picture,</text_slice>
            </slice>
            <slice>
              <time_slice>4:27</time_slice>
              <text_slice>no matter what happened in the
first two stages, in the third</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>stage we're going to have
two possible choices.</text_slice>
            </slice>
            <slice>
              <time_slice>4:35</time_slice>
              <text_slice>So how many leaves are there
at the end of this tree?</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>That's simple.</text_slice>
            </slice>
            <slice>
              <time_slice>4:42</time_slice>
              <text_slice>It's just the product of
these three numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>The number of possible leaves
that we have out there is 4</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>times 3 times 2.</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>Number of choices at each stage
gets multiplied, and</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>that gives us the number
of overall choices.</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>So this is the general rule, the
general trick that we are</text_slice>
            </slice>
            <slice>
              <time_slice>5:01</time_slice>
              <text_slice>going to use over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>5:03</time_slice>
              <text_slice>So let's apply it to some very
simple problems as a warm up.</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>How many license plates can you
make if you're allowed to</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>use three letters and then
followed by four digits?</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>At least if you're dealing with
the English alphabet, you</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>have 26 choices for
the first letter.</text_slice>
            </slice>
            <slice>
              <time_slice>5:23</time_slice>
              <text_slice>Then you have 26 choices
for the second letter.</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>And then 26 choices for
the third letter.</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>And then we start the digits.</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>We have 10 choices for the first
digit, 10 choices for</text_slice>
            </slice>
            <slice>
              <time_slice>5:34</time_slice>
              <text_slice>the second digit, 10 choices for
the third, 10 choices for</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>the last one.</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>Let's make it a little more
complicated, suppose that</text_slice>
            </slice>
            <slice>
              <time_slice>5:43</time_slice>
              <text_slice>we're interested in license
plates where no letter can be</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>repeated and no digit
can be repeated.</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>So you have to use different
letters, different digits.</text_slice>
            </slice>
            <slice>
              <time_slice>5:53</time_slice>
              <text_slice>How many license plates
can you make?</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>OK, let's choose the
first letter,</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>and we have 26 choices.</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>Now, I'm ready to choose my
second letter, how many</text_slice>
            </slice>
            <slice>
              <time_slice>6:02</time_slice>
              <text_slice>choices do I have?</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>I have 25, because I already
used one letter.</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>I have the 25 remaining letters
to choose from.</text_slice>
            </slice>
            <slice>
              <time_slice>6:11</time_slice>
              <text_slice>For the next letter,
how many choices?</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>Well, I used up two of
my letters, so I</text_slice>
            </slice>
            <slice>
              <time_slice>6:17</time_slice>
              <text_slice>only have 24 available.</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>And then we start with the
digits, 10 choices for the</text_slice>
            </slice>
            <slice>
              <time_slice>6:22</time_slice>
              <text_slice>first digit, 9 choices for the
second, 8 for the third, 7 for</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>the last one.</text_slice>
            </slice>
            <slice>
              <time_slice>6:30</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>So, now, let's bring some
symbols in a related problem.</text_slice>
            </slice>
            <slice>
              <time_slice>6:38</time_slice>
              <text_slice>You are given a set that
consists of n elements and</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>you're supposed to take
those n elements and</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>put them in a sequence.</text_slice>
            </slice>
            <slice>
              <time_slice>6:50</time_slice>
              <text_slice>That is to order them.</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>Any possible ordering of those
elements is called a</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>permutation.</text_slice>
            </slice>
            <slice>
              <time_slice>6:57</time_slice>
              <text_slice>So for example, if we have the
set 1, 2, 3, 4, a possible</text_slice>
            </slice>
            <slice>
              <time_slice>7:02</time_slice>
              <text_slice>permutation is the
list 2, 3, 4, 1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>That's one possible
permutation.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>And there's lots of possible
permutations, of course, the</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>question is how many
are there.</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>OK, let's think about building
this permutation by choosing</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>one at a time.</text_slice>
            </slice>
            <slice>
              <time_slice>7:21</time_slice>
              <text_slice>Which of these elements goes
into each one of these slots?</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>How many choices for the number
that goes into the</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>first slot or the elements?</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>Well, we can choose any one of
the available elements, so we</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>have n choices.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>Let's say this element goes
here, having used up that</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>element, we're left with n minus
1 elements and we can</text_slice>
            </slice>
            <slice>
              <time_slice>7:45</time_slice>
              <text_slice>pick any one of these and bring
it into the second slot.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>So here we have n choices, here
we're going to have n</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>minus 1 choices, then how
many we put there will</text_slice>
            </slice>
            <slice>
              <time_slice>7:55</time_slice>
              <text_slice>have n minus 2 choices.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>And you go down until the end.</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>What happens at this point
when you are to</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>pick the last element?</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>Well, you've used n minus of
them, there's only one</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>left in your bag.</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>You're forced to use that one.</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>So the last stage, you're going
to have only one choice.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>So, basically, the number of
possible permutations is the</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>product of all integers
from n down to one, or</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>from one up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>And there's a symbol that we
use for this number, it's</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>called n factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>So n factorial is the number of
permutations of n objects.</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>The number of ways that you can
order n objects that are</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>given to you.</text_slice>
            </slice>
            <slice>
              <time_slice>8:39</time_slice>
              <text_slice>Now, a different equation.</text_slice>
            </slice>
            <slice>
              <time_slice>8:42</time_slice>
              <text_slice>We have n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>Let's say the elements
are 1, 1,2, up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>8:48</time_slice>
              <text_slice>And it's a set.</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>And we want to create
a subset.</text_slice>
            </slice>
            <slice>
              <time_slice>8:54</time_slice>
              <text_slice>How many possible subsets
are there?</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>So speaking of subsets means
looking at each one of the</text_slice>
            </slice>
            <slice>
              <time_slice>9:02</time_slice>
              <text_slice>elements and deciding whether
you're going to put it in to</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>subsets or not.</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>For example, I could choose
to put 1 in, but 2 I'm not</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>putting it in, 3 I'm not putting
it in, 4 I'm putting</text_slice>
            </slice>
            <slice>
              <time_slice>9:17</time_slice>
              <text_slice>it, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>So that's how you
create a subset.</text_slice>
            </slice>
            <slice>
              <time_slice>9:21</time_slice>
              <text_slice>You look at each one of the
elements and you say, OK, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>going to put it in the subset,
or I'm not going to put it.</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>So think of these as consisting
of stages.</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>At each stage you look at
one element, and you</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>make a binary decision.</text_slice>
            </slice>
            <slice>
              <time_slice>9:35</time_slice>
              <text_slice>Do I put it in the
subset, or not?</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>So therefore, how many
subsets are there?</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>Well, I have two choices
for the first element.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>Am I going to put in
the subset, or not?</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>I have two choices for the
next element, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>For each one of the elements,
we have two choices.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>So the overall number of choices
is 2 to the power n.</text_slice>
            </slice>
            <slice>
              <time_slice>10:02</time_slice>
              <text_slice>So, conclusion--</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>the number of subsets, often n
element set, is 2 to the n.</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>So in particular, if we take n
equal to 1, let's check that</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>our answer makes sense.</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>If we have n equal to one, how
many subsets does it have?</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>So we're dealing with
a set of just one.</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>What are the subsets?</text_slice>
            </slice>
            <slice>
              <time_slice>10:33</time_slice>
              <text_slice>One subset is this one.</text_slice>
            </slice>
            <slice>
              <time_slice>10:37</time_slice>
              <text_slice>Do we have other subsets
of the one element set?</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>Yes, we have the empty set.</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>That's the second one.</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>These are the two possible
subsets of</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>this particular set.</text_slice>
            </slice>
            <slice>
              <time_slice>10:50</time_slice>
              <text_slice>So 2 subsets when n is equal to
1, that checks the answer.</text_slice>
            </slice>
            <slice>
              <time_slice>10:56</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>11:00</time_slice>
              <text_slice>OK, so having gone so far, we
can do our first example now.</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>So we are given a die
and we're going</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>to roll it 6 times.</text_slice>
            </slice>
            <slice>
              <time_slice>11:16</time_slice>
              <text_slice>OK, let's make some assumptions
about the rolls.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>Let's assume that the rolls are
independent, and that the</text_slice>
            </slice>
            <slice>
              <time_slice>11:29</time_slice>
              <text_slice>die is also fair.</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>So this means that the
probability of any particular</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>outcome of the die rolls--</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>for example, so we have 6 rolls,
one particular outcome</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>could be 3,3,1,6,5.</text_slice>
            </slice>
            <slice>
              <time_slice>11:48</time_slice>
              <text_slice>So that's one possible
outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>What's the probability
of this outcome?</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>There's probability 1/6 that
this happens, 1/6 that this</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>happens, 1/6 that this
happens, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:00</time_slice>
              <text_slice>So the probability that
the outcome is this</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>is 1/6 to the sixth.</text_slice>
            </slice>
            <slice>
              <time_slice>12:10</time_slice>
              <text_slice>What did I use to come
up with this answer?</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>I used independence, so I
multiplied the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>12:17</time_slice>
              <text_slice>the first roll gives me a 2,
times the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>second roll gives me
a 3, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>And then I used the assumption
that the die is fair, so that</text_slice>
            </slice>
            <slice>
              <time_slice>12:26</time_slice>
              <text_slice>the probability of 2 is
1/6, the probably of 3</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>is 1/6, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>So if I were to spell it out,
it's the probability that we</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>get the 2 in the first roll,
times the probability of 3 in</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>the second roll, times the
probability of the</text_slice>
            </slice>
            <slice>
              <time_slice>12:40</time_slice>
              <text_slice>5 in the last roll.</text_slice>
            </slice>
            <slice>
              <time_slice>12:42</time_slice>
              <text_slice>So by independence, I can
multiply probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>And because the die is fair,
each one of these numbers is</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>1/6 to the sixth.</text_slice>
            </slice>
            <slice>
              <time_slice>12:53</time_slice>
              <text_slice>And so the same calculation
would apply no matter what</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>numbers I would put in here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:00</time_slice>
              <text_slice>So all possible outcomes
are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>Let's start with this.</text_slice>
            </slice>
            <slice>
              <time_slice>13:08</time_slice>
              <text_slice>So since all possible outcomes
are equally likely to find an</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>answer to a probability
question, if we're dealing</text_slice>
            </slice>
            <slice>
              <time_slice>13:15</time_slice>
              <text_slice>with some particular event, so
the event is that all rolls</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>give different numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>13:22</time_slice>
              <text_slice>That's our event A. And our
sample space is some set</text_slice>
            </slice>
            <slice>
              <time_slice>13:31</time_slice>
              <text_slice>capital omega.</text_slice>
            </slice>
            <slice>
              <time_slice>13:32</time_slice>
              <text_slice>We know that the answer is going
to be the cardinality of</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>the set A, divided by the
cardinality of the set omega.</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>So let's deal with the
easy one first.</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>How many elements are there
in the sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>How many possible outcomes
are there when you</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>roll a dice 6 times?</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>You have 6 choices for
the first roll.</text_slice>
            </slice>
            <slice>
              <time_slice>13:54</time_slice>
              <text_slice>You have 6 choices for the
second roll and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>13:57</time_slice>
              <text_slice>So the overall number
of outcomes is going</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>to be 6 to the sixth.</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>So number of elements in
the sample space is 6</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>to the sixth power.</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>And I guess this checks
with this.</text_slice>
            </slice>
            <slice>
              <time_slice>14:14</time_slice>
              <text_slice>We have 6 to the sixth outcomes,
each one has this</text_slice>
            </slice>
            <slice>
              <time_slice>14:18</time_slice>
              <text_slice>much probability,
so the overall</text_slice>
            </slice>
            <slice>
              <time_slice>14:20</time_slice>
              <text_slice>probability is equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>Right?</text_slice>
            </slice>
            <slice>
              <time_slice>14:24</time_slice>
              <text_slice>So the probability of an
individual outcome is one over</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>how many possible outcomes
we have, which is this.</text_slice>
            </slice>
            <slice>
              <time_slice>14:32</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>So how about the numerator?</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>We are interested in outcomes
in which the numbers that we</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>get are all different.</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>So what is an outcome in which
the numbers are all different?</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>So the die has 6 faces.</text_slice>
            </slice>
            <slice>
              <time_slice>14:56</time_slice>
              <text_slice>We roll it 6 times.</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>We're going to get 6
different numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>This means that we're going to
exhaust all the possible</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>numbers, but they can appear
in any possible sequence.</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>So an outcome that makes this
event happen is a list of the</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>numbers from 1 to 6,
but arranged in</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>some arbitrary order.</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>So the possible outcomes that
make event A happen are just</text_slice>
            </slice>
            <slice>
              <time_slice>15:23</time_slice>
              <text_slice>the permutations of the
numbers from 1 to 6.</text_slice>
            </slice>
            <slice>
              <time_slice>15:31</time_slice>
              <text_slice>One possible outcome that makes
our events to happen--</text_slice>
            </slice>
            <slice>
              <time_slice>15:33</time_slice>
              <text_slice>it would be this.</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>Here we have 6 possible numbers,
but any other list of</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>this kind in which none
of the numbers is</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>repeated would also do.</text_slice>
            </slice>
            <slice>
              <time_slice>15:46</time_slice>
              <text_slice>So number of outcomes that make
the event happen is the</text_slice>
            </slice>
            <slice>
              <time_slice>15:51</time_slice>
              <text_slice>number of permutations
of 6 elements.</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>So it's 6 factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>And so the final answer is
going to be 6 factorial</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>divided by 6 to the sixth.</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>All right, so that's a typical
way that's one solves problems</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>We know how to count
certain things.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>For example, here we knew how to
count permutations, and we</text_slice>
            </slice>
            <slice>
              <time_slice>16:14</time_slice>
              <text_slice>used our knowledge to count the
elements of the set that</text_slice>
            </slice>
            <slice>
              <time_slice>16:16</time_slice>
              <text_slice>we need to deal with.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>So now let's get to a slightly
more difficult problem.</text_slice>
            </slice>
            <slice>
              <time_slice>16:30</time_slice>
              <text_slice>We're given once more a
set with n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>We already know how many subsets
that set has, but now</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>we would be interested in
subsets that have exactly k</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>elements in them.</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>So we start with our big set
that has n elements, and we</text_slice>
            </slice>
            <slice>
              <time_slice>17:05</time_slice>
              <text_slice>want to construct a subset
that has k elements.</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>Out of those n I'm
going to choose k</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>and put them in there.</text_slice>
            </slice>
            <slice>
              <time_slice>17:16</time_slice>
              <text_slice>In how many ways
can I do this?</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>More concrete way of thinking
about this problem--</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>you have n people in some group
and you want to form a</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>committee by picking people from
that group, and you want</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>to form a committee
with k people.</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>Where k is a given number.</text_slice>
            </slice>
            <slice>
              <time_slice>17:32</time_slice>
              <text_slice>For example, a 5 person
committee.</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>How many 5 person committees
are possible if you're</text_slice>
            </slice>
            <slice>
              <time_slice>17:37</time_slice>
              <text_slice>starting with 100 people?</text_slice>
            </slice>
            <slice>
              <time_slice>17:39</time_slice>
              <text_slice>So that's what we
want to count.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>How many k element subsets
are there?</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>We don't yet know the answer,
but let's give a name to it.</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>And the name is going to be this
particular symbol, which</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>we read as n choose k.</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>Out of n elements, we want
to choose k of them.</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>18:02</time_slice>
              <text_slice>That may be a little tricky.</text_slice>
            </slice>
            <slice>
              <time_slice>18:04</time_slice>
              <text_slice>So what we're going to do is
to instead figure out a</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>somewhat easier problem,
which is going to be--</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>in how many ways can I pick k
out of these people and puts</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>them in a particular order?</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>So how many possible ordered
lists can I make that consist</text_slice>
            </slice>
            <slice>
              <time_slice>18:30</time_slice>
              <text_slice>of k people?</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>By ordered, I mean that we take
those k people and we say</text_slice>
            </slice>
            <slice>
              <time_slice>18:35</time_slice>
              <text_slice>this is the first person
in the community.</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>That's the second person
in the committee.</text_slice>
            </slice>
            <slice>
              <time_slice>18:39</time_slice>
              <text_slice>That's the third person in
the committee and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>So in how many ways
can we do this?</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>Out of these n, we want to
choose just k of them and put</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>them in slots.</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>One after the other.</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>So this is pretty much like the
license plate problem we</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>solved just a little earlier.</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>So we have n choices for who
we put as the top person in</text_slice>
            </slice>
            <slice>
              <time_slice>19:06</time_slice>
              <text_slice>the community.</text_slice>
            </slice>
            <slice>
              <time_slice>19:07</time_slice>
              <text_slice>We can pick anyone and have
them be the first person.</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>Then I'm going to choose
the second</text_slice>
            </slice>
            <slice>
              <time_slice>19:13</time_slice>
              <text_slice>person in the committee.</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>I've used up 1 person.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>So I'm going to have n
minus 1 choices here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:21</time_slice>
              <text_slice>And now, at this stage I've used
up 2 people, so I have n</text_slice>
            </slice>
            <slice>
              <time_slice>19:25</time_slice>
              <text_slice>minus 2 choices here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:28</time_slice>
              <text_slice>And this keeps going on.</text_slice>
            </slice>
            <slice>
              <time_slice>19:31</time_slice>
              <text_slice>Well, what is going to
be the last number?</text_slice>
            </slice>
            <slice>
              <time_slice>19:34</time_slice>
              <text_slice>Is it's n minus k?</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>Well, not really.</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>I'm starting subtracting numbers
after the second one,</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>so by the end I will have
subtracted k minus 1.</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>So that's how many choices I
will have for the last person.</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>So this is the number
of ways--</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>the product of these numbers
there gives me the number of</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>ways that I can create ordered
lists consisting of k people</text_slice>
            </slice>
            <slice>
              <time_slice>20:08</time_slice>
              <text_slice>out of the n that
we started with.</text_slice>
            </slice>
            <slice>
              <time_slice>20:11</time_slice>
              <text_slice>Now, you can do a little bit of
algebra and check that this</text_slice>
            </slice>
            <slice>
              <time_slice>20:15</time_slice>
              <text_slice>expression here is the same
as that expression.</text_slice>
            </slice>
            <slice>
              <time_slice>20:17</time_slice>
              <text_slice>Why is this?</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>This factorial has all the
products from 1 up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>20:22</time_slice>
              <text_slice>This factorial has all
the products from 1</text_slice>
            </slice>
            <slice>
              <time_slice>20:25</time_slice>
              <text_slice>up to n minus k.</text_slice>
            </slice>
            <slice>
              <time_slice>20:26</time_slice>
              <text_slice>So you get cancellations.</text_slice>
            </slice>
            <slice>
              <time_slice>20:28</time_slice>
              <text_slice>And what's left is all the
products starting from the</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>next number after here, which
is this particular number.</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>So the number of possible ways
of creating such ordered lists</text_slice>
            </slice>
            <slice>
              <time_slice>20:42</time_slice>
              <text_slice>is n factorial divided by
n minus k factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>Now, a different way that I
could make an ordered list--</text_slice>
            </slice>
            <slice>
              <time_slice>20:53</time_slice>
              <text_slice>instead of picking the people
one at a time, I could first</text_slice>
            </slice>
            <slice>
              <time_slice>20:57</time_slice>
              <text_slice>choose my k people who are going
to be in the committee,</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>and then put them in order.</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>And tell them out of these k,
you are the first, you are the</text_slice>
            </slice>
            <slice>
              <time_slice>21:07</time_slice>
              <text_slice>second, you are the third.</text_slice>
            </slice>
            <slice>
              <time_slice>21:10</time_slice>
              <text_slice>Starting with this k
people, in how many</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>ways can I order them?</text_slice>
            </slice>
            <slice>
              <time_slice>21:15</time_slice>
              <text_slice>That's the number
of permutations.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>Starting with a set with k
objects, in how many ways can</text_slice>
            </slice>
            <slice>
              <time_slice>21:25</time_slice>
              <text_slice>I put them in a specific
order?</text_slice>
            </slice>
            <slice>
              <time_slice>21:28</time_slice>
              <text_slice>How many specific orders
are there?</text_slice>
            </slice>
            <slice>
              <time_slice>21:31</time_slice>
              <text_slice>That's basically the question.</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>In how many ways can
I permute these k</text_slice>
            </slice>
            <slice>
              <time_slice>21:34</time_slice>
              <text_slice>people and arrange them.</text_slice>
            </slice>
            <slice>
              <time_slice>21:36</time_slice>
              <text_slice>So the number of ways
that you can do</text_slice>
            </slice>
            <slice>
              <time_slice>21:38</time_slice>
              <text_slice>this step is k factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>So in how many ways can I
start with a set with n</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>elements, go through this
process, and end up with a</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>sorted list with k elements?</text_slice>
            </slice>
            <slice>
              <time_slice>21:55</time_slice>
              <text_slice>By the rule that--</text_slice>
            </slice>
            <slice>
              <time_slice>21:57</time_slice>
              <text_slice>when we have stages, the total
number of stages is how many</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>choices we had in the first
stage, times how many choices</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>we had in the second stage.</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>The number of ways that this
process can happen is this</text_slice>
            </slice>
            <slice>
              <time_slice>22:12</time_slice>
              <text_slice>times that.</text_slice>
            </slice>
            <slice>
              <time_slice>22:14</time_slice>
              <text_slice>This is a different way that
that process could happen.</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>And the number of possible
of ways is this number.</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>No matter which way we carry out
that process, in the end</text_slice>
            </slice>
            <slice>
              <time_slice>22:27</time_slice>
              <text_slice>we have the possible ways of
arranging k people out of the</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>n that we started with.</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>So the final answer that we get
when we count should be</text_slice>
            </slice>
            <slice>
              <time_slice>22:40</time_slice>
              <text_slice>either this, or this
times that.</text_slice>
            </slice>
            <slice>
              <time_slice>22:44</time_slice>
              <text_slice>Both are equally valid ways of
counting, so both should give</text_slice>
            </slice>
            <slice>
              <time_slice>22:47</time_slice>
              <text_slice>us the same answer.</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>So we get this equality here.</text_slice>
            </slice>
            <slice>
              <time_slice>22:52</time_slice>
              <text_slice>So these two expressions
corresponds to two different</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>ways of constructing ordered
lists of k people starting</text_slice>
            </slice>
            <slice>
              <time_slice>23:01</time_slice>
              <text_slice>with n people initially.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>And now that we have this
relation, we can send the k</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>factorial to the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>And that tells us what
that number, n choose</text_slice>
            </slice>
            <slice>
              <time_slice>23:13</time_slice>
              <text_slice>k, is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>23:16</time_slice>
              <text_slice>So this formula-- it's written
here in red, because you're</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>going to see it a zillion
times until</text_slice>
            </slice>
            <slice>
              <time_slice>23:22</time_slice>
              <text_slice>the end of the semester--</text_slice>
            </slice>
            <slice>
              <time_slice>23:23</time_slice>
              <text_slice>they are called the binomial
coefficients.</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>And they tell us the number of
possible ways that we can</text_slice>
            </slice>
            <slice>
              <time_slice>23:34</time_slice>
              <text_slice>create a k element subset,
starting with a</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>set that has n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>23:41</time_slice>
              <text_slice>It's always good to do a sanity
check to formulas by</text_slice>
            </slice>
            <slice>
              <time_slice>23:44</time_slice>
              <text_slice>considering extreme cases.</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>So let's take the case where
k is equal to n.</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>What's the right answer
in this case?</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>How many n elements subsets
are there out</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>of an element set?</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>Well, your subset needs
to include every one.</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>You don't have any choices.</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>There's only one choice.</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>It's the set itself.</text_slice>
            </slice>
            <slice>
              <time_slice>24:12</time_slice>
              <text_slice>So the answer should
be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>That's the number of n element
subsets, starting with a set</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>with n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>Let's see if the formula gives
us the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>We have n factorial divided by
k, which is n in our case--</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>n factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>And then n minus k
is 0 factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>24:36</time_slice>
              <text_slice>So if our formula is correct, we
should have this equality.</text_slice>
            </slice>
            <slice>
              <time_slice>24:42</time_slice>
              <text_slice>And what's the way to
make that correct?</text_slice>
            </slice>
            <slice>
              <time_slice>24:45</time_slice>
              <text_slice>Well, it depends what kind
of meaning do we</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>give to this symbol?</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>How do we define
zero factorial?</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>I guess in some ways
it's arbitrary.</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>We're going to define it
in a way that makes</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>this formula right.</text_slice>
            </slice>
            <slice>
              <time_slice>24:59</time_slice>
              <text_slice>So the definition that we will
be using is that whenever you</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>have 0 factorial, it's going
to stand for the number 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>So let's check that this is
also correct, at the other</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>extreme case.</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>If we let k equal to 0, what
does the formula give us?</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>It gives us, again, n factorial
divided by 0</text_slice>
            </slice>
            <slice>
              <time_slice>25:20</time_slice>
              <text_slice>factorial times n factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>According to our convention,
this again is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>So there is one subset of our
set that we started with that</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>has zero elements.</text_slice>
            </slice>
            <slice>
              <time_slice>25:35</time_slice>
              <text_slice>Which subset is it?</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>It's the empty set.</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>So the empty set is the single
subset of the set that we</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>started with that happens to
have exactly zero elements.</text_slice>
            </slice>
            <slice>
              <time_slice>25:49</time_slice>
              <text_slice>So the formula checks in this
extreme case as well.</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>So we're comfortable using it.</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>Now these factorials and these
coefficients are really messy</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>algebraic objects.</text_slice>
            </slice>
            <slice>
              <time_slice>26:03</time_slice>
              <text_slice>There's lots of beautiful
identities that they satisfy,</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>which you can prove
algebraically sometimes by</text_slice>
            </slice>
            <slice>
              <time_slice>26:10</time_slice>
              <text_slice>using induction and having
cancellations happen</text_slice>
            </slice>
            <slice>
              <time_slice>26:13</time_slice>
              <text_slice>all over the place.</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>But it's really messy.</text_slice>
            </slice>
            <slice>
              <time_slice>26:17</time_slice>
              <text_slice>Sometimes you can bypass those
calculations by being clever</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>and using your understanding
of what these</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>coefficients stand for.</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>So here's a typical example.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>What is the sum of those
binomial coefficients?</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>I fix n, and sum over
all possible cases.</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>So if you're an algebra genius,
you're going to take</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>this expression here, plug it in
here, and then start doing</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>algebra furiously.</text_slice>
            </slice>
            <slice>
              <time_slice>26:51</time_slice>
              <text_slice>And half an hour later, you
may get the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>But now let's try
to be clever.</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>What does this really do?</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>What does that formula count?</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>We're considering k
element subsets.</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>That's this number.</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>And we're considering the number
of k element subsets</text_slice>
            </slice>
            <slice>
              <time_slice>27:12</time_slice>
              <text_slice>for different choices of k.</text_slice>
            </slice>
            <slice>
              <time_slice>27:14</time_slice>
              <text_slice>The first term in this sum
counts how many 0-element</text_slice>
            </slice>
            <slice>
              <time_slice>27:18</time_slice>
              <text_slice>subsets we have.</text_slice>
            </slice>
            <slice>
              <time_slice>27:20</time_slice>
              <text_slice>The next term in this sum counts
how many 1-element</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>subsets we have.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>The next term counts how many
2-element subsets we have.</text_slice>
            </slice>
            <slice>
              <time_slice>27:30</time_slice>
              <text_slice>So in the end, what
have we counted?</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>We've counted the total
number of subsets.</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>We've considered all possible
cardinalities.</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>We've counted the number
of subsets of size k.</text_slice>
            </slice>
            <slice>
              <time_slice>27:46</time_slice>
              <text_slice>We've considered all
possible sizes k.</text_slice>
            </slice>
            <slice>
              <time_slice>27:49</time_slice>
              <text_slice>The overall count is
going to be the</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>total number of subsets.</text_slice>
            </slice>
            <slice>
              <time_slice>27:57</time_slice>
              <text_slice>And we know what this is.</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>A couple of slides ago, we
discussed that this number is</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>equal to 2 to the n.</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>So, nice, clean and simple
answer, which is easy to guess</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>once you give an interpretation
to the</text_slice>
            </slice>
            <slice>
              <time_slice>28:15</time_slice>
              <text_slice>algebraic expression that you
have in front of you.</text_slice>
            </slice>
            <slice>
              <time_slice>28:21</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>28:22</time_slice>
              <text_slice>So let's move again to sort of
an example in which those</text_slice>
            </slice>
            <slice>
              <time_slice>28:27</time_slice>
              <text_slice>binomial coefficients are
going to show up.</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>So here's the setting--</text_slice>
            </slice>
            <slice>
              <time_slice>28:34</time_slice>
              <text_slice>n independent coin tosses,
and each coin toss has a</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>probability, P, of resulting
in heads.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>So this is our probabilistic
experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>Suppose we do 6 tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>What's the probability that we
get this particular sequence</text_slice>
            </slice>
            <slice>
              <time_slice>28:53</time_slice>
              <text_slice>of outcomes?</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>Because of independence, we
can multiply probability.</text_slice>
            </slice>
            <slice>
              <time_slice>28:59</time_slice>
              <text_slice>So it's going to be the
probability that the first</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>toss results in heads, times
the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>29:05</time_slice>
              <text_slice>second toss results in tails,
times the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>third one results in tails,
times probability of heads,</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>times probability of heads,
times probability of heads,</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>which is just P to the fourth
times (1 minus P) squared.</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>So that's the probability of
this particular sequence.</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>How about a different
sequence?</text_slice>
            </slice>
            <slice>
              <time_slice>29:26</time_slice>
              <text_slice>If I had 4 tails and 2 heads,
but in a different order--</text_slice>
            </slice>
            <slice>
              <time_slice>29:39</time_slice>
              <text_slice>let's say if we considered
this particular outcome--</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>would the answer be different?</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>We would still have P, times P,
times P, times P, times (1</text_slice>
            </slice>
            <slice>
              <time_slice>29:49</time_slice>
              <text_slice>minus P), times (1 minus P).</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>We would get again,
the same answer.</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>So what you observe from just
this example is that, more</text_slice>
            </slice>
            <slice>
              <time_slice>29:59</time_slice>
              <text_slice>generally, the probability
of obtaining a particular</text_slice>
            </slice>
            <slice>
              <time_slice>30:03</time_slice>
              <text_slice>sequence of heads and tails is
P to a power, equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>number of heads.</text_slice>
            </slice>
            <slice>
              <time_slice>30:10</time_slice>
              <text_slice>So here we had 4 heads.</text_slice>
            </slice>
            <slice>
              <time_slice>30:12</time_slice>
              <text_slice>So there's P to the
fourth showing up.</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>And then (1 minus P) to the
power number of tails.</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>So every k head sequence--</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>every outcome in which we have
exactly k heads, has the same</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>probability, which is going to
be P to the k, (1 minus p), to</text_slice>
            </slice>
            <slice>
              <time_slice>30:37</time_slice>
              <text_slice>the (n minus k).</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>This is the probability of any
particular sequence that has</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>exactly k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>So that's the probability
of a particular</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>sequence with k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>So now let's ask the question,
what is the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>my experiment results in exactly
k heads, but in some</text_slice>
            </slice>
            <slice>
              <time_slice>30:57</time_slice>
              <text_slice>arbitrary order?</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>So the heads could
show up anywhere.</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>So there's a number
of different ways</text_slice>
            </slice>
            <slice>
              <time_slice>31:04</time_slice>
              <text_slice>that this can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>31:05</time_slice>
              <text_slice>What's the overall probability
that this event takes place?</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>So the probability of an event
taking place is the sum of the</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>probabilities of all the
individual ways that</text_slice>
            </slice>
            <slice>
              <time_slice>31:19</time_slice>
              <text_slice>the event can occur.</text_slice>
            </slice>
            <slice>
              <time_slice>31:22</time_slice>
              <text_slice>So it's the sum of the
probabilities of all the</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>outcomes that make
the event happen.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>The different ways that we can
obtain k heads are the number</text_slice>
            </slice>
            <slice>
              <time_slice>31:31</time_slice>
              <text_slice>of different sequences that
contain exactly k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>We just figured out that any
sequence with exactly k heads</text_slice>
            </slice>
            <slice>
              <time_slice>31:44</time_slice>
              <text_slice>has this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>So to do this summation, we just
need to take the common</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>probability of each individual
k head sequence, times how</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>many terms we have
in this sum.</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>So what we're left to do now
is to figure out how many k</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>head sequences are there.</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>How many outcomes are there in
which we have exactly k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>32:18</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>So what are the ways that I can
describe to you a sequence</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>with k heads?</text_slice>
            </slice>
            <slice>
              <time_slice>32:30</time_slice>
              <text_slice>I can take my n slots
that corresponds to</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>the different tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>I'm interested in particular
sequences that</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>have exactly k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>32:47</time_slice>
              <text_slice>So what I need to do is to
choose k slots and assign</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>heads to them.</text_slice>
            </slice>
            <slice>
              <time_slice>33:05</time_slice>
              <text_slice>So to specify a sequence that
has exactly k heads is the</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>same thing as drawing this
picture and telling you which</text_slice>
            </slice>
            <slice>
              <time_slice>33:17</time_slice>
              <text_slice>are the k slots that happened
to have heads.</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>So I need to choose out of those
n slots, k of them, and</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>assign them heads.</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>In how many ways can I
choose this k slots?</text_slice>
            </slice>
            <slice>
              <time_slice>33:35</time_slice>
              <text_slice>Well, it's the question of
starting with a set of n slots</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>and choosing k slots out
of the n available.</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>So the number of k head
sequences is the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>number of k element subsets of
the set of slots that we</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>started with, which are
the n slots 1 up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>We know what that number is.</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>We counted, before, the number
of k element subsets, starting</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>with a set with n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>34:20</time_slice>
              <text_slice>And we gave a symbol to that
number, which is that</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>thing, n choose k.</text_slice>
            </slice>
            <slice>
              <time_slice>34:24</time_slice>
              <text_slice>So this is the final answer
that we obtain.</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>So these are the so-called
binomial probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>And they gave us the
probabilities for different</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>numbers of heads starting with
a fair coin that's being</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>tossed a number of times.</text_slice>
            </slice>
            <slice>
              <time_slice>34:42</time_slice>
              <text_slice>This formula is correct, of
course, for reasonable values</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>of k, meaning its correct for
k equals 0, 1, up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>34:52</time_slice>
              <text_slice>If k is bigger than n, what's
the probability of k heads?</text_slice>
            </slice>
            <slice>
              <time_slice>34:57</time_slice>
              <text_slice>If k is bigger than n, there's
no way to obtain k heads, so</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>that probability is,
of course, zero.</text_slice>
            </slice>
            <slice>
              <time_slice>35:03</time_slice>
              <text_slice>So these probabilities only
makes sense for the numbers k</text_slice>
            </slice>
            <slice>
              <time_slice>35:07</time_slice>
              <text_slice>that are possible, given
that we have n tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>35:13</time_slice>
              <text_slice>And now a question similar
to the one we had in</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>the previous slide.</text_slice>
            </slice>
            <slice>
              <time_slice>35:18</time_slice>
              <text_slice>If I write down this
summation--</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>even worse algebra than the one
in the previous slide--</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>what do you think this number
will turn out to be?</text_slice>
            </slice>
            <slice>
              <time_slice>35:35</time_slice>
              <text_slice>It should be 1 because this
is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>35:39</time_slice>
              <text_slice>of obtaining k heads.</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>When we do the summation, what
we're doing is we're</text_slice>
            </slice>
            <slice>
              <time_slice>35:45</time_slice>
              <text_slice>considering the probability of
0 heads, plus the probability</text_slice>
            </slice>
            <slice>
              <time_slice>35:48</time_slice>
              <text_slice>of 1 head, plus the probability
of 2 heads, plus</text_slice>
            </slice>
            <slice>
              <time_slice>35:50</time_slice>
              <text_slice>the probability of n heads.</text_slice>
            </slice>
            <slice>
              <time_slice>35:52</time_slice>
              <text_slice>We've exhausted all the
possibilities in our</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>35:55</time_slice>
              <text_slice>So the overall probability,
when you exhaust all</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>possibilities, must
be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>So that's yet another beautiful
formula that</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>evaluates into something
really simple.</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>And if you tried to prove this
identity algebraically, of</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>course, you would have to
suffer quite a bit.</text_slice>
            </slice>
            <slice>
              <time_slice>36:16</time_slice>
              <text_slice>So now armed with the binomial
probabilities, we can do the</text_slice>
            </slice>
            <slice>
              <time_slice>36:20</time_slice>
              <text_slice>harder problems.</text_slice>
            </slice>
            <slice>
              <time_slice>36:23</time_slice>
              <text_slice>So let's take the same
experiment again.</text_slice>
            </slice>
            <slice>
              <time_slice>36:27</time_slice>
              <text_slice>We flip a coin independently
10 times.</text_slice>
            </slice>
            <slice>
              <time_slice>36:32</time_slice>
              <text_slice>So these 10 tosses
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>36:37</time_slice>
              <text_slice>We flip it 10 times.</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>We don't see the result, but
somebody comes and tells us,</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>you know, there were exactly
3 heads in the 10</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>tosses that you had.</text_slice>
            </slice>
            <slice>
              <time_slice>36:49</time_slice>
              <text_slice>OK?</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>So a certain event happened.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>And now you're asked to find
the probability of another</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>event, which is that the first
2 tosses were heads.</text_slice>
            </slice>
            <slice>
              <time_slice>37:01</time_slice>
              <text_slice>Let's call that event A. OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:08</time_slice>
              <text_slice>So are we in the setting
of discrete</text_slice>
            </slice>
            <slice>
              <time_slice>37:14</time_slice>
              <text_slice>uniform probability laws?</text_slice>
            </slice>
            <slice>
              <time_slice>37:16</time_slice>
              <text_slice>When we toss a coin multiple
times, is it the case that all</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>outcomes are equally likely?</text_slice>
            </slice>
            <slice>
              <time_slice>37:24</time_slice>
              <text_slice>All sequences are
equally likely?</text_slice>
            </slice>
            <slice>
              <time_slice>37:27</time_slice>
              <text_slice>That's the case if you
have a fair coin--</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>that all sequences are
equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>37:32</time_slice>
              <text_slice>But if your coin is not fair,
of course, heads/heads is</text_slice>
            </slice>
            <slice>
              <time_slice>37:37</time_slice>
              <text_slice>going to have a different
probability than tails/tails.</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>If your coin is biased towards
heads, then heads/heads is</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>going to be more likely.</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>So we're not quite in
the uniform setting.</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>Our overall sample space, omega,
does not have equally</text_slice>
            </slice>
            <slice>
              <time_slice>37:53</time_slice>
              <text_slice>likely elements.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>Do we care about that?</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>Not necessarily.</text_slice>
            </slice>
            <slice>
              <time_slice>37:59</time_slice>
              <text_slice>All the action now happens
inside the event B that we are</text_slice>
            </slice>
            <slice>
              <time_slice>38:04</time_slice>
              <text_slice>told has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>So we have our big sample
space, omega.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>Elements of that sample space
are not equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>We are told that a certain
event B occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>And inside that event B, we're
asked to find the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>probability that A has
also occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>Now here's the lucky thing,
inside the event B, all</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>outcomes are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>The outcomes inside B are the
sequences of 10 tosses that</text_slice>
            </slice>
            <slice>
              <time_slice>38:40</time_slice>
              <text_slice>have exactly 3 heads.</text_slice>
            </slice>
            <slice>
              <time_slice>38:42</time_slice>
              <text_slice>Every 3-head sequence has
this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>So the elements of
B are equally</text_slice>
            </slice>
            <slice>
              <time_slice>38:50</time_slice>
              <text_slice>likely with each other.</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>Once we condition on the event
B having occurred, what</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>happens to the probabilities
of the different outcomes</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>inside here?</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>Well, conditional probability
laws keep the same proportions</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>as the unconditional ones.</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>The elements of B were equally
likely when we started, so</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>they're equally likely once we
are told that B has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>39:21</time_slice>
              <text_slice>So to do with this problem, we
need to just transport us to</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>this smaller universe and think
about what's happening</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>in that little universe.</text_slice>
            </slice>
            <slice>
              <time_slice>39:32</time_slice>
              <text_slice>In that little universe,
all elements of</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>B are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>So to find the probability of
some subset of that set, we</text_slice>
            </slice>
            <slice>
              <time_slice>39:43</time_slice>
              <text_slice>only need to count the
cardinality of B, and count</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>the cardinality of A.
So let's do that.</text_slice>
            </slice>
            <slice>
              <time_slice>39:51</time_slice>
              <text_slice>Number of outcomes in B--</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>in how many ways can we get
3 heads out of 10 tosses?</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>That's the number we considered
before, and</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>it's 10 choose 3.</text_slice>
            </slice>
            <slice>
              <time_slice>40:06</time_slice>
              <text_slice>This is the number of
3-head sequences</text_slice>
            </slice>
            <slice>
              <time_slice>40:11</time_slice>
              <text_slice>when you have 10 tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>40:13</time_slice>
              <text_slice>Now let's look at the event A.
The event A is that the first</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>2 tosses where heads, but we're
living now inside this</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>universe B. Given that B
occurred, how many elements</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>does A have in there?</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>In how many ways can A happen
inside the B universe.</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>If you're told that the
first 2 were heads--</text_slice>
            </slice>
            <slice>
              <time_slice>40:46</time_slice>
              <text_slice>sorry.</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>So out of the outcomes in B that
have 3 heads, how many</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>start with heads/heads?</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>Well, if it starts with
heads/heads, then the only</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>uncertainty is the location
of the third head.</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>So we started with heads/heads,
we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>have three heads, the question
is, where is that third head</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>It has eight possibilities.</text_slice>
            </slice>
            <slice>
              <time_slice>41:16</time_slice>
              <text_slice>So slot 1 is heads, slot 2 is
heads, the third heads can be</text_slice>
            </slice>
            <slice>
              <time_slice>41:20</time_slice>
              <text_slice>anywhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>So there's 8 possibilities
for where the third</text_slice>
            </slice>
            <slice>
              <time_slice>41:25</time_slice>
              <text_slice>head is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>41:29</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>So what we have counted here is
really the cardinality of A</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>intersection B, which is out of
the elements in B, how many</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>of them make A happen, divided
by the cardinality of B. And</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>that gives us the answer, which
is going to be 10 choose</text_slice>
            </slice>
            <slice>
              <time_slice>41:53</time_slice>
              <text_slice>3, divided by 8.</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>And I should probably redraw a
little bit of the picture that</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>they have here.</text_slice>
            </slice>
            <slice>
              <time_slice>42:02</time_slice>
              <text_slice>The set A is not necessarily
contained in B. It could also</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>have stuff outside B. So the
event that the first 2 tosses</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>are heads can happen with a
total of 3 heads, but it can</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>also happen with a different
total number of heads.</text_slice>
            </slice>
            <slice>
              <time_slice>42:22</time_slice>
              <text_slice>But once we are transported
inside the set B, what we need</text_slice>
            </slice>
            <slice>
              <time_slice>42:27</time_slice>
              <text_slice>to count is just this part of
A. It's A intersection B and</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>compare it with the total number
of elements in the set</text_slice>
            </slice>
            <slice>
              <time_slice>42:35</time_slice>
              <text_slice>B. Did I write it the
opposite way?</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>Yes.</text_slice>
            </slice>
            <slice>
              <time_slice>42:41</time_slice>
              <text_slice>So this is 8 over 10 choose 3.</text_slice>
            </slice>
            <slice>
              <time_slice>42:49</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>42:49</time_slice>
              <text_slice>So we're going to close with a
more difficult problem now.</text_slice>
            </slice>
            <slice>
              <time_slice>42:57</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>43:00</time_slice>
              <text_slice>This business of n choose k has
to do with starting with a</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>set and picking a subset
of k elements.</text_slice>
            </slice>
            <slice>
              <time_slice>43:11</time_slice>
              <text_slice>Another way of thinking of that
is that we start with a</text_slice>
            </slice>
            <slice>
              <time_slice>43:15</time_slice>
              <text_slice>set with n elements and you
choose a subset that has k,</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>which means that there's n
minus k that are left.</text_slice>
            </slice>
            <slice>
              <time_slice>43:24</time_slice>
              <text_slice>Picking a subset is the same as
partitioning our set into</text_slice>
            </slice>
            <slice>
              <time_slice>43:29</time_slice>
              <text_slice>two pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>Now let's generalize this
question and start counting</text_slice>
            </slice>
            <slice>
              <time_slice>43:36</time_slice>
              <text_slice>partitions in general.</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>Somebody gives you a set
that has n elements.</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>Somebody gives you also
certain numbers--</text_slice>
            </slice>
            <slice>
              <time_slice>43:44</time_slice>
              <text_slice>n1, n2, n3, let's say,
n4, where these</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>numbers add up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>And you're asked to partition
this set into four subsets</text_slice>
            </slice>
            <slice>
              <time_slice>43:58</time_slice>
              <text_slice>where each one of the subsets
has this particular</text_slice>
            </slice>
            <slice>
              <time_slice>44:01</time_slice>
              <text_slice>cardinality.</text_slice>
            </slice>
            <slice>
              <time_slice>44:02</time_slice>
              <text_slice>So you're asking to cut it into
four pieces, each one</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>having the prescribed
cardinality.</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>In how many ways can we
do this partitioning?</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>n choose k was the answer when
we partitioned in two pieces,</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>what's the answer
more generally?</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>For a concrete example of a
partition, you have your 52</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>card deck and you deal, as in
bridge, by giving 13 cards to</text_slice>
            </slice>
            <slice>
              <time_slice>44:32</time_slice>
              <text_slice>each one of the players.</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>Assuming that the dealing is
done fairly and with a well</text_slice>
            </slice>
            <slice>
              <time_slice>44:38</time_slice>
              <text_slice>shuffled deck of cards, every
particular partition of the 52</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>cards into four hands, that is
four subsets of 13 each,</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>should be equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>44:52</time_slice>
              <text_slice>So we take the 52 cards and we
partition them into subsets of</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>13, 13, 13, and 13.</text_slice>
            </slice>
            <slice>
              <time_slice>44:58</time_slice>
              <text_slice>And we assume that all possible
partitions, all</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>possible ways of dealing the
cards are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>So we are again in a setting
where we can use counting,</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>because all the possible
outcomes are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>45:10</time_slice>
              <text_slice>So an outcome of the experiment
is the hands that</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>each player ends up getting.</text_slice>
            </slice>
            <slice>
              <time_slice>45:17</time_slice>
              <text_slice>And when you get the cards in
your hands, it doesn't matter</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>in which order that
you got them.</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>It only matters what cards
you have on you.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>So it only matters which subset
of the cards you got.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>So what's the cardinality of
the sample space in this</text_slice>
            </slice>
            <slice>
              <time_slice>45:35</time_slice>
              <text_slice>experiment?</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>So let's do it for the concrete
numbers that we have</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>for the problem of partitioning
52 cards.</text_slice>
            </slice>
            <slice>
              <time_slice>45:49</time_slice>
              <text_slice>So think of dealing as follows--
you shuffle the deck</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>perfectly, and then you take the
top 13 cards and give them</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>to one person.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>In how many possible hands are
there for that person?</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>Out of the 52 cards, I choose 13
at random and give them to</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>the first person.</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>Having done that, what
happens next?</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>I'm left with 39 cards.</text_slice>
            </slice>
            <slice>
              <time_slice>46:16</time_slice>
              <text_slice>And out of those 39 cards, I
pick 13 of them and give them</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>to the second person.</text_slice>
            </slice>
            <slice>
              <time_slice>46:22</time_slice>
              <text_slice>Now I'm left with 26 cards.</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>Out of those 26, I choose 13,
give them to the third person.</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>And for the last person there
isn't really any choice.</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>Out of the 13, I have to give
that person all 13.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>And that number is
just equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>So we don't care about it.</text_slice>
            </slice>
            <slice>
              <time_slice>46:43</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>46:43</time_slice>
              <text_slice>So next thing you do is to write
down the formulas for</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>these numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>46:49</time_slice>
              <text_slice>So, for example, here you
would have 52 factorial,</text_slice>
            </slice>
            <slice>
              <time_slice>46:52</time_slice>
              <text_slice>divided by 13 factorial,
times 39</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>factorial, and you continue.</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>And then there are nice
cancellations that happen.</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>This 39 factorial is going to
cancel the 39 factorial that</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>comes from there, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>After you do the cancellations
and all the algebra, you're</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>left with this particular
answer, which is the number of</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>possible partitions of 52 cards
into four players where</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>each player gets exactly
13 hands.</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>If you were to generalize this
formula to the setting that we</text_slice>
            </slice>
            <slice>
              <time_slice>47:25</time_slice>
              <text_slice>have here, the more general
formula is--</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>you have n factorial, where n is
the number of objects that</text_slice>
            </slice>
            <slice>
              <time_slice>47:33</time_slice>
              <text_slice>you are distributing, divided
by the product of the</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>factorials of the--</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>OK, here I'm doing it for
the case where we split</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>it into four sets.</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>So that would be the answer when
we partition a set into</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>four subsets of prescribed
cardinalities.</text_slice>
            </slice>
            <slice>
              <time_slice>47:57</time_slice>
              <text_slice>And you can guess how that
formula would generalize if</text_slice>
            </slice>
            <slice>
              <time_slice>48:00</time_slice>
              <text_slice>you want to split it into
five sets or six sets.</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>So far we just figured out the
size of the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>48:10</time_slice>
              <text_slice>Now we need to look at our
event, which is the event that</text_slice>
            </slice>
            <slice>
              <time_slice>48:14</time_slice>
              <text_slice>each player gets an ace, let's
call that event A. In how many</text_slice>
            </slice>
            <slice>
              <time_slice>48:20</time_slice>
              <text_slice>ways can that event happens?</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>How many possible hands are
there in which every player</text_slice>
            </slice>
            <slice>
              <time_slice>48:26</time_slice>
              <text_slice>has exactly one ace?</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>So I need to think about the
sequential process by which I</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>distribute the cards so that
everybody gets exactly one</text_slice>
            </slice>
            <slice>
              <time_slice>48:36</time_slice>
              <text_slice>ace, and then try to think
in how many ways can that</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>sequential process happen.</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>So one way of making sure that
everybody gets exactly one ace</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>is the following--</text_slice>
            </slice>
            <slice>
              <time_slice>48:46</time_slice>
              <text_slice>I take the four aces and I
distribute them randomly to</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>the four players, but making
sure that each one gets</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>exactly one ace.</text_slice>
            </slice>
            <slice>
              <time_slice>48:55</time_slice>
              <text_slice>In how many ways can
that happen?</text_slice>
            </slice>
            <slice>
              <time_slice>48:57</time_slice>
              <text_slice>I take the ace of spades and I
send it to a random person out</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>of the four.</text_slice>
            </slice>
            <slice>
              <time_slice>49:03</time_slice>
              <text_slice>So there's 4 choices for this.</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>Then I'm left with 3
aces to distribute.</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>That person already
gotten an ace.</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>I take the next ace, and
I give it to one of</text_slice>
            </slice>
            <slice>
              <time_slice>49:17</time_slice>
              <text_slice>the 3 people remaining.</text_slice>
            </slice>
            <slice>
              <time_slice>49:19</time_slice>
              <text_slice>So there's 3 choices
for how to do that.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>And then for the next ace,
there's 2 people who have not</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>yet gotten an ace,
and they give it</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>randomly to one of them.</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>So these are the possible ways
of distributing for the 4</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>aces, so that each person
gets exactly one.</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>It's actually the same
as this problem.</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>Starting with a set of four
things, in how many ways can I</text_slice>
            </slice>
            <slice>
              <time_slice>49:48</time_slice>
              <text_slice>partition them into four subsets
where the first set</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>has one element, the second has
one element, the third one</text_slice>
            </slice>
            <slice>
              <time_slice>49:56</time_slice>
              <text_slice>has another element,
and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>49:58</time_slice>
              <text_slice>So it agrees with that formula
by giving us 4 factorial.</text_slice>
            </slice>
            <slice>
              <time_slice>50:05</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>So there are different ways
of distributing the aces.</text_slice>
            </slice>
            <slice>
              <time_slice>50:09</time_slice>
              <text_slice>And then there's different
ways of distributing the</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>remaining 48 cards.</text_slice>
            </slice>
            <slice>
              <time_slice>50:13</time_slice>
              <text_slice>How many ways are there?</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>Well, I have 48 cards that I'm
going to distribute to four</text_slice>
            </slice>
            <slice>
              <time_slice>50:18</time_slice>
              <text_slice>players by giving 12
cards to each one.</text_slice>
            </slice>
            <slice>
              <time_slice>50:22</time_slice>
              <text_slice>It's exactly the same question
as the one we had here, except</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>that now it's 48 cards,
12 to each person.</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>And that gives us this
particular count.</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>So putting all that together
gives us the different ways</text_slice>
            </slice>
            <slice>
              <time_slice>50:39</time_slice>
              <text_slice>that we can distribute the cards
to the four players so</text_slice>
            </slice>
            <slice>
              <time_slice>50:43</time_slice>
              <text_slice>that each one gets
exactly one ace.</text_slice>
            </slice>
            <slice>
              <time_slice>50:45</time_slice>
              <text_slice>The number of possible ways
is going to be this four</text_slice>
            </slice>
            <slice>
              <time_slice>50:48</time_slice>
              <text_slice>factorial, coming from here,
times this number--</text_slice>
            </slice>
            <slice>
              <time_slice>50:54</time_slice>
              <text_slice>this gives us the number of
ways that the event of</text_slice>
            </slice>
            <slice>
              <time_slice>50:56</time_slice>
              <text_slice>interest can happen--</text_slice>
            </slice>
            <slice>
              <time_slice>50:58</time_slice>
              <text_slice>and then the denominator is the
cardinality of our sample</text_slice>
            </slice>
            <slice>
              <time_slice>51:02</time_slice>
              <text_slice>space, which is this number.</text_slice>
            </slice>
            <slice>
              <time_slice>51:04</time_slice>
              <text_slice>So this looks like
a horrible mess.</text_slice>
            </slice>
            <slice>
              <time_slice>51:07</time_slice>
              <text_slice>It turns out that this
expression does simplify to</text_slice>
            </slice>
            <slice>
              <time_slice>51:10</time_slice>
              <text_slice>something really,
really simple.</text_slice>
            </slice>
            <slice>
              <time_slice>51:13</time_slice>
              <text_slice>And if you look at the textbook
for this problem, you</text_slice>
            </slice>
            <slice>
              <time_slice>51:16</time_slice>
              <text_slice>will see an alternative
derivation that gives you a</text_slice>
            </slice>
            <slice>
              <time_slice>51:18</time_slice>
              <text_slice>short cut to the same
numerical answer.</text_slice>
            </slice>
            <slice>
              <time_slice>51:22</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>51:23</time_slice>
              <text_slice>So that basically concludes
chapter one.</text_slice>
            </slice>
            <slice>
              <time_slice>51:25</time_slice>
              <text_slice>From next time we're going to
consider introducing random</text_slice>
            </slice>
            <slice>
              <time_slice>51:29</time_slice>
              <text_slice>variables and make the subject
even more interesting.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Classical Statistical Inference - I (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l23/</lecture_pdf_url>
      <lectureno>23</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Estimate a mean Condence intervals (CIs)
2 An estimate may not be informativeX1,...,X n: i.i.d., mean , variance  n
enough
Xi=+Wi
An 1condence interval
W nc2i: i.i.d., mean, 0, varia e  +is a (random) interval [n,n],
X X n= s ean = Mn= s.t.+1+
n ample m+P( n
nn)1,
often =0.05, or 0.25, or 0.01
Properties: interpretation is subtle
E[n]= (unbiased) CI in estimation of the mean
n=(X1++XWLLN: n)/n
n(consistency)normal tables: (1.96) = 1
MSE:20.05/2
/n
P|n|96n1. .
0 95 (CLT)/
Sample mean often turns out to also be 1.96  1.96 P + 0.95the ML estimate.n nn  n
E.g., if2XiN(,), i.i.d. 
More generally: let zbe s.t. (z)=1/2
z zPn n n+n
1
The case of unknown 
Option 1: use upper bound on 
ifXiBernoulli: 1/2
Option 2: use ad hoc estimate of 
ifXiBernoulli( ):=
(1)
Option 3: Use generic estimate
of the variance
Start from2=E[(Xi)2]
21n
n=
(Xini=12)2
(but do not know )
S2 1n
n=
(Xin)2
n1i=12
(unbiased: E[S2n]=2)
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 23 Classical statistics
Readings: Section 9.1N
(not responsible for t-based condence  X pX(x;)Estimatorintervals, in pp. 471-473)
also for vectors Xand :
Outline
pX,...,X (x1,...,x n;,...,1 n1 m)
Classical statistics
These are NOT conditional probabilities;Maximum likelihood (ML) estimation
is NOT randomEstimating a sample mean
mathematically: many models,Condence intervals (CIs)one for each possible value of 
CIs using an estimated variance
Problem types:
Hypothesis testing:
H0:=1/2 versus H1:=3/4
Composite hypotheses:H
0:=1/2 versus H1:=1/2
Estimation: design an estimator ,
to keep estimation errorsmall
Maximum Likelihood Estimation Desirable properties of estimators
(should hold FOR ALL !!!)Model, with unknown parameter(s):
XpX(x;)Unbiased: E[n]=
Pick that makes data most likely
exponential example, with n= 1:
ML=a r gm a x pX(x;)E[1/X 1]= =
(biased)
Compare to Bayesian MAP estimation:Consistent: n(in probability)
MAP =a r gm a x p|X(|x) 
exponential example:
p (x|)p() (X1+ + Xn)/n E[X]=1 /X MAP =a r gm a x| 
 p(x) can use this to show that:X
n=n/(X 1+ + Xn) 1/ E[X]=
Example: X1,...,X n: i.i.d., exponential( ) 
Small mean d error (MSE)n
maxsquare
x
ei
 [()2E ]=v a r ( )+(E[])2
i=1
=v a r ( ) as)2
+ (bin
max nlog
i
xi
=1
n n ML= n=x1++xn X1++Xn
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x |)
X

p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
Estimator
p()
pX|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|) pX(x;)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10

1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-5-discrete-variables-probability-expectations/</video_url>
          <video_title>Lecture 5: Discrete Random Variables; Probability Mass Functions; Expectations</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation, or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:17</time_slice>
              <text_slice>ocw.mit.edu,</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>0:23</time_slice>
              <text_slice>So let us start.</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>So today we're starting a
new unit in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>We have covered, so far, the
basics of probability theory--</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>the main concepts and tools, as
far as just probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>are concerned.</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>But if that was all that there
is in this subject, the</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>subject would not
be rich enough.</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>What makes probability theory
a lot more interesting and</text_slice>
            </slice>
            <slice>
              <time_slice>1:16</time_slice>
              <text_slice>richer is that we can also talk
about random variables,</text_slice>
            </slice>
            <slice>
              <time_slice>1:19</time_slice>
              <text_slice>which are ways of assigning
numerical results to the</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>outcomes of an experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>So we're going to define what
random variables are, and then</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>we're going to describe them
using so-called probability</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>mass functions.</text_slice>
            </slice>
            <slice>
              <time_slice>1:36</time_slice>
              <text_slice>Basically some numerical values
are more likely to</text_slice>
            </slice>
            <slice>
              <time_slice>1:39</time_slice>
              <text_slice>occur than other numerical
values, and we capture this by</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>assigning probabilities
to them the usual way.</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>And we represent these in
a compact way using the</text_slice>
            </slice>
            <slice>
              <time_slice>1:49</time_slice>
              <text_slice>so-called probability
mass functions.</text_slice>
            </slice>
            <slice>
              <time_slice>1:51</time_slice>
              <text_slice>We're going to see a couple of
examples of random variables,</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>some of which we have already
seen but with different</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>terminology.</text_slice>
            </slice>
            <slice>
              <time_slice>1:59</time_slice>
              <text_slice>And so far, it's going to be
just a couple of definitions</text_slice>
            </slice>
            <slice>
              <time_slice>2:04</time_slice>
              <text_slice>and calculations of
the type that you</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>already know how to do.</text_slice>
            </slice>
            <slice>
              <time_slice>2:08</time_slice>
              <text_slice>But then we're going to
introduce the one new, big</text_slice>
            </slice>
            <slice>
              <time_slice>2:11</time_slice>
              <text_slice>concept of the day.</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>So up to here it's going to be
mostly an exercise in notation</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>and definitions.</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>But then we got our big concept
which is the concept</text_slice>
            </slice>
            <slice>
              <time_slice>2:20</time_slice>
              <text_slice>of the expected value of a
random variable, which is some</text_slice>
            </slice>
            <slice>
              <time_slice>2:24</time_slice>
              <text_slice>kind of average value of
the random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>And then we're going to also
talk, very briefly, about</text_slice>
            </slice>
            <slice>
              <time_slice>2:30</time_slice>
              <text_slice>close distance of the
expectation, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>concept of the variance
of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>So what is a random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>It's an assignment of a
numerical value to every</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>possible outcome of
the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>So here's the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>2:51</time_slice>
              <text_slice>The sample space is this class,
and we've got lots of</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>students in here.</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>This is our sample
space, omega.</text_slice>
            </slice>
            <slice>
              <time_slice>3:00</time_slice>
              <text_slice>I'm interested in the height
of a random student.</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>So I'm going to use a real line
where I record height,</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>and let's say this is
height in inches.</text_slice>
            </slice>
            <slice>
              <time_slice>3:12</time_slice>
              <text_slice>And the experiment happens,
I pick a random student.</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>And I go and measure the height
of that random student,</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>and that gives me a
specific number.</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>So what's a good number
in inches?</text_slice>
            </slice>
            <slice>
              <time_slice>3:25</time_slice>
              <text_slice>Let's say 60.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>Or I pick another student, and
that student has a height of</text_slice>
            </slice>
            <slice>
              <time_slice>3:33</time_slice>
              <text_slice>71 inches, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>So this is the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>These are the outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>3:39</time_slice>
              <text_slice>These are the numerical values
of the random variable that we</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>call height.</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>So mathematically, what are
we dealing with here?</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>We're basically dealing with a
function from the sample space</text_slice>
            </slice>
            <slice>
              <time_slice>3:54</time_slice>
              <text_slice>into the real numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>3:56</time_slice>
              <text_slice>That function takes as argument,
an outcome of the</text_slice>
            </slice>
            <slice>
              <time_slice>4:01</time_slice>
              <text_slice>experiment, that is a typical
student, and produces the</text_slice>
            </slice>
            <slice>
              <time_slice>4:04</time_slice>
              <text_slice>value of that function, which
is the height of that</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>particular student.</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>So we think of an abstract
object that we denote by a</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>capital H, which is the random
variable called height.</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>And that random variable is
essentially this particular</text_slice>
            </slice>
            <slice>
              <time_slice>4:21</time_slice>
              <text_slice>function that we talked
about here.</text_slice>
            </slice>
            <slice>
              <time_slice>4:25</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>4:26</time_slice>
              <text_slice>So there's a distinction that
we're making here--</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>H is height in the abstract.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>It's the function.</text_slice>
            </slice>
            <slice>
              <time_slice>4:33</time_slice>
              <text_slice>These numbers here are
particular numerical values</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>that this function takes when
you choose one particular</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>outcome of the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>Now, when you have a single
probability experiment, you</text_slice>
            </slice>
            <slice>
              <time_slice>4:44</time_slice>
              <text_slice>can have multiple random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>4:47</time_slice>
              <text_slice>So perhaps, instead of just
height, I'm also interested in</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>the weight of a typical
student.</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>And so when the experiment
happens, I</text_slice>
            </slice>
            <slice>
              <time_slice>4:58</time_slice>
              <text_slice>pick that random student--</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>this is the height
of the student.</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>But that student would also
have a weight, and I could</text_slice>
            </slice>
            <slice>
              <time_slice>5:05</time_slice>
              <text_slice>record it here.</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>And similarly, every student
is going to have their own</text_slice>
            </slice>
            <slice>
              <time_slice>5:09</time_slice>
              <text_slice>particular weight.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>So the weight function is a
different function from the</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>sample space to the real
numbers, and it's a different</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:18</time_slice>
              <text_slice>So the point I'm making here is
that a single probabilistic</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>experiment may involve several
interesting random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>5:26</time_slice>
              <text_slice>I may be interested in the
height of a random student or</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>the weight of the
random student.</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>These are different
random variables</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>that could be of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>5:35</time_slice>
              <text_slice>I can also do other things.</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>Suppose I define an object such
as H bar, which is 2.58.</text_slice>
            </slice>
            <slice>
              <time_slice>5:44</time_slice>
              <text_slice>What does that correspond to?</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>Well, this is the height
in centimeters.</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>Now, H bar is a function of H
itself, but if you were to</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>draw the picture, the picture
would go this way.</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>60 gets mapped to 150, 71 gets
mapped to, oh, that's</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>too hard for me.</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>OK, gets mapped to something,
and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>6:10</time_slice>
              <text_slice>So H bar is also a
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>Once I pick a particular
student, that particular</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>outcome determines completely
the numerical value of H bar,</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>which is the height of that
student but measured in</text_slice>
            </slice>
            <slice>
              <time_slice>6:29</time_slice>
              <text_slice>centimeters.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>What we have here is actually
a random variable, which is</text_slice>
            </slice>
            <slice>
              <time_slice>6:33</time_slice>
              <text_slice>defined as a function of another
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>And the point that this example
is trying to make is</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>that functions of
random variables</text_slice>
            </slice>
            <slice>
              <time_slice>6:42</time_slice>
              <text_slice>are also random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>The experiment happens, the
experiment determines a</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>numerical value for
this object.</text_slice>
            </slice>
            <slice>
              <time_slice>6:49</time_slice>
              <text_slice>And once you have the numerical
value for this</text_slice>
            </slice>
            <slice>
              <time_slice>6:51</time_slice>
              <text_slice>object, that determines
also the numerical</text_slice>
            </slice>
            <slice>
              <time_slice>6:54</time_slice>
              <text_slice>value for that object.</text_slice>
            </slice>
            <slice>
              <time_slice>6:55</time_slice>
              <text_slice>So given an outcome, the
numerical value of this</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>particular object
is determined.</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>So H bar is itself a function
from the sample space, from</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>outcomes to numerical values.</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>And that makes it a random
variable according to the</text_slice>
            </slice>
            <slice>
              <time_slice>7:11</time_slice>
              <text_slice>formal definition that
we have here.</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>So the formal definition is that
the random variable is</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>not random, it's not a variable,
it's just a function</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>from the sample space
to the real numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>That's the abstract, right way
of thinking about them.</text_slice>
            </slice>
            <slice>
              <time_slice>7:29</time_slice>
              <text_slice>Now, random variables can
be of different types.</text_slice>
            </slice>
            <slice>
              <time_slice>7:32</time_slice>
              <text_slice>They can be discrete
or continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>Suppose that I measure the
heights in inches, but I round</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>to the nearest inch.</text_slice>
            </slice>
            <slice>
              <time_slice>7:40</time_slice>
              <text_slice>Then the numerical values I'm
going to get here would be</text_slice>
            </slice>
            <slice>
              <time_slice>7:43</time_slice>
              <text_slice>just integers.</text_slice>
            </slice>
            <slice>
              <time_slice>7:45</time_slice>
              <text_slice>So that would make
it an integer</text_slice>
            </slice>
            <slice>
              <time_slice>7:47</time_slice>
              <text_slice>valued random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>7:48</time_slice>
              <text_slice>And this is a discrete
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>Or maybe I have a scale for
measuring height which is</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>infinitely precise and records
your height to an infinite</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>number of digits of precision.</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>In that case, your height
would be just a</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>general real number.</text_slice>
            </slice>
            <slice>
              <time_slice>8:05</time_slice>
              <text_slice>So we would have a random
variable that takes values in</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>the entire set of
real numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>Well, I guess not really
negative numbers, but the set</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>of non-negative numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>8:16</time_slice>
              <text_slice>And that would be a continuous
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>8:19</time_slice>
              <text_slice>It takes values in
a continuous set.</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>So we will be talking about both
discrete and continuous</text_slice>
            </slice>
            <slice>
              <time_slice>8:25</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>The first thing we will do
will be to devote a few</text_slice>
            </slice>
            <slice>
              <time_slice>8:28</time_slice>
              <text_slice>lectures on discrete random
variables, because discrete is</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>always easier.</text_slice>
            </slice>
            <slice>
              <time_slice>8:33</time_slice>
              <text_slice>And then we're going to
repeat everything in</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>the continuous setting.</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>So discrete is easier, and
it's the right place to</text_slice>
            </slice>
            <slice>
              <time_slice>8:41</time_slice>
              <text_slice>understand all the concepts,
even those who may appear to</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>be elementary.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>And then you will be set to
understand what's going on</text_slice>
            </slice>
            <slice>
              <time_slice>8:50</time_slice>
              <text_slice>when we go to the
continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>So in the continuous case, you
get all the complications of</text_slice>
            </slice>
            <slice>
              <time_slice>8:54</time_slice>
              <text_slice>calculus and some extra math
that comes in there.</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>So it's important to have been
down all the concepts very</text_slice>
            </slice>
            <slice>
              <time_slice>9:00</time_slice>
              <text_slice>well in the easy, discrete case
so that you don't have</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>conceptual hurdles when
you move on to</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>Now, one important remark that
may seem trivial but it's</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>actually very important so that
you don't get tangled up</text_slice>
            </slice>
            <slice>
              <time_slice>9:17</time_slice>
              <text_slice>between different types
of concepts--</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>there's a fundamental
distinction between the random</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>variable itself, and
the numerical</text_slice>
            </slice>
            <slice>
              <time_slice>9:26</time_slice>
              <text_slice>values that it takes.</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>Abstractly speaking, or
mathematically speaking, a</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>random variable, x, or H in this
example, is a function.</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>9:39</time_slice>
              <text_slice>Maybe if you like programming
the words "procedure" or</text_slice>
            </slice>
            <slice>
              <time_slice>9:43</time_slice>
              <text_slice>"sub-routine" might be better.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>So what's the sub-routine
height?</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>Given a student, I take that
student, force them on the</text_slice>
            </slice>
            <slice>
              <time_slice>9:51</time_slice>
              <text_slice>scale and measure them.</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>That's the sub-routine that
measures heights.</text_slice>
            </slice>
            <slice>
              <time_slice>9:56</time_slice>
              <text_slice>It's really a function that
takes students as input and</text_slice>
            </slice>
            <slice>
              <time_slice>10:00</time_slice>
              <text_slice>produces numbers as output.</text_slice>
            </slice>
            <slice>
              <time_slice>10:02</time_slice>
              <text_slice>The sub-routine we denoted
by capital H.</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>That's the random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>10:07</time_slice>
              <text_slice>But once you plug in a
particular student into that</text_slice>
            </slice>
            <slice>
              <time_slice>10:10</time_slice>
              <text_slice>sub-routine, you end up getting
a particular number.</text_slice>
            </slice>
            <slice>
              <time_slice>10:14</time_slice>
              <text_slice>This is the numerical output
of that sub-routine or the</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>numerical value of
that function.</text_slice>
            </slice>
            <slice>
              <time_slice>10:19</time_slice>
              <text_slice>And that numerical value is an
element of the real numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>10:24</time_slice>
              <text_slice>So the numerical value is a
real number, where this</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>capital X is a function from
omega to the real numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>10:35</time_slice>
              <text_slice>So they are very different
types of objects.</text_slice>
            </slice>
            <slice>
              <time_slice>10:38</time_slice>
              <text_slice>And the way that we keep track
of what we're talking about at</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>any given time is by using
capital letters for random</text_slice>
            </slice>
            <slice>
              <time_slice>10:45</time_slice>
              <text_slice>variables and lower case
letters for numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>So now once we have a random
variable at hand, that random</text_slice>
            </slice>
            <slice>
              <time_slice>11:00</time_slice>
              <text_slice>variable takes on different
numerical values.</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>And we want to describe to say
something about the relative</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>likelihoods of the different
numerical values that the</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>random variable can take.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>So here's our sample space,
and here's the real line.</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>And there's a bunch of outcomes
that gave rise to one</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>particular numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>11:30</time_slice>
              <text_slice>There's another numerical value
that arises if we have</text_slice>
            </slice>
            <slice>
              <time_slice>11:33</time_slice>
              <text_slice>this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>There's another numerical value
that arises if we have</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>So our sample space is here.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>The real numbers are here.</text_slice>
            </slice>
            <slice>
              <time_slice>11:42</time_slice>
              <text_slice>And what we want to do is to ask
the question, how likely</text_slice>
            </slice>
            <slice>
              <time_slice>11:46</time_slice>
              <text_slice>is that particular numerical
value to occur?</text_slice>
            </slice>
            <slice>
              <time_slice>11:50</time_slice>
              <text_slice>So what we're essentially asking
is, how likely is it</text_slice>
            </slice>
            <slice>
              <time_slice>11:53</time_slice>
              <text_slice>that we obtain an outcome that
leads to that particular</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>numerical value?</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>We calculate that overall
probability of that numerical</text_slice>
            </slice>
            <slice>
              <time_slice>12:02</time_slice>
              <text_slice>value and we represent that
probability using a bar so</text_slice>
            </slice>
            <slice>
              <time_slice>12:07</time_slice>
              <text_slice>that we end up generating
a bar graph.</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>So that could be a possible
bar graph</text_slice>
            </slice>
            <slice>
              <time_slice>12:16</time_slice>
              <text_slice>associated with this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>The size of this bar is the
total probability that our</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>random variable took on this
numerical value, which is just</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>the sum of the probabilities of
the different outcomes that</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>led to that numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>So the thing that we're plotting
here, the bar graph--</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>we give a name to it.</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>It's a function, which we denote
by lowercase b, capital</text_slice>
            </slice>
            <slice>
              <time_slice>12:43</time_slice>
              <text_slice>X. The capital X indicates which
random variable we're</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>talking about.</text_slice>
            </slice>
            <slice>
              <time_slice>12:48</time_slice>
              <text_slice>And it's a function of little
x, which is the range of</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>values that our random
variable is taking.</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>So in mathematical notation, the
value of the PMF at some</text_slice>
            </slice>
            <slice>
              <time_slice>13:04</time_slice>
              <text_slice>particular number, little x,
is the probability that our</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>random variable takes on the
numerical value, little x.</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>And if you want to be precise
about what this means, it's</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>the overall probability of all
outcomes for which the random</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>variable ends up taking
that value, little x.</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>So this is the overall
probability of all omegas that</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>lead to that particular
numerical</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>value, x, of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>13:39</time_slice>
              <text_slice>So what do we know about PMFs?</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>Since there are probabilities,
all these entries in the bar</text_slice>
            </slice>
            <slice>
              <time_slice>13:47</time_slice>
              <text_slice>graph have to be non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>Also, if you exhaust all the
possible values of little x's,</text_slice>
            </slice>
            <slice>
              <time_slice>13:54</time_slice>
              <text_slice>you will have exhausted all the
possible outcomes here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:57</time_slice>
              <text_slice>Because every outcome leads
to some particular x.</text_slice>
            </slice>
            <slice>
              <time_slice>14:01</time_slice>
              <text_slice>So the sum of these
probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>should be equal to one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>This is the second
relation here.</text_slice>
            </slice>
            <slice>
              <time_slice>14:06</time_slice>
              <text_slice>So this relation tell
us that some little</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>x is going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>They happen with different
probabilities, but when you</text_slice>
            </slice>
            <slice>
              <time_slice>14:15</time_slice>
              <text_slice>consider all the possible little
x's together, one of</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>those little x's is going
to be realized.</text_slice>
            </slice>
            <slice>
              <time_slice>14:21</time_slice>
              <text_slice>Probabilities need
to add to one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>14:26</time_slice>
              <text_slice>So let's get our first example
of a non-trivial bar graph.</text_slice>
            </slice>
            <slice>
              <time_slice>14:31</time_slice>
              <text_slice>Consider the experiment where
I start with a coin and I</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>start flipping it
over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>And I do this until I obtain
heads for the first time.</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>So what are possible outcomes
of this experiment?</text_slice>
            </slice>
            <slice>
              <time_slice>14:45</time_slice>
              <text_slice>One possible outcome is that
I obtain heads at the first</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>toss, and then I stop.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>In this case, my random variable
takes the value 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:55</time_slice>
              <text_slice>Or it's possible that I obtain
tails and then heads.</text_slice>
            </slice>
            <slice>
              <time_slice>14:59</time_slice>
              <text_slice>How many tosses did it take
until heads appeared?</text_slice>
            </slice>
            <slice>
              <time_slice>15:02</time_slice>
              <text_slice>This would be x equals to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>Or more generally, I might
obtain tails for k minus 1</text_slice>
            </slice>
            <slice>
              <time_slice>15:09</time_slice>
              <text_slice>times, and then obtain heads
at the k-th time, in which</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>case, our random variable takes
the value, little k.</text_slice>
            </slice>
            <slice>
              <time_slice>15:19</time_slice>
              <text_slice>So that's the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>So capital X is a well defined
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>It's the number of tosses it
takes until I see heads for</text_slice>
            </slice>
            <slice>
              <time_slice>15:29</time_slice>
              <text_slice>the first time.</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>These are the possible
outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>These are elements of
our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>15:34</time_slice>
              <text_slice>And these are the values of X
depending on the outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>Clearly X is a function
of the outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>15:43</time_slice>
              <text_slice>You tell me the outcome, I'm
going to tell you what X is.</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>So what we want to do now is to
calculate the PMF of X. So</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>Px of k is, by definition, the
probability that our random</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>variable takes the value k.</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>For the random variable to take
the value of k, the first</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>head appears at toss number k.</text_slice>
            </slice>
            <slice>
              <time_slice>16:09</time_slice>
              <text_slice>The only way that this event
can happen is if we obtain</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>this sequence of events.</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>T's the first k minus
1 times, tails, and</text_slice>
            </slice>
            <slice>
              <time_slice>16:19</time_slice>
              <text_slice>heads at the k-th flip.</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>So this event, that the random
variable is equal to k, is the</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>same as this event, k minus 1
tails followed by 1 head.</text_slice>
            </slice>
            <slice>
              <time_slice>16:30</time_slice>
              <text_slice>What's the probability
of that event?</text_slice>
            </slice>
            <slice>
              <time_slice>16:32</time_slice>
              <text_slice>We're assuming that the coin
tosses are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>So to find the probability
of this event, we need to</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>multiply the probability of
tails, times the probability</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>of tails, times the probability
of tails.</text_slice>
            </slice>
            <slice>
              <time_slice>16:43</time_slice>
              <text_slice>We multiply k minus one times,
times the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>16:47</time_slice>
              <text_slice>heads, which puts an
extra p at the end.</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>And this is the formula for the
so-called geometric PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>16:56</time_slice>
              <text_slice>And why do we call
it geometric?</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>Because if you go and plot the
bar graph of this random</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>variable, X, we start
at 1 with a certain</text_slice>
            </slice>
            <slice>
              <time_slice>17:10</time_slice>
              <text_slice>number, which is p.</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>And then at 2 we get p(1-p).</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>At 3 we're going to get
something smaller, it's p</text_slice>
            </slice>
            <slice>
              <time_slice>17:23</time_slice>
              <text_slice>times (1-p)-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>17:25</time_slice>
              <text_slice>And the bars keep going down
at the rate of geometric</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>progression.</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>Each bar is smaller than the
previous bar, because each</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>time we get an extra factor
of 1-p involved.</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>So the shape of this
PMF is the graph</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>of a geometric sequence.</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>For that reason, we say that
it's the geometric PMF, and we</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>call X also a geometric
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>17:51</time_slice>
              <text_slice>So the number of coin tosses
until the first head is a</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>geometric random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>17:58</time_slice>
              <text_slice>So this was an example
of how to compute the</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>PMF of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>18:02</time_slice>
              <text_slice>This was an easy example,
because this event could be</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>realized in one and
only one way.</text_slice>
            </slice>
            <slice>
              <time_slice>18:09</time_slice>
              <text_slice>So to find the probability of
this, we just needed to find</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>the probability of this
particular outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>More generally, there's going
to be many outcomes that can</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>lead to the same numerical
value.</text_slice>
            </slice>
            <slice>
              <time_slice>18:22</time_slice>
              <text_slice>And we need to keep track
of all of them.</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>For example, in this picture,
if I want to find this value</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>of the PMF, I need to add up the
probabilities of all the</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>outcomes that leads
to that value.</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>So the general procedure
is exactly what</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>this picture suggests.</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>To find this probability, you go
and identify which outcomes</text_slice>
            </slice>
            <slice>
              <time_slice>18:43</time_slice>
              <text_slice>lead to this numerical value,
and add their probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>18:47</time_slice>
              <text_slice>So let's do a simple example.</text_slice>
            </slice>
            <slice>
              <time_slice>18:49</time_slice>
              <text_slice>I take a tetrahedral die.</text_slice>
            </slice>
            <slice>
              <time_slice>18:51</time_slice>
              <text_slice>I toss it twice.</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>And there's lots of random
variables that you can</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>associate with the
same experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>So the outcome of the first
throw, we can call it F.</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>That's a random variable because
it's determined once</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>you tell me what happens
in the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>The outcome of the
second throw is</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>another random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:13</time_slice>
              <text_slice>The minimum of the two throws
is also a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>Once I do the experiment, this
random variable takes on a</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>specific numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>So suppose I do the experiment
and I get a 2 and a 3.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>So this random variable is going
to take the numerical</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>value of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>This is going to take the
numerical value of 3.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>This is going to take the
numerical value of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>And now suppose that I want to
calculate the PMF of this</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>What I will need to do is to
calculate Px(0), Px(1), Px(2),</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>Px(3), and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>Let's not do the entire
calculation then, let's just</text_slice>
            </slice>
            <slice>
              <time_slice>19:50</time_slice>
              <text_slice>calculate one of the
entries of the PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>So Px(2)--</text_slice>
            </slice>
            <slice>
              <time_slice>19:56</time_slice>
              <text_slice>that's the probability that the
minimum of the two throws</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>gives us a 2.</text_slice>
            </slice>
            <slice>
              <time_slice>20:00</time_slice>
              <text_slice>And this can happen
in many ways.</text_slice>
            </slice>
            <slice>
              <time_slice>20:04</time_slice>
              <text_slice>There are five ways that
it can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>Those are all of the outcomes
for which the smallest of the</text_slice>
            </slice>
            <slice>
              <time_slice>20:11</time_slice>
              <text_slice>two is equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>20:13</time_slice>
              <text_slice>That's five outcomes assuming
that the tetrahedral die is</text_slice>
            </slice>
            <slice>
              <time_slice>20:18</time_slice>
              <text_slice>fair and the tosses
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>20:20</time_slice>
              <text_slice>Each one of these outcomes
has probability of 1/16.</text_slice>
            </slice>
            <slice>
              <time_slice>20:24</time_slice>
              <text_slice>There's five of them, so
we get an answer, 5/16.</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>Conceptually, this is just the
procedure that you use to</text_slice>
            </slice>
            <slice>
              <time_slice>20:33</time_slice>
              <text_slice>calculate PMFs the way that
you construct this</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>particular bar graph.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>You consider all the possible
values of your random</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>variable, and for each one of
those random variables you</text_slice>
            </slice>
            <slice>
              <time_slice>20:43</time_slice>
              <text_slice>find the probability that the
random variable takes on that</text_slice>
            </slice>
            <slice>
              <time_slice>20:47</time_slice>
              <text_slice>value by adding the
probabilities of all the</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>possible outcomes that
leads to that</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>particular numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>So let's do another, more
interesting one.</text_slice>
            </slice>
            <slice>
              <time_slice>20:57</time_slice>
              <text_slice>So let's revisit the
coin tossing</text_slice>
            </slice>
            <slice>
              <time_slice>21:00</time_slice>
              <text_slice>problem from last time.</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>Let us fix a number n, and we
decide to flip a coin n</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>consecutive times.</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>Each time the coin tosses
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>And each one of the tosses will
have a probability, p, of</text_slice>
            </slice>
            <slice>
              <time_slice>21:19</time_slice>
              <text_slice>obtaining heads.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>Let's consider the random
variable, which is the total</text_slice>
            </slice>
            <slice>
              <time_slice>21:23</time_slice>
              <text_slice>number of heads that
have been obtained.</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>Well, that's something that
we dealt with last time.</text_slice>
            </slice>
            <slice>
              <time_slice>21:29</time_slice>
              <text_slice>We know the probabilities for
different numbers of heads,</text_slice>
            </slice>
            <slice>
              <time_slice>21:33</time_slice>
              <text_slice>but we're just going
to do the same now</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>using today's notation.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>So let's, for concreteness,
n equal to 4.</text_slice>
            </slice>
            <slice>
              <time_slice>21:41</time_slice>
              <text_slice>Px is the PMF of that random
variable, X. Px(2) is meant to</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>be, by definition, it's the
probability that a random</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>variable takes the value of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>21:54</time_slice>
              <text_slice>So this is the probability
that we have, exactly two</text_slice>
            </slice>
            <slice>
              <time_slice>21:57</time_slice>
              <text_slice>heads in our four tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>The event of exactly two heads
can happen in multiple ways.</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>And here I've written
down the different</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>ways that it can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>It turns out that there's
exactly six</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>ways that it can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>And each one of these ways,
luckily enough, has the same</text_slice>
            </slice>
            <slice>
              <time_slice>22:15</time_slice>
              <text_slice>probability--</text_slice>
            </slice>
            <slice>
              <time_slice>22:16</time_slice>
              <text_slice>p-squared times (1-p)-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>22:19</time_slice>
              <text_slice>So that gives us the value for
the PMF evaluated at 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:24</time_slice>
              <text_slice>So here we just counted
explicitly that we have six</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>possible ways that this can
happen, and this gave rise to</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>this factor of 6.</text_slice>
            </slice>
            <slice>
              <time_slice>22:32</time_slice>
              <text_slice>But this factor of 6 turns
out to be the same as</text_slice>
            </slice>
            <slice>
              <time_slice>22:37</time_slice>
              <text_slice>this 4 choose 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>If you remember definition from
last time, 4 choose 2 is</text_slice>
            </slice>
            <slice>
              <time_slice>22:42</time_slice>
              <text_slice>4 factorial divided by 2
factorial, divided by 2</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>factorial, which is
indeed equal to 6.</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>And this is the more general
formula that</text_slice>
            </slice>
            <slice>
              <time_slice>22:52</time_slice>
              <text_slice>you would be using.</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>In general, if you have n tosses
and you're interested</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>in the probability of obtaining
k heads, the</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>probability of that event is
given by this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>So that's the formula that
we derived last time.</text_slice>
            </slice>
            <slice>
              <time_slice>23:08</time_slice>
              <text_slice>Except that last time we didn't
use this notation.</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>We just said the probability of
k heads is equal to this.</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>Today we introduce the
extra notation.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>And also having that notation,
we may be tempted to also plot</text_slice>
            </slice>
            <slice>
              <time_slice>23:22</time_slice>
              <text_slice>a bar graph for the Px.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>In this case, for the coin
tossing problem.</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>And if you plot that bar graph
as a function of k when n is a</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>fairly large number, what you
will end up obtaining is a bar</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>graph that has a shape of
something like this.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>So certain values of k are more
likely than others, and</text_slice>
            </slice>
            <slice>
              <time_slice>23:58</time_slice>
              <text_slice>the more likely values
are somewhere in the</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>middle of the range.</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>And extreme values--</text_slice>
            </slice>
            <slice>
              <time_slice>24:03</time_slice>
              <text_slice>too few heads or too many
heads, are unlikely.</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>Now, the miraculous thing is
that it turns out that this</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>curve gets a pretty definite
shape, like a so-called bell</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>curve, when n is big.</text_slice>
            </slice>
            <slice>
              <time_slice>24:20</time_slice>
              <text_slice>This is a very deep and central
fact from probability</text_slice>
            </slice>
            <slice>
              <time_slice>24:24</time_slice>
              <text_slice>theory that we will get to
in a couple of months.</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>For now, it just could be
a curious observation.</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>If you go into MATLAB and put
this formula in and ask MATLAB</text_slice>
            </slice>
            <slice>
              <time_slice>24:38</time_slice>
              <text_slice>to plot it for you, you're going
to get an interesting</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>shape of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>And later on we will have to
sort of understand where this</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>is coming from and whether
there's a nice, simple formula</text_slice>
            </slice>
            <slice>
              <time_slice>24:50</time_slice>
              <text_slice>for the asymptotic
form that we get.</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>So, so far I've said essentially
nothing new, just</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>a little bit of notation and
this little conceptual thing</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>that you have to think of random
variables as functions</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>in the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>So now it's time to introduce
something new.</text_slice>
            </slice>
            <slice>
              <time_slice>25:11</time_slice>
              <text_slice>This is the big concept
of the day.</text_slice>
            </slice>
            <slice>
              <time_slice>25:14</time_slice>
              <text_slice>In some sense it's
an easy concept.</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>But it's the most central, most
important concept that we</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>have to deal with random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>It's the concept
of the expected</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>value of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>25:30</time_slice>
              <text_slice>So the expected value is meant
to be, let's speak loosely,</text_slice>
            </slice>
            <slice>
              <time_slice>25:34</time_slice>
              <text_slice>something like an average,
where you interpret</text_slice>
            </slice>
            <slice>
              <time_slice>25:38</time_slice>
              <text_slice>probabilities as something
like frequencies.</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>So you play a certain game and
your rewards are going to be--</text_slice>
            </slice>
            <slice>
              <time_slice>25:49</time_slice>
              <text_slice>use my standard numbers--</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>your rewards are going
to be one dollar</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>with probability 1/6.</text_slice>
            </slice>
            <slice>
              <time_slice>25:58</time_slice>
              <text_slice>It's going to be 2 dollars with
probability 1/2, and four</text_slice>
            </slice>
            <slice>
              <time_slice>26:04</time_slice>
              <text_slice>dollars with probability 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>So this is a plot of the PMF
of some random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>If you play that game and you
get so many dollars with this</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>probability, and so on, how much
do you expect to get on</text_slice>
            </slice>
            <slice>
              <time_slice>26:18</time_slice>
              <text_slice>the average if you play the
game a zillion times?</text_slice>
            </slice>
            <slice>
              <time_slice>26:21</time_slice>
              <text_slice>Well, you can think
as follows--</text_slice>
            </slice>
            <slice>
              <time_slice>26:23</time_slice>
              <text_slice>one sixth of the time I'm
going to get one dollar.</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>One half of the time that
outcome is going to happen and</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>I'm going to get two dollars.</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>And one third of the time the
other outcome happens, and I'm</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>going to get four dollars.</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>And you evaluate that number
and it turns out to be 2.5.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>So that's a reasonable way of
calculating the average payoff</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>if you think of these
probabilities as the</text_slice>
            </slice>
            <slice>
              <time_slice>26:52</time_slice>
              <text_slice>frequencies with which you
obtain the different payoffs.</text_slice>
            </slice>
            <slice>
              <time_slice>26:56</time_slice>
              <text_slice>And loosely speaking, it doesn't
hurt to think of</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>probabilities as frequencies
when you try to make sense of</text_slice>
            </slice>
            <slice>
              <time_slice>27:02</time_slice>
              <text_slice>various things.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>So what did we do here?</text_slice>
            </slice>
            <slice>
              <time_slice>27:06</time_slice>
              <text_slice>We took the probabilities of the
different outcomes, of the</text_slice>
            </slice>
            <slice>
              <time_slice>27:11</time_slice>
              <text_slice>different numerical values, and
multiplied them with the</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>corresponding numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>27:17</time_slice>
              <text_slice>Similarly here, we have
a probability and the</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>corresponding numerical value
and we added up over all x's.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>So that's what we did.</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>It looks like an interesting
quantity to deal with.</text_slice>
            </slice>
            <slice>
              <time_slice>27:29</time_slice>
              <text_slice>So we're going to give a name to
it, and we're going to call</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>it the expected value of
a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>So this formula just captures
the calculation that we did.</text_slice>
            </slice>
            <slice>
              <time_slice>27:39</time_slice>
              <text_slice>How do we interpret the
expected value?</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>So the one interpretation
is the one that I</text_slice>
            </slice>
            <slice>
              <time_slice>27:46</time_slice>
              <text_slice>used in this example.</text_slice>
            </slice>
            <slice>
              <time_slice>27:48</time_slice>
              <text_slice>You can think of it as the
average that you get over a</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>large number of repetitions
of an experiment where you</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>interpret the probabilities as
the frequencies with which the</text_slice>
            </slice>
            <slice>
              <time_slice>27:59</time_slice>
              <text_slice>different numerical
values can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>There's another interpretation
that's a little more visual</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>and that's kind of insightful,
if you remember your freshman</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>physics, this kind of formula
gives you the center of</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>gravity of an object
of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>If you take that picture
literally and think of this as</text_slice>
            </slice>
            <slice>
              <time_slice>28:17</time_slice>
              <text_slice>a mass of one sixth sitting
here, and the mass of one half</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>sitting here, and one third
sitting there, and you ask me</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>what's the center of gravity
of that structure.</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>This is the formula that gives
you the center of gravity.</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>Now what's the center
of gravity?</text_slice>
            </slice>
            <slice>
              <time_slice>28:30</time_slice>
              <text_slice>It's the place where if you put
your pen right underneath,</text_slice>
            </slice>
            <slice>
              <time_slice>28:34</time_slice>
              <text_slice>that diagram will stay in place
and will not fall on one</text_slice>
            </slice>
            <slice>
              <time_slice>28:38</time_slice>
              <text_slice>side and will not fall
on the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>So in this thing, by picture,
since the 4 is a little more</text_slice>
            </slice>
            <slice>
              <time_slice>28:44</time_slice>
              <text_slice>to the right and a little
heavier, the center of gravity</text_slice>
            </slice>
            <slice>
              <time_slice>28:47</time_slice>
              <text_slice>should be somewhere
around here.</text_slice>
            </slice>
            <slice>
              <time_slice>28:50</time_slice>
              <text_slice>And that's what for
the math gave us.</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>It turns out to be
two and a half.</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>Once you have this
interpretation about centers</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>of gravity, sometimes
you can calculate</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>expectations pretty fast.</text_slice>
            </slice>
            <slice>
              <time_slice>29:01</time_slice>
              <text_slice>So here's our new
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>29:04</time_slice>
              <text_slice>It's the uniform random variable
in which each one of</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>the numerical values
is equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>29:10</time_slice>
              <text_slice>Here there's a total of n plus
1 possible numerical values.</text_slice>
            </slice>
            <slice>
              <time_slice>29:13</time_slice>
              <text_slice>So each one of them has
probability 1 over (n + 1).</text_slice>
            </slice>
            <slice>
              <time_slice>29:17</time_slice>
              <text_slice>Let's calculate the expected
value of this random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>We can take the formula
literally and consider all</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>possible outcomes, or all
possible numerical values, and</text_slice>
            </slice>
            <slice>
              <time_slice>29:28</time_slice>
              <text_slice>weigh them by their
corresponding probability, and</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>do this calculation and
obtain an answer.</text_slice>
            </slice>
            <slice>
              <time_slice>29:35</time_slice>
              <text_slice>But I gave you the intuition
of centers of gravity.</text_slice>
            </slice>
            <slice>
              <time_slice>29:38</time_slice>
              <text_slice>Can you use that intuition
to guess the answer?</text_slice>
            </slice>
            <slice>
              <time_slice>29:41</time_slice>
              <text_slice>What's the center of gravity
infrastructure of this kind?</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>We have symmetry.</text_slice>
            </slice>
            <slice>
              <time_slice>29:47</time_slice>
              <text_slice>So it should be in the middle.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>And what's the middle?</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>It's the average of the
two end points.</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>So without having to do the
algebra, you know that's the</text_slice>
            </slice>
            <slice>
              <time_slice>29:57</time_slice>
              <text_slice>answer is going to
be n over 2.</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>So this is a moral that you
should keep whenever you have</text_slice>
            </slice>
            <slice>
              <time_slice>30:05</time_slice>
              <text_slice>PMF, which is symmetric around
a certain point.</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>That certain point is going
to be the expected value</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>associated with this
particular PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:22</time_slice>
              <text_slice>So having defined the expected
value, what is there that's</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>left for us to do?</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>Well, we want to investigate how
it behaves, what kind of</text_slice>
            </slice>
            <slice>
              <time_slice>30:37</time_slice>
              <text_slice>properties does it have, and
also how do you calculate</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>expected values of complicated
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>So the first complication that
we're going to start with is</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>the case where we deal with a
function of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>So let me redraw this same
picture as before.</text_slice>
            </slice>
            <slice>
              <time_slice>31:05</time_slice>
              <text_slice>We have omega.</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>This is our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>31:09</time_slice>
              <text_slice>This is the real line.</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>And we have a random variable
that gives rise to various</text_slice>
            </slice>
            <slice>
              <time_slice>31:17</time_slice>
              <text_slice>values for X. So the random
variable is capital X, and</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>every outcome leads to a
particular numerical value x</text_slice>
            </slice>
            <slice>
              <time_slice>31:28</time_slice>
              <text_slice>for our random variable X. So
capital X is really the</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>function that maps these points
into the real line.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>And then I consider a function
of this random variable, call</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>it capital Y, and it's
a function of my</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>previous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>31:49</time_slice>
              <text_slice>And this new random variable Y
takes numerical values that</text_slice>
            </slice>
            <slice>
              <time_slice>31:54</time_slice>
              <text_slice>are completely determined once
I know the numerical value of</text_slice>
            </slice>
            <slice>
              <time_slice>31:58</time_slice>
              <text_slice>capital X. And perhaps you get
a diagram of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>So X is a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>Once you have an outcome, this
determines the value of x.</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>Y is also a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>Once you have the outcome,
that determines</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>the value of y.</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>Y is completely determined once
you know X. We have a</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>formula for how to calculate
the expected value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>Suppose that you're interested
in calculating the expected</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>value of Y. How would
you go about it?</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>The only thing you have in your
hands is the definition,</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>so you could start by just
using the definition.</text_slice>
            </slice>
            <slice>
              <time_slice>32:47</time_slice>
              <text_slice>And what does this entail?</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>It entails for every particular
value of y, collect</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>all the outcomes that leads
to that value of y.</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>Find their probability.</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>Do the same here.</text_slice>
            </slice>
            <slice>
              <time_slice>33:02</time_slice>
              <text_slice>For that value, collect
those outcomes.</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>Find their probability
and weight by y.</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>So this formula does the
addition over this line.</text_slice>
            </slice>
            <slice>
              <time_slice>33:13</time_slice>
              <text_slice>We consider the different
outcomes and add things up.</text_slice>
            </slice>
            <slice>
              <time_slice>33:17</time_slice>
              <text_slice>There's an alternative way of
doing the same accounting</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>where instead of doing the
addition over those numbers,</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>we do the addition up here.</text_slice>
            </slice>
            <slice>
              <time_slice>33:26</time_slice>
              <text_slice>We consider the different
possible values of x, and we</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>think as follows--</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>for each possible value of x,
that value is going to occur</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>with this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>And if that value has occurred,
this is how much I'm</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>getting, the g of x.</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>So I'm considering the
probability of this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>And in that case, y
takes this value.</text_slice>
            </slice>
            <slice>
              <time_slice>33:56</time_slice>
              <text_slice>Then I'm considering the
probabilities of this outcome.</text_slice>
            </slice>
            <slice>
              <time_slice>34:00</time_slice>
              <text_slice>And in that case, g of x
takes again that value.</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>Then I consider this particular
x, it happens with</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>this much probability, and in
that case, g of x takes that</text_slice>
            </slice>
            <slice>
              <time_slice>34:11</time_slice>
              <text_slice>value, and similarly here.</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>We end up doing exactly the same
arithmetic, it's only a</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>question whether we bundle
things together.</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>That is, if we calculate the
probability of this, then</text_slice>
            </slice>
            <slice>
              <time_slice>34:25</time_slice>
              <text_slice>we're bundling these
two cases together.</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>Whereas if we do the addition
up here, we do a separate</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>calculation--</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>this probability times this
number, and then this</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>probability times that number.</text_slice>
            </slice>
            <slice>
              <time_slice>34:37</time_slice>
              <text_slice>So it's just a simple
rearrangement of the way that</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>we do the calculations, but it
does make a big difference in</text_slice>
            </slice>
            <slice>
              <time_slice>34:45</time_slice>
              <text_slice>practice if you actually want
to calculate expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>34:49</time_slice>
              <text_slice>So the second procedure that I
mentioned, where you do the</text_slice>
            </slice>
            <slice>
              <time_slice>34:52</time_slice>
              <text_slice>addition by running
over the x-axis</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>corresponds to this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>34:59</time_slice>
              <text_slice>Consider all possibilities for x
and when that x happens, how</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>much money are you getting?</text_slice>
            </slice>
            <slice>
              <time_slice>35:07</time_slice>
              <text_slice>That gives you the average money
that you are getting.</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>35:11</time_slice>
              <text_slice>So I kind of hand waved and
argued that it's just a</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>different way of accounting, of
course one needs to prove</text_slice>
            </slice>
            <slice>
              <time_slice>35:17</time_slice>
              <text_slice>this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>35:19</time_slice>
              <text_slice>And fortunately it
can be proved.</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>You're going to see that
in recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>35:23</time_slice>
              <text_slice>Most people, once they're a
little comfortable with the</text_slice>
            </slice>
            <slice>
              <time_slice>35:25</time_slice>
              <text_slice>concepts of probability,
actually believe that this is</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>true by definition.</text_slice>
            </slice>
            <slice>
              <time_slice>35:30</time_slice>
              <text_slice>In fact it's not true
by definition.</text_slice>
            </slice>
            <slice>
              <time_slice>35:31</time_slice>
              <text_slice>It's called the law of the
unconscious statistician.</text_slice>
            </slice>
            <slice>
              <time_slice>35:34</time_slice>
              <text_slice>It's something that you always
do, but it's something that</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>does require justification.</text_slice>
            </slice>
            <slice>
              <time_slice>35:40</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>So this gives us basically a
shortcut for calculating</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>expected values of functions of
a random variable without</text_slice>
            </slice>
            <slice>
              <time_slice>35:47</time_slice>
              <text_slice>having to find the PMF
of that function.</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>We can work with the PMF of
the original function.</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>So we're going to use this
property over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>36:00</time_slice>
              <text_slice>Before we start using it, one
general word of caution--</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>the average of a function of a
random variable, in general,</text_slice>
            </slice>
            <slice>
              <time_slice>36:10</time_slice>
              <text_slice>is not the same as the function
of the average.</text_slice>
            </slice>
            <slice>
              <time_slice>36:16</time_slice>
              <text_slice>So these two operations of
taking averages and taking</text_slice>
            </slice>
            <slice>
              <time_slice>36:20</time_slice>
              <text_slice>functions do not commute.</text_slice>
            </slice>
            <slice>
              <time_slice>36:23</time_slice>
              <text_slice>What this inequality tells you
is that, in general, you can</text_slice>
            </slice>
            <slice>
              <time_slice>36:28</time_slice>
              <text_slice>not reason on the average.</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>So we're going to see instances
where this property</text_slice>
            </slice>
            <slice>
              <time_slice>36:38</time_slice>
              <text_slice>is not true.</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>You're going to see
lots of them.</text_slice>
            </slice>
            <slice>
              <time_slice>36:41</time_slice>
              <text_slice>Let me just throw it here that
it's something that's not true</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>in general, but we will be
interested in the exceptions</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>where a relation like
this is true.</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>But these will be
the exceptions.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>So in general, expectations
are average,</text_slice>
            </slice>
            <slice>
              <time_slice>36:56</time_slice>
              <text_slice>something like averages.</text_slice>
            </slice>
            <slice>
              <time_slice>36:58</time_slice>
              <text_slice>But the function of an average
is not the same as the average</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>of the function.</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>So now let's go to properties
of expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>Suppose that alpha is a real
number, and I ask you, what's</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>the expected value of
that real number?</text_slice>
            </slice>
            <slice>
              <time_slice>37:17</time_slice>
              <text_slice>So for example, if I write
down this expression--</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>expected value of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>37:23</time_slice>
              <text_slice>What is this?</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>Well, we defined random
variables and we defined</text_slice>
            </slice>
            <slice>
              <time_slice>37:29</time_slice>
              <text_slice>expectations of random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>So for this to make syntactic
sense, this thing inside here</text_slice>
            </slice>
            <slice>
              <time_slice>37:35</time_slice>
              <text_slice>should be a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>37:37</time_slice>
              <text_slice>Is 2 --</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>the number 2 --- is it
a random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>37:44</time_slice>
              <text_slice>In some sense, yes.</text_slice>
            </slice>
            <slice>
              <time_slice>37:48</time_slice>
              <text_slice>It's the random variable that
takes, always, the value of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>So suppose that you have some
experiment and that experiment</text_slice>
            </slice>
            <slice>
              <time_slice>37:59</time_slice>
              <text_slice>always outputs 2 whenever
it happens.</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>Then you can say, yes, it's
a random experiment but it</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>always gives me 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>The value of the random
variable is</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>always 2 no matter what.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>It's kind of a degenerate random
variable that doesn't</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>have any real randomness in it,
but it's still useful to</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>think of it as a special case.</text_slice>
            </slice>
            <slice>
              <time_slice>38:20</time_slice>
              <text_slice>So it corresponds to a function
from the sample space</text_slice>
            </slice>
            <slice>
              <time_slice>38:23</time_slice>
              <text_slice>to the real line that takes
only one value.</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>No matter what the outcome is,
it always gives me a 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>If you have a random variable
that always gives you a 2,</text_slice>
            </slice>
            <slice>
              <time_slice>38:34</time_slice>
              <text_slice>what is the expected
value going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>38:37</time_slice>
              <text_slice>The only entry that shows
up in this summation</text_slice>
            </slice>
            <slice>
              <time_slice>38:40</time_slice>
              <text_slice>is that number 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>The probability of a 2 is equal
to 1, and the value of</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>that random variable
is equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:48</time_slice>
              <text_slice>So it's the number itself.</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>So the average value in an
experiment that always gives</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>you 2's is 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>38:57</time_slice>
              <text_slice>So that's nice and simple.</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>Now let's go to our experiment
where age was</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>your height in inches.</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>And I know your height in
inches, but I'm interested in</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>your height measured
in centimeters.</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>How is that going
to be related to</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>your height in inches?</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>Well, if you take your height
in inches and convert it to</text_slice>
            </slice>
            <slice>
              <time_slice>39:27</time_slice>
              <text_slice>centimeters, I have another
random variable, which is</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>always, no matter what, two and
a half times bigger than</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>the random variable
I started with.</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>If you take some quantity and
always multiplied by two and a</text_slice>
            </slice>
            <slice>
              <time_slice>39:40</time_slice>
              <text_slice>half what happens to the average
of that quantity?</text_slice>
            </slice>
            <slice>
              <time_slice>39:43</time_slice>
              <text_slice>It also gets multiplied
by two and a half.</text_slice>
            </slice>
            <slice>
              <time_slice>39:46</time_slice>
              <text_slice>So you get a relation like
this, which says that the</text_slice>
            </slice>
            <slice>
              <time_slice>39:52</time_slice>
              <text_slice>average height of a student
measured in centimeters is two</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>and a half times the
average height of a</text_slice>
            </slice>
            <slice>
              <time_slice>39:58</time_slice>
              <text_slice>student measured in inches.</text_slice>
            </slice>
            <slice>
              <time_slice>40:01</time_slice>
              <text_slice>So that makes perfect
intuitive sense.</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>If you generalize it, it gives
us this relation, that if you</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>have a number, you can pull it
outside the expectation and</text_slice>
            </slice>
            <slice>
              <time_slice>40:13</time_slice>
              <text_slice>you get the right result.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>So this is a case where you
can reason on the average.</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>If you take a number, such as
height, and multiply it by a</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>certain number, you can
reason on the average.</text_slice>
            </slice>
            <slice>
              <time_slice>40:25</time_slice>
              <text_slice>I multiply the numbers
by two, the averages</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>will go up by two.</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>So this is an exception to this
cautionary statement that</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>I had up there.</text_slice>
            </slice>
            <slice>
              <time_slice>40:35</time_slice>
              <text_slice>How do we prove that
this fact is true?</text_slice>
            </slice>
            <slice>
              <time_slice>40:39</time_slice>
              <text_slice>Well, we can use the expected
value rule here, which tells</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>us that the expected value of
alpha X, this is our g of X,</text_slice>
            </slice>
            <slice>
              <time_slice>40:52</time_slice>
              <text_slice>essentially, is going to be
the sum over all x's of my</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>function, g of X, times the
probability of the x's.</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>In our particular case, g of X
is alpha times X. And we have</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>those probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>41:12</time_slice>
              <text_slice>And the alpha goes outside
the summation.</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>So we get alpha, sum over x's,
x Px of x, which is alpha</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>times the expected value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>So that's how you prove this
relation formally using this</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>rule up here.</text_slice>
            </slice>
            <slice>
              <time_slice>41:32</time_slice>
              <text_slice>And the next formula that
I have here also gets</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>proved the same way.</text_slice>
            </slice>
            <slice>
              <time_slice>41:37</time_slice>
              <text_slice>What does this formula
tell you?</text_slice>
            </slice>
            <slice>
              <time_slice>41:41</time_slice>
              <text_slice>If I take everybody's height
in centimeters--</text_slice>
            </slice>
            <slice>
              <time_slice>41:46</time_slice>
              <text_slice>we already multiplied
by alpha--</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>and the gods give everyone
a bonus of ten extra</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>centimeters.</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>What's going to happen to the
average height of the class?</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>Well, it will just go up by
an extra ten centimeters.</text_slice>
            </slice>
            <slice>
              <time_slice>42:02</time_slice>
              <text_slice>So this expectation is going to
be giving you the bonus of</text_slice>
            </slice>
            <slice>
              <time_slice>42:08</time_slice>
              <text_slice>beta just adds a beta to the
average height in centimeters,</text_slice>
            </slice>
            <slice>
              <time_slice>42:15</time_slice>
              <text_slice>which we also know to be alpha
times the expected</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>value of X, plus beta.</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>So this is a linearity property
of expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>42:29</time_slice>
              <text_slice>If you take a linear function
of a single random variable,</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>the expected value of that
linear function is the linear</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>function of the expected
value.</text_slice>
            </slice>
            <slice>
              <time_slice>42:41</time_slice>
              <text_slice>So this is our big exception to
this cautionary note, that</text_slice>
            </slice>
            <slice>
              <time_slice>42:44</time_slice>
              <text_slice>we have equal if g is linear.</text_slice>
            </slice>
            <slice>
              <time_slice>42:55</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>43:00</time_slice>
              <text_slice>So let's get to the last
concept of the day.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>What kind of functions
of random</text_slice>
            </slice>
            <slice>
              <time_slice>43:07</time_slice>
              <text_slice>variables may be of interest?</text_slice>
            </slice>
            <slice>
              <time_slice>43:11</time_slice>
              <text_slice>One possibility might be the
average value of X-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>43:18</time_slice>
              <text_slice>Why is it interesting?</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>Well, why not.</text_slice>
            </slice>
            <slice>
              <time_slice>43:21</time_slice>
              <text_slice>It's the simplest function
that you can think of.</text_slice>
            </slice>
            <slice>
              <time_slice>43:27</time_slice>
              <text_slice>So if you want to calculate
the expected value of</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>X-squared, you would use this
general rule for how you can</text_slice>
            </slice>
            <slice>
              <time_slice>43:35</time_slice>
              <text_slice>calculate expected values of
functions of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>You consider all the
possible x's.</text_slice>
            </slice>
            <slice>
              <time_slice>43:41</time_slice>
              <text_slice>For each x, you see what's the
probability that it occurs.</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>And if that x occurs, you
consider and see how big</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>x-squared is.</text_slice>
            </slice>
            <slice>
              <time_slice>43:52</time_slice>
              <text_slice>Now, the more interesting
quantity, a more interesting</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>expectation that you can
calculate has to do not with</text_slice>
            </slice>
            <slice>
              <time_slice>43:58</time_slice>
              <text_slice>x-squared, but with the distance
of x from the mean</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>and then squared.</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>So let's try to parse what
we've got up here.</text_slice>
            </slice>
            <slice>
              <time_slice>44:10</time_slice>
              <text_slice>Let's look just at the
quantity inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>What kind of quantity is it?</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>It's a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>44:22</time_slice>
              <text_slice>X is random, the random
variable, expected value of X</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>Subtract a number from a random
variable, you get</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>another random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>44:32</time_slice>
              <text_slice>Take a random variable and
square it, you get another</text_slice>
            </slice>
            <slice>
              <time_slice>44:35</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>So the thing inside here is a
legitimate random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>What kind of random
variable is it?</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>So suppose that we have our
experiment and we have</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>different x's that can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>44:52</time_slice>
              <text_slice>And the mean of X in this
picture might be somewhere</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>around here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:00</time_slice>
              <text_slice>I do the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>45:02</time_slice>
              <text_slice>I obtain some numerical
value of x.</text_slice>
            </slice>
            <slice>
              <time_slice>45:05</time_slice>
              <text_slice>Let's say I obtain this
numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>45:09</time_slice>
              <text_slice>I look at the distance from
the mean, which is this</text_slice>
            </slice>
            <slice>
              <time_slice>45:13</time_slice>
              <text_slice>length, and I take the
square of that.</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>Each time that I do the
experiment, I go and record my</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>distance from the mean
and square it.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>So I give more emphasis
to big distances.</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>And then I take the average over
all possible outcomes,</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>all possible numerical values.</text_slice>
            </slice>
            <slice>
              <time_slice>45:35</time_slice>
              <text_slice>So I'm trying to compute
the average squared</text_slice>
            </slice>
            <slice>
              <time_slice>45:39</time_slice>
              <text_slice>distance from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>This corresponds to
this formula here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>So the picture that I drew
corresponds to that.</text_slice>
            </slice>
            <slice>
              <time_slice>45:51</time_slice>
              <text_slice>For every possible numerical
value of x, that numerical</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>value corresponds to a certain
distance from the mean</text_slice>
            </slice>
            <slice>
              <time_slice>45:59</time_slice>
              <text_slice>squared, and I weight according
to how likely is</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>that particular value
of x to arise.</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>So this measures the
average squared</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>distance from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>Now, because of that expected
value rule, of course, this</text_slice>
            </slice>
            <slice>
              <time_slice>46:17</time_slice>
              <text_slice>thing is the same as
that expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>It's the average value of the
random variable, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>46:23</time_slice>
              <text_slice>squared distance
from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>With this probability, the
random variable takes on this</text_slice>
            </slice>
            <slice>
              <time_slice>46:29</time_slice>
              <text_slice>numerical value, and the squared
distance from the mean</text_slice>
            </slice>
            <slice>
              <time_slice>46:33</time_slice>
              <text_slice>ends up taking that particular
numerical value.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>So why is the variance
interesting?</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>It tells us how far away from
the mean we expect to be on</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>the average.</text_slice>
            </slice>
            <slice>
              <time_slice>46:46</time_slice>
              <text_slice>Well, actually we're not
counting distances from the</text_slice>
            </slice>
            <slice>
              <time_slice>46:49</time_slice>
              <text_slice>mean, it's distances squared.</text_slice>
            </slice>
            <slice>
              <time_slice>46:51</time_slice>
              <text_slice>So it gives more emphasis to the
kind of outliers in here.</text_slice>
            </slice>
            <slice>
              <time_slice>46:56</time_slice>
              <text_slice>But it's a measure of
how spread out the</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>distribution is.</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>A big variance means that those
bars go far to the left</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>and to the right, typically.</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>Where as a small variance would
mean that all those bars</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>are tightly concentrated
around the mean value.</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>It's the average squared
deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>Small variance means that
we generally have small</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>deviations.</text_slice>
            </slice>
            <slice>
              <time_slice>47:19</time_slice>
              <text_slice>Large variances mean that
we generally have large</text_slice>
            </slice>
            <slice>
              <time_slice>47:22</time_slice>
              <text_slice>deviations.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>Now as a practical matter, when
you want to calculate the</text_slice>
            </slice>
            <slice>
              <time_slice>47:27</time_slice>
              <text_slice>variance, there's a handy
formula which I'm not proving</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>but you will see it
in recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:33</time_slice>
              <text_slice>It's just two lines
of algebra.</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>And it allows us to calculate it
in a somewhat simpler way.</text_slice>
            </slice>
            <slice>
              <time_slice>47:40</time_slice>
              <text_slice>We need to calculate the
expected value of the random</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>variable and the expected value
of the squares of the</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>random variable, and
these two are going</text_slice>
            </slice>
            <slice>
              <time_slice>47:47</time_slice>
              <text_slice>to give us the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>So to summarize what we did
up here, the variance, by</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>definition, is given
by this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>47:57</time_slice>
              <text_slice>It's the expected value of
the squared deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>But we have the equivalent
formula, which comes from</text_slice>
            </slice>
            <slice>
              <time_slice>48:06</time_slice>
              <text_slice>application of the expected
value rule, to the function g</text_slice>
            </slice>
            <slice>
              <time_slice>48:13</time_slice>
              <text_slice>of X, equals to x minus the
(expected value of X)-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>48:26</time_slice>
              <text_slice>So this is the definition.</text_slice>
            </slice>
            <slice>
              <time_slice>48:27</time_slice>
              <text_slice>This comes from the expected
value rule.</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>What are some properties
of the variance?</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>Of course variances are
always non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>Why is it always non-negative?</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>Well, you look at the definition
and your just</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>adding up non-negative things.</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>We're adding squared
deviations.</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>So when you add non-negative
things, you get something</text_slice>
            </slice>
            <slice>
              <time_slice>48:50</time_slice>
              <text_slice>non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>The next question is, how do
things scale if you take a</text_slice>
            </slice>
            <slice>
              <time_slice>48:55</time_slice>
              <text_slice>linear function of a
random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>48:59</time_slice>
              <text_slice>Let's think about the
effects of beta.</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>If I take a random variable and
add the constant to it,</text_slice>
            </slice>
            <slice>
              <time_slice>49:06</time_slice>
              <text_slice>how does this affect the amount
of spread that we have?</text_slice>
            </slice>
            <slice>
              <time_slice>49:09</time_slice>
              <text_slice>It doesn't affect--</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>whatever the spread of this
thing is, if I add the</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>constant beta, it just moves
this diagram here, but the</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>spread doesn't grow
or get reduced.</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>The thing is that when I'm
adding a constant to a random</text_slice>
            </slice>
            <slice>
              <time_slice>49:24</time_slice>
              <text_slice>variable, all the x's that are
going to appear are further to</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>the right, but the expected
value also moves to the right.</text_slice>
            </slice>
            <slice>
              <time_slice>49:32</time_slice>
              <text_slice>And since we're only interested
in distances from</text_slice>
            </slice>
            <slice>
              <time_slice>49:35</time_slice>
              <text_slice>the mean, these distances
do not get affected.</text_slice>
            </slice>
            <slice>
              <time_slice>49:39</time_slice>
              <text_slice>x gets increased by something.</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>The mean gets increased by
that same something.</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>The difference stays the same.</text_slice>
            </slice>
            <slice>
              <time_slice>49:46</time_slice>
              <text_slice>So adding a constant to a random
variable doesn't do</text_slice>
            </slice>
            <slice>
              <time_slice>49:49</time_slice>
              <text_slice>anything to it's variance.</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>But if I multiply a random
variable by a constant alpha,</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>what is that going to
do to its variance?</text_slice>
            </slice>
            <slice>
              <time_slice>49:58</time_slice>
              <text_slice>Because we have a square here,
when I multiply my random</text_slice>
            </slice>
            <slice>
              <time_slice>50:04</time_slice>
              <text_slice>variable by a constant, this x
gets multiplied by a constant,</text_slice>
            </slice>
            <slice>
              <time_slice>50:08</time_slice>
              <text_slice>the mean gets multiplied by a
constant, the square gets</text_slice>
            </slice>
            <slice>
              <time_slice>50:12</time_slice>
              <text_slice>multiplied by the square
of that constant.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>And because of that reason, we
get this square of alpha</text_slice>
            </slice>
            <slice>
              <time_slice>50:18</time_slice>
              <text_slice>showing up here.</text_slice>
            </slice>
            <slice>
              <time_slice>50:20</time_slice>
              <text_slice>So that's how variances
transform under linear</text_slice>
            </slice>
            <slice>
              <time_slice>50:22</time_slice>
              <text_slice>transformations.</text_slice>
            </slice>
            <slice>
              <time_slice>50:23</time_slice>
              <text_slice>You multiply your random
variable by constant, the</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>variance goes up by the square
of that same constant.</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>50:31</time_slice>
              <text_slice>That's it for today.</text_slice>
            </slice>
            <slice>
              <time_slice>50:32</time_slice>
              <text_slice>See you on Wednesday.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Multiple Continuous Random Variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Conditioning
Recall
P(xXx+)fX(x) 
By analogy, would like:
P(xXx+|Yy)fX(|Yx|y)
This leads us to the denition:
fX,Y(x, y)fX Y(x|y)= iffY(y)&gt;0|fY(y)
For given y, conditional PDF is a
(normalized) section of the joint PDF
If independent, fX,Y=fXfY, we obtain
fX Y(x|y)=f X(x)|
Area of slice = Height of marginal
density at x
Slice through
density surface
for fixed xRenormalizing slices for
fixed x gives conditional
densities for Y given X = xJoint, Marginal and Conditional Densities
Image by MIT OpenCourseWare, adapted from
Probability, by J. Pittman, 1999.
Stick-breaking example
Break a stick of length twice:
break at X: uniform in [0, 1];
break again at Y, uniform in [0,X ]
f (y| x)f(x)  Y   |X X 
x L    y 
fX,Y(x, y)=f X(x)fY X(y|x)=|
on the set:
 y 
L
L x 
E[Y |X=x]=
yfY(y|X )dy=|X =x1fX,Y(x, y)= , 0yx x
 y 
L
L x 
fY(y)=
fX,Y(x, y)dx
1=
dx
yx
1 = log , 0y  y
 1  E[Y]=
yfY(y)dy=
ylog dy=
0 0 y 4
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 9 Continuous r.v.s and pdfs
Readings: Sections 3.4-3.5f (x)XSample Space
Outline
PDF review a b x Event { a &lt; X &lt; b }
Multiple random variables
conditioning
bindependence P(aXb)= f
a
Examples
X(x)dx
P(xXx+)fX(x) 
Summary of concepts
E[g(X)] =
g(x)f X(x)dx
pX(x) fX(x)
F(x)
X
xpX(x) E[X]
x
xfX(x)dx
var(X)
pX,Y(x, y) fX,Y(x, y)
pX|A(x) fX A(x)|
pX Y(x y| |) fX(x|Y |y)
Joint PDF fX,Y(x, y) Buons needle
Parallel lines at distance d
Needle of length (assume &lt;d)
FindP(needle intersects one of the lines)
P((X, Y )S)=
fX,Y(x, y)dx dy
S q
x
l
dInterpretation:
(  +  + ) ( ) 2Px X x ,y Y y  fX,Yx, y 
X[0,d / 2]: distance of needle midpoint
to nearest line
Expectations: Model: X,uniform, independent
( ) =0 2 0[ ( )] =
(f x, xd/,   /2EgX, Y gx, y)fX,Y(x, y)dx dy X,    
 
From the joint to the marginal: Intersect if Xsin2
fX(x) P(xXx+)=
PXsin
=
fX(x)f()dx d2 x sin2
4/2(/2) sin 
= dx dd0 0
XandYare called independent if
4 /2 2=
sinfX,Y(x, y)= X( )d=f x f Y(y), for all x, yd02 d
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-6-discrete-random-variable-examples-joint-pmfs/</video_url>
          <video_title>Lecture 6: Discrete Random Variable Examples; Joint PMFs</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Markov Chains - I (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Example Generic convergence questions:
Does rij(n) converge to something?
0.5 0.8
0.50.5 0.5
1 2 32 11 1
0.2n odd: r22(n)= n even: r22(n)=
n=0 n=1 n=2 n= 100 n= 101 Does the limit depend on initial
r11(n) state?
r12(n) 0.4
r21(n) 3 4 1 20.3 0.3
r22(n)r(n)=11
r(n)=31
r(n)=21
Recurrent and transient states
State iisrecurrent if:
starting from i,
and from wherever you can go,
there is a way of returning to i
If not recurrent, called transient
3 4
6 75
1 2
8
itransient:
P(Xn=i)0,
ivisited nite number of times
Recurrent class :
collection of recurrent states thatcommunicate with each otherand with no other state
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 16 Checkout counter model
Discrete time n=0,1,...Markov Processes  I
Customer arrivals: Bernoulli( )pReadings: Sections 7.17.2
geometric interarrival times
Customer service times: geometric( q)
Lecture outline
Checkout counter example State Xn: number of customers at
time n
Markov process denition
n-step transition probabilities
Classication of states
2 0 1 3...9 10
Finite state Markov chains n-step transition probabilities
State occupancy probabilities,Xn: state after ntransitions
given initial state i:
belongs to a nite set, e.g., {1, . . . , m}
r(n)= P(X=j X =i)X0is either given or random ij n |0
Markov property/assumption: Time 0 Time  n-1 Time  n
(given current state, the past does not
matter)1
r (n-1) p i1 1j...
pij=P(Xn+1=j|Xn=i)
i k
=P(Xn+1=j|X(n-1)n=i, Xr n1, . . . , X 0) ik pkj j...
r (n-1) p im mj
Model specication:m
identify the possible states Key recursion:
the possible transitionsmidentifyrij(n)=/summationdisplay
rik(n
identify the transition probabilities k=11)pkj
With random initial state:
m
P(Xn=j)=
i/summationdisplay
P(X0=i)rij(n)
=1
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-7-multiple-variables-expectations-independence/</video_url>
          <video_title>Lecture 7: Multiple Discrete Random Variables: Expectations, Conditioning, Independence</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Poisson Process - I (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Example Interarrival Times
You get email according to a Poisson Yktime of kth arrival
process at a rate of = 5 messages per
Erlang distribution:hour. You check your email every thirty
minutes. k 1ykey
fY(y)= ,y 0k(k1)!
Prob(no new messages) =
Prob(one new message) =
Time of rst arrival (k = 1):
exponential
:fY(y)= ey,y10
Memoryless property: The time to the
next arrival is independent of the past
Bernoulli/Poisson Relation Merging P
oisson Processes
Sum of independent Poisson random
! ! ! ! ! ! ! !
n = t /!variables is Poisson
x x x np  ="t
0 = Time p "!Arrivals Merging of independent Poisson processes
is Poisson
Red bulb flashes
 (Poisson)POISSON BERNOULLI
All  flashes"Times of Arrival Continuous Discrete 1  (Poisson)
Arrival Rate /uni ime p/per trial" t t 2
Green bulb flashesPMF of # of Arrivals Poisson Binomial (Poisson)
Interarrival Time Distr. Exponential Geometric
What is the probability that the next
Time to k-th arrival Erlang Pascal arrival comes from the rst process?     
Interarrival Times
Yktime of kth arrival
Erlang distribution:
fYk(y) =kyk1ey
(k1)!, y 0
k=1
k=2
k=3
yfY (y)
k
First-order interarrival times (k = 1):
exponential
fY1(y) =ey, y 0
Memoryless property: The time to the
next arrival is independent of the past       
Bernoulli/Poisson Relation
POISSON BERNOULLI
Times of Arrival Continuous Discrete
Arrival Rate /unit time p/per trial
PMF of # of Arrivals Poisson Binomial
Interarrival Time Distr. Exponential Geometric
Time to k-th arrival Erlang Pascal     
Adding Poisson Processes
Sum of independent Poisson random vari-
ables is Poisson
Sum of independent Poisson processes
is Poisson
All  flashes (Poisson)Red bulb flashes (Poisson)
"1
"2
Green bulb flashes (Poisson)
What is the probability that the next
arrival comes from the rst process?     
Interarrival Times
Yktime of kth arrival
Erlang distribution:
fYk(y) =kyk1ey
(k1)!, y 0
k=1
k=2
k=3
yfY (y)
k
First-order interarrival times (k = 1):
exponential
fY1(y) =ey, y 0
Memoryless property: The time to the
next arrival is independent of the past       
Bernoulli/Poisson Relation
Time 0x x x!
Arrivalsp ="!! ! ! ! ! ! !n = t /!
np  ="t
POISSON BERNOULLI
Times of Arrival Continuous Discrete
Arrival Rate /unit time p/per trial
PMF of # of Arrivals Poisson Binomial
Interarrival Time Distr. Exponential Geometric
Time to k-th arrival Erlang Pascal     
Adding Poisson Processes
Sum of independent Poisson random vari-
ables is Poisson
Sum of independent Poisson processes
is Poisson
What is the probability that the next
arrival comes from the rst process?
2flr (l) 
l0 r = 1 
r = 2 
r = 3 
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 14 Bernoulli review
Discrete time; success probability pThe Poisson process
Number of arrivals in m otnti e sl s:Readings: Start Section 6.2.
binomial pmf
Interarrival times: geometric pmfLecture outline
Time to ar : Pascal p fkrivals mReview of Bernoulli process
MemorylessnessDenition of Poisson process
Distribution of number of arrivals
Distribution of interarrival times
Other properties of the Poisson process
Denition of the Poisson process PMF of Number of Arrivals N
t t t t t t !1 3 ! 1 2 3 2
xx xx xxxx xxx xx xx xxxx xxx
0 0
Time Time
Time homogeneity:
Finely discretize [0,t ]: approximately BernoulliP(k,) = Prob. of karrivals in interval
of duration 
Nt(of discrete approximation): binomial
Numbers of arrivals in disjoint time
intervals are independent
Taking 0 (or n ) gives:
Small interval probabilities:()ke
For VERY small : P(k,)= ,k =0,1,...
1
 k!
if
), k = 0;
P(k, , ifk= 1; E[Nt]= t,v ar(N t
0 ifk&gt;1.
: r ival r tet)=
,
a r a   
LECTURE 16
The Poisson process
Readings: Start Section 5.2.
Lecture outline
Review of Bernoulli process
Denition of Poisson process
Distribution of number of arrivals
Distribution of interarrival times
Other properties of the Poisson process  
Bernoulli review
Discrete time; success probability p
Number of arrivals in ntime slots:
binomial pmf
Interarrival time pmf: geometric pmf
Time to karrivals: Pascal pmf
Memorylessness       
Denition of the Poisson process
P(k,) = Prob. of karrivals in interval
of duration 
Assumptions:
Numbers of arrivals in disjoint time in-
tervals are independent
For VERY small :
P(k,)

1 ifk= 0
 ifk= 1
0 if k &gt;1
= arrival rate    
PMF of Number of Arrivals N
P(k,) =()ke
k!, k = 0, 1, . . .
E[N] =
2
N=
MN(s) = et(es1)
Example: You get email according to a
Poisson process at a rate of = 0.4 mes-
sages per hour. You check your email every
thirty minutes.
Prob(no new messages)=
Prob(one new message)=  
LECTURE 16
The Poisson process
Readings: Start Section 5.2.
Lecture outline
Review of Bernoulli process
Denition of Poisson process
Distribution of number of arrivals
Distribution of interarrival times
Other properties of the Poisson process  
Bernoulli review
Discrete time; success probability p
Number of arrivals in ntime slots:
binomial pmf
Interarrival time pmf: geometric pmf
Time to karrivals: Pascal pmf
Memorylessness       
Denition of the Poisson process
P(k,) = Prob. of karrivals in interval
of duration 
Assumptions:
Numbers of arrivals in disjoint time in-
tervals are independent
For VERY small :
P(k,)

1 ifk= 0
 ifk= 1
0 if k &gt;1
= arrival rate    
PMF of Number of Arrivals N
P(k,) =()ke
k!, k = 0, 1, . . .
E[N] =
2
N=
MN(s) = et(es1)
Example: You get email according to a
Poisson process at a rate of = 0.4 mes-
sages per hour. You check your email every
thirty minutes.
Prob(no new messages)=
Prob(one new message)=
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-8-continuous-random-variables/</video_url>
          <video_title>Lecture 8: Continuous Random Variables</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high-quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>JOHN TSITSIKLIS: OK.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>We can start.</text_slice>
            </slice>
            <slice>
              <time_slice>0:24</time_slice>
              <text_slice>Good morning.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>So we're going to start
now a new unit.</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>For the next couple of lectures,
we will be talking</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>about continuous random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>So this is new material
which is not going</text_slice>
            </slice>
            <slice>
              <time_slice>0:36</time_slice>
              <text_slice>to be in the quiz.</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>You are going to have a long
break next week without any</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>lecture, just a quiz and
recitation and tutorial.</text_slice>
            </slice>
            <slice>
              <time_slice>0:45</time_slice>
              <text_slice>So what's going to happen
in this new unit?</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>Basically, we want to do
everything that we did for</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>discrete random variables,
reintroduce the same sort of</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>concepts but see how they apply
and how they need to be</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>modified in order to talk about
random variables that</text_slice>
            </slice>
            <slice>
              <time_slice>1:02</time_slice>
              <text_slice>take continuous values.</text_slice>
            </slice>
            <slice>
              <time_slice>1:04</time_slice>
              <text_slice>At some level, it's
all the same.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>At some level, it's quite a bit
harder because when things</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>are continuous, calculus
comes in.</text_slice>
            </slice>
            <slice>
              <time_slice>1:12</time_slice>
              <text_slice>So the calculations that you
have to do on the side</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>sometimes need a little
bit more thinking.</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>In terms of new concepts,
there's not going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>whole lot today, some analogs
of things we have done.</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>We're going to introduce the
concept of cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>distribution functions, which
allows us to deal with</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>discrete and continuous
random variables, all</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>of them in one shot.</text_slice>
            </slice>
            <slice>
              <time_slice>1:34</time_slice>
              <text_slice>And finally, introduce a famous
kind of continuous</text_slice>
            </slice>
            <slice>
              <time_slice>1:37</time_slice>
              <text_slice>random variable, the normal
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>OK, so what's the story?</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>Continuous random variables are
random variables that take</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>values over the continuum.</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>So the numerical value of the
random variable can be any</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>real number.</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>They don't take values just
in a discrete set.</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>So we have our sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>The experiment happens.</text_slice>
            </slice>
            <slice>
              <time_slice>2:02</time_slice>
              <text_slice>We get some omega, a sample
point in the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>2:05</time_slice>
              <text_slice>And once that point is
determined, it determines the</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>numerical value of the
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>Remember, random variables are
functions on the sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>2:15</time_slice>
              <text_slice>You pick a sample point.</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>This determines the numerical
value of the random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>So that numerical value is going
to be some real number</text_slice>
            </slice>
            <slice>
              <time_slice>2:23</time_slice>
              <text_slice>on that line.</text_slice>
            </slice>
            <slice>
              <time_slice>2:26</time_slice>
              <text_slice>Now we want to say something
about the distribution of the</text_slice>
            </slice>
            <slice>
              <time_slice>2:28</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>We want to say which values are
more likely than others to</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>occur in a certain sense.</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>For example, you may be
interested in a particular</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>event, the event that the random
variable takes values</text_slice>
            </slice>
            <slice>
              <time_slice>2:40</time_slice>
              <text_slice>in the interval from a to b.</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>And we want to say something
about the</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>probability of that event.</text_slice>
            </slice>
            <slice>
              <time_slice>2:45</time_slice>
              <text_slice>In principle, how
is this done?</text_slice>
            </slice>
            <slice>
              <time_slice>2:48</time_slice>
              <text_slice>You go back to the sample space,
and you find all those</text_slice>
            </slice>
            <slice>
              <time_slice>2:52</time_slice>
              <text_slice>outcomes for which the value of
the random variable happens</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>to be in that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>The probability that the random
variable falls here is</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>the same as the probability of
all outcomes that make the</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>random variable to
fall in there.</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>So in principle, you can work on
the original sample space,</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>find the probability of this
event, and you would be done.</text_slice>
            </slice>
            <slice>
              <time_slice>3:14</time_slice>
              <text_slice>But similar to what happened in
chapter 2, we want to kind</text_slice>
            </slice>
            <slice>
              <time_slice>3:18</time_slice>
              <text_slice>of push the sample space in the
background and just work</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>directly on the real
axis and talk about</text_slice>
            </slice>
            <slice>
              <time_slice>3:26</time_slice>
              <text_slice>probabilities up here.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>So we want now a way to specify
probabilities, how</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>they are bunched together, or
arranged, along the real line.</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>So what did we do for discrete
random variables?</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>We introduced PMFs, probability
mass functions.</text_slice>
            </slice>
            <slice>
              <time_slice>3:44</time_slice>
              <text_slice>And the way that we described
the random variable was by</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>saying this point has so much
mass on top of it, that point</text_slice>
            </slice>
            <slice>
              <time_slice>3:50</time_slice>
              <text_slice>has so much mass on top
of it, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>And so we assigned a total
amount of 1 unit of</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>3:58</time_slice>
              <text_slice>We assigned it to different
masses, which we put at</text_slice>
            </slice>
            <slice>
              <time_slice>4:01</time_slice>
              <text_slice>different points on
the real axis.</text_slice>
            </slice>
            <slice>
              <time_slice>4:04</time_slice>
              <text_slice>So that's what you do if
somebody gives you a pound of</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>discrete stuff, a pound of
mass in little chunks.</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>And you place those chunks
at a few points.</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>Now, in the continuous case,
this total unit of probability</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>mass does not sit just on
discrete points but is spread</text_slice>
            </slice>
            <slice>
              <time_slice>4:25</time_slice>
              <text_slice>all over the real axis.</text_slice>
            </slice>
            <slice>
              <time_slice>4:28</time_slice>
              <text_slice>So now we're going to have a
unit of mass that spreads on</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>top of the real axis.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>How do we describe masses that
are continuously spread?</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>The way we describe them is
by specifying densities.</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>That is, how thick is the mass
that's sitting here?</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>How dense is the mass that's
sitting there?</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>So that's exactly what
we're going to do.</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>We're going to introduce the
concept of a probability</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>density function that tells us
how probabilities accumulate</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>at different parts
of the real axis.</text_slice>
            </slice>
            <slice>
              <time_slice>5:03</time_slice>
              <text_slice>So here's an example or a
picture of a possible</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>probability density function.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>What does that density function
kind of convey</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>intuitively?</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>Well, that these x's
are relatively</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>less likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>5:19</time_slice>
              <text_slice>Those x's are somewhat more
likely to occur because the</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>density is higher.</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>Now, for a more formal
definition, we're going to say</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>that a random variable X is said
to be continuous if it</text_slice>
            </slice>
            <slice>
              <time_slice>5:35</time_slice>
              <text_slice>can be described by a
density function in</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>the following sense.</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>We have a density function.</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>And we calculate probabilities
of falling inside an interval</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>by finding the area under
the curve that sits</text_slice>
            </slice>
            <slice>
              <time_slice>5:52</time_slice>
              <text_slice>on top of that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>5:54</time_slice>
              <text_slice>So that's sort of the defining
relation for</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>It's an implicit definition.</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>And it tells us a random
variable is continuous if we</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>can calculate probabilities
this way.</text_slice>
            </slice>
            <slice>
              <time_slice>6:06</time_slice>
              <text_slice>So the probability of falling
in this interval is the area</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>under this curve.</text_slice>
            </slice>
            <slice>
              <time_slice>6:10</time_slice>
              <text_slice>Mathematically, it's the
integral of the density over</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>this particular interval.</text_slice>
            </slice>
            <slice>
              <time_slice>6:17</time_slice>
              <text_slice>If the density happens to be
constant over that interval,</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>the area under the curve would
be the length of the interval</text_slice>
            </slice>
            <slice>
              <time_slice>6:23</time_slice>
              <text_slice>times the height of
the density, which</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>sort of makes sense.</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>Now, because the density is not
constant but it kind of</text_slice>
            </slice>
            <slice>
              <time_slice>6:32</time_slice>
              <text_slice>moves around, what you need is
to write down an integral.</text_slice>
            </slice>
            <slice>
              <time_slice>6:35</time_slice>
              <text_slice>Now, this formula is very much
analogous to what you would do</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>for discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>For a discrete random variable,
how do you calculate</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>this probability?</text_slice>
            </slice>
            <slice>
              <time_slice>6:45</time_slice>
              <text_slice>You look at all x's
in this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>And you add the probability mass
function over that range.</text_slice>
            </slice>
            <slice>
              <time_slice>6:54</time_slice>
              <text_slice>So just for comparison, this
would be the formula for the</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>discrete case--</text_slice>
            </slice>
            <slice>
              <time_slice>7:01</time_slice>
              <text_slice>the sum over all x's in the
interval from a to b over the</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>probability mass function.</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>And there is a syntactic analogy
that's happening here</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>and which will be a persistent
theme when we deal with</text_slice>
            </slice>
            <slice>
              <time_slice>7:16</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>Sums get replaced
by integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>In the discrete case, you add.</text_slice>
            </slice>
            <slice>
              <time_slice>7:24</time_slice>
              <text_slice>In the continuous case,
you integrate.</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>Mass functions get replaced
by density functions.</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>So you can take pretty much any
formula from the discrete</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>case and translate it to a
continuous analog of that</text_slice>
            </slice>
            <slice>
              <time_slice>7:40</time_slice>
              <text_slice>formula, as we're
going to see.</text_slice>
            </slice>
            <slice>
              <time_slice>7:43</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>7:47</time_slice>
              <text_slice>So let's take this
now as our model.</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>What is the probability that
the random variable takes a</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>specific value if we have a
continuous random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>Well, this would be the case.</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>It's a case of a trivial
interval, where the two end</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>points coincide.</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>So it would be the integral
from a to itself.</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>So you're integrating just
over a single point.</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>Now, when you integrate over
a single point, the</text_slice>
            </slice>
            <slice>
              <time_slice>8:12</time_slice>
              <text_slice>integral is just 0.</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>The area under the curve, if
you're only looking at a</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>single point, it's 0.</text_slice>
            </slice>
            <slice>
              <time_slice>8:19</time_slice>
              <text_slice>So big property of continuous
random variables is that any</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>individual point has
0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>In particular, when you look at
the value of the density,</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>the density does not tell you
the probability of that point.</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>The point itself has
0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>So the density tells you
something a little different.</text_slice>
            </slice>
            <slice>
              <time_slice>8:42</time_slice>
              <text_slice>We are going to see shortly
what that is.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>Before we get there,
can the density be</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>an arbitrary function?</text_slice>
            </slice>
            <slice>
              <time_slice>8:54</time_slice>
              <text_slice>Almost, but not quite.</text_slice>
            </slice>
            <slice>
              <time_slice>8:56</time_slice>
              <text_slice>There are two things
that we want.</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>First, since densities
are used to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>9:00</time_slice>
              <text_slice>probabilities, and since
probabilities must be</text_slice>
            </slice>
            <slice>
              <time_slice>9:02</time_slice>
              <text_slice>non-negative, the density should
also be non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>Otherwise you would be getting
negative probabilities, which</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>is not a good thing.</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>So that's a basic property
that any density function</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>should obey.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>The second property that we
need is that the overall</text_slice>
            </slice>
            <slice>
              <time_slice>9:21</time_slice>
              <text_slice>probability of the entire real
line should be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:25</time_slice>
              <text_slice>So if you ask me, what is the
probability that x falls</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>between minus infinity and plus
infinity, well, we are</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>sure that x is going to
fall in that range.</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>So the probability of that
event should be 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>So the probability of being
between minus infinity and</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>plus infinity should be 1, which
means that the integral</text_slice>
            </slice>
            <slice>
              <time_slice>9:43</time_slice>
              <text_slice>from minus infinity to plus
infinity should be 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:46</time_slice>
              <text_slice>So that just tells us that
there's 1 unit of total</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>probability that's being
spread over our space.</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>Now, what's the best way to
think intuitively about what</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>the density function does?</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>The interpretation that I find
most natural and easy to</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>convey the meaning of a
density is to look at</text_slice>
            </slice>
            <slice>
              <time_slice>10:10</time_slice>
              <text_slice>probabilities of small
intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>10:13</time_slice>
              <text_slice>So let us take an x somewhere
here and then x plus delta</text_slice>
            </slice>
            <slice>
              <time_slice>10:18</time_slice>
              <text_slice>just next to it.</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>So delta is a small number.</text_slice>
            </slice>
            <slice>
              <time_slice>10:23</time_slice>
              <text_slice>And let's look at the
probability of the event that</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>we get a value in that range.</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>For continuous random variables,
the way we find the</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>probability of falling in that
range is by integrating the</text_slice>
            </slice>
            <slice>
              <time_slice>10:35</time_slice>
              <text_slice>density over that range.</text_slice>
            </slice>
            <slice>
              <time_slice>10:37</time_slice>
              <text_slice>So we're drawing this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>And we want to take the
area under this curve.</text_slice>
            </slice>
            <slice>
              <time_slice>10:46</time_slice>
              <text_slice>Now, what happens if delta
is a fairly small number?</text_slice>
            </slice>
            <slice>
              <time_slice>10:50</time_slice>
              <text_slice>If delta is pretty small, our
density is not going to change</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>much over that range.</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>So you can pretend that
the density is</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>approximately constant.</text_slice>
            </slice>
            <slice>
              <time_slice>11:01</time_slice>
              <text_slice>And so to find the area under
the curve, you just take the</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>base times the height.</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>And it doesn't matter where
exactly you take the height in</text_slice>
            </slice>
            <slice>
              <time_slice>11:10</time_slice>
              <text_slice>that interval, because the
density doesn't change very</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>much over that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>And so the integral becomes just
base times the height.</text_slice>
            </slice>
            <slice>
              <time_slice>11:19</time_slice>
              <text_slice>So for small intervals, the
probability of a small</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>interval is approximately
the density times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>11:30</time_slice>
              <text_slice>So densities essentially
give us</text_slice>
            </slice>
            <slice>
              <time_slice>11:32</time_slice>
              <text_slice>probabilities of small intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>And if you want to think about
it a little differently, you</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>can take that delta from
here and send it to</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>the denominator there.</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>And what this tells you
is that the density is</text_slice>
            </slice>
            <slice>
              <time_slice>11:48</time_slice>
              <text_slice>probability per unit length for
intervals of small length.</text_slice>
            </slice>
            <slice>
              <time_slice>11:55</time_slice>
              <text_slice>So the units of density are
probability per unit length.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>Densities are not
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>12:01</time_slice>
              <text_slice>They are rates at which
probabilities accumulate,</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>probabilities per unit length.</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>And since densities are not
probabilities, they don't have</text_slice>
            </slice>
            <slice>
              <time_slice>12:09</time_slice>
              <text_slice>to be less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>Ordinary probabilities always
must be less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>12:14</time_slice>
              <text_slice>But density is a different
kind of thing.</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>It can get pretty big
in some places.</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>It can even sort of blow
up in some places.</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>As long as the total area under
the curve is 1, other</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>than that, the curve can do
anything that it wants.</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>Now, the density prescribes
for us the</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>probability of intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>Sometimes we may want to find
the probability of more</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>general sets.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>How would we do that?</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>Well, for nice sets, you will
just integrate the density</text_slice>
            </slice>
            <slice>
              <time_slice>12:51</time_slice>
              <text_slice>over that nice set.</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>I'm not quite defining
what "nice" means.</text_slice>
            </slice>
            <slice>
              <time_slice>12:56</time_slice>
              <text_slice>That's a pretty technical
topic in the theory of</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>13:00</time_slice>
              <text_slice>But for our purposes, usually we
will take b to be something</text_slice>
            </slice>
            <slice>
              <time_slice>13:04</time_slice>
              <text_slice>like a union of intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>So how do you find the
probability of falling in the</text_slice>
            </slice>
            <slice>
              <time_slice>13:10</time_slice>
              <text_slice>union of two intervals?</text_slice>
            </slice>
            <slice>
              <time_slice>13:11</time_slice>
              <text_slice>Well, you find the probability
of falling in that interval</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>plus the probability of falling
in that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>So it's the integral over this
interval plus the integral</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>over that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>13:20</time_slice>
              <text_slice>And you think of this as just
integrating over the union of</text_slice>
            </slice>
            <slice>
              <time_slice>13:24</time_slice>
              <text_slice>the two intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>So once you can calculate
probabilities of intervals,</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>then usually you are in
business, and you can</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>calculate anything else
you might want.</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>So the probability density
function is a complete</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>description of any statistical
information we might be</text_slice>
            </slice>
            <slice>
              <time_slice>13:39</time_slice>
              <text_slice>interested in for a continuous
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>So now we can start walking
through the concepts and the</text_slice>
            </slice>
            <slice>
              <time_slice>13:47</time_slice>
              <text_slice>definitions that we have for
discrete random variables and</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>translate them to the
continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>13:54</time_slice>
              <text_slice>The first big concept is the
concept of the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>One can start with a
mathematical definition.</text_slice>
            </slice>
            <slice>
              <time_slice>14:01</time_slice>
              <text_slice>And here we put down
a definition by</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>just translating notation.</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>Wherever we have a sum in
the discrete case, we</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>now write an integral.</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>And wherever we had the
probability mass function, we</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>now throw in the probability
density function.</text_slice>
            </slice>
            <slice>
              <time_slice>14:20</time_slice>
              <text_slice>This formula--</text_slice>
            </slice>
            <slice>
              <time_slice>14:22</time_slice>
              <text_slice>you may have seen it in
freshman physics--</text_slice>
            </slice>
            <slice>
              <time_slice>14:24</time_slice>
              <text_slice>basically, it again gives you
the center of gravity of the</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>picture that you have when
you have the density.</text_slice>
            </slice>
            <slice>
              <time_slice>14:31</time_slice>
              <text_slice>It's the center of gravity of
the object sitting underneath</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>the probability density
function.</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>So that the interpretation
still applies.</text_slice>
            </slice>
            <slice>
              <time_slice>14:40</time_slice>
              <text_slice>It's also true that our
conceptual interpretation of</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>what an expectation means is
also valid in this case.</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>That is, if you repeat an
experiment a zillion times,</text_slice>
            </slice>
            <slice>
              <time_slice>14:51</time_slice>
              <text_slice>each time drawing an independent
sample of your</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>random variable x, in the long
run, the average that you are</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>going to get should be
the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>One can reason in a hand-waving
way, sort of</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>intuitively, the way we did it
for the case of discrete</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>15:08</time_slice>
              <text_slice>But this is also a theorem
of some sort.</text_slice>
            </slice>
            <slice>
              <time_slice>15:11</time_slice>
              <text_slice>It's a limit theorem that we're
going to visit later on</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>Having defined the expectation
and having claimed that the</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>interpretation of the
expectation is that same as</text_slice>
            </slice>
            <slice>
              <time_slice>15:23</time_slice>
              <text_slice>before, then we can start taking
just any formula you've</text_slice>
            </slice>
            <slice>
              <time_slice>15:26</time_slice>
              <text_slice>seen before and just
translate it.</text_slice>
            </slice>
            <slice>
              <time_slice>15:28</time_slice>
              <text_slice>So for example, to find the
expected value of a function</text_slice>
            </slice>
            <slice>
              <time_slice>15:31</time_slice>
              <text_slice>of a continuous random variable,
you do not have to</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>find the PDF or PMF of g(X).</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>You can just work directly with
the original distribution</text_slice>
            </slice>
            <slice>
              <time_slice>15:43</time_slice>
              <text_slice>of the random variable
capital X.</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>And this formula is the same
as for the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>15:48</time_slice>
              <text_slice>Sums get replaced
by integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>And PMFs get replaced by PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>And in particular, the variance
of a random variable</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>is defined again the same way.</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>The variance is the expected
value, the average of the</text_slice>
            </slice>
            <slice>
              <time_slice>16:03</time_slice>
              <text_slice>distance of X from the mean
and then squared.</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>So it's the expected value for
a random variable that takes</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>these numerical values.</text_slice>
            </slice>
            <slice>
              <time_slice>16:12</time_slice>
              <text_slice>And same formula as before,
integral and F instead of</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>summation, and the P.</text_slice>
            </slice>
            <slice>
              <time_slice>16:19</time_slice>
              <text_slice>And the formulas that we have
derived or formulas that you</text_slice>
            </slice>
            <slice>
              <time_slice>16:23</time_slice>
              <text_slice>have seen for the discrete case,
they all go through the</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>16:27</time_slice>
              <text_slice>So for example, the useful
relation for variances, which</text_slice>
            </slice>
            <slice>
              <time_slice>16:31</time_slice>
              <text_slice>is this one, remains true.</text_slice>
            </slice>
            <slice>
              <time_slice>16:37</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>16:37</time_slice>
              <text_slice>So time for an example.</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>The most simple example of a
continuous random variable</text_slice>
            </slice>
            <slice>
              <time_slice>16:43</time_slice>
              <text_slice>that there is, is the so-called</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>uniform random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>16:48</time_slice>
              <text_slice>So the uniform random variable
is described by a density</text_slice>
            </slice>
            <slice>
              <time_slice>16:51</time_slice>
              <text_slice>which is 0 except over
an interval.</text_slice>
            </slice>
            <slice>
              <time_slice>16:55</time_slice>
              <text_slice>And over that interval,
it is constant.</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>What is it meant to convey?</text_slice>
            </slice>
            <slice>
              <time_slice>17:00</time_slice>
              <text_slice>It's trying to convey the idea
that all x's in this range are</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>17:06</time_slice>
              <text_slice>Well, that doesn't
say very much.</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>Any individual x has
0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>So it's conveying a little
more than that.</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>What it is saying is that if I
take an interval of a given</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>length delta, and I take another
interval of the same</text_slice>
            </slice>
            <slice>
              <time_slice>17:22</time_slice>
              <text_slice>length, delta, under the uniform
distribution, these</text_slice>
            </slice>
            <slice>
              <time_slice>17:26</time_slice>
              <text_slice>two intervals are going to have
the same probability.</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>So being uniform means that
intervals of same length have</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>the same probability.</text_slice>
            </slice>
            <slice>
              <time_slice>17:35</time_slice>
              <text_slice>So no interval is more likely
than any other to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>And in that sense, it conveys
the idea of sort of complete</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>17:45</time_slice>
              <text_slice>Any little interval in our range
is equally likely as any</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>other little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>17:49</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>17:50</time_slice>
              <text_slice>So what's the formula
for this density?</text_slice>
            </slice>
            <slice>
              <time_slice>17:53</time_slice>
              <text_slice>I only told you the range.</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>What's the height?</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>Well, the area under the density
must be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>Total probability
is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>18:02</time_slice>
              <text_slice>And so the height, inescapably,
is going to be 1</text_slice>
            </slice>
            <slice>
              <time_slice>18:07</time_slice>
              <text_slice>over (b minus a).</text_slice>
            </slice>
            <slice>
              <time_slice>18:09</time_slice>
              <text_slice>That's the height that makes
the density integrate to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>So that's the formula.</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>And if you don't want to lose
one point in your exam, you</text_slice>
            </slice>
            <slice>
              <time_slice>18:21</time_slice>
              <text_slice>have to say that it's
also 0, otherwise.</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>18:27</time_slice>
              <text_slice>All right?</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>That's sort of the
complete answer.</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>How about the expected value
of this random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>18:35</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>18:36</time_slice>
              <text_slice>You can find the expected value
in two different ways.</text_slice>
            </slice>
            <slice>
              <time_slice>18:39</time_slice>
              <text_slice>One is to start with
the definition.</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>And so you integrate
over the range of</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>interest times the density.</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>And you figure out what that
integral is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>Or you can be a little
more clever.</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>Since the center-of-gravity
interpretation is still true,</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>it must be the center of gravity
of this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>19:03</time_slice>
              <text_slice>And the center of gravity is,
of course, the midpoint.</text_slice>
            </slice>
            <slice>
              <time_slice>19:06</time_slice>
              <text_slice>Whenever you have symmetry,
the mean is always the</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>midpoint of the diagram that
gives you the PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>So that's the expected
value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>19:24</time_slice>
              <text_slice>Finally, regarding the variance,
well, there you will</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>have to do a little
bit of calculus.</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>We can write down
the definition.</text_slice>
            </slice>
            <slice>
              <time_slice>19:33</time_slice>
              <text_slice>So it's an integral
instead of a sum.</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>A typical value of the random
variable minus the expected</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>value, squared, times
the density.</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>And we integrate.</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>You do this integral, and you
find it's (b minus a) squared</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>over that number, which
happens to be 12.</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>Maybe more interesting is the
standard deviation itself.</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>And you see that the standard
deviation is proportional to</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>the width of that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>20:05</time_slice>
              <text_slice>This agrees with our intuition,
that the standard</text_slice>
            </slice>
            <slice>
              <time_slice>20:07</time_slice>
              <text_slice>deviation is meant to capture a
sense of how spread out our</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>distribution is.</text_slice>
            </slice>
            <slice>
              <time_slice>20:14</time_slice>
              <text_slice>And the standard deviation has
the same units as the random</text_slice>
            </slice>
            <slice>
              <time_slice>20:17</time_slice>
              <text_slice>variable itself.</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>So it's sort of good to-- you
can interpret it in a</text_slice>
            </slice>
            <slice>
              <time_slice>20:22</time_slice>
              <text_slice>reasonable way based
on that picture.</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>OK, yes.</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>Now, let's go up one level and
think about the following.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>So we have formulas for the
discrete case, formulas for</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>20:42</time_slice>
              <text_slice>So you can write them
side by side.</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>One has sums, the other
has integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>20:47</time_slice>
              <text_slice>Suppose you want to make an
argument and say that</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>something is true for every
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>20:52</time_slice>
              <text_slice>You would essentially need to
do two separate proofs, for</text_slice>
            </slice>
            <slice>
              <time_slice>20:55</time_slice>
              <text_slice>discrete and for continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>20:57</time_slice>
              <text_slice>Is there some way of dealing
with random variables just one</text_slice>
            </slice>
            <slice>
              <time_slice>21:00</time_slice>
              <text_slice>at a time, in one shot, using
a sort of uniform notation?</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>Is there a unifying concept?</text_slice>
            </slice>
            <slice>
              <time_slice>21:07</time_slice>
              <text_slice>Luckily, there is one.</text_slice>
            </slice>
            <slice>
              <time_slice>21:10</time_slice>
              <text_slice>It's the notion of the
cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>function of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>And it's a concept that applies
equally well to</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>discrete and continuous
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>So it's an object that we can
use to describe distributions</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>in both cases, using just
one piece of notation.</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>So what's the definition?</text_slice>
            </slice>
            <slice>
              <time_slice>21:33</time_slice>
              <text_slice>It's the probability that the
random variable takes values</text_slice>
            </slice>
            <slice>
              <time_slice>21:36</time_slice>
              <text_slice>less than a certain
number little x.</text_slice>
            </slice>
            <slice>
              <time_slice>21:39</time_slice>
              <text_slice>So you go to the diagram, and
you see what's the probability</text_slice>
            </slice>
            <slice>
              <time_slice>21:41</time_slice>
              <text_slice>that I'm falling to
the left of this.</text_slice>
            </slice>
            <slice>
              <time_slice>21:44</time_slice>
              <text_slice>And you specify those
probabilities for all x's.</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>In the continuous case, you
calculate those probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>21:51</time_slice>
              <text_slice>using the integral formula.</text_slice>
            </slice>
            <slice>
              <time_slice>21:53</time_slice>
              <text_slice>So you integrate from
here up to x.</text_slice>
            </slice>
            <slice>
              <time_slice>21:55</time_slice>
              <text_slice>In the discrete case, to find
the probability to the left of</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>some point, you go here, and
you add probabilities again</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>from the left.</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>So the way that the cumulative
distribution function is</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>calculated is a little different
in the continuous</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>and discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>In one case you integrate.</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>In the other, you sum.</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>But leaving aside how it's being
calculated, what the</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>concept is, it's the same
concept in both cases.</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>So let's see what the shape of
the cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>22:25</time_slice>
              <text_slice>function would be in
the two cases.</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>So here what we want is to
record for every little x the</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>probability of falling
to the left of x.</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>So let's start here.</text_slice>
            </slice>
            <slice>
              <time_slice>22:38</time_slice>
              <text_slice>Probability of falling to
the left of here is 0--</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>0, 0, 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>Once we get here and we start
moving to the right, the</text_slice>
            </slice>
            <slice>
              <time_slice>22:47</time_slice>
              <text_slice>probability of falling to the
left of here is the area of</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>this little rectangle.</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>And the area of that little
rectangle increases linearly</text_slice>
            </slice>
            <slice>
              <time_slice>22:57</time_slice>
              <text_slice>as I keep moving.</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>So accordingly, the CDF
increases linearly until I get</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>to that point.</text_slice>
            </slice>
            <slice>
              <time_slice>23:04</time_slice>
              <text_slice>At that point, what's
the value of my CDF?</text_slice>
            </slice>
            <slice>
              <time_slice>23:08</time_slice>
              <text_slice>1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>I have accumulated all the
probability there is.</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>I have integrated it.</text_slice>
            </slice>
            <slice>
              <time_slice>23:13</time_slice>
              <text_slice>This total area has
to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>So it reaches 1, and then
there's no more probability to</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>be accumulated.</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>It just stays at 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:23</time_slice>
              <text_slice>So the value here
is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:28</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>23:30</time_slice>
              <text_slice>How would you find the density
if somebody gave you the CDF?</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>The CDF is the integral
of the density.</text_slice>
            </slice>
            <slice>
              <time_slice>23:39</time_slice>
              <text_slice>Therefore, the density is the
derivative of the CDF.</text_slice>
            </slice>
            <slice>
              <time_slice>23:43</time_slice>
              <text_slice>So you look at this picture
and take the derivative.</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>Derivative is 0 here, 0 here.</text_slice>
            </slice>
            <slice>
              <time_slice>23:48</time_slice>
              <text_slice>And it's a constant
up there, which</text_slice>
            </slice>
            <slice>
              <time_slice>23:51</time_slice>
              <text_slice>corresponds to that constant.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>So more generally, and an
important thing to know, is</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>that the derivative of the CDF
is equal to the density--</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>almost, with a little
bit of an exception.</text_slice>
            </slice>
            <slice>
              <time_slice>24:14</time_slice>
              <text_slice>What's the exception?</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>At those places where the CDF
does not have a derivative--</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>here where it has a corner--</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>the derivative is undefined.</text_slice>
            </slice>
            <slice>
              <time_slice>24:23</time_slice>
              <text_slice>And in some sense, the
density is also</text_slice>
            </slice>
            <slice>
              <time_slice>24:26</time_slice>
              <text_slice>ambiguous at that point.</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>Is my density at the endpoint,
is it 0 or is it 1?</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>It doesn't really matter.</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>If you change the density at
just a single point, it's not</text_slice>
            </slice>
            <slice>
              <time_slice>24:36</time_slice>
              <text_slice>going to affect the
value of any</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>integral you ever calculate.</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>So the value of the density at
the endpoint, you can leave it</text_slice>
            </slice>
            <slice>
              <time_slice>24:44</time_slice>
              <text_slice>as being ambiguous, or
you can specify it.</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>It doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>So at all places where the
CDF has a derivative,</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>this will be true.</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>At those places where you have
corners, which do show up</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>sometimes, well, you
don't really care.</text_slice>
            </slice>
            <slice>
              <time_slice>25:01</time_slice>
              <text_slice>How about the discrete case?</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>In the discrete case, the CDF
has a more peculiar shape.</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>So let's do the calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>We want to find the
probability of b</text_slice>
            </slice>
            <slice>
              <time_slice>25:10</time_slice>
              <text_slice>to the left of here.</text_slice>
            </slice>
            <slice>
              <time_slice>25:11</time_slice>
              <text_slice>That probability is 0, 0, 0.</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>Once we cross that point, the
probability of being to the</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>left of here is 1/6.</text_slice>
            </slice>
            <slice>
              <time_slice>25:19</time_slice>
              <text_slice>So as soon as we cross the
point 1, we get the</text_slice>
            </slice>
            <slice>
              <time_slice>25:22</time_slice>
              <text_slice>probability of 1/6, which means
that the size of the</text_slice>
            </slice>
            <slice>
              <time_slice>25:25</time_slice>
              <text_slice>jump that we have here is 1/6.</text_slice>
            </slice>
            <slice>
              <time_slice>25:29</time_slice>
              <text_slice>Now, question.</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>At this point 1, which is the
correct value of the CDF?</text_slice>
            </slice>
            <slice>
              <time_slice>25:35</time_slice>
              <text_slice>Is it 0, or is it 1/6?</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>It's 1/6 because--</text_slice>
            </slice>
            <slice>
              <time_slice>25:40</time_slice>
              <text_slice>you need to look carefully
at the definitions, the</text_slice>
            </slice>
            <slice>
              <time_slice>25:42</time_slice>
              <text_slice>probability of x being less
than or equal to little x.</text_slice>
            </slice>
            <slice>
              <time_slice>25:46</time_slice>
              <text_slice>If I take little x to be 1,
it's the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>25:49</time_slice>
              <text_slice>capital X is less than
or equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:51</time_slice>
              <text_slice>So it includes the event
that x is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>So it includes this
probability here.</text_slice>
            </slice>
            <slice>
              <time_slice>25:58</time_slice>
              <text_slice>So at jump points, the correct
value of the CDF is going to</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>be this one.</text_slice>
            </slice>
            <slice>
              <time_slice>26:04</time_slice>
              <text_slice>And now as I trace, x is
going to the right.</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>As soon as I cross this point,
I have added another 3/6</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>So that 3/6 causes a
jump to the CDF.</text_slice>
            </slice>
            <slice>
              <time_slice>26:20</time_slice>
              <text_slice>And that determines
the new value.</text_slice>
            </slice>
            <slice>
              <time_slice>26:23</time_slice>
              <text_slice>And finally, once I cross
the last point, I get</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>another jump of 2/6.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>A general moral from these two
examples and these pictures.</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>CDFs are well defined
in both cases.</text_slice>
            </slice>
            <slice>
              <time_slice>26:39</time_slice>
              <text_slice>For the case of continuous
random variables, the CDF will</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>be a continuous function.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>It starts from 0.</text_slice>
            </slice>
            <slice>
              <time_slice>26:46</time_slice>
              <text_slice>It eventually goes to 1
and goes smoothly--</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>well, continuously from smaller
to higher values.</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>It can only go up.</text_slice>
            </slice>
            <slice>
              <time_slice>26:55</time_slice>
              <text_slice>It cannot go down since we're
accumulating more and more</text_slice>
            </slice>
            <slice>
              <time_slice>26:58</time_slice>
              <text_slice>probability as we are
going to the right.</text_slice>
            </slice>
            <slice>
              <time_slice>27:00</time_slice>
              <text_slice>In the discrete case, again
it starts from 0,</text_slice>
            </slice>
            <slice>
              <time_slice>27:03</time_slice>
              <text_slice>and it goes to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>But it does it in a
staircase manner.</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>And you get a jump at each place
where the PMF assigns a</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>positive mass.</text_slice>
            </slice>
            <slice>
              <time_slice>27:14</time_slice>
              <text_slice>So jumps in the CDF are
associated with point masses</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>in our distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>27:20</time_slice>
              <text_slice>In the continuous case, we don't
have any point masses,</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>so we do not have any
jumps either.</text_slice>
            </slice>
            <slice>
              <time_slice>27:30</time_slice>
              <text_slice>Now, besides saving
us notation--</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>we don't have to deal
with discrete</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>and continuous twice--</text_slice>
            </slice>
            <slice>
              <time_slice>27:39</time_slice>
              <text_slice>CDFs give us actually a little
more flexibility.</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>Not all random variables are
continuous or discrete.</text_slice>
            </slice>
            <slice>
              <time_slice>27:46</time_slice>
              <text_slice>You can cook up random variables
that are kind of</text_slice>
            </slice>
            <slice>
              <time_slice>27:49</time_slice>
              <text_slice>neither or a mixture
of the two.</text_slice>
            </slice>
            <slice>
              <time_slice>27:53</time_slice>
              <text_slice>An example would be, let's
say you play a game.</text_slice>
            </slice>
            <slice>
              <time_slice>27:59</time_slice>
              <text_slice>And with a certain probability,
you get a certain</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>number of dollars
in your hands.</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>So you flip a coin.</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>And with probability 1/2, you
get a reward of 1/2 dollars.</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>And with probability 1/2, you
are led to a dark room where</text_slice>
            </slice>
            <slice>
              <time_slice>28:18</time_slice>
              <text_slice>you spin a wheel of fortune.</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>And that wheel of fortune gives
you a random reward</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>between 0 and 1.</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>So any of these outcomes
is possible.</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>And the amount that you're
going to get,</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>let's say, is uniform.</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>So you flip a coin.</text_slice>
            </slice>
            <slice>
              <time_slice>28:35</time_slice>
              <text_slice>And depending on the outcome of
the coin, either you get a</text_slice>
            </slice>
            <slice>
              <time_slice>28:38</time_slice>
              <text_slice>certain value or you get a
value that ranges over a</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>continuous interval.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>So what kind of random
variable is it?</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>Is it continuous?</text_slice>
            </slice>
            <slice>
              <time_slice>28:50</time_slice>
              <text_slice>Well, continuous random
variables assign 0 probability</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>to individual points.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>Is it the case here?</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>No, because you have positive
probability of</text_slice>
            </slice>
            <slice>
              <time_slice>29:00</time_slice>
              <text_slice>obtaining 1/2 dollar.</text_slice>
            </slice>
            <slice>
              <time_slice>29:04</time_slice>
              <text_slice>So our random variable
is not continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>Is it discrete?</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>It's not discrete, because our
random variable can take</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>values also over a
continuous range.</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>So we call such a random
variable a</text_slice>
            </slice>
            <slice>
              <time_slice>29:16</time_slice>
              <text_slice>mixed random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>29:19</time_slice>
              <text_slice>If you were to draw its
distribution very loosely,</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>probably you would want to draw
a picture like this one,</text_slice>
            </slice>
            <slice>
              <time_slice>29:33</time_slice>
              <text_slice>which kind of conveys the
idea of what's going on.</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>So just think of this as a
drawing of masses that are</text_slice>
            </slice>
            <slice>
              <time_slice>29:39</time_slice>
              <text_slice>sitting over a table.</text_slice>
            </slice>
            <slice>
              <time_slice>29:41</time_slice>
              <text_slice>We place an object that weighs
half a pound, but it's an</text_slice>
            </slice>
            <slice>
              <time_slice>29:47</time_slice>
              <text_slice>object that takes zero space.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>So half a pound is just sitting
on top of that point.</text_slice>
            </slice>
            <slice>
              <time_slice>29:53</time_slice>
              <text_slice>And we take another half-pound
of probability and spread it</text_slice>
            </slice>
            <slice>
              <time_slice>29:57</time_slice>
              <text_slice>uniformly over that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>So this is like a piece that
comes from mass functions.</text_slice>
            </slice>
            <slice>
              <time_slice>30:04</time_slice>
              <text_slice>And that's a piece that looks
more like a density function.</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>And we just throw them together
in the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>30:10</time_slice>
              <text_slice>I'm not trying to associate
any formal</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>meaning with this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>30:14</time_slice>
              <text_slice>It's just a schematic of how
probabilities are distributed,</text_slice>
            </slice>
            <slice>
              <time_slice>30:18</time_slice>
              <text_slice>help us visualize
what's going on.</text_slice>
            </slice>
            <slice>
              <time_slice>30:20</time_slice>
              <text_slice>Now, if you have taken classes
on systems and all of that,</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>you may have seen the concept
of an impulse function.</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>And you my start saying that,
oh, I should treat this</text_slice>
            </slice>
            <slice>
              <time_slice>30:33</time_slice>
              <text_slice>mathematically as a so-called
impulse function.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>But we do not need this for our
purposes in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>Just think of this as a nice
picture that conveys what's</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>going on in this particular
case.</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>So now, what would the CDF
look like in this case?</text_slice>
            </slice>
            <slice>
              <time_slice>30:51</time_slice>
              <text_slice>The CDF is always well defined,
no matter what kind</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>of random variable you have.</text_slice>
            </slice>
            <slice>
              <time_slice>30:57</time_slice>
              <text_slice>So the fact that it's not
continuous, it's not discrete</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>shouldn't be a problem as
long as we can calculate</text_slice>
            </slice>
            <slice>
              <time_slice>31:01</time_slice>
              <text_slice>probabilities of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>31:04</time_slice>
              <text_slice>So the probability of falling
to the left here is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>Once I start crossing there, the
probability of falling to</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>the left of a point increases
linearly with</text_slice>
            </slice>
            <slice>
              <time_slice>31:13</time_slice>
              <text_slice>how far I have gone.</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>So we get this linear
increase.</text_slice>
            </slice>
            <slice>
              <time_slice>31:17</time_slice>
              <text_slice>But as soon as I cross that
point, I accumulate another</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>1/2 unit of probability
instantly.</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>And once I accumulate that 1/2
unit, it means that my CDF is</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>going to have a jump of 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>31:30</time_slice>
              <text_slice>And then afterwards, I still
keep accumulating probability</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>at a fixed rate, the rate
being the density.</text_slice>
            </slice>
            <slice>
              <time_slice>31:36</time_slice>
              <text_slice>And I keep accumulating, again,
at a linear rate until</text_slice>
            </slice>
            <slice>
              <time_slice>31:39</time_slice>
              <text_slice>I settle to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>So this is a CDF that has
certain pieces where it</text_slice>
            </slice>
            <slice>
              <time_slice>31:46</time_slice>
              <text_slice>increases continuously.</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>And that corresponds to the
continuous part of our</text_slice>
            </slice>
            <slice>
              <time_slice>31:50</time_slice>
              <text_slice>randomize variable.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>And it also has some places
where it has discrete jumps.</text_slice>
            </slice>
            <slice>
              <time_slice>31:55</time_slice>
              <text_slice>And those district jumps
correspond to places in which</text_slice>
            </slice>
            <slice>
              <time_slice>31:57</time_slice>
              <text_slice>we have placed a
positive mass.</text_slice>
            </slice>
            <slice>
              <time_slice>32:00</time_slice>
              <text_slice>And by the--</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>OK, yeah.</text_slice>
            </slice>
            <slice>
              <time_slice>32:03</time_slice>
              <text_slice>So this little 0 shouldn't
be there.</text_slice>
            </slice>
            <slice>
              <time_slice>32:06</time_slice>
              <text_slice>So let's cross it out.</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>So finally, we're going to take
the remaining time and</text_slice>
            </slice>
            <slice>
              <time_slice>32:15</time_slice>
              <text_slice>introduce our new friend.</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>It's going to be the Gaussian
or normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>32:23</time_slice>
              <text_slice>So it's the most important
distribution there is in all</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>of probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>It's plays a very
central role.</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>It shows up all over
the place.</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>We'll see later in the
class in more detail</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>why it shows up.</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>But the quick preview
is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>If you have a phenomenon in
which you measure a certain</text_slice>
            </slice>
            <slice>
              <time_slice>32:46</time_slice>
              <text_slice>quantity, but that quantity is
made up of lots and lots of</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>random contributions--</text_slice>
            </slice>
            <slice>
              <time_slice>32:52</time_slice>
              <text_slice>so your random variable is
actually the sum of lots and</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>lots of independent little
random variables--</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>then invariability, no matter
what kind of distribution the</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>little random variables have,
their sum will turn out to</text_slice>
            </slice>
            <slice>
              <time_slice>33:08</time_slice>
              <text_slice>have approximately a normal
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>So this makes the normal
distribution to arise very</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>naturally in lots and
lots of contexts.</text_slice>
            </slice>
            <slice>
              <time_slice>33:16</time_slice>
              <text_slice>Whenever you have noise that's
comprised of lots of different</text_slice>
            </slice>
            <slice>
              <time_slice>33:21</time_slice>
              <text_slice>independent pieces of noise,
then the end result will be a</text_slice>
            </slice>
            <slice>
              <time_slice>33:26</time_slice>
              <text_slice>random variable that's normal.</text_slice>
            </slice>
            <slice>
              <time_slice>33:28</time_slice>
              <text_slice>So we are going to come back
to that topic later.</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>But that's the preview comment,
basically to argue</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>that it's an important one.</text_slice>
            </slice>
            <slice>
              <time_slice>33:37</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>33:37</time_slice>
              <text_slice>And there's a special case.</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>If you are dealing with a
binomial distribution, which</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>is the sum of lots of Bernoulli
random variables,</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>again you would expect that
the binomial would start</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>looking like a normal if you
have many, many-- a large</text_slice>
            </slice>
            <slice>
              <time_slice>33:51</time_slice>
              <text_slice>number of point fields.</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>So what's the math
involved here?</text_slice>
            </slice>
            <slice>
              <time_slice>33:56</time_slice>
              <text_slice>Let's parse the formula for
the density of the normal.</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>What we start with is the
function X squared over 2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:07</time_slice>
              <text_slice>And if you are to plot X
squared over 2, it's a</text_slice>
            </slice>
            <slice>
              <time_slice>34:09</time_slice>
              <text_slice>parabola, and it has
this shape --</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>X squared over 2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>Then what do we do?</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>We take the negative exponential
of this.</text_slice>
            </slice>
            <slice>
              <time_slice>34:20</time_slice>
              <text_slice>So when X squared over
2 is 0, then negative</text_slice>
            </slice>
            <slice>
              <time_slice>34:24</time_slice>
              <text_slice>exponential is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>When X squared over 2 increases,
the negative</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>exponential of that falls off,
and it falls off pretty fast.</text_slice>
            </slice>
            <slice>
              <time_slice>34:37</time_slice>
              <text_slice>So as this goes up, the
formula for the</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>density goes down.</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>And because exponentials are
pretty strong in how quickly</text_slice>
            </slice>
            <slice>
              <time_slice>34:45</time_slice>
              <text_slice>they fall off, this means that
the tails of this distribution</text_slice>
            </slice>
            <slice>
              <time_slice>34:49</time_slice>
              <text_slice>actually do go down
pretty fast.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>So that explains the shape
of the normal PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>34:57</time_slice>
              <text_slice>How about this factor 1
over square root 2 pi?</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>Where does this come from?</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>Well, the integral has
to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>So you have to go and do your
calculus exercise and find the</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>integral of this the minus X
squared over 2 function and</text_slice>
            </slice>
            <slice>
              <time_slice>35:18</time_slice>
              <text_slice>then figure out, what constant
do I need to put in front so</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>that the integral
is equal to 1?</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>How do you evaluate
that integral?</text_slice>
            </slice>
            <slice>
              <time_slice>35:26</time_slice>
              <text_slice>Either you go to Mathematica
or Wolfram's Alpha or</text_slice>
            </slice>
            <slice>
              <time_slice>35:30</time_slice>
              <text_slice>whatever, and it tells
you what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>35:33</time_slice>
              <text_slice>Or it's a very beautiful
calculus exercise that you may</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>have seen at some point.</text_slice>
            </slice>
            <slice>
              <time_slice>35:39</time_slice>
              <text_slice>You throw in another exponential
of this kind, you</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>bring in polar coordinates, and
somehow the answer comes</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>beautifully out there.</text_slice>
            </slice>
            <slice>
              <time_slice>35:48</time_slice>
              <text_slice>But in any case, this is the
constant that you need to make</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>it integrate to 1 and to be
a legitimate density.</text_slice>
            </slice>
            <slice>
              <time_slice>35:56</time_slice>
              <text_slice>We call this the standard
normal.</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>And for the standard normal,
what is the expected value?</text_slice>
            </slice>
            <slice>
              <time_slice>36:02</time_slice>
              <text_slice>Well, the symmetry, so
it's equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>36:05</time_slice>
              <text_slice>What is the variance?</text_slice>
            </slice>
            <slice>
              <time_slice>36:07</time_slice>
              <text_slice>Well, here there's
no shortcut.</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>You have to do another
calculus exercise.</text_slice>
            </slice>
            <slice>
              <time_slice>36:12</time_slice>
              <text_slice>And you find that the variance
is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>So this is a normal that's
centered around 0.</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>How about other types of normals
that are centered at</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>different places?</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>So we can do the same
kind of thing.</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>Instead of centering it at 0,
we can take some place where</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>we want to center it, write down
a quadratic such as (X</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>minus mu) squared, and then
take the negative</text_slice>
            </slice>
            <slice>
              <time_slice>36:44</time_slice>
              <text_slice>exponential of that.</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>And that gives us a normal
density that's centered at mu.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>Now, I may wish to control
the width of my density.</text_slice>
            </slice>
            <slice>
              <time_slice>37:01</time_slice>
              <text_slice>To control the width of my
density, equivalently I can</text_slice>
            </slice>
            <slice>
              <time_slice>37:04</time_slice>
              <text_slice>control the width
of my parabola.</text_slice>
            </slice>
            <slice>
              <time_slice>37:07</time_slice>
              <text_slice>If my parabola is narrower, if
my parabola looks like this,</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>what's going to happen
to the density?</text_slice>
            </slice>
            <slice>
              <time_slice>37:17</time_slice>
              <text_slice>It's going to fall
off much faster.</text_slice>
            </slice>
            <slice>
              <time_slice>37:26</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>37:26</time_slice>
              <text_slice>How do I make my parabola
narrower or wider?</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>I do it by putting in a
constant down here.</text_slice>
            </slice>
            <slice>
              <time_slice>37:35</time_slice>
              <text_slice>So by putting a sigma here, this
stretches or widens my</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>parabola by a factor of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>37:42</time_slice>
              <text_slice>Let's see.</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>Which way does it go?</text_slice>
            </slice>
            <slice>
              <time_slice>37:44</time_slice>
              <text_slice>If sigma is very small,
this is a big number.</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>My parabola goes up quickly,
which means my normal falls</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>off very fast.</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>So small sigma corresponds
to a narrower density.</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>And so it, therefore, should be
intuitive that the standard</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>deviation is proportional
to sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>38:11</time_slice>
              <text_slice>Because that's the amount
by which you</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>are scaling the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>38:15</time_slice>
              <text_slice>And indeed, the standard
deviation is sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>And so the variance
is sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>So all that we have done here
to create a general normal</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>with a given mean and variance
is to take this picture, shift</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>it in space so that the mean
sits at mu instead of 0, and</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>then scale it by a
factor of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>38:38</time_slice>
              <text_slice>This gives us a normal
with a given</text_slice>
            </slice>
            <slice>
              <time_slice>38:41</time_slice>
              <text_slice>mean and a given variance.</text_slice>
            </slice>
            <slice>
              <time_slice>38:42</time_slice>
              <text_slice>And the formula for
it is this one.</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>38:48</time_slice>
              <text_slice>Now, normal random variables
have some wonderful</text_slice>
            </slice>
            <slice>
              <time_slice>38:52</time_slice>
              <text_slice>properties.</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>And one of them is that they
behave nicely when you take</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>linear functions of them.</text_slice>
            </slice>
            <slice>
              <time_slice>39:02</time_slice>
              <text_slice>So let's fix some constants
a and b, suppose that X is</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>normal, and look at this
linear function Y.</text_slice>
            </slice>
            <slice>
              <time_slice>39:13</time_slice>
              <text_slice>What is the expected
value of Y?</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>Here we don't need
anything special.</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>We know that the expected value
of a linear function is</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>the linear function of
the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>So the expected value is this.</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>How about the variance?</text_slice>
            </slice>
            <slice>
              <time_slice>39:33</time_slice>
              <text_slice>We know that the variance of a
linear function doesn't care</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>about the constant term.</text_slice>
            </slice>
            <slice>
              <time_slice>39:37</time_slice>
              <text_slice>But the variance gets multiplied
by a squared.</text_slice>
            </slice>
            <slice>
              <time_slice>39:40</time_slice>
              <text_slice>So we get these variance, where
sigma squared is the</text_slice>
            </slice>
            <slice>
              <time_slice>39:46</time_slice>
              <text_slice>variance of the original
normal.</text_slice>
            </slice>
            <slice>
              <time_slice>39:49</time_slice>
              <text_slice>So have we used so far the
property that X is normal?</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>No, we haven't.</text_slice>
            </slice>
            <slice>
              <time_slice>39:55</time_slice>
              <text_slice>This calculation here is true
in general when you take a</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>linear function of a
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>40:02</time_slice>
              <text_slice>But if X is normal, we get the
other additional fact that Y</text_slice>
            </slice>
            <slice>
              <time_slice>40:08</time_slice>
              <text_slice>is also going to be normal.</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>So that's the nontrivial
part of the fact that</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>I'm claiming here.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>So linear functions of normal
random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>40:19</time_slice>
              <text_slice>themselves normal.</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>How do we convince ourselves
about it?</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>It's something that we will do
formerly in about two or three</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>lectures from today.</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>So we're going to prove it.</text_slice>
            </slice>
            <slice>
              <time_slice>40:35</time_slice>
              <text_slice>But if you think about it
intuitively, normal means this</text_slice>
            </slice>
            <slice>
              <time_slice>40:39</time_slice>
              <text_slice>particular bell-shaped curve.</text_slice>
            </slice>
            <slice>
              <time_slice>40:42</time_slice>
              <text_slice>And that bell-shaped curve could
be sitting anywhere and</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>could be scaled in any way.</text_slice>
            </slice>
            <slice>
              <time_slice>40:47</time_slice>
              <text_slice>So you start with a
bell-shaped curve.</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>If you take X, which is bell
shaped, and you multiply it by</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>a constant, what does that do?</text_slice>
            </slice>
            <slice>
              <time_slice>40:57</time_slice>
              <text_slice>Multiplying by a constant is
just like scaling the axis or</text_slice>
            </slice>
            <slice>
              <time_slice>41:01</time_slice>
              <text_slice>changing the units with which
you're measuring it.</text_slice>
            </slice>
            <slice>
              <time_slice>41:03</time_slice>
              <text_slice>So it will take a bell shape
and spread it or narrow it.</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>But it will still
be a bell shape.</text_slice>
            </slice>
            <slice>
              <time_slice>41:10</time_slice>
              <text_slice>And then when you add the
constant, you just take that</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>bell and move it elsewhere.</text_slice>
            </slice>
            <slice>
              <time_slice>41:16</time_slice>
              <text_slice>So under linear transformations,
bell shapes</text_slice>
            </slice>
            <slice>
              <time_slice>41:19</time_slice>
              <text_slice>will remain bell shapes, just
sitting at a different place</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>and with a different width.</text_slice>
            </slice>
            <slice>
              <time_slice>41:25</time_slice>
              <text_slice>And that sort of the intuition
of why normals remain normals</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>under this kind of
transformation.</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>So why is this useful?</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>Well, OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:37</time_slice>
              <text_slice>We have a formula
for the density.</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>But usually we want to calculate
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>How will you calculate
probabilities?</text_slice>
            </slice>
            <slice>
              <time_slice>41:45</time_slice>
              <text_slice>If I ask you, what's the
probability that the normal is</text_slice>
            </slice>
            <slice>
              <time_slice>41:48</time_slice>
              <text_slice>less than 3, how
do you find it?</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>You need to integrate the
density from minus</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>infinity up to 3.</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>Unfortunately, the integral of
the expression that shows up</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>that you would have to
calculate, an integral of this</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>kind from, let's say, minus
infinity to some number, is</text_slice>
            </slice>
            <slice>
              <time_slice>42:12</time_slice>
              <text_slice>something that's not known
in closed form.</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>So if you're looking for a
closed-form formula for this--</text_slice>
            </slice>
            <slice>
              <time_slice>42:23</time_slice>
              <text_slice>X bar--</text_slice>
            </slice>
            <slice>
              <time_slice>42:25</time_slice>
              <text_slice>if you're looking for a
closed-form formula that gives</text_slice>
            </slice>
            <slice>
              <time_slice>42:27</time_slice>
              <text_slice>you the value of this integral
as a function of X bar, you're</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>not going to find it.</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>So what can we do?</text_slice>
            </slice>
            <slice>
              <time_slice>42:36</time_slice>
              <text_slice>Well, since it's a useful
integral, we can</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>just tabulate it.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>Calculate it once and for all,
for all values of X bar up to</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>some precision, and have
that table, and use it.</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>That's what one does.</text_slice>
            </slice>
            <slice>
              <time_slice>42:53</time_slice>
              <text_slice>OK, but now there is a catch.</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>Are we going to write down a
table for every conceivable</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>type of normal distribution--</text_slice>
            </slice>
            <slice>
              <time_slice>43:01</time_slice>
              <text_slice>that is, for every possible
mean and every variance?</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>I guess that would be
a pretty long table.</text_slice>
            </slice>
            <slice>
              <time_slice>43:07</time_slice>
              <text_slice>You don't want to do that.</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>Fortunately, it's enough to
have a table with the</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>numerical values only for
the standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>And once you have those, you can
use them in a clever way</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>to calculate probabilities for
the more general case.</text_slice>
            </slice>
            <slice>
              <time_slice>43:24</time_slice>
              <text_slice>So let's see how this is done.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>So our starting point is that
someone has graciously</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>calculated for us the values
of the CDF, the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>43:36</time_slice>
              <text_slice>distribution function, that is
the probability of falling</text_slice>
            </slice>
            <slice>
              <time_slice>43:40</time_slice>
              <text_slice>below a certain point for
the standard normal</text_slice>
            </slice>
            <slice>
              <time_slice>43:44</time_slice>
              <text_slice>and at various places.</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>How do we read this table?</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>The probability that X is
less than, let's say,</text_slice>
            </slice>
            <slice>
              <time_slice>43:55</time_slice>
              <text_slice>0.63 is this number.</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>This number, 0.7357, is the
probability that the standard</text_slice>
            </slice>
            <slice>
              <time_slice>44:04</time_slice>
              <text_slice>normal is below 0.63.</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>So the table refers to
the standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>But someone, let's say, gives
us some other numbers and</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>tells us we're dealing with a
normal with a certain mean and</text_slice>
            </slice>
            <slice>
              <time_slice>44:22</time_slice>
              <text_slice>a certain variance.</text_slice>
            </slice>
            <slice>
              <time_slice>44:23</time_slice>
              <text_slice>And we want to calculate the
probability that the value of</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>that random variable is less
than or equal to 3.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>How are we going to do it?</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>Well, there's a standard trick,
which is so-called</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>standardizing a random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>Standardizing a random variable</text_slice>
            </slice>
            <slice>
              <time_slice>44:41</time_slice>
              <text_slice>stands for the following.</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>You look at the random
variable, and you</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>subtract the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>This makes it a random
variable with 0 mean.</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>And then if I divide by the
standard deviation, what</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>happens to the variance of
this random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>44:58</time_slice>
              <text_slice>Dividing by a number divides the
variance by sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>45:03</time_slice>
              <text_slice>The original variance of
X was sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>So when I divide by sigma, I
end up with unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>45:11</time_slice>
              <text_slice>So after I do this
transformation, I get a random</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>variable that has 0 mean
and unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>It is also normal.</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>Why is its normal?</text_slice>
            </slice>
            <slice>
              <time_slice>45:23</time_slice>
              <text_slice>Because this expression is a
linear function of the X that</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>I started with.</text_slice>
            </slice>
            <slice>
              <time_slice>45:30</time_slice>
              <text_slice>It's a linear function of a
normal random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>Therefore, it is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>And it is a standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>So by taking a general normal
random variable and doing this</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>standardization, you end up
with a standard normal to</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>which you can then
apply the table.</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>Sometimes one calls this
the normalized score.</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>If you're thinking about test
results, how would you</text_slice>
            </slice>
            <slice>
              <time_slice>45:59</time_slice>
              <text_slice>interpret this number?</text_slice>
            </slice>
            <slice>
              <time_slice>46:00</time_slice>
              <text_slice>It tells you how many standard
deviations are you</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>away from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>This is how much you are
away from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>And you count it in terms
of how many standard</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>deviations it is.</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>So this number being equal to 3
tells you that X happens to</text_slice>
            </slice>
            <slice>
              <time_slice>46:19</time_slice>
              <text_slice>be 3 standard deviations
above the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:23</time_slice>
              <text_slice>And I guess if you're looking
at your quiz scores, very</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>often that's the kind of number
that you think about.</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>So it's a useful quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>46:32</time_slice>
              <text_slice>But it's also useful for doing
the calculation we're now</text_slice>
            </slice>
            <slice>
              <time_slice>46:35</time_slice>
              <text_slice>going to do.</text_slice>
            </slice>
            <slice>
              <time_slice>46:36</time_slice>
              <text_slice>So suppose that X has a mean of
2 and a variance of 16, so</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>a standard deviation of 4.</text_slice>
            </slice>
            <slice>
              <time_slice>46:43</time_slice>
              <text_slice>And we're going to calculate the
probability of this event.</text_slice>
            </slice>
            <slice>
              <time_slice>46:46</time_slice>
              <text_slice>This event is described in terms
of this X that has ugly</text_slice>
            </slice>
            <slice>
              <time_slice>46:49</time_slice>
              <text_slice>means and variances.</text_slice>
            </slice>
            <slice>
              <time_slice>46:51</time_slice>
              <text_slice>But we can take this event
and rewrite it as</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>an equivalent event.</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>X less than 3 is this same as
X minus 2 being less than 3</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>minus 2, which is the same as
this ratio being less than</text_slice>
            </slice>
            <slice>
              <time_slice>47:06</time_slice>
              <text_slice>that ratio.</text_slice>
            </slice>
            <slice>
              <time_slice>47:08</time_slice>
              <text_slice>So I'm subtracting from both
sides of the inequality the</text_slice>
            </slice>
            <slice>
              <time_slice>47:11</time_slice>
              <text_slice>mean and then dividing by
the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:14</time_slice>
              <text_slice>This event is the same
as that event.</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>Why do we like this
better than that?</text_slice>
            </slice>
            <slice>
              <time_slice>47:19</time_slice>
              <text_slice>We like it because this is the
standardized, or normalized,</text_slice>
            </slice>
            <slice>
              <time_slice>47:23</time_slice>
              <text_slice>version of X. We know that
this is standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>47:28</time_slice>
              <text_slice>And so we're asking the
question, what's the</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>probability that the standard
normal is less than this</text_slice>
            </slice>
            <slice>
              <time_slice>47:34</time_slice>
              <text_slice>number, which is 1/4?</text_slice>
            </slice>
            <slice>
              <time_slice>47:37</time_slice>
              <text_slice>So that's the key property, that
this is normal (0, 1).</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>And so we can look up now with
the table and ask for the</text_slice>
            </slice>
            <slice>
              <time_slice>47:48</time_slice>
              <text_slice>probability that the standard
normal random variable</text_slice>
            </slice>
            <slice>
              <time_slice>47:51</time_slice>
              <text_slice>is less than 0.25.</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>Where is that going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>47:55</time_slice>
              <text_slice>0.2, 0.25, it's here.</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>So the answer is 0.987.</text_slice>
            </slice>
            <slice>
              <time_slice>48:09</time_slice>
              <text_slice>So I guess this is just a drill
that you could learn in</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>high school.</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>You didn't have to come here
to learn about it.</text_slice>
            </slice>
            <slice>
              <time_slice>48:18</time_slice>
              <text_slice>But it's a drill that's very
useful when we will be</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>calculating normal probabilities
all the time.</text_slice>
            </slice>
            <slice>
              <time_slice>48:24</time_slice>
              <text_slice>So make sure you know how to
use the table and how to</text_slice>
            </slice>
            <slice>
              <time_slice>48:27</time_slice>
              <text_slice>massage a general normal
random variable into a</text_slice>
            </slice>
            <slice>
              <time_slice>48:30</time_slice>
              <text_slice>standard normal random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>So just one more minute to look
at the big picture and</text_slice>
            </slice>
            <slice>
              <time_slice>48:37</time_slice>
              <text_slice>take stock of what we
have done so far</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>and where we're going.</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>Chapter 2 was this part of the
picture, where we dealt with</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>48:50</time_slice>
              <text_slice>And this time, today, we
started talking about</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>48:56</time_slice>
              <text_slice>And we introduced the density
function, which is the analog</text_slice>
            </slice>
            <slice>
              <time_slice>49:00</time_slice>
              <text_slice>of the probability
mass function.</text_slice>
            </slice>
            <slice>
              <time_slice>49:03</time_slice>
              <text_slice>We have the concepts
of expectation and</text_slice>
            </slice>
            <slice>
              <time_slice>49:05</time_slice>
              <text_slice>variance and CDF.</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>And this kind of notation
applies to both discrete and</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>continuous cases.</text_slice>
            </slice>
            <slice>
              <time_slice>49:11</time_slice>
              <text_slice>They are calculated the same way
in both cases except that</text_slice>
            </slice>
            <slice>
              <time_slice>49:17</time_slice>
              <text_slice>in the continuous case,
you use sums.</text_slice>
            </slice>
            <slice>
              <time_slice>49:19</time_slice>
              <text_slice>In the discrete case,
you use integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>So on that side, you
have integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>49:25</time_slice>
              <text_slice>In this case, you have sums.</text_slice>
            </slice>
            <slice>
              <time_slice>49:27</time_slice>
              <text_slice>In this case, you always have
Fs in your formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>In this case, you always have
Ps in your formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>49:33</time_slice>
              <text_slice>So what's there that's left
for us to do is to look at</text_slice>
            </slice>
            <slice>
              <time_slice>49:37</time_slice>
              <text_slice>these two concepts, joint
probability mass functions and</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>conditional mass functions, and
figure out what would be</text_slice>
            </slice>
            <slice>
              <time_slice>49:47</time_slice>
              <text_slice>the equivalent concepts on
the continuous side.</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>So we will need some notion of
a joint density when we're</text_slice>
            </slice>
            <slice>
              <time_slice>49:55</time_slice>
              <text_slice>dealing with multiple
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>49:57</time_slice>
              <text_slice>And we will also need the
concept of conditional</text_slice>
            </slice>
            <slice>
              <time_slice>50:00</time_slice>
              <text_slice>density, again for the case of
continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>50:03</time_slice>
              <text_slice>The intuition and the meaning
of these objects is going to</text_slice>
            </slice>
            <slice>
              <time_slice>50:07</time_slice>
              <text_slice>be exactly the same as here,
only a little subtler because</text_slice>
            </slice>
            <slice>
              <time_slice>50:14</time_slice>
              <text_slice>densities are not
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>50:16</time_slice>
              <text_slice>They're rates at which
probabilities accumulate.</text_slice>
            </slice>
            <slice>
              <time_slice>50:18</time_slice>
              <text_slice>So that adds a little bit of
potential confusion here,</text_slice>
            </slice>
            <slice>
              <time_slice>50:22</time_slice>
              <text_slice>which, hopefully, we will fully
resolve in the next</text_slice>
            </slice>
            <slice>
              <time_slice>50:24</time_slice>
              <text_slice>couple of sections.</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>50:27</time_slice>
              <text_slice>Thank you.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Central Limit Theorem (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Apply to binomial The 1/2 correction for binomial
approximation
Fixp, where 0 &lt;p&lt; 1
P(Sn21) = P(Sn&lt;22),
Xi: Bernoulli(p)because Snis integer
Sn=X1++Xn: Binomial(n, p )Compromise: consider P(Sn21.5)
mean np, variance np(1p)
CDF of Snnp
np(1 p)standard normal

Example
18 192021 22
n= 36, p=0.5; nd P(Sn21)
Exact answer:
2136136
=0.8785k2k=0
De MoivreLaplace CLT (for binomial) Poisson vs. normal approximations of
the binomialWhen the 1/2 correction is used, CLT
can also approximate the binomial p.m.f.(not just the binomial CDF)Poisson arrivals during unit interval equals:
sum of n(independent) Poisson arrivals
P(Sn= 19) = P(18.5Sn19.5) during nintervals of length 1/n
Letn , apply CLT (??)18.5Sn19.5 
Poisson=normal (????)18.518 Sn18 19.5 18
333
.17ZBinomial( )nn, p0 0.5
pxed, n : normal
P(Sn= 19) P(0.17Z0.5)npxed, n ,p0: Poisson
=P(Z0.5)P(Z0.17)p=1/100, n= 100: Poisson
=0 .69150.5675 p=1/10, n= 500: normal
=0 .124
Exact answer:
361125119 236
=0.
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 20 Usefulness
THE CENTRAL LIMIT THEOREM universal; only means, variances matter
accurate computational shortcutReadings: Section 5.4
justication of normal models
 ,Xni.i.d., nite variance2X1,... 
Standardized Sn=X1++Xn:What exactly does it say?
SnE[Sn]SnnE[X]Zn== CDF of Znconverges to normal CDF
Snnnot a statement about convergence of
E[Zn]=0 , var(Z )=1 PDFs or PMFsn
LetZbe a standard normal r.v.
(zero mean, unit variance)Normal approximation
Treat Znas if normalTheorem: For every c:
also treat Snas if normal
P(Znc)P(Zc)
P(Zc) is the standard normal CDF,
(c), available from the normal tables
Can we use it when nis moderate?
Yes, but no nice theorems to this eect
Symmetry helps a lot
0.14 0.1
n =4
0.12
0.08
0.1
0.060.08
0.060.04 The pollsters problem using the CLT
0.04
0.02
0.02 f: fraction of population that  ...
0 00 5 10 15 20 0 5 10 15 20 25 30 35
ith (randomly selected) person polled:
0.25 0.035
n =32
0.03
0.2 1
0.025
,if yes,X=
0.15i
0.02 0,if no.
0.0150.1
0.01
0.05
0.005
0Mn=(X1++Xn)/n
00 2 4 6 8 100 120 140 160 180 200Suppose we want:
P(M f .01) .05
0.12 0.0|n
8 |
n = 160.07n = 80.1
0.06 Event of interest: |Mnf|.01
0.08
0.05
0.06 0.04
0.03X1++Xn
0.04nf01
0.02
0.02n.
0.01
0
0
0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 
1 0.06
0.9 n = 32
X1+ Xn+nf .01 n
0.05
0.8
0.7n
0.04
0.6
0.5 0.03
0.4
0.02
0.3
0.2
0.01
0.1P(|Mnf|.01)P(|Z |.01n/)
0 0
0 1 2 3 4 5 6 7 30 40 50 60 70 80 90 100 P(|Z |.02n)n =2
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-9-multiple-continuous-random-variables/</video_url>
          <video_title>Lecture 9: Multiple Continuous Random Variables</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high-quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:23</time_slice>
              <text_slice>JOHN TSITSIKLIS:
OK let's start.</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>So we've had the quiz.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>And I guess there's both good
and bad news in it.</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>Yesterday, as you know,
the bad news.</text_slice>
            </slice>
            <slice>
              <time_slice>0:31</time_slice>
              <text_slice>The average was a little
lower than what</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>we would have wanted.</text_slice>
            </slice>
            <slice>
              <time_slice>0:36</time_slice>
              <text_slice>On the other hand, the good news
is that the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>0:39</time_slice>
              <text_slice>was nicely spread.</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>And that's the main purpose of
this quiz is basically for you</text_slice>
            </slice>
            <slice>
              <time_slice>0:44</time_slice>
              <text_slice>to calibrate and see roughly
where you are standing.</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>The other piece of the good
news is that, as you know,</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>this quiz doesn't count for very
much in your final grade.</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>So it's really a matter of
calibration and to get your</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>mind set appropriately to
prepare for the second quiz,</text_slice>
            </slice>
            <slice>
              <time_slice>1:02</time_slice>
              <text_slice>which counts a lot more.</text_slice>
            </slice>
            <slice>
              <time_slice>1:04</time_slice>
              <text_slice>And it's more substantial.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>And we'll make sure that
the second quiz will</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>have a higher average.</text_slice>
            </slice>
            <slice>
              <time_slice>1:12</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>1:12</time_slice>
              <text_slice>So let's go to our material.</text_slice>
            </slice>
            <slice>
              <time_slice>1:15</time_slice>
              <text_slice>We're talking now
these days about</text_slice>
            </slice>
            <slice>
              <time_slice>1:18</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>And I'll remind you what
we discussed last time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:23</time_slice>
              <text_slice>I'll remind you of the concept
of the probability density</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>function of a single
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>1:28</time_slice>
              <text_slice>And then we're going to rush
through all the concepts that</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>we covered for the case of
discrete random variables and</text_slice>
            </slice>
            <slice>
              <time_slice>1:34</time_slice>
              <text_slice>discuss their analogs for
the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>1:37</time_slice>
              <text_slice>And talk about notions
such as conditioning</text_slice>
            </slice>
            <slice>
              <time_slice>1:40</time_slice>
              <text_slice>independence and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>So the big picture is here.</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>We have all those concepts that
we developed for the case</text_slice>
            </slice>
            <slice>
              <time_slice>1:49</time_slice>
              <text_slice>of discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>1:52</time_slice>
              <text_slice>And now we will just talk about
their analogs in the</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>We already discussed this analog
last week, the density</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>of a single random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:04</time_slice>
              <text_slice>Then there are certain concepts
that show up both in</text_slice>
            </slice>
            <slice>
              <time_slice>2:08</time_slice>
              <text_slice>the discrete and the
continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>So we have the cumulative
distribution function, which</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>is a description of the
probability distribution of a</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>random variable and which
applies whether you have a</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>discrete or continuous
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:23</time_slice>
              <text_slice>Then there's the notion
of the expected value.</text_slice>
            </slice>
            <slice>
              <time_slice>2:26</time_slice>
              <text_slice>And in the two cases, the
expected value is calculated</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>in a slightly different way,
but not very different.</text_slice>
            </slice>
            <slice>
              <time_slice>2:32</time_slice>
              <text_slice>We have sums in one case,
integrals in the other.</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>And this is the general
pattern that</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>we're going to have.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>Formulas for the discrete case
translate to corresponding</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>formulas or expressions in
the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>We generically replace sums by
integrals, and we replace must</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>functions with density
functions.</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>Then the new pieces for today
are going to be mostly the</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>notion of a joint density
function, which is how we</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>describe the probability
distribution of two random</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>variables that are somehow
related, in general, and then</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>the notion of a conditional
density function that tells us</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>the distribution of one random
variable X when you're told</text_slice>
            </slice>
            <slice>
              <time_slice>3:15</time_slice>
              <text_slice>the value of another random
variable Y. There's another</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>concept, which is the
conditional PDF given that the</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>certain event has happened.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>This is a concept that's
in some ways simpler.</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>You've already seen a little
bit of that in last week's</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>recitation and tutorial.</text_slice>
            </slice>
            <slice>
              <time_slice>3:33</time_slice>
              <text_slice>The idea is that we have a
single random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>3:35</time_slice>
              <text_slice>It's described by a density.</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>Then you're told that the
certain event has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>Your model changes
the universe that</text_slice>
            </slice>
            <slice>
              <time_slice>3:42</time_slice>
              <text_slice>you are dealing with.</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>In the new universe, you are
dealing with a new density</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>function, the one that applies
given the knowledge that we</text_slice>
            </slice>
            <slice>
              <time_slice>3:51</time_slice>
              <text_slice>have that the certain
event has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>3:56</time_slice>
              <text_slice>So what exactly did
we say about</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>continuous random variables?</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>The first thing is the
definition, that a random</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>variable is said to be
continuous if we are given a</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>certain object that we call
the probability density</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>function and we can calculate
interval probabilities given</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>this density function.</text_slice>
            </slice>
            <slice>
              <time_slice>4:18</time_slice>
              <text_slice>So the definition is that the
random variable is continuous</text_slice>
            </slice>
            <slice>
              <time_slice>4:21</time_slice>
              <text_slice>if you can calculate
probabilities associated with</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>that random variable
given that formula.</text_slice>
            </slice>
            <slice>
              <time_slice>4:27</time_slice>
              <text_slice>So this formula tells you that
the probability that your</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>random variable falls inside
this interval is the area</text_slice>
            </slice>
            <slice>
              <time_slice>4:33</time_slice>
              <text_slice>under the density curve.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>There's a few properties
that a density</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>function must satisfy.</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>Since we're talking about
probabilities, and</text_slice>
            </slice>
            <slice>
              <time_slice>4:42</time_slice>
              <text_slice>probabilities are non-negative,
we have that the</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>density function is always
a non-negative function.</text_slice>
            </slice>
            <slice>
              <time_slice>4:49</time_slice>
              <text_slice>The total probability over
the entire real line</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>must be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>So the integral when you
integrate over the entire real</text_slice>
            </slice>
            <slice>
              <time_slice>4:58</time_slice>
              <text_slice>line has to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>4:59</time_slice>
              <text_slice>That's the second property.</text_slice>
            </slice>
            <slice>
              <time_slice>5:01</time_slice>
              <text_slice>Another property that you get is
that if you let a equal to</text_slice>
            </slice>
            <slice>
              <time_slice>5:05</time_slice>
              <text_slice>b, this integral becomes 0.</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>And that tells you that the
probability of a single point</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>in the continuous case
is always equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>So these are formal
properties.</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>When you want to think
intuitively, the best way to</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>think about what the density
function is to think in terms</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>of little intervals, the
probability that my random</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>variable falls inside
the little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>Well, inside that little
interval, the density function</text_slice>
            </slice>
            <slice>
              <time_slice>5:35</time_slice>
              <text_slice>here is roughly constant.</text_slice>
            </slice>
            <slice>
              <time_slice>5:36</time_slice>
              <text_slice>So that integral becomes the
value of the density times the</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>length of the interval over
which you are integrating,</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>which is delta.</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>And so the density function
basically gives us</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>probabilities of little events,
of small events.</text_slice>
            </slice>
            <slice>
              <time_slice>5:54</time_slice>
              <text_slice>And the density is to be
interpreted as probability per</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>unit length at a certain
place in the diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>6:02</time_slice>
              <text_slice>So in that place in the diagram,
the probability per</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>unit length around this
neighborhood would be the</text_slice>
            </slice>
            <slice>
              <time_slice>6:07</time_slice>
              <text_slice>height of the density function
at that point.</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>What else?</text_slice>
            </slice>
            <slice>
              <time_slice>6:13</time_slice>
              <text_slice>We have a formula for
calculating expected values of</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>functions of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>6:17</time_slice>
              <text_slice>In the discrete case, we had the
formula where here we had</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>the sum, and instead of the
density, we had the PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>6:25</time_slice>
              <text_slice>The same formula is also valid
in the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>6:29</time_slice>
              <text_slice>And it's not too hard to derive,
but we will not do it.</text_slice>
            </slice>
            <slice>
              <time_slice>6:35</time_slice>
              <text_slice>But let's think of the
intuition of what</text_slice>
            </slice>
            <slice>
              <time_slice>6:36</time_slice>
              <text_slice>this formula says.</text_slice>
            </slice>
            <slice>
              <time_slice>6:38</time_slice>
              <text_slice>You're trying to figure out on
the average how much g(X) is</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>6:42</time_slice>
              <text_slice>And then you reason, and you
say, well, X may turn out to</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>take a particular value or a
small interval of values.</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>This is the probability
that X falls</text_slice>
            </slice>
            <slice>
              <time_slice>6:54</time_slice>
              <text_slice>inside the small interval.</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>And when that happens, g(X)
takes that value.</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>So this fraction of the time,
you fall in the little</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>neighborhood of x, and
you get so much.</text_slice>
            </slice>
            <slice>
              <time_slice>7:07</time_slice>
              <text_slice>Then you average over all the
possible x's that can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>And that gives you the average
value of the function g(X).</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>So this is the easy stuff.</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>Now let's get to the
new material.</text_slice>
            </slice>
            <slice>
              <time_slice>7:23</time_slice>
              <text_slice>We want to talk about multiple
random variables</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>So we want to talk now about two
random variables that are</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>continuous, and in some sense
that they are jointly</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>And let's see what this means.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>The definition is similar to
the definition we had for a</text_slice>
            </slice>
            <slice>
              <time_slice>7:40</time_slice>
              <text_slice>single random variable, where
I take this formula here as</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>the definition of continuous
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>Two random variables are said to
be jointly continuous if we</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>can calculate probabilities by
integrating a certain function</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>that we call the joint
density function</text_slice>
            </slice>
            <slice>
              <time_slice>8:01</time_slice>
              <text_slice>over the set of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>So we have our two-dimensional
plane.</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>This is the x-y plane.</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>There's a certain event S that
we're interested in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>We want to calculate
the probability.</text_slice>
            </slice>
            <slice>
              <time_slice>8:15</time_slice>
              <text_slice>How do we do that?</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>We are given this function
f_(X,Y), the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>It's a function of the two
arguments x and y.</text_slice>
            </slice>
            <slice>
              <time_slice>8:25</time_slice>
              <text_slice>So think of that function as
being some kind of surface</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>that sits on top of the
two-dimensional plane.</text_slice>
            </slice>
            <slice>
              <time_slice>8:34</time_slice>
              <text_slice>The probability of falling
inside the set S, we calculate</text_slice>
            </slice>
            <slice>
              <time_slice>8:39</time_slice>
              <text_slice>it by looking at the volume
under the surface, that volume</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>that sits on top of S. So the
surface underneath it has a</text_slice>
            </slice>
            <slice>
              <time_slice>8:50</time_slice>
              <text_slice>certain total volume.</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>What should that total
volume be?</text_slice>
            </slice>
            <slice>
              <time_slice>8:54</time_slice>
              <text_slice>Well, we think of these volumes
as probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>So the total probability
should be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:00</time_slice>
              <text_slice>The total volume under this
surface, should be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:05</time_slice>
              <text_slice>So that's one property
that we want our</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>density function to have.</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>So when you integrate over the
entire space, this is of the</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>volume under your surface.</text_slice>
            </slice>
            <slice>
              <time_slice>9:22</time_slice>
              <text_slice>That should be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:24</time_slice>
              <text_slice>Of course, since we're talking
about probabilities, the joint</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>density should be a non-negative
function.</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>So think of the situation
as having one pound of</text_slice>
            </slice>
            <slice>
              <time_slice>9:34</time_slice>
              <text_slice>probability that's spread
all over your space.</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>And the height of this joint
density function basically</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>tells you how much probability
tends to be accumulated in</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>certain regions of space
as opposed to other</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>parts of the space.</text_slice>
            </slice>
            <slice>
              <time_slice>9:49</time_slice>
              <text_slice>So wherever the density is big,
that means that this is</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>an area of the two-dimensional
plane that's</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>more likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>9:56</time_slice>
              <text_slice>Where the density is small, that
means that those x-y's</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>are less likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>You have already seen
one example</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>of continuous densities.</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>That was the example we had in
the very beginning of the</text_slice>
            </slice>
            <slice>
              <time_slice>10:08</time_slice>
              <text_slice>class with a uniform</text_slice>
            </slice>
            <slice>
              <time_slice>10:10</time_slice>
              <text_slice>distribution on the unit square.</text_slice>
            </slice>
            <slice>
              <time_slice>10:13</time_slice>
              <text_slice>That was a special
case of a density</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>function that was constant.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>So all places in the unit square
were roughly equally</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>likely as any other places.</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>But in other models, some parts
of the space may be more</text_slice>
            </slice>
            <slice>
              <time_slice>10:25</time_slice>
              <text_slice>likely than others.</text_slice>
            </slice>
            <slice>
              <time_slice>10:27</time_slice>
              <text_slice>And we describe those relative
likelihoods using</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>this density function.</text_slice>
            </slice>
            <slice>
              <time_slice>10:31</time_slice>
              <text_slice>So if somebody gives us the
density function, this</text_slice>
            </slice>
            <slice>
              <time_slice>10:33</time_slice>
              <text_slice>determines for us probabilities
of all the</text_slice>
            </slice>
            <slice>
              <time_slice>10:38</time_slice>
              <text_slice>subsets of the two-dimensional
plane.</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>Now for an intuitive
interpretation, it's good to</text_slice>
            </slice>
            <slice>
              <time_slice>10:45</time_slice>
              <text_slice>think about small events.</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>So let's take a particular x
here and then x plus delta.</text_slice>
            </slice>
            <slice>
              <time_slice>10:51</time_slice>
              <text_slice>So this is a small interval.</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>Take another small interval
here that goes from y to y</text_slice>
            </slice>
            <slice>
              <time_slice>10:56</time_slice>
              <text_slice>plus delta.</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>And let's look at the event that
x falls here and y falls</text_slice>
            </slice>
            <slice>
              <time_slice>11:03</time_slice>
              <text_slice>right there.</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>What is this event?</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>Well, this is the event
that will fall</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>inside this little rectangle.</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>Using this rule for calculating
probabilities,</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>what is the probability of that
rectangle going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>11:19</time_slice>
              <text_slice>Well, it should be the integral
of the density over</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>this rectangle.</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>Or it's the volume under the
surface that sits on top of</text_slice>
            </slice>
            <slice>
              <time_slice>11:29</time_slice>
              <text_slice>that rectangle.</text_slice>
            </slice>
            <slice>
              <time_slice>11:31</time_slice>
              <text_slice>Now, if the rectangle is very
small, the joint density is</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>not going to change very much
in that neighborhood.</text_slice>
            </slice>
            <slice>
              <time_slice>11:36</time_slice>
              <text_slice>So we can treat it
as a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>So the volume is going to
be the height times</text_slice>
            </slice>
            <slice>
              <time_slice>11:42</time_slice>
              <text_slice>the area of the base.</text_slice>
            </slice>
            <slice>
              <time_slice>11:44</time_slice>
              <text_slice>The height at that point is
whatever the function happens</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>to be around that point.</text_slice>
            </slice>
            <slice>
              <time_slice>11:49</time_slice>
              <text_slice>And the area of the base
is delta squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>So this is the intuitive way
to understand what a joint</text_slice>
            </slice>
            <slice>
              <time_slice>11:58</time_slice>
              <text_slice>density function really
tells you.</text_slice>
            </slice>
            <slice>
              <time_slice>12:01</time_slice>
              <text_slice>It specifies for you
probabilities of little</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>squares, of little rectangles.</text_slice>
            </slice>
            <slice>
              <time_slice>12:08</time_slice>
              <text_slice>And it allows you to think of
the joint density function as</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>probability per unit area.</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>So these are the units of the
density, its probability per</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>unit area in the neighborhood
of a certain point.</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>So what do we do with this
density function once we have</text_slice>
            </slice>
            <slice>
              <time_slice>12:26</time_slice>
              <text_slice>it in our hands?</text_slice>
            </slice>
            <slice>
              <time_slice>12:28</time_slice>
              <text_slice>Well, we can use it to calculate
expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>Suppose that you have a
function of two random</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>variables described by
a joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>12:38</time_slice>
              <text_slice>You can find, perhaps, the
distribution of this random</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>variable and then use the
basic definition of the</text_slice>
            </slice>
            <slice>
              <time_slice>12:45</time_slice>
              <text_slice>expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>Or you can calculate
expectations directly, using</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>the distribution of the original
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>This is a formula that's again
identical to the formula that</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>we had for the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>In the discrete case,
we had a double sum</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>here, and we had PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>So the intuition behind this
formula is the same that one</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>had for the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>13:08</time_slice>
              <text_slice>It's just that the mechanics
are different.</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>Then something that we did in
the discrete case was to find</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>a way to go from the joint
density of the two random</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>variables taken together to the
density of just one of the</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>So we had a formula for
the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>Let's see how things are
going to work out in</text_slice>
            </slice>
            <slice>
              <time_slice>13:33</time_slice>
              <text_slice>the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>So in the continuous
case, we have here</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>our two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>And we have a density
for them.</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>And let's say that we want to
calculate the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>x falls inside this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>So we're looking at the
probability that our random</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>variable X falls in the interval
from little x to x</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>plus delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>Now, by the properties that we
already have for interpreting</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>the density function of a single
random variable, the</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>probability of a little interval
is approximately the</text_slice>
            </slice>
            <slice>
              <time_slice>14:14</time_slice>
              <text_slice>density of that single random
variable times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:18</time_slice>
              <text_slice>And now we want to find a
formula for this marginal</text_slice>
            </slice>
            <slice>
              <time_slice>14:22</time_slice>
              <text_slice>density in terms of
the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>14:26</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>14:26</time_slice>
              <text_slice>So this is the probability
that x</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>falls inside this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>14:30</time_slice>
              <text_slice>In terms of the two-dimensional
plane, this is</text_slice>
            </slice>
            <slice>
              <time_slice>14:34</time_slice>
              <text_slice>the probability that (x,y)
falls inside this strip.</text_slice>
            </slice>
            <slice>
              <time_slice>14:40</time_slice>
              <text_slice>So to find that probability,
we need to calculate the</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>probability that (x,y) falls in
here, which is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>the double integral over the
interval over this strip, of</text_slice>
            </slice>
            <slice>
              <time_slice>14:55</time_slice>
              <text_slice>the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>15:05</time_slice>
              <text_slice>And what are we integrating
over?</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>y goes from minus infinity
to plus infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>And the dummy variable x goes
from little x to x plus delta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>So to integrate over this strip,
what we do is for any</text_slice>
            </slice>
            <slice>
              <time_slice>15:31</time_slice>
              <text_slice>given y, we integrate
in this dimension.</text_slice>
            </slice>
            <slice>
              <time_slice>15:34</time_slice>
              <text_slice>This is the x integral.</text_slice>
            </slice>
            <slice>
              <time_slice>15:36</time_slice>
              <text_slice>And then we integrate over
the y dimension.</text_slice>
            </slice>
            <slice>
              <time_slice>15:40</time_slice>
              <text_slice>Now what is this
inner integral?</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>Because x only varies very
little, this is approximately</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>constant in that range.</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>So the integral with
respect to x just</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>becomes delta times f(x,y).</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>And then we've got our dy.</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>So this is what the inner
integral will evaluate to.</text_slice>
            </slice>
            <slice>
              <time_slice>16:11</time_slice>
              <text_slice>We are integrating over
the little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>So we're keeping y fixed.</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>Integrating over here, we take
the value of the density times</text_slice>
            </slice>
            <slice>
              <time_slice>16:22</time_slice>
              <text_slice>how much we're integrating
over.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>And we get this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>16:27</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>16:28</time_slice>
              <text_slice>Now, this expression must be
equal to that expression.</text_slice>
            </slice>
            <slice>
              <time_slice>16:33</time_slice>
              <text_slice>So if we cancel the deltas, we
see that the marginal density</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>must be equal to the integral of
the joint density, where we</text_slice>
            </slice>
            <slice>
              <time_slice>16:44</time_slice>
              <text_slice>have integrated out
the value of y.</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>So this formula should come as
no surprise at this point.</text_slice>
            </slice>
            <slice>
              <time_slice>16:59</time_slice>
              <text_slice>It's exactly the same as the
formula that we had for</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>But now we are replacing the
sum with an integral.</text_slice>
            </slice>
            <slice>
              <time_slice>17:06</time_slice>
              <text_slice>And instead of using the
joint PMF, we are</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>using the joint PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>Then, continuing going down the
list of things we did for</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>discrete random variables, we
can now introduce a definition</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>of the notion of independence
of two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>And by analogy with the discrete
case, we define</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>independence to be the
following condition.</text_slice>
            </slice>
            <slice>
              <time_slice>17:33</time_slice>
              <text_slice>Two random variables are
independent if and only if</text_slice>
            </slice>
            <slice>
              <time_slice>17:37</time_slice>
              <text_slice>their joint density function
factors out as a product of</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>their marginal densities.</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>And this property needs to
be true for all x and y.</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>So this is the formal
definition.</text_slice>
            </slice>
            <slice>
              <time_slice>17:49</time_slice>
              <text_slice>Operationally and intuitively,
what does it mean?</text_slice>
            </slice>
            <slice>
              <time_slice>17:53</time_slice>
              <text_slice>Well, intuitively it means
the same thing as in</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>Knowing anything about X
shouldn't tell you anything</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>about Y. That is, information
about X is not going to change</text_slice>
            </slice>
            <slice>
              <time_slice>18:05</time_slice>
              <text_slice>your beliefs about Y. We are
going to come back to this</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>statement in a second.</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>The other thing that it
allows you to do--</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>I'm not going to derive this--
is it allows you to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>probabilities by multiplying
individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>So if you ask for the
probability that x falls in a</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>certain set A and y falls in a
certain set B, then you can</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>calculate that probability
by multiplying individual</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>18:41</time_slice>
              <text_slice>This takes just two lines of
derivation, which I'm not</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>going to do.</text_slice>
            </slice>
            <slice>
              <time_slice>18:47</time_slice>
              <text_slice>But it comes back to
the usual notion of</text_slice>
            </slice>
            <slice>
              <time_slice>18:51</time_slice>
              <text_slice>independence of events.</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>Basically, operationally
independence means that you</text_slice>
            </slice>
            <slice>
              <time_slice>18:56</time_slice>
              <text_slice>can multiply probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>So now let's look
at an example.</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>There's a sort of pretty famous
and classical one.</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>It goes back a lot more
than a 100 years.</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>And it's the famous
Needle of Buffon.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>Buffon was a French naturalist
who, for some reason, also</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>decided to play with
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>And look at the following
problem.</text_slice>
            </slice>
            <slice>
              <time_slice>19:24</time_slice>
              <text_slice>So you have the two-dimensional
plane.</text_slice>
            </slice>
            <slice>
              <time_slice>19:28</time_slice>
              <text_slice>And on the plane we draw a
bunch of parallel lines.</text_slice>
            </slice>
            <slice>
              <time_slice>19:33</time_slice>
              <text_slice>And those parallel lines are
separated by a length.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>And the lines are apart
at distance d.</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>And we throw a needle at random,
completely at random.</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>And we'll have to give a meaning
to what "completely at</text_slice>
            </slice>
            <slice>
              <time_slice>20:01</time_slice>
              <text_slice>random" means.</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>And when we throw a needle,
there's two possibilities.</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>Either the needle is going to
fall in a way that does not</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>intersect any of the lines, or
it's going to fall in a way</text_slice>
            </slice>
            <slice>
              <time_slice>20:13</time_slice>
              <text_slice>that it intersects
one of the lines.</text_slice>
            </slice>
            <slice>
              <time_slice>20:15</time_slice>
              <text_slice>We're taking the needle to be
shorter than this distance, so</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>the needle cannot intersect
two lines simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>20:22</time_slice>
              <text_slice>It either intersects 0, or it
intersects one of the lines.</text_slice>
            </slice>
            <slice>
              <time_slice>20:26</time_slice>
              <text_slice>The question is to find the
probability that the needle is</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>going to intersect a line.</text_slice>
            </slice>
            <slice>
              <time_slice>20:32</time_slice>
              <text_slice>What's the probability
of this?</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>20:35</time_slice>
              <text_slice>We are going to approach this
problem by using our standard</text_slice>
            </slice>
            <slice>
              <time_slice>20:40</time_slice>
              <text_slice>four-step procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>20:42</time_slice>
              <text_slice>Set up your sample space,
describe a probability law on</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>that sample space, identify
the event of interest, and</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>then calculate.</text_slice>
            </slice>
            <slice>
              <time_slice>20:53</time_slice>
              <text_slice>These four steps basically
correspond to these three</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>bullets and then the last
equation down here.</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>So first thing is to set
up a sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>21:06</time_slice>
              <text_slice>We need some variables to
describe what happened in the</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>21:10</time_slice>
              <text_slice>So what happens in the
experiment is that the needle</text_slice>
            </slice>
            <slice>
              <time_slice>21:14</time_slice>
              <text_slice>lands somewhere.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>And where it lands, we can
describe this by specifying</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>the location of the center
of the needle.</text_slice>
            </slice>
            <slice>
              <time_slice>21:24</time_slice>
              <text_slice>And what do we mean by the
location of the center?</text_slice>
            </slice>
            <slice>
              <time_slice>21:27</time_slice>
              <text_slice>Well, we can take as our
variable to be the distance</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>from the center of the needle
to the nearest line.</text_slice>
            </slice>
            <slice>
              <time_slice>21:36</time_slice>
              <text_slice>So it tells us the vertical
distance of the center of the</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>needle from the nearest line.</text_slice>
            </slice>
            <slice>
              <time_slice>21:45</time_slice>
              <text_slice>The other thing that
matters is the</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>orientation of the needle.</text_slice>
            </slice>
            <slice>
              <time_slice>21:49</time_slice>
              <text_slice>So we need one more variable,
which we take to be the angle</text_slice>
            </slice>
            <slice>
              <time_slice>21:53</time_slice>
              <text_slice>that the needle is forming
with the lines.</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>We can put the angle here,
or you can put in there.</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>Yes, it's still the
same angle.</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>So we have these two variables
that described what happened</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>in the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>And we can take our sample space
to be the set of all</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>possible x's and theta's.</text_slice>
            </slice>
            <slice>
              <time_slice>22:14</time_slice>
              <text_slice>What are the possible x's?</text_slice>
            </slice>
            <slice>
              <time_slice>22:16</time_slice>
              <text_slice>The lines are d apart, so the
nearest line is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>22:20</time_slice>
              <text_slice>anywhere between
0 and d/2 away.</text_slice>
            </slice>
            <slice>
              <time_slice>22:24</time_slice>
              <text_slice>So that tells us what the
possible x's will be.</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>As for theta, it really
depends how</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>you define your angle.</text_slice>
            </slice>
            <slice>
              <time_slice>22:33</time_slice>
              <text_slice>We are going to define our theta
to be the acute angle</text_slice>
            </slice>
            <slice>
              <time_slice>22:37</time_slice>
              <text_slice>that's formed between the needle
and a line, if you were</text_slice>
            </slice>
            <slice>
              <time_slice>22:44</time_slice>
              <text_slice>to extend it.</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>So theta is going to be
something between 0 and pi/2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:50</time_slice>
              <text_slice>So I guess these red pieces
really correspond to the part</text_slice>
            </slice>
            <slice>
              <time_slice>22:54</time_slice>
              <text_slice>of setting up the
sample space.</text_slice>
            </slice>
            <slice>
              <time_slice>22:58</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>22:58</time_slice>
              <text_slice>So that's part one.</text_slice>
            </slice>
            <slice>
              <time_slice>23:00</time_slice>
              <text_slice>Second part is we
need a model.</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>Let's take our model to be that
we basically know nothing</text_slice>
            </slice>
            <slice>
              <time_slice>23:08</time_slice>
              <text_slice>about how the needle falls.</text_slice>
            </slice>
            <slice>
              <time_slice>23:10</time_slice>
              <text_slice>It can fall in any possible way,
and all possible ways are</text_slice>
            </slice>
            <slice>
              <time_slice>23:13</time_slice>
              <text_slice>equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>Now, if you have those parallel
lines, and you close</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>your eyes completely and throw a
needle completely at random,</text_slice>
            </slice>
            <slice>
              <time_slice>23:22</time_slice>
              <text_slice>any x should be equally
likely.</text_slice>
            </slice>
            <slice>
              <time_slice>23:25</time_slice>
              <text_slice>So we describe that situation by
saying that X should have a</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>uniform distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>That is, it should have a
constant density over the</text_slice>
            </slice>
            <slice>
              <time_slice>23:33</time_slice>
              <text_slice>range of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>Similarly, if you kind of spin
your needle completely at</text_slice>
            </slice>
            <slice>
              <time_slice>23:39</time_slice>
              <text_slice>random, any angle should be as
likely as any other angle.</text_slice>
            </slice>
            <slice>
              <time_slice>23:43</time_slice>
              <text_slice>And we decide to model this
situation by saying that theta</text_slice>
            </slice>
            <slice>
              <time_slice>23:47</time_slice>
              <text_slice>also has a uniform
distribution over</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>the range of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>23:54</time_slice>
              <text_slice>And finally, where we put it
should have nothing to do with</text_slice>
            </slice>
            <slice>
              <time_slice>23:58</time_slice>
              <text_slice>how much we rotate it.</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>And we capture this
mathematically by saying that</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>X is going to be independent
of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>Now, this is going
to be our model.</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>I'm not deriving the model
from anything.</text_slice>
            </slice>
            <slice>
              <time_slice>24:11</time_slice>
              <text_slice>I'm only saying that this sounds
like a model that does</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>not assume any knowledge or
preference for certain values</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>of x rather than other
values of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>In the absence of any other
particular information you</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>might have in your hands, that's
the most reasonable</text_slice>
            </slice>
            <slice>
              <time_slice>24:28</time_slice>
              <text_slice>model to come up with.</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>So you model the problem
that way.</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>So what's the formula for
the joint density?</text_slice>
            </slice>
            <slice>
              <time_slice>24:35</time_slice>
              <text_slice>It's going to be the
product of the</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>densities of X and Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>Why is it the product?</text_slice>
            </slice>
            <slice>
              <time_slice>24:42</time_slice>
              <text_slice>This is because we assumed
independence.</text_slice>
            </slice>
            <slice>
              <time_slice>24:45</time_slice>
              <text_slice>And the density of X, since
it's uniform, and since it</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>needs to integrate to 1, that
density needs to be 2/d.</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>That's the density of X.
And the density of</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>Theta needs to be 2/pi.</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>That's the value for the density
of Theta so that the</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>overall probability over this
interval ends up being 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>So now we do have our joint
density in our hands.</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>The next thing to do
is to identify</text_slice>
            </slice>
            <slice>
              <time_slice>25:14</time_slice>
              <text_slice>the event of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>And this is best done
in a picture.</text_slice>
            </slice>
            <slice>
              <time_slice>25:20</time_slice>
              <text_slice>And there's two possible
situations</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>that one could have.</text_slice>
            </slice>
            <slice>
              <time_slice>25:25</time_slice>
              <text_slice>Either the needle falls this
way, or it falls this way.</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>So how can we tell if one or the
other is going to happen?</text_slice>
            </slice>
            <slice>
              <time_slice>25:38</time_slice>
              <text_slice>It has to do with whether this
interval here is smaller than</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>that or bigger than that.</text_slice>
            </slice>
            <slice>
              <time_slice>25:50</time_slice>
              <text_slice>So we are comparing
the height of this</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>interval to that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>This interval here
is capital X.</text_slice>
            </slice>
            <slice>
              <time_slice>25:58</time_slice>
              <text_slice>This interval here,
what is it?</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>This is half of the length of
the needle, which is l/2.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>To find this height, we take l/2
and multiply it with the</text_slice>
            </slice>
            <slice>
              <time_slice>26:10</time_slice>
              <text_slice>sine of the angle
that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>26:13</time_slice>
              <text_slice>So the length of this
interval up here is</text_slice>
            </slice>
            <slice>
              <time_slice>26:18</time_slice>
              <text_slice>l/2 times sine theta.</text_slice>
            </slice>
            <slice>
              <time_slice>26:23</time_slice>
              <text_slice>If this is smaller than
x, the needle does not</text_slice>
            </slice>
            <slice>
              <time_slice>26:28</time_slice>
              <text_slice>intersect the line.</text_slice>
            </slice>
            <slice>
              <time_slice>26:30</time_slice>
              <text_slice>If this is bigger than
x, then the needle</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>intersects the line.</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>So the event of interest, that
the needle intersects the</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>line, is described this way
in terms of x and theta.</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>And now that we have the event
of interest described</text_slice>
            </slice>
            <slice>
              <time_slice>26:46</time_slice>
              <text_slice>mathematically, all that we
need to do is to find the</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>probability of this event, we
integrate the joint density</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>over the part of (x, theta)
space in which this</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>inequality is true.</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>So it's a double integral over
the set of all x's and theta's</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>where this is true.</text_slice>
            </slice>
            <slice>
              <time_slice>27:06</time_slice>
              <text_slice>The way to do this integral is
we fix theta, and we integrate</text_slice>
            </slice>
            <slice>
              <time_slice>27:11</time_slice>
              <text_slice>for x's that go from 0
up to that number.</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>And theta can be anything
between 0 and pi/2.</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>So the integral over this set
is basically this double</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>integral here.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>We already have a formula
for the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>It's 4 over pi d, so
we put it here.</text_slice>
            </slice>
            <slice>
              <time_slice>27:30</time_slice>
              <text_slice>And now, fortunately,
this is a pretty</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>easy integral to evaluate.</text_slice>
            </slice>
            <slice>
              <time_slice>27:34</time_slice>
              <text_slice>The integral with respect to x
-- there's nothing in here.</text_slice>
            </slice>
            <slice>
              <time_slice>27:37</time_slice>
              <text_slice>So the integral is just the
length of the interval over</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>which we're integrating.</text_slice>
            </slice>
            <slice>
              <time_slice>27:42</time_slice>
              <text_slice>It's l/2 sine theta.</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>And then we need to integrate
this with respect to theta.</text_slice>
            </slice>
            <slice>
              <time_slice>27:47</time_slice>
              <text_slice>We know that the integral of a
sine is a negative cosine.</text_slice>
            </slice>
            <slice>
              <time_slice>27:53</time_slice>
              <text_slice>You plug in the values for
the negative cosine</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>at the two end points.</text_slice>
            </slice>
            <slice>
              <time_slice>27:58</time_slice>
              <text_slice>I'm sure you can do
this integral .</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>And we finally obtain the
answer, which is amazingly</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>simple for such a pretty
complicated-looking problem.</text_slice>
            </slice>
            <slice>
              <time_slice>28:08</time_slice>
              <text_slice>It's 2l over pi d.</text_slice>
            </slice>
            <slice>
              <time_slice>28:12</time_slice>
              <text_slice>So some people a long, long time
ago, after they looked at</text_slice>
            </slice>
            <slice>
              <time_slice>28:15</time_slice>
              <text_slice>this answer, they said that
maybe that gives us an</text_slice>
            </slice>
            <slice>
              <time_slice>28:19</time_slice>
              <text_slice>interesting way where one could
estimate the value by</text_slice>
            </slice>
            <slice>
              <time_slice>28:22</time_slice>
              <text_slice>pi, for example,
experimentally.</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>How do you do that?</text_slice>
            </slice>
            <slice>
              <time_slice>28:27</time_slice>
              <text_slice>Fix l and d, the dimensions
of the problem.</text_slice>
            </slice>
            <slice>
              <time_slice>28:32</time_slice>
              <text_slice>Throw a million needles on
your piece of paper.</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>See how often your needless
do intersect the line.</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>That gives you a number
for this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>You know l and d, so you can
use that to infer pi.</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>And there's an apocryphal story
about a wounded soldier</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>in a hospital after the
American Civil War who</text_slice>
            </slice>
            <slice>
              <time_slice>28:55</time_slice>
              <text_slice>actually had heard about this
and was spending his time in</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>the hospital throwing needles
on pieces of paper.</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>I don't know if it's
true or not.</text_slice>
            </slice>
            <slice>
              <time_slice>29:04</time_slice>
              <text_slice>But let's do something
similar here.</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>So let's look at this diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>We fix the dimensions.</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>This is supposed to
be our little d.</text_slice>
            </slice>
            <slice>
              <time_slice>29:15</time_slice>
              <text_slice>That's supposed to
be our little l.</text_slice>
            </slice>
            <slice>
              <time_slice>29:18</time_slice>
              <text_slice>We have the formula from the
previous slide that p</text_slice>
            </slice>
            <slice>
              <time_slice>29:22</time_slice>
              <text_slice>is 2l over pi d.</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>In this instance, we choose
d to be twice l.</text_slice>
            </slice>
            <slice>
              <time_slice>29:29</time_slice>
              <text_slice>So this number is 1/pi.</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>So the probability that the
needle hits the line is 1/pi.</text_slice>
            </slice>
            <slice>
              <time_slice>29:37</time_slice>
              <text_slice>So I need needles that are
3.1 centimeters long.</text_slice>
            </slice>
            <slice>
              <time_slice>29:41</time_slice>
              <text_slice>I couldn't find such needles.</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>But I could find paper clips
that are 3.1 centimeters long.</text_slice>
            </slice>
            <slice>
              <time_slice>29:47</time_slice>
              <text_slice>So let's start throwing paper
clips at random and see how</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>many of them will end up
intersecting the lines.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>Good.</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>So out of eight paper clips,
we have exactly four that</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>intersected the line.</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>So our estimate for the
probability of intersecting</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>the line is 1/2, which gives us
an estimate for the value</text_slice>
            </slice>
            <slice>
              <time_slice>30:18</time_slice>
              <text_slice>of pi, which is two.</text_slice>
            </slice>
            <slice>
              <time_slice>30:22</time_slice>
              <text_slice>Well, I mean, within an
engineering approximation,</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>we're in the right
ballpark, right?</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>So this might look like a
silly way of trying to</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>estimate pi.</text_slice>
            </slice>
            <slice>
              <time_slice>30:33</time_slice>
              <text_slice>And it probably is.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>On the other hand, this kind of
methodology is being used</text_slice>
            </slice>
            <slice>
              <time_slice>30:41</time_slice>
              <text_slice>especially by physicists and
also by statisticians.</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>It's used a lot.</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>When is it used?</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>If you have an integral to
calculate, such as this</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>integral, but you're not lucky,
and your functions are</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>not so simple where you can do
your calculations by hand, and</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>maybe the dimensions are
larger-- instead of two random</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>variables you have 100
random variables, so</text_slice>
            </slice>
            <slice>
              <time_slice>31:04</time_slice>
              <text_slice>it's a 100-fold integral--</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>then there's no way to do
that in the computer.</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>But the way that you can
actually do it is by</text_slice>
            </slice>
            <slice>
              <time_slice>31:14</time_slice>
              <text_slice>generating random samples of
your random variables, doing</text_slice>
            </slice>
            <slice>
              <time_slice>31:18</time_slice>
              <text_slice>that simulation over and
over many times.</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>That is, by interpreting an
integral as a probability, you</text_slice>
            </slice>
            <slice>
              <time_slice>31:25</time_slice>
              <text_slice>can use simulation to estimate
that probability.</text_slice>
            </slice>
            <slice>
              <time_slice>31:29</time_slice>
              <text_slice>And that gives you a way of
calculating integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>31:32</time_slice>
              <text_slice>And physicists do actually use
that a lot, as well as</text_slice>
            </slice>
            <slice>
              <time_slice>31:36</time_slice>
              <text_slice>statisticians, computer
scientists, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>31:39</time_slice>
              <text_slice>It's a so-called Monte
Carlo method</text_slice>
            </slice>
            <slice>
              <time_slice>31:41</time_slice>
              <text_slice>for evaluating integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>31:43</time_slice>
              <text_slice>And it's a basic piece of the
toolbox in science these days.</text_slice>
            </slice>
            <slice>
              <time_slice>31:50</time_slice>
              <text_slice>Finally, the harder concept
of the day is the idea of</text_slice>
            </slice>
            <slice>
              <time_slice>31:54</time_slice>
              <text_slice>conditioning.</text_slice>
            </slice>
            <slice>
              <time_slice>31:55</time_slice>
              <text_slice>And here things become a little
subtle when you deal</text_slice>
            </slice>
            <slice>
              <time_slice>31:58</time_slice>
              <text_slice>with continuous random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>32:00</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>32:02</time_slice>
              <text_slice>First, remember again our basic
interpretation of what a</text_slice>
            </slice>
            <slice>
              <time_slice>32:05</time_slice>
              <text_slice>density is.</text_slice>
            </slice>
            <slice>
              <time_slice>32:06</time_slice>
              <text_slice>A density gives us</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>probabilities of little intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>So how should we define
conditional densities?</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>Conditional densities should
again give us probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>little intervals, but inside a
conditional world where we</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>have been told something about
the other random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>So what we would like to be
true is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>We would like to define a
concept of a conditional</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>density of a random variable X
given the value of another</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>random variable Y. And it should
behave the following</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>way, that the conditional
density gives us the</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>probability of little
intervals--</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>same as here--</text_slice>
            </slice>
            <slice>
              <time_slice>32:44</time_slice>
              <text_slice>given that we are told
the value of y.</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>And here's where the
subtleties come.</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>The main thing to notice is
that here I didn't write</text_slice>
            </slice>
            <slice>
              <time_slice>32:54</time_slice>
              <text_slice>"equal," I wrote "approximately
equal." Why do</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>we need that?</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>Well, the thing is that
conditional probabilities are</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>not defined when you condition
on an event that has 0</text_slice>
            </slice>
            <slice>
              <time_slice>33:08</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>So we need the conditioning
event here to have posed this</text_slice>
            </slice>
            <slice>
              <time_slice>33:13</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>So instead of saying that Y is
exactly equal to little y, we</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>want to instead say we're in a
new universe where capital Y</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>is very close to little y.</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>And then this notion of "very
close" kind of takes the limit</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>and takes it to be
infinitesimally close.</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>So this is the way to interpret
conditional</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>33:40</time_slice>
              <text_slice>That's what they should mean.</text_slice>
            </slice>
            <slice>
              <time_slice>33:42</time_slice>
              <text_slice>Now, in practice, when you
actually use probability, you</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>forget about that subtlety.</text_slice>
            </slice>
            <slice>
              <time_slice>33:46</time_slice>
              <text_slice>And you say, well, I've been
told that Y is equal to 1.3.</text_slice>
            </slice>
            <slice>
              <time_slice>33:50</time_slice>
              <text_slice>Give me the conditional
distribution of X. But</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>formally or rigorously, you
should say I'm being told that</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>Y is infinitesimally
close to 1.3.</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>Tell me the distribution of X.</text_slice>
            </slice>
            <slice>
              <time_slice>34:03</time_slice>
              <text_slice>Now, if this is what we want,
what should this quantity be?</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>It's a conditional probability,
so it should be</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>the probability of two
things happening--</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>X being close to little x, Y
being close to little y.</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>And that's basically given to
us by the joint density</text_slice>
            </slice>
            <slice>
              <time_slice>34:20</time_slice>
              <text_slice>divided by the probability of
the conditioning event, which</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>has something to do with the
density of Y itself.</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>And if you do things carefully,
you see that the</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>only way to satisfy this
relation is to define the</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>conditional density by this
particular formula.</text_slice>
            </slice>
            <slice>
              <time_slice>34:38</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:38</time_slice>
              <text_slice>Big discussion to come down in
the end to what you should</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>have probably guessed by now.</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>We just take any formulas and
expressions from the discrete</text_slice>
            </slice>
            <slice>
              <time_slice>34:49</time_slice>
              <text_slice>case and replace PMFs by PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>So the conditional PDF is
defined by this formula where</text_slice>
            </slice>
            <slice>
              <time_slice>34:58</time_slice>
              <text_slice>here we have joint PDF and
marginal PDF, as opposed to</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>the discrete case where we
had the joint PMF and</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>the marginal PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>35:07</time_slice>
              <text_slice>So in some sense, it's just
a syntactic change.</text_slice>
            </slice>
            <slice>
              <time_slice>35:11</time_slice>
              <text_slice>In another sense, it's a little
subtler on how you</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>actually interpret it.</text_slice>
            </slice>
            <slice>
              <time_slice>35:17</time_slice>
              <text_slice>Speaking about interpretation,
what are some ways of thinking</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>about the joint density?</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>Well, the best way to think
about it is that somebody has</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>fixed little y for you.</text_slice>
            </slice>
            <slice>
              <time_slice>35:27</time_slice>
              <text_slice>So little y is being
fixed here.</text_slice>
            </slice>
            <slice>
              <time_slice>35:31</time_slice>
              <text_slice>And we look at this density
as a function of X.</text_slice>
            </slice>
            <slice>
              <time_slice>35:35</time_slice>
              <text_slice>I've told you what Y is.</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>Tell me what you know about X.
And you tell me that X has a</text_slice>
            </slice>
            <slice>
              <time_slice>35:39</time_slice>
              <text_slice>certain distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>What does that distribution
look like?</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>It has exactly the same shape
as the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>35:50</time_slice>
              <text_slice>Remember, we fixed Y. So
this is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>35:53</time_slice>
              <text_slice>So the only thing that varies
is X. So we get the function</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>that behaves like the joint
density when you fix y, which</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>is really you take the joint
density, and you</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>take a slice of it.</text_slice>
            </slice>
            <slice>
              <time_slice>36:05</time_slice>
              <text_slice>You fix a y, and you see
how it varies with x.</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>So in that sense, the
conditional PDF is just a</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>slice of the joint PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>36:14</time_slice>
              <text_slice>But we need to divide by a
certain number, which just</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>scales it and changes
its shape.</text_slice>
            </slice>
            <slice>
              <time_slice>36:19</time_slice>
              <text_slice>We're coming back to a
picture in a second.</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>But before going to the picture,
lets go back to the</text_slice>
            </slice>
            <slice>
              <time_slice>36:25</time_slice>
              <text_slice>interpretation of
independence.</text_slice>
            </slice>
            <slice>
              <time_slice>36:27</time_slice>
              <text_slice>If the two random the variables
are independent,</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>according to our definition in
the previous slide, the joint</text_slice>
            </slice>
            <slice>
              <time_slice>36:33</time_slice>
              <text_slice>density is going to factor
as the product of</text_slice>
            </slice>
            <slice>
              <time_slice>36:36</time_slice>
              <text_slice>the marginal densities.</text_slice>
            </slice>
            <slice>
              <time_slice>36:37</time_slice>
              <text_slice>The density of Y in the
numerator cancels the density</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>in the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>And we're just left with
the density of X.</text_slice>
            </slice>
            <slice>
              <time_slice>36:44</time_slice>
              <text_slice>So in the case of independence,
what we get is</text_slice>
            </slice>
            <slice>
              <time_slice>36:46</time_slice>
              <text_slice>that the conditional is the
same as the marginal.</text_slice>
            </slice>
            <slice>
              <time_slice>36:49</time_slice>
              <text_slice>And that solidifies our
intuition that in the case of</text_slice>
            </slice>
            <slice>
              <time_slice>36:52</time_slice>
              <text_slice>independence, being told
something about the value of Y</text_slice>
            </slice>
            <slice>
              <time_slice>36:58</time_slice>
              <text_slice>does not change our beliefs
about how X is distributed.</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>So whatever we expected about X
is going to remain true even</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>after we are told something
about Y.</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>So let's look at
some pictures.</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>Here is what the joint
PDF might look like.</text_slice>
            </slice>
            <slice>
              <time_slice>37:16</time_slice>
              <text_slice>Here we've got our
x and y-axis.</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>And if you want to calculate the
probability of a certain</text_slice>
            </slice>
            <slice>
              <time_slice>37:23</time_slice>
              <text_slice>event, what you do is you look
at that event and you see how</text_slice>
            </slice>
            <slice>
              <time_slice>37:27</time_slice>
              <text_slice>much of that mass is sitting
on top of that event.</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>Now let's start slicing.</text_slice>
            </slice>
            <slice>
              <time_slice>37:35</time_slice>
              <text_slice>Let's fix a value of x and look
along that slice where we</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>obtain this function.</text_slice>
            </slice>
            <slice>
              <time_slice>37:48</time_slice>
              <text_slice>Now what does that slice do?</text_slice>
            </slice>
            <slice>
              <time_slice>37:52</time_slice>
              <text_slice>That slice tells us for that
particular x what the possible</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>values of y are going to be
and how likely they are.</text_slice>
            </slice>
            <slice>
              <time_slice>38:00</time_slice>
              <text_slice>If we integrate over all
y's, what do we get?</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>Integrating over all y's just
gives us the marginal density</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>of X. It's the calculation
that we did here.</text_slice>
            </slice>
            <slice>
              <time_slice>38:15</time_slice>
              <text_slice>By integrating over all y's, we
find the marginal density</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>of X. So the total area under
that slice gives us the</text_slice>
            </slice>
            <slice>
              <time_slice>38:27</time_slice>
              <text_slice>marginal density of X. And by
looking at the different</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>slices, we find how likely the
different values of x are</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>38:36</time_slice>
              <text_slice>How about the conditional?</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>If we're interested in the
conditional of Y given X, how</text_slice>
            </slice>
            <slice>
              <time_slice>38:48</time_slice>
              <text_slice>would you think about it?</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>This refers to a universe where
we are told that capital</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>X takes on a specific value.</text_slice>
            </slice>
            <slice>
              <time_slice>38:57</time_slice>
              <text_slice>So we put ourselves in
the universe where</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>this line has happened.</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>There's still possible values
of y that can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>And this shape kind of tells us
the relative likelihoods of</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>the different y's.</text_slice>
            </slice>
            <slice>
              <time_slice>39:10</time_slice>
              <text_slice>And this is indeed going to be
the shape of the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>distribution of Y given
that X has occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>On the other hand, the
conditional distribution must</text_slice>
            </slice>
            <slice>
              <time_slice>39:21</time_slice>
              <text_slice>add up to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>So the total probability over
all of the different y's in</text_slice>
            </slice>
            <slice>
              <time_slice>39:25</time_slice>
              <text_slice>this universe, that
total probability</text_slice>
            </slice>
            <slice>
              <time_slice>39:27</time_slice>
              <text_slice>should be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:29</time_slice>
              <text_slice>Here it's not equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>The total area is the
marginal density.</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>To make it equal to 1, we need
to divide by the marginal</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>density, which is basically to
renormalize this shape so that</text_slice>
            </slice>
            <slice>
              <time_slice>39:44</time_slice>
              <text_slice>the total area under that slice,
under that shape, is</text_slice>
            </slice>
            <slice>
              <time_slice>39:48</time_slice>
              <text_slice>equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>So we start with the joint.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>We take the slices.</text_slice>
            </slice>
            <slice>
              <time_slice>39:55</time_slice>
              <text_slice>And then we adjust the slices
so that every slice has an</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>area underneath equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>And this gives us
the conditional.</text_slice>
            </slice>
            <slice>
              <time_slice>40:05</time_slice>
              <text_slice>So for example, down here--</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>you can not even see it
in this diagram--</text_slice>
            </slice>
            <slice>
              <time_slice>40:11</time_slice>
              <text_slice>but after you renormalize it
so that its total area is</text_slice>
            </slice>
            <slice>
              <time_slice>40:15</time_slice>
              <text_slice>equal to 1, you get this sort of
narrow spike that goes up.</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>And so this is a plot of the
conditional distributions that</text_slice>
            </slice>
            <slice>
              <time_slice>40:22</time_slice>
              <text_slice>you get for the different
values of x.</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>Given a particular value of x,
you're going to get this</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>certain conditional
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>So this picture is worth about
as much as anything else in</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>this particular chapter.</text_slice>
            </slice>
            <slice>
              <time_slice>40:38</time_slice>
              <text_slice>Make sure you kind of understand
exactly all these</text_slice>
            </slice>
            <slice>
              <time_slice>40:42</time_slice>
              <text_slice>pieces of the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>40:47</time_slice>
              <text_slice>And finally, let's go, in the
remaining time, through an</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>example where we're going to
throw in the bucket all the</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>concepts and notations that
we have introduced so far.</text_slice>
            </slice>
            <slice>
              <time_slice>40:58</time_slice>
              <text_slice>So the example is as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>We start with a stick that
has a certain length.</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>And we break it a completely
random location.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>And--</text_slice>
            </slice>
            <slice>
              <time_slice>41:09</time_slice>
              <text_slice>yes, this 1 should be l.</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>So it has length l.</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>And we're going to break
it at the random place.</text_slice>
            </slice>
            <slice>
              <time_slice>41:19</time_slice>
              <text_slice>And we call that random place
where we break it, we call it</text_slice>
            </slice>
            <slice>
              <time_slice>41:21</time_slice>
              <text_slice>X.</text_slice>
            </slice>
            <slice>
              <time_slice>41:24</time_slice>
              <text_slice>X can be anywhere, uniform
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>So this means that X has a
density that goes from 0 to l.</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>I guess this capital L is
supposed to be the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>41:34</time_slice>
              <text_slice>lower-case l.</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>So that's the density of X. And
since the density needs to</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>integrate to 1, the height of
that density has to be 1/l.</text_slice>
            </slice>
            <slice>
              <time_slice>41:46</time_slice>
              <text_slice>Now, having broken the stick
and given that we are left</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>with this piece of the stick,
I'm now going to break it</text_slice>
            </slice>
            <slice>
              <time_slice>41:53</time_slice>
              <text_slice>again at a completely random
place, meaning I'm going to</text_slice>
            </slice>
            <slice>
              <time_slice>41:56</time_slice>
              <text_slice>choose a point where I break it
uniformly over the length</text_slice>
            </slice>
            <slice>
              <time_slice>41:59</time_slice>
              <text_slice>of the stick.</text_slice>
            </slice>
            <slice>
              <time_slice>42:00</time_slice>
              <text_slice>What does this mean?</text_slice>
            </slice>
            <slice>
              <time_slice>42:02</time_slice>
              <text_slice>And let's call Y the location
where I break it.</text_slice>
            </slice>
            <slice>
              <time_slice>42:05</time_slice>
              <text_slice>So Y is going to range
between 0 and x.</text_slice>
            </slice>
            <slice>
              <time_slice>42:10</time_slice>
              <text_slice>x is the stick that
I'm left with.</text_slice>
            </slice>
            <slice>
              <time_slice>42:11</time_slice>
              <text_slice>So I'm going to break it
somewhere in between.</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>So I pick a y between 0 and x.</text_slice>
            </slice>
            <slice>
              <time_slice>42:21</time_slice>
              <text_slice>And of course, x
is less than l.</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>And I'm going to
break it there.</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>So y is uniform between
0 and x.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>What does that mean, that the
density of y, given that you</text_slice>
            </slice>
            <slice>
              <time_slice>42:36</time_slice>
              <text_slice>have already told me x, ranges
from 0 to little x?</text_slice>
            </slice>
            <slice>
              <time_slice>42:42</time_slice>
              <text_slice>If I told you that the first
break happened at a particular</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>x, then y can only range
over this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>And I'm assuming a uniform</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>distribution over that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>So we have this kind of shape.</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>And that fixes for
us the height of</text_slice>
            </slice>
            <slice>
              <time_slice>43:00</time_slice>
              <text_slice>the conditional density.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>So what's the joint density of
those two random variables?</text_slice>
            </slice>
            <slice>
              <time_slice>43:11</time_slice>
              <text_slice>By the definition of conditional
densities, the</text_slice>
            </slice>
            <slice>
              <time_slice>43:14</time_slice>
              <text_slice>conditional was defined as the
ratio of this divided by that.</text_slice>
            </slice>
            <slice>
              <time_slice>43:18</time_slice>
              <text_slice>So we can find the joint density
by taking the marginal</text_slice>
            </slice>
            <slice>
              <time_slice>43:21</time_slice>
              <text_slice>and then multiplying
by the conditional.</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>This is the same formula as
in the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>This is our very familiar
multiplication rule, but</text_slice>
            </slice>
            <slice>
              <time_slice>43:29</time_slice>
              <text_slice>adjusted to the case of
continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>So Ps become Fs.</text_slice>
            </slice>
            <slice>
              <time_slice>43:34</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>43:35</time_slice>
              <text_slice>So we do have a formula
for this.</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>What is it?</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>It's 1/l--</text_slice>
            </slice>
            <slice>
              <time_slice>43:40</time_slice>
              <text_slice>that's the density of X --</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>times 1/x, which is the
conditional density of Y. This</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>is the formula for the
joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>But we must be careful.</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>This is a formula that's
not valid anywhere.</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>It's only valid for the x's
and y's that are possible.</text_slice>
            </slice>
            <slice>
              <time_slice>43:57</time_slice>
              <text_slice>And the x's and y's that are
possible are given by these</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>inequalities.</text_slice>
            </slice>
            <slice>
              <time_slice>44:01</time_slice>
              <text_slice>So x can range from 0 to
l, and y can only be</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>smaller than x.</text_slice>
            </slice>
            <slice>
              <time_slice>44:07</time_slice>
              <text_slice>So this is the formula
for the density on</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>this part of our space.</text_slice>
            </slice>
            <slice>
              <time_slice>44:12</time_slice>
              <text_slice>The density is 0
anywhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>44:16</time_slice>
              <text_slice>So what does it look like?</text_slice>
            </slice>
            <slice>
              <time_slice>44:18</time_slice>
              <text_slice>It's basically a 1/x function.</text_slice>
            </slice>
            <slice>
              <time_slice>44:20</time_slice>
              <text_slice>So it's sort of constant
along that dimension.</text_slice>
            </slice>
            <slice>
              <time_slice>44:23</time_slice>
              <text_slice>But as x goes to 0, your
density goes up and</text_slice>
            </slice>
            <slice>
              <time_slice>44:27</time_slice>
              <text_slice>can even blow up.</text_slice>
            </slice>
            <slice>
              <time_slice>44:29</time_slice>
              <text_slice>It sort of looks like a sail
that's raised and somewhat</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>curved and has a point up
there going to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>44:37</time_slice>
              <text_slice>So this is the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>Now once you have in your hands
a joint density, then</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>you can answer in principle
any problem.</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>It's just a matter of plugging
in and doing computations.</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>How about calculating something
like a conditional</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>expectation of Y given
a value of x?</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>That's a concept we have
not defined so far.</text_slice>
            </slice>
            <slice>
              <time_slice>45:02</time_slice>
              <text_slice>But how should we define it?</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>Means the reasonable thing.</text_slice>
            </slice>
            <slice>
              <time_slice>45:06</time_slice>
              <text_slice>We'll define it the same way
as ordinary expectations</text_slice>
            </slice>
            <slice>
              <time_slice>45:09</time_slice>
              <text_slice>except that since we're given
some conditioning information,</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>we should use the probability
distribution that applies to</text_slice>
            </slice>
            <slice>
              <time_slice>45:17</time_slice>
              <text_slice>that particular situation.</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>So in a situation where we are
told the value of x, the</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>distribution that applies is the
conditional distribution</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>of Y. So it's going to be the
conditional density of Y given</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>the value of x.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>Now, we know what this is.</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>It's given by 1/x.</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>So we need to integrate
y times 1/x dy.</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>And what should we
integrate over?</text_slice>
            </slice>
            <slice>
              <time_slice>45:48</time_slice>
              <text_slice>Well, given the value of x, y
can only range from 0 to x.</text_slice>
            </slice>
            <slice>
              <time_slice>45:53</time_slice>
              <text_slice>So this is what we get.</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>And you do your integral, and
you get that this is x/2.</text_slice>
            </slice>
            <slice>
              <time_slice>46:01</time_slice>
              <text_slice>Is it a surprise?</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>It shouldn't be.</text_slice>
            </slice>
            <slice>
              <time_slice>46:04</time_slice>
              <text_slice>This is just the expected value
of Y in a universe where</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>X has been realized and Y is
given by this distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>Y is uniform between 0 and x.</text_slice>
            </slice>
            <slice>
              <time_slice>46:17</time_slice>
              <text_slice>The expected value of Y should
be the midpoint of this</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>interval, which is x/2.</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>Now let's do fancier stuff.</text_slice>
            </slice>
            <slice>
              <time_slice>46:28</time_slice>
              <text_slice>Since we have the joint
distribution, we should be</text_slice>
            </slice>
            <slice>
              <time_slice>46:31</time_slice>
              <text_slice>able to calculate
the marginal.</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>What is the distribution of Y?</text_slice>
            </slice>
            <slice>
              <time_slice>46:36</time_slice>
              <text_slice>After breaking the stick twice,
how big is the little</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>piece that I'm left with?</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>How do we find this?</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>To find the marginal, we just
take the joint and integrate</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>out the variable that
we don't want.</text_slice>
            </slice>
            <slice>
              <time_slice>46:52</time_slice>
              <text_slice>A particular y can happen
in many ways.</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>It can happen together
with any x.</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>So we consider all the possible
x's that can go</text_slice>
            </slice>
            <slice>
              <time_slice>47:00</time_slice>
              <text_slice>together with this y and average
over all those x's.</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>So we plug in the formula for
the joint density from the</text_slice>
            </slice>
            <slice>
              <time_slice>47:09</time_slice>
              <text_slice>previous slide.</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>We know that it's 1/lx.</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>And what's the range
of the x's?</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>So to find the density of Y for
a particular y up here,</text_slice>
            </slice>
            <slice>
              <time_slice>47:22</time_slice>
              <text_slice>I'm going to integrate
over x's.</text_slice>
            </slice>
            <slice>
              <time_slice>47:26</time_slice>
              <text_slice>The density is 0
here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>The density is nonzero
only in this part.</text_slice>
            </slice>
            <slice>
              <time_slice>47:32</time_slice>
              <text_slice>So I need to integrate over x's
going from here to there.</text_slice>
            </slice>
            <slice>
              <time_slice>47:37</time_slice>
              <text_slice>So what's the "here"?</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>This line goes up at
the slope of 1.</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>So this is the line
x equals y.</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>So if I fix y, it means that
my integral starts from a</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>value of x that is
also equal to y.</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>So where the integral starts
from is at x equals y.</text_slice>
            </slice>
            <slice>
              <time_slice>47:58</time_slice>
              <text_slice>And it goes all the way until
the end of the length of our</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>stick, which is l.</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>So we need to integrate
from little y up to l.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>So that's something that
almost always comes up.</text_slice>
            </slice>
            <slice>
              <time_slice>48:12</time_slice>
              <text_slice>It's not enough to have just
this formula for integrating</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>the joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>You need to keep track
of different regions.</text_slice>
            </slice>
            <slice>
              <time_slice>48:19</time_slice>
              <text_slice>And if the joint density is 0
in some regions, then you</text_slice>
            </slice>
            <slice>
              <time_slice>48:23</time_slice>
              <text_slice>exclude those regions from
the range of integration.</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>So the range of integration is
only over those values where</text_slice>
            </slice>
            <slice>
              <time_slice>48:32</time_slice>
              <text_slice>the particular formula is valid,
the places where the</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>joint density is nonzero.</text_slice>
            </slice>
            <slice>
              <time_slice>48:37</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>The integral of 1/x dx, that
gives you a logarithm.</text_slice>
            </slice>
            <slice>
              <time_slice>48:41</time_slice>
              <text_slice>So we evaluate this integral,
and we get an</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>expression of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>So the density of Y has a
somewhat unexpected shape.</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>So it's a logarithmic
function.</text_slice>
            </slice>
            <slice>
              <time_slice>48:55</time_slice>
              <text_slice>And it goes this way.</text_slice>
            </slice>
            <slice>
              <time_slice>48:59</time_slice>
              <text_slice>It's for y going all
the way to l.</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>When y is equal to l, the
logarithm of 1 is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>But when y approaches 0,
logarithm of something big</text_slice>
            </slice>
            <slice>
              <time_slice>49:12</time_slice>
              <text_slice>blows up, and we get a
shape of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>Finally, we can calculate the
expected value of Y. And we</text_slice>
            </slice>
            <slice>
              <time_slice>49:25</time_slice>
              <text_slice>can do this by using the
definition of the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>49:29</time_slice>
              <text_slice>So integral of y times
the density of y.</text_slice>
            </slice>
            <slice>
              <time_slice>49:33</time_slice>
              <text_slice>We already found what that
density is, so we</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>can plug it in here.</text_slice>
            </slice>
            <slice>
              <time_slice>49:38</time_slice>
              <text_slice>And we're integrating over
the range of possible</text_slice>
            </slice>
            <slice>
              <time_slice>49:40</time_slice>
              <text_slice>y's, from 0 to l.</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>Now this involves the integral
for y log y, which I'm sure</text_slice>
            </slice>
            <slice>
              <time_slice>49:46</time_slice>
              <text_slice>you have encountered in your
calculus classes but maybe do</text_slice>
            </slice>
            <slice>
              <time_slice>49:49</time_slice>
              <text_slice>not remember how to do it.</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>In any case, you look it
up in some integral</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>tables or do it by parts.</text_slice>
            </slice>
            <slice>
              <time_slice>49:55</time_slice>
              <text_slice>And you get the final
answer of l/4.</text_slice>
            </slice>
            <slice>
              <time_slice>49:59</time_slice>
              <text_slice>And at this point, you say,
that's a really simple answer.</text_slice>
            </slice>
            <slice>
              <time_slice>50:02</time_slice>
              <text_slice>Shouldn't I have expected
it to be l/4?</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>I guess, yes.</text_slice>
            </slice>
            <slice>
              <time_slice>50:07</time_slice>
              <text_slice>I mean, when you break it once,
the expected value of</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>what you are left with is going
to be 1/2 of what you</text_slice>
            </slice>
            <slice>
              <time_slice>50:14</time_slice>
              <text_slice>started with.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>When you break it the next time,
the expected length of</text_slice>
            </slice>
            <slice>
              <time_slice>50:19</time_slice>
              <text_slice>what you're left with should be
1/2 of the piece that you</text_slice>
            </slice>
            <slice>
              <time_slice>50:23</time_slice>
              <text_slice>are now breaking.</text_slice>
            </slice>
            <slice>
              <time_slice>50:24</time_slice>
              <text_slice>So each time that you break it
at random, you expected it to</text_slice>
            </slice>
            <slice>
              <time_slice>50:27</time_slice>
              <text_slice>become smaller by
a factor of 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>50:29</time_slice>
              <text_slice>So if you break it twice, you
are left something that's</text_slice>
            </slice>
            <slice>
              <time_slice>50:31</time_slice>
              <text_slice>expected to be 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>This is reasoning on the
average, which happens to give</text_slice>
            </slice>
            <slice>
              <time_slice>50:37</time_slice>
              <text_slice>you the right answer
in this case.</text_slice>
            </slice>
            <slice>
              <time_slice>50:39</time_slice>
              <text_slice>But again, there's the warning
that reasoning on the average</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>doesn't always give you
the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>50:44</time_slice>
              <text_slice>So be careful about doing
arguments of this type.</text_slice>
            </slice>
            <slice>
              <time_slice>50:48</time_slice>
              <text_slice>Very good.</text_slice>
            </slice>
            <slice>
              <time_slice>50:48</time_slice>
              <text_slice>See you on Wednesday.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Multiple Discrete Random Variables: Expectations, Conditioning, Independence (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 7 Review
Readings: Finish Chapter 2
pX(x)= P(X=x)
Lecture outline
pX,Y(x, y)=P(X=x, Y=y)
Multiple random variables
p )Joint PMFX Y(x|y)=P(X=x Y =y| |
Conditioning
IndependencepX(x)=
More on expectations/summationdisplay
pX,Y(x, y)
y
Binomial distribution revisited pX,Y(x, y)=pX(x)pY X(y x| |)
A hat problem
Independent random variables Expectations
pX,Y,Z (x, y, z )=pX(x)pY X(y|x)pZ X,Y (z|x, y) [ | | EX]=/summationdisplay
xpX(x)
x
E[g(X, Y )] = g(x, y)pX,Y(x, y)Random variables X,Y,Zarex y
independent if:/summationdisplay/summationdisplay
pX,Y,Z (x, y, z )=pX(x) pY(y)pZ(z) In general: E[g(X, Y )] = gE[X],E[Y]
for all x, y, z/parenleftBig /parenrightBig
y E[X+]=E[X]+
41/20 2/20 2/20E[X+Y+Z]=E[X]+E[Y]+E[Z]
32/20 4/20 1/20 2/20
2 1/20 3/20 1/20 IfX,Yare independent:
1 1/20E[XY]=E[X]E[Y]
x12 3 4
E[g(X)h(Y )] =E[g(X)]
Independent?E[h(Y)]
What if we condition on X2
and Y3?/negationslash
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Variances Binomial mean and variance
Var( aX)=2aVar( X) X= # of successes in nindependent
trials
Var( X+a) = Var( X)
probability of success p
nnLetZ=X+Y.kE[X]= k p (1kp)nk
IfX,Yare independent: k/summationdisplay
=0/parenleftBig /parenrightBig
Var( X+Y) = Var( X) + Var( Y)
1,if success in trial i,Xi=0,otherwise
Examples:
E[Xi]=
IfX=Y, Var( X+Y)=
IfX=Y, Var( X+Y)= E[X]=
IfX,Yindep., and Z=X3Y,
Var( Z)=Var( Xi)=
Var( X)=
The hat problem Variance in the hat problem
 people throw their hats in a box and Var( )= [2 2n X EX](E[X]) = E[2X]1
then pick one at random.
X: number of people who get their own
hat2=/summationdisplay2X Xi+
i i,j/summationdisplay
XiXj
:i=jFindE[X]
E[2X]=i
Xi=
1,ifiselects own hat
0,otherwise.
P(X1X2= 1) = P(X1= 1)P(X2=1| X1= 1)
X=X1+X2++Xn=
P(Xi= 1) =
E[Xi]=
Are the Xiindependent? E[2X]=
E[X]= Var( X)=/negationslash
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-10-continuous-bayes-rule-derived-distributions/</video_url>
          <video_title>Lecture 10: Continuous Bayes' Rule; Derived Distributions</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:23</time_slice>
              <text_slice>PROFESSOR: So today's agenda
is to say a few more things</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>about continuous random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>0:28</time_slice>
              <text_slice>Mainly we're going to talk a
little bit about inference.</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>This is a topic that we're going
to revisit at the end of</text_slice>
            </slice>
            <slice>
              <time_slice>0:35</time_slice>
              <text_slice>the semester.</text_slice>
            </slice>
            <slice>
              <time_slice>0:36</time_slice>
              <text_slice>But there's a few things
that we can</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>already say at this point.</text_slice>
            </slice>
            <slice>
              <time_slice>0:40</time_slice>
              <text_slice>And then the new topic for
today is the subject of</text_slice>
            </slice>
            <slice>
              <time_slice>0:44</time_slice>
              <text_slice>derived distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>0:45</time_slice>
              <text_slice>Basically if you know the
distribution of one random</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>variable, and you have a
function of that random</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>variable, how to find a</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>distribution for that function.</text_slice>
            </slice>
            <slice>
              <time_slice>0:54</time_slice>
              <text_slice>And it's a fairly mechanical
skill, but that's an important</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>one, so we're going
to go through it.</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>So let's see where we stand.</text_slice>
            </slice>
            <slice>
              <time_slice>1:02</time_slice>
              <text_slice>Here is the big picture.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>That's all we have
done so far.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>We have talked about discrete
random variables, which we</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>described by probability
mass function.</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>So if we have multiple random
variables, we describe them</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>with the a joint
mass function.</text_slice>
            </slice>
            <slice>
              <time_slice>1:16</time_slice>
              <text_slice>And then we define conditional
probabilities, or conditional</text_slice>
            </slice>
            <slice>
              <time_slice>1:19</time_slice>
              <text_slice>PMFs, and the three are related
according to this</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>formula, which is, you can
think of it either as the</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>definition of conditional
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>Or as the multiplication rule,
the probability of two things</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>happening is the product of the
probabilities of the first</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>thing happening, and then the
second happening, given that</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>the first has happened.</text_slice>
            </slice>
            <slice>
              <time_slice>1:39</time_slice>
              <text_slice>There's another relation between
this, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>probability of x occurring, is
the sum of the different</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>probabilities of the different
ways that x may occur, which</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>is in conjunction with different
values of y.</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>And there's an analog of all
that in the continuous world,</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>where all you do is to replace
p's by f's, and replace sums</text_slice>
            </slice>
            <slice>
              <time_slice>2:02</time_slice>
              <text_slice>by integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>So the formulas all
look the same.</text_slice>
            </slice>
            <slice>
              <time_slice>2:05</time_slice>
              <text_slice>The interpretations are a little
more subtle, so the f's</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>are not probabilities, they're
probability densities.</text_slice>
            </slice>
            <slice>
              <time_slice>2:11</time_slice>
              <text_slice>So they're probabilities per
unit length, or in the case of</text_slice>
            </slice>
            <slice>
              <time_slice>2:16</time_slice>
              <text_slice>joint PDf's, these are
probabilities per unit area.</text_slice>
            </slice>
            <slice>
              <time_slice>2:20</time_slice>
              <text_slice>So they're densities
of some sort.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>Probably the more subtle concept
to understand what it</text_slice>
            </slice>
            <slice>
              <time_slice>2:26</time_slice>
              <text_slice>really is the conditional
density.</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>In some sense, it's simple.</text_slice>
            </slice>
            <slice>
              <time_slice>2:30</time_slice>
              <text_slice>It's just the density of X in
a world where you have been</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>told the value of the random
variable Y. It's a function</text_slice>
            </slice>
            <slice>
              <time_slice>2:40</time_slice>
              <text_slice>that has two arguments, but the
best way to think about it</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>is to say that we fixed y.</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>We're told the value of the
random variable Y, and we look</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>at it as a function of x.</text_slice>
            </slice>
            <slice>
              <time_slice>2:52</time_slice>
              <text_slice>So as a function of x, the
denominator is a constant, and</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>it just looks like the
joint density.</text_slice>
            </slice>
            <slice>
              <time_slice>2:59</time_slice>
              <text_slice>when we keep y fixed.</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>So it's really a function of
one argument, just the</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>argument x.</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>And it has the same shape as the
joint's density when you</text_slice>
            </slice>
            <slice>
              <time_slice>3:10</time_slice>
              <text_slice>take that slice of it.</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>So conditional PDFs are just
slices of joint PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>There's a bunch of concepts,
expectations, variances,</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>cumulative distribution
functions that apply equally</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>well for to both universes
of discrete or</text_slice>
            </slice>
            <slice>
              <time_slice>3:26</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>So why is probability useful?</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>Probability is useful because,
among other things, we use it</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>to make sense of the
world around us.</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>We use it to make inferences
about things that we do not</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>see directly.</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>And this is done in a
very simple manner</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>using the base rule.</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>We've already seen some of that,
and now we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>revisit it with a bunch of
different variations.</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>And the variations come because
sometimes our random</text_slice>
            </slice>
            <slice>
              <time_slice>3:58</time_slice>
              <text_slice>variable are discrete, sometimes
they're continuous,</text_slice>
            </slice>
            <slice>
              <time_slice>4:01</time_slice>
              <text_slice>or we can have a combination
of the two.</text_slice>
            </slice>
            <slice>
              <time_slice>4:04</time_slice>
              <text_slice>So the big picture is that
there's some unknown random</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>variable out of there, and we
know the distribution that's</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>And in the discrete case, it's
going to be given by PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>4:16</time_slice>
              <text_slice>In the continuous case,
it's given a PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>Then we have some phenomenon,
some noisy phenomenon or some</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>measuring device, and that
measuring device produces</text_slice>
            </slice>
            <slice>
              <time_slice>4:28</time_slice>
              <text_slice>observable random variables Y.</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>We don't know what x is, but we
have some beliefs about how</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>X is distributed.</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>We observe the random variable
Y. We need a</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>model of this box.</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>And the model of that box is
going to be either a PMF, for</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>the random variable Y. And that
model tells us, if the</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>true state of the world is X,
how do we expect to Y to be</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>distributed?</text_slice>
            </slice>
            <slice>
              <time_slice>4:58</time_slice>
              <text_slice>That's for the case where
Y is this discrete.</text_slice>
            </slice>
            <slice>
              <time_slice>5:01</time_slice>
              <text_slice>If Y is a continuous, you might
instead have a density</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>for Y, or something
of that form.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>So in either case, this
should be a function</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>that's known to us.</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>This is our model of the
measuring device.</text_slice>
            </slice>
            <slice>
              <time_slice>5:18</time_slice>
              <text_slice>And now having observed
y, we want to make</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>inferences about x.</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>What does it mean to
make inferences?</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>Well the most complete answer in
the inference problem is to</text_slice>
            </slice>
            <slice>
              <time_slice>5:29</time_slice>
              <text_slice>tell me the probability
distribution</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>of the unknown quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>5:34</time_slice>
              <text_slice>But when I say the probability
distribution, I</text_slice>
            </slice>
            <slice>
              <time_slice>5:36</time_slice>
              <text_slice>don't mean this one.</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>I mean the probability
distribution that takes into</text_slice>
            </slice>
            <slice>
              <time_slice>5:41</time_slice>
              <text_slice>account the measurements
that you got.</text_slice>
            </slice>
            <slice>
              <time_slice>5:43</time_slice>
              <text_slice>So the output of an inference
problem is to come up with the</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>distribution of X, the unknown
quantity, given what we have</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>already observed.</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>And in the discrete case, it
would be an object like that.</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>If X is continuous, it would
be an object of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>6:13</time_slice>
              <text_slice>OK, so we're given conditional
probabilities of this type,</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>and we want to get conditional
distributions of the opposite</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>type where the order of the</text_slice>
            </slice>
            <slice>
              <time_slice>6:23</time_slice>
              <text_slice>conditioning is being reversed.</text_slice>
            </slice>
            <slice>
              <time_slice>6:25</time_slice>
              <text_slice>So the starting point
is always a formula</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>such as this one.</text_slice>
            </slice>
            <slice>
              <time_slice>6:30</time_slice>
              <text_slice>The probability of x happening,
and then y</text_slice>
            </slice>
            <slice>
              <time_slice>6:33</time_slice>
              <text_slice>happening given that
x happens.</text_slice>
            </slice>
            <slice>
              <time_slice>6:36</time_slice>
              <text_slice>This is the probability that
a particular x and y happen</text_slice>
            </slice>
            <slice>
              <time_slice>6:40</time_slice>
              <text_slice>simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>6:42</time_slice>
              <text_slice>But this is also equal to the
probability that y happens,</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>and then that x happens, given
that y has happened.</text_slice>
            </slice>
            <slice>
              <time_slice>6:53</time_slice>
              <text_slice>And you take this expression
and send one term to the</text_slice>
            </slice>
            <slice>
              <time_slice>6:57</time_slice>
              <text_slice>denominator of the other side,
and this gives us the base</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>rule for the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>Which is this one that you have
already seen, and you</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>have played with it.</text_slice>
            </slice>
            <slice>
              <time_slice>7:07</time_slice>
              <text_slice>So this is what the formula
looks like in</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>And the typical example where
both random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>7:14</time_slice>
              <text_slice>discrete is the one we discussed
some time ago.</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>X is, let's say, a binary
variable, or whether an</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>airplane is present
up there or not.</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>Y is a discrete measurement, for
example, whether our radar</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>beeped or it didn't beep.</text_slice>
            </slice>
            <slice>
              <time_slice>7:30</time_slice>
              <text_slice>And we make inferences and
calculate the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>7:33</time_slice>
              <text_slice>the plane is there, or the
probability that the plane is</text_slice>
            </slice>
            <slice>
              <time_slice>7:37</time_slice>
              <text_slice>not there, given the measurement
that we have made.</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>And of course X and Y do not
need to be just binary.</text_slice>
            </slice>
            <slice>
              <time_slice>7:43</time_slice>
              <text_slice>They could be more general
discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:47</time_slice>
              <text_slice>So how does the story change
in the continuous case?</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>First, what's a possible
application of</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>the continuous case?</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>Well, think of X as being some
signal that takes values over</text_slice>
            </slice>
            <slice>
              <time_slice>7:59</time_slice>
              <text_slice>a continuous range.</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>Let's say X is the current
through a resistor.</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>And then you have some measuring
device that measures</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>currents, but that device is
noisy, it gets hit, let's say</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>for example, by Gaussian
noise.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>And the Y that you observe is a
noisy version of X. But your</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>instruments are analog, so
you measure things on</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>a continuous scale.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>What are you going to
do in that case?</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>Well the inference problem, the
output of the inference</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>problem, is going to be the
conditional distribution of X.</text_slice>
            </slice>
            <slice>
              <time_slice>8:33</time_slice>
              <text_slice>What do you think your current
is based on a particular value</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>of Y that you have observed?</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>So the output of our inference
problem is, given the specific</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>value of Y, to calculate this
entire function as a function</text_slice>
            </slice>
            <slice>
              <time_slice>8:48</time_slice>
              <text_slice>of x, and then go and plot it.</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>How do we calculate it?</text_slice>
            </slice>
            <slice>
              <time_slice>8:53</time_slice>
              <text_slice>You go through the same
calculation as in the discrete</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>case, except that all of the
x's gets replaced by p's.</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>In the continuous case, it's
equally true that the joint's</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>density is the product of the
marginal density with the</text_slice>
            </slice>
            <slice>
              <time_slice>9:07</time_slice>
              <text_slice>conditional density.</text_slice>
            </slice>
            <slice>
              <time_slice>9:09</time_slice>
              <text_slice>So the formula is still
valid with just a</text_slice>
            </slice>
            <slice>
              <time_slice>9:11</time_slice>
              <text_slice>little change of notation.</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>So we end up with the same
formula here, except that we</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>replace x's with p's.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>So all of these functions
are known to us.</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>We have formulas for them.</text_slice>
            </slice>
            <slice>
              <time_slice>9:25</time_slice>
              <text_slice>We fix a specific value of y,
we plug it in, so we're left</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>with a function of x.</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>And that gives us the posterior
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>Actually there's also a
denominator term that's not</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>necessarily given to us, but we
can always calculate it if</text_slice>
            </slice>
            <slice>
              <time_slice>9:42</time_slice>
              <text_slice>we have the marginal of X,
and we have the model for</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>measuring device.</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>Then we can always find the
marginal distribution of Y. So</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>this quantity, that number, is
in general a known one, as</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>well, and doesn't give
us any problems.</text_slice>
            </slice>
            <slice>
              <time_slice>9:58</time_slice>
              <text_slice>So to complicate things a little
bit, we can also look</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>into situations where our two
random variables are of</text_slice>
            </slice>
            <slice>
              <time_slice>10:07</time_slice>
              <text_slice>different kinds.</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>For example, one random variable
could be discrete,</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>and the other it might
be continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>And there's two versions.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>Here one version is when X is
discrete, but Y is continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>What's an example of this?</text_slice>
            </slice>
            <slice>
              <time_slice>10:25</time_slice>
              <text_slice>Well suppose that I send a
single bit of information so</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>my X is 0 or 1.</text_slice>
            </slice>
            <slice>
              <time_slice>10:34</time_slice>
              <text_slice>And what I measure is Y,
which is X plus, let's</text_slice>
            </slice>
            <slice>
              <time_slice>10:39</time_slice>
              <text_slice>say, Gaussian noise.</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>This is the standard example
that shows up in any textbook</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>on communication, or
signal processing.</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>You send a single bit, but what
you observe is a noisy</text_slice>
            </slice>
            <slice>
              <time_slice>10:58</time_slice>
              <text_slice>version of that bit.</text_slice>
            </slice>
            <slice>
              <time_slice>11:02</time_slice>
              <text_slice>You start with a model
of your x's.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>These would be your prior
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>For example, you might be
believe that either 0 or 1 are</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>equally likely, in which case
your PMF gives equal weight to</text_slice>
            </slice>
            <slice>
              <time_slice>11:16</time_slice>
              <text_slice>two possible values.</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>And then we need a model of
our measuring device.</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>This is one specific model.</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>The general model would have
a shape such as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>Y has a distribution,
its density.</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>And that density, however,
depends on the value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>So when x is 0, we might get
a density of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>11:46</time_slice>
              <text_slice>And when x is 1, we might
get the density</text_slice>
            </slice>
            <slice>
              <time_slice>11:50</time_slice>
              <text_slice>of a different kind.</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>So these are the conditional
densities of y in a universe</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>that's specified by a particular
value of x.</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>And then we go ahead and
do our inference.</text_slice>
            </slice>
            <slice>
              <time_slice>12:09</time_slice>
              <text_slice>OK, what's the right formula
for doing this inference?</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>We need a formula that's sort of
an analog of this one, but</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>applies to the case where we
have two random variables of</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>different kinds.</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>So let me just redo this
calculation here.</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>Except that I'm not going to
have a probability of taking</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>specific values.</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>It will have to be something
a little different.</text_slice>
            </slice>
            <slice>
              <time_slice>12:36</time_slice>
              <text_slice>So here's how it goes.</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>Let's look at the probability
that X takes a specific value</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>that makes sense in the discrete
case, but for the</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>continuous random variable,
let's look at the probability</text_slice>
            </slice>
            <slice>
              <time_slice>12:50</time_slice>
              <text_slice>that it takes values in
some little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>12:53</time_slice>
              <text_slice>And now this probability of
two things happening, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>going to write it
as a product.</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>And I'm going to write
this as a product in</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>two different ways.</text_slice>
            </slice>
            <slice>
              <time_slice>13:01</time_slice>
              <text_slice>So one way is to say that this
is the probability that X</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>takes that value and then given
that X takes that value,</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>the probability that Y falls
inside that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>So this is our usual
multiplication rule for</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>multiplying probabilities, but
I can use the multiplication</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>rule also in a different way.</text_slice>
            </slice>
            <slice>
              <time_slice>13:27</time_slice>
              <text_slice>It's the probability
that Y falls in</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>the range of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>13:33</time_slice>
              <text_slice>And then the probability that X
takes the value of interest</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>given that Y satisfies
the first condition.</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>So this is something that's
definitely true.</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>We're just using the
multiplication rule.</text_slice>
            </slice>
            <slice>
              <time_slice>13:57</time_slice>
              <text_slice>And now let's translate it
into PMF is PDF notation.</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>So the entry up there is the
PMF of X evaluated at x.</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>The second entry, what is it?</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>Well probabilities of
little intervals are</text_slice>
            </slice>
            <slice>
              <time_slice>14:12</time_slice>
              <text_slice>given to us by densities.</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>But we are in the conditional
universe where X takes on a</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>particular value.</text_slice>
            </slice>
            <slice>
              <time_slice>14:20</time_slice>
              <text_slice>So it's going to be the density
of Y given the value</text_slice>
            </slice>
            <slice>
              <time_slice>14:27</time_slice>
              <text_slice>of X times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:30</time_slice>
              <text_slice>So probabilities of little
intervals are given by the</text_slice>
            </slice>
            <slice>
              <time_slice>14:32</time_slice>
              <text_slice>density times the length of
the little interval, but</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>because we're working in the
conditional universe, it has</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>to be the conditional density.</text_slice>
            </slice>
            <slice>
              <time_slice>14:41</time_slice>
              <text_slice>Now let's try the second
expression.</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>This is the probability
that the Y falls</text_slice>
            </slice>
            <slice>
              <time_slice>14:46</time_slice>
              <text_slice>into the little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>So that's the density
of Y times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:51</time_slice>
              <text_slice>And then here we have an
object which is the</text_slice>
            </slice>
            <slice>
              <time_slice>14:53</time_slice>
              <text_slice>conditional probability X in a
universe where the value of Y</text_slice>
            </slice>
            <slice>
              <time_slice>14:59</time_slice>
              <text_slice>is given to us.</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>Now this relation is sort
of approximate.</text_slice>
            </slice>
            <slice>
              <time_slice>15:08</time_slice>
              <text_slice>This is true for very small
delta in the limit.</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>But we can cancel the deltas
from both sides, and we're</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>left with a formula that links
together PMFs and PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>Now this may look terribly
confusing because there's both</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>p's and f's involved.</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>But the logic should be clear.</text_slice>
            </slice>
            <slice>
              <time_slice>15:29</time_slice>
              <text_slice>If a random variable
is discrete, it's</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>described by PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>15:34</time_slice>
              <text_slice>So here we're talking about
the PMF of X in some</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>particular universe.</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>X is discrete, so
it has a PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>15:41</time_slice>
              <text_slice>Similarly here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>Y is continuous so it's
described by a PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>15:45</time_slice>
              <text_slice>And even in the conditional
universe where I tell you the</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>value of X, Y is still a
continuous random variable, so</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>it's been described by a PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>So this is the basic
relation that links</text_slice>
            </slice>
            <slice>
              <time_slice>15:55</time_slice>
              <text_slice>together PMF and PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>In this mixed the world.</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>And now in this inequality,
you can take this term and</text_slice>
            </slice>
            <slice>
              <time_slice>16:04</time_slice>
              <text_slice>send it to the new denominator
to the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>And what you end up with
is the formula</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>that we have up here.</text_slice>
            </slice>
            <slice>
              <time_slice>16:11</time_slice>
              <text_slice>And this is a formula that we
can use to make inferences</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>about the discrete random
variable X when we're told the</text_slice>
            </slice>
            <slice>
              <time_slice>16:18</time_slice>
              <text_slice>value of the continuous random
variable Y. The probability</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>that X takes on a particular
value has something</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>to do with the prior.</text_slice>
            </slice>
            <slice>
              <time_slice>16:31</time_slice>
              <text_slice>And other than that, it's
proportional to this quantity,</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>the conditional of Y given X.
So these are the quantities</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>that we plotted here.</text_slice>
            </slice>
            <slice>
              <time_slice>16:43</time_slice>
              <text_slice>Suppose that the x's are equally
likely in your prior,</text_slice>
            </slice>
            <slice>
              <time_slice>16:47</time_slice>
              <text_slice>so we don't really care
about that term.</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>It tells us that the posterior
of X is proportional to that</text_slice>
            </slice>
            <slice>
              <time_slice>16:55</time_slice>
              <text_slice>particular density under
the given x's.</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>So in this picture, if I were to
get a particular y here, I</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>would say that x equals 1
has a probability that's</text_slice>
            </slice>
            <slice>
              <time_slice>17:07</time_slice>
              <text_slice>proportional to this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:09</time_slice>
              <text_slice>x equals 0 has a probability
that's</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>proportional to this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>So the ratio of these two
quantities gives us the</text_slice>
            </slice>
            <slice>
              <time_slice>17:16</time_slice>
              <text_slice>relative odds of the different
x's given the y</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>that we have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>So we're going to come back to
this topic and redo plenty of</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>examples of these kinds towards
the end of the class,</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>when we spend some
time dedicated</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>to inference problems.</text_slice>
            </slice>
            <slice>
              <time_slice>17:36</time_slice>
              <text_slice>But already at this stage, we
sort of have the basic skills</text_slice>
            </slice>
            <slice>
              <time_slice>17:39</time_slice>
              <text_slice>to deal with a lot of that.</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>And it's useful at this
point to pull all</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>the formulas together.</text_slice>
            </slice>
            <slice>
              <time_slice>17:45</time_slice>
              <text_slice>So finally let's look at the
last case that's remaining.</text_slice>
            </slice>
            <slice>
              <time_slice>17:49</time_slice>
              <text_slice>Here we have a continuous
phenomenon that we're trying</text_slice>
            </slice>
            <slice>
              <time_slice>17:54</time_slice>
              <text_slice>to measure, but our measurements
are discrete.</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>What's an example where
this might happen?</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>So you have some device that
emits light, and you drive it</text_slice>
            </slice>
            <slice>
              <time_slice>18:05</time_slice>
              <text_slice>with a current that has
a certain intensity.</text_slice>
            </slice>
            <slice>
              <time_slice>18:07</time_slice>
              <text_slice>You don't know what that
current is, and it's a</text_slice>
            </slice>
            <slice>
              <time_slice>18:09</time_slice>
              <text_slice>continuous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>But the device emits
light by sending</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>out individual photons.</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>And your measurement is some
other device that counts how</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>many photons did you get
in a single second.</text_slice>
            </slice>
            <slice>
              <time_slice>18:23</time_slice>
              <text_slice>So if we have devices that emit
a very low intensity you</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>can actually start counting
individual photons as they're</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>being observed.</text_slice>
            </slice>
            <slice>
              <time_slice>18:32</time_slice>
              <text_slice>So we have a discrete
measurement, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>18:35</time_slice>
              <text_slice>number of problems, and we
have a continuous hidden</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>random variable that we're
trying to estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>18:43</time_slice>
              <text_slice>What do we do in this case?</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>Well we start again with a
formula of this kind, and send</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>the p term to the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>And that's the formula that we
use there, except that the</text_slice>
            </slice>
            <slice>
              <time_slice>18:58</time_slice>
              <text_slice>roles of x's and y's
are interchanged.</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>So since here we have Y being
discrete, we should change all</text_slice>
            </slice>
            <slice>
              <time_slice>19:06</time_slice>
              <text_slice>the subscripts.</text_slice>
            </slice>
            <slice>
              <time_slice>19:07</time_slice>
              <text_slice>It would be p_Y f_X given
y f_X, and P(Y given X).</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>So just change all
those subscripts.</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>Because now what we're used to
be continuous became discrete,</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>and vice versa.</text_slice>
            </slice>
            <slice>
              <time_slice>19:25</time_slice>
              <text_slice>Take that formula, send
the other terms to the</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>denominator, and we have a
formula for the density, or X,</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>given the particular
measurements for Y that we</text_slice>
            </slice>
            <slice>
              <time_slice>19:34</time_slice>
              <text_slice>have obtained.</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>In some sense that's all there
is in Bayesian inference.</text_slice>
            </slice>
            <slice>
              <time_slice>19:41</time_slice>
              <text_slice>It's using these very simple
one line formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>But why are there people then
who make their living solving</text_slice>
            </slice>
            <slice>
              <time_slice>19:51</time_slice>
              <text_slice>inference problems?</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>Well, the devil is
in the details.</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>As we're going to discuss,
there are some real world</text_slice>
            </slice>
            <slice>
              <time_slice>19:57</time_slice>
              <text_slice>issues of how exactly do you
design your f's, how do you</text_slice>
            </slice>
            <slice>
              <time_slice>20:01</time_slice>
              <text_slice>model your system, then how do
you do your calculations.</text_slice>
            </slice>
            <slice>
              <time_slice>20:04</time_slice>
              <text_slice>This might not be always easy.</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>For example, there's certain
integrals or sums that have to</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>be evaluated, which may be
hard to do and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>So this object is
a lot of richer</text_slice>
            </slice>
            <slice>
              <time_slice>20:14</time_slice>
              <text_slice>than just these formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>On the other hand, at the
conceptual level, that's the</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>basis for Bayesian inference,
that these</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>are the basic concepts.</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>All right, so now let's change
gear and move to the new</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>subject, which is the topic of
finding the distribution of a</text_slice>
            </slice>
            <slice>
              <time_slice>20:36</time_slice>
              <text_slice>functional for a random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>We call those distributions
derived distributions, because</text_slice>
            </slice>
            <slice>
              <time_slice>20:42</time_slice>
              <text_slice>we're given the distribution
of X. We're interested in a</text_slice>
            </slice>
            <slice>
              <time_slice>20:45</time_slice>
              <text_slice>function of X. We want to derive
the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>that function based on
the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>that we already know.</text_slice>
            </slice>
            <slice>
              <time_slice>20:53</time_slice>
              <text_slice>So it could be a function of
just one random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>20:56</time_slice>
              <text_slice>It could be a function of
several random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>20:59</time_slice>
              <text_slice>So one example that we are going
to solve at some point,</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>let's say you have to run the
variables X and Y. Somebody</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>tells you their distribution,
for example, is a uniform of</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>the square.</text_slice>
            </slice>
            <slice>
              <time_slice>21:10</time_slice>
              <text_slice>For some reason, you're
interested in the ratio of</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>these two random variables,
and you want to find the</text_slice>
            </slice>
            <slice>
              <time_slice>21:14</time_slice>
              <text_slice>distribution of that ratio.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>You can think of lots of cases
where your random variable of</text_slice>
            </slice>
            <slice>
              <time_slice>21:21</time_slice>
              <text_slice>interest is created by taking
some other unknown variables</text_slice>
            </slice>
            <slice>
              <time_slice>21:25</time_slice>
              <text_slice>and taking a function of them.</text_slice>
            </slice>
            <slice>
              <time_slice>21:27</time_slice>
              <text_slice>And so it's legitimate to care
about the distribution of that</text_slice>
            </slice>
            <slice>
              <time_slice>21:31</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>21:33</time_slice>
              <text_slice>A caveat, however.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>There's an important case where
you don't need to find</text_slice>
            </slice>
            <slice>
              <time_slice>21:39</time_slice>
              <text_slice>the distribution of that
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>21:41</time_slice>
              <text_slice>And this is when you want to
calculate the expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>21:44</time_slice>
              <text_slice>If all you care about is the
expected value of this</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>function of the random
variables, you can work</text_slice>
            </slice>
            <slice>
              <time_slice>21:50</time_slice>
              <text_slice>directly with the distribution
of the original random</text_slice>
            </slice>
            <slice>
              <time_slice>21:53</time_slice>
              <text_slice>variables without ever having
to find the PDF of g.</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>So you don't do unnecessary work
if it's not needed, but</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>if it's needed, or if you're
asked to do it,</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>then you just do it.</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>So how do we find the
distribution of the function?</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>As a warm-up, let's look
at the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>22:17</time_slice>
              <text_slice>Suppose that X is a discrete
random variable and takes</text_slice>
            </slice>
            <slice>
              <time_slice>22:21</time_slice>
              <text_slice>certain values.</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>We have a function g that
maps x's into y's.</text_slice>
            </slice>
            <slice>
              <time_slice>22:27</time_slice>
              <text_slice>And we want to find the
probability mass function for</text_slice>
            </slice>
            <slice>
              <time_slice>22:30</time_slice>
              <text_slice>Y.</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>So for example, if I'm
interested in finding the</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>probability that Y takes on
this particular value, how</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>would they find it?</text_slice>
            </slice>
            <slice>
              <time_slice>22:42</time_slice>
              <text_slice>Well I ask, what are the
different ways that these</text_slice>
            </slice>
            <slice>
              <time_slice>22:46</time_slice>
              <text_slice>particular y value can happen?</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>And the different ways that it
can happen is either if x</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>takes this value, or if
X takes that value.</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>So we identify this event in the
y space with that event in</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>the x space.</text_slice>
            </slice>
            <slice>
              <time_slice>23:04</time_slice>
              <text_slice>These two events
are identical.</text_slice>
            </slice>
            <slice>
              <time_slice>23:06</time_slice>
              <text_slice>X falls in this set if and only
if Y falls in that set.</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>Therefore, the probability of
Y falling in that set is the</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>probability of X falling
in that set.</text_slice>
            </slice>
            <slice>
              <time_slice>23:17</time_slice>
              <text_slice>The probability of X falling in
that set is just the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>the individual probabilities
of the x's in this set.</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>So we just add the probabilities
of the different</text_slice>
            </slice>
            <slice>
              <time_slice>23:27</time_slice>
              <text_slice>x's where the summation is taken
over all x's that leads</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>to that particular value of y.</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>Very good.</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>So that's all there is
in the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>23:39</time_slice>
              <text_slice>It's a very nice and simple.</text_slice>
            </slice>
            <slice>
              <time_slice>23:41</time_slice>
              <text_slice>So let's transfer
these methods to</text_slice>
            </slice>
            <slice>
              <time_slice>23:43</time_slice>
              <text_slice>the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>Suppose we are in the
continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>23:47</time_slice>
              <text_slice>Suppose that X and Y now can
take values anywhere.</text_slice>
            </slice>
            <slice>
              <time_slice>23:52</time_slice>
              <text_slice>And I try to use same methods
and I ask, what is the</text_slice>
            </slice>
            <slice>
              <time_slice>23:55</time_slice>
              <text_slice>probability that Y is going
to take this value?</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>At least if the diagram is this
way, you would say this</text_slice>
            </slice>
            <slice>
              <time_slice>24:03</time_slice>
              <text_slice>is the same as the probability
that X takes this value.</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>So I can find the probability
of Y being this in terms of</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>the probability of
X being that.</text_slice>
            </slice>
            <slice>
              <time_slice>24:12</time_slice>
              <text_slice>Is this useful?</text_slice>
            </slice>
            <slice>
              <time_slice>24:14</time_slice>
              <text_slice>In the continuous
case, it's not.</text_slice>
            </slice>
            <slice>
              <time_slice>24:16</time_slice>
              <text_slice>Because in the continuous case,
any single value has 0</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>So what you're going to get out
of this argument is that</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>the probability Y takes this
value is 0, is equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>24:29</time_slice>
              <text_slice>probability that X takes that
value which also 0.</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>That doesn't help us.</text_slice>
            </slice>
            <slice>
              <time_slice>24:34</time_slice>
              <text_slice>We want to do something more.</text_slice>
            </slice>
            <slice>
              <time_slice>24:36</time_slice>
              <text_slice>We want to actually find,
perhaps, the density of Y, as</text_slice>
            </slice>
            <slice>
              <time_slice>24:40</time_slice>
              <text_slice>opposed to the probabilities
of individual y's.</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>So to find the density of Y,
you might argue as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>I'm looking at an interval for
y, and I ask what's the</text_slice>
            </slice>
            <slice>
              <time_slice>24:51</time_slice>
              <text_slice>probability of falling
in this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>And you go back and find the
corresponding set of x's that</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>leads to those y's, and equate
those two probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>25:02</time_slice>
              <text_slice>The probability of all of those
y's collectively should</text_slice>
            </slice>
            <slice>
              <time_slice>25:04</time_slice>
              <text_slice>be equal to the probability of
all of the x's that map into</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>that interval collectively.</text_slice>
            </slice>
            <slice>
              <time_slice>25:11</time_slice>
              <text_slice>And this way you can
relate the two.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>As far as the mechanics go, in
many cases it's easier to not</text_slice>
            </slice>
            <slice>
              <time_slice>25:22</time_slice>
              <text_slice>to work with little intervals,
but instead to work with</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>cumulative distribution
functions that used to work</text_slice>
            </slice>
            <slice>
              <time_slice>25:30</time_slice>
              <text_slice>with sort of big intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>25:32</time_slice>
              <text_slice>So you can instead do
a different picture.</text_slice>
            </slice>
            <slice>
              <time_slice>25:35</time_slice>
              <text_slice>Look at this set of y's.</text_slice>
            </slice>
            <slice>
              <time_slice>25:38</time_slice>
              <text_slice>This is the set of y's
that are smaller</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>than a certain value.</text_slice>
            </slice>
            <slice>
              <time_slice>25:43</time_slice>
              <text_slice>The probability of this set
is given by the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>25:46</time_slice>
              <text_slice>distribution of the
random variable Y.</text_slice>
            </slice>
            <slice>
              <time_slice>25:49</time_slice>
              <text_slice>Now this set of y's gets
produced by some corresponding</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>set of x's.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>Maybe these are the x's that
map into y's in that set.</text_slice>
            </slice>
            <slice>
              <time_slice>26:04</time_slice>
              <text_slice>And then we argue as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>26:06</time_slice>
              <text_slice>The probability that the Y falls
in this interval is the</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>same as the probability that
X falls in that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>So the event of Y falling here
and the event of X falling</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>there are the same, so their
probabilities must be equal.</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>And then I do the calculations
here.</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>And I end up getting the
cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>26:25</time_slice>
              <text_slice>function of Y. Once I have the
cumulative, I can get the</text_slice>
            </slice>
            <slice>
              <time_slice>26:28</time_slice>
              <text_slice>density by just differentiating.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>So this is the general cookbook
procedure that we</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>will be using to calculate
it derived distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>We're interested in a random
variable Y, which is a</text_slice>
            </slice>
            <slice>
              <time_slice>26:43</time_slice>
              <text_slice>function of the x's.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>We will aim at obtaining the
cumulative distribution of Y.</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>Somehow, manage to calculate the
probability of this event.</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>Once we get it, and what I mean
by get it, I don't mean</text_slice>
            </slice>
            <slice>
              <time_slice>26:58</time_slice>
              <text_slice>getting it for a single
value of little y.</text_slice>
            </slice>
            <slice>
              <time_slice>27:00</time_slice>
              <text_slice>You need to get this
for all little y's.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>So you need to get the
function itself, the</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>cumulative distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>Once you get it in that form,
then you can calculate the</text_slice>
            </slice>
            <slice>
              <time_slice>27:12</time_slice>
              <text_slice>derivative at any particular
point.</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>And this is going to give
you the density of Y.</text_slice>
            </slice>
            <slice>
              <time_slice>27:18</time_slice>
              <text_slice>So a simple two-step
procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>The devil is in the details of
how you carry the mechanics.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>So let's do one first example.</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>Suppose that X is a uniform
random variable, takes values</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>between 0 and 2.</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>We're interested in the random
variable Y, which is the cube</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>of X. What kind of distribution</text_slice>
            </slice>
            <slice>
              <time_slice>27:37</time_slice>
              <text_slice>is it going to have?</text_slice>
            </slice>
            <slice>
              <time_slice>27:38</time_slice>
              <text_slice>Now first notice that Y takes
values between 0 and 8.</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>So X is uniform, so all the
x's are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>27:51</time_slice>
              <text_slice>You might then say, well, in
that case, all the y's should</text_slice>
            </slice>
            <slice>
              <time_slice>27:55</time_slice>
              <text_slice>be equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>So Y might also have a
uniform distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>Is this true?</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>We'll find out.</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>So let's start applying the
cookbook procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>28:06</time_slice>
              <text_slice>We want to find first the
cumulative distribution of the</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>random variable Y, which by
definition is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>that the random variable is
less than or equal to a</text_slice>
            </slice>
            <slice>
              <time_slice>28:17</time_slice>
              <text_slice>certain number.</text_slice>
            </slice>
            <slice>
              <time_slice>28:18</time_slice>
              <text_slice>That's what we want to find.</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>What we have in our hands is the
distribution of X. That's</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>what we need to work with.</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>So the first step that you need
to do is to look at this</text_slice>
            </slice>
            <slice>
              <time_slice>28:30</time_slice>
              <text_slice>events and translate it, and
write it in terms of the</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>random variable about which you
know you have information.</text_slice>
            </slice>
            <slice>
              <time_slice>28:39</time_slice>
              <text_slice>So Y is X cubed, so this event
is the same as that event.</text_slice>
            </slice>
            <slice>
              <time_slice>28:44</time_slice>
              <text_slice>So now we can forget
about the y's.</text_slice>
            </slice>
            <slice>
              <time_slice>28:46</time_slice>
              <text_slice>It's just an exercise involving
a single random</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>variable with a known
distribution and we want to</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>calculate the probability
of some event.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>So we're looking
at this event.</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>X cubed being less than or equal
to Y. We massage that</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>expression so that's it involves
X directly, so let's</text_slice>
            </slice>
            <slice>
              <time_slice>29:06</time_slice>
              <text_slice>take cubic roots of both sides
of this inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>This event is the same as the
event that X is less than or</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>equal to Y to the 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>Now with a uniform distribution
on [0,2], what is</text_slice>
            </slice>
            <slice>
              <time_slice>29:19</time_slice>
              <text_slice>that probability going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>29:22</time_slice>
              <text_slice>It's the probability of being in
the interval from 0 to y to</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>the 1/3, so it's going to be in
the area under the uniform</text_slice>
            </slice>
            <slice>
              <time_slice>29:34</time_slice>
              <text_slice>going up to that point.</text_slice>
            </slice>
            <slice>
              <time_slice>29:37</time_slice>
              <text_slice>And what's the area under
that uniform?</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>So here's x.</text_slice>
            </slice>
            <slice>
              <time_slice>29:44</time_slice>
              <text_slice>Here is the distribution
of X. It goes up to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>The distribution of
X is this one.</text_slice>
            </slice>
            <slice>
              <time_slice>29:53</time_slice>
              <text_slice>We want to go up to
y to the 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>29:56</time_slice>
              <text_slice>So the probability for this
event happening is this area.</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>And the area is equal to the
base, which is y to the 1/3</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>times the height.</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>What is the height?</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>Well since the density must
integrate to 1, the total area</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>under the curve has to be 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>So the height here is 1/2, and
that explains why we get the</text_slice>
            </slice>
            <slice>
              <time_slice>30:19</time_slice>
              <text_slice>1/2 factor down there.</text_slice>
            </slice>
            <slice>
              <time_slice>30:22</time_slice>
              <text_slice>So that's the formula for the
cumulative distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>And then the rest is easy.</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>You just take derivatives.</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>You differentiate this
expression with respect to y</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>1/2 times 1/3, and y
drops by one power.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>So you get y to 2/3 in
the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>So if you wish to plot this,
it's 1/y to the 2/3.</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>So when y goes to 0, it sort
of blows up and it</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>goes on this way.</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>Is this picture correct
the way I've drawn it?</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>What's wrong with it?</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>[? AUDIENCE:  Something. ?]</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>PROFESSOR: Yes.</text_slice>
            </slice>
            <slice>
              <time_slice>31:13</time_slice>
              <text_slice>y only takes values
from 0 to 8.</text_slice>
            </slice>
            <slice>
              <time_slice>31:17</time_slice>
              <text_slice>This formula that I wrote here
is only correct when the</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>preview picture applies.</text_slice>
            </slice>
            <slice>
              <time_slice>31:25</time_slice>
              <text_slice>I took my y to the 1/3 to
be between 0 and 2.</text_slice>
            </slice>
            <slice>
              <time_slice>31:31</time_slice>
              <text_slice>So this formula here is only
correct for y between 0 and 8.</text_slice>
            </slice>
            <slice>
              <time_slice>31:43</time_slice>
              <text_slice>And for that reason, the formula
for the derivative is</text_slice>
            </slice>
            <slice>
              <time_slice>31:46</time_slice>
              <text_slice>also true only for a
y between 0 and 8.</text_slice>
            </slice>
            <slice>
              <time_slice>31:50</time_slice>
              <text_slice>And any other values of why are
impossible, so they get</text_slice>
            </slice>
            <slice>
              <time_slice>31:55</time_slice>
              <text_slice>zero density.</text_slice>
            </slice>
            <slice>
              <time_slice>31:57</time_slice>
              <text_slice>So to complete the picture
here, the PDF of y has a</text_slice>
            </slice>
            <slice>
              <time_slice>32:04</time_slice>
              <text_slice>cut-off of 8, and it's also
0 everywhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>And one thing that we see is
that the distribution of Y is</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>not uniform.</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>Certain y's are more likely than
others, even though we</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>started with a uniform random</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>variable X. All right.</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>So we will keep doing examples
of this kind, a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>32:36</time_slice>
              <text_slice>progressively more interesting
or more complicated.</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>So that's going to continue
in the next lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>You're going to see plenty of
examples in your recitations</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>and tutorials and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>So let's do one that's pretty
similar to the one that we</text_slice>
            </slice>
            <slice>
              <time_slice>32:52</time_slice>
              <text_slice>did, but it's going to add to
just a small twist in how we</text_slice>
            </slice>
            <slice>
              <time_slice>32:57</time_slice>
              <text_slice>do the mechanics.</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>OK so you set your
cruise control</text_slice>
            </slice>
            <slice>
              <time_slice>33:02</time_slice>
              <text_slice>when you start driving.</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>And you keep driving at the
constants based at the</text_slice>
            </slice>
            <slice>
              <time_slice>33:06</time_slice>
              <text_slice>constant speed.</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>Where you set your cruise
control is somewhere</text_slice>
            </slice>
            <slice>
              <time_slice>33:09</time_slice>
              <text_slice>between 30 and 60.</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>You're going to drive
a distance of 200.</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>And so the time it's going to
take for your trip is 200 over</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>the setting of your
cruise control.</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>So it's 200/V.</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>Somebody gives you the
distribution of V, and they</text_slice>
            </slice>
            <slice>
              <time_slice>33:26</time_slice>
              <text_slice>tell you not only it's between
30 and 60, it's roughly</text_slice>
            </slice>
            <slice>
              <time_slice>33:29</time_slice>
              <text_slice>equally likely to be anything
between 30 and 60, so we have</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>a uniform distribution
over that range.</text_slice>
            </slice>
            <slice>
              <time_slice>33:36</time_slice>
              <text_slice>So we have a distribution of
V. We want to find the</text_slice>
            </slice>
            <slice>
              <time_slice>33:40</time_slice>
              <text_slice>distribution of the random
variable T, which is the time</text_slice>
            </slice>
            <slice>
              <time_slice>33:43</time_slice>
              <text_slice>it takes till your trip ends.</text_slice>
            </slice>
            <slice>
              <time_slice>33:49</time_slice>
              <text_slice>So how are we going
to proceed?</text_slice>
            </slice>
            <slice>
              <time_slice>33:51</time_slice>
              <text_slice>We'll use the exact same
cookbook procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>We're going to start by
finding the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>distribution of T.
What is this?</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>By definition, the cumulative
distribution is the</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>probability that T is less
than a certain number.</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>Now we don't know the
distribution of T, so we</text_slice>
            </slice>
            <slice>
              <time_slice>34:15</time_slice>
              <text_slice>cannot to work with these
event directly.</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>But we take that event and
translate it into T-space.</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>So we replace the t's by what we
know T to be in terms of V</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>or</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>the v's All right.</text_slice>
            </slice>
            <slice>
              <time_slice>34:36</time_slice>
              <text_slice>So we have the distribution
of V. So now let's</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>calculate this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:42</time_slice>
              <text_slice>Let's massage this event and
rewrite it as the probability</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>that V is larger or
equal to 200/T.</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>So what is this going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>So let's say that 200/T
is some number that</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>falls inside the range.</text_slice>
            </slice>
            <slice>
              <time_slice>35:19</time_slice>
              <text_slice>So that's going to be true if
200/T is bigger than 30, and</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>less than 60.</text_slice>
            </slice>
            <slice>
              <time_slice>35:26</time_slice>
              <text_slice>Which means that t is
less than 30/200.</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>No, 200/30.</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>And bigger than 200/60.</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>So for t's inside that range,
this number 200/t falls inside</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>that range.</text_slice>
            </slice>
            <slice>
              <time_slice>35:52</time_slice>
              <text_slice>This is the range of t's that
are possible, given the</text_slice>
            </slice>
            <slice>
              <time_slice>35:55</time_slice>
              <text_slice>description of the problem
the we have set up.</text_slice>
            </slice>
            <slice>
              <time_slice>35:59</time_slice>
              <text_slice>So for t's in that range, what
is the probability that V is</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>bigger than this number?</text_slice>
            </slice>
            <slice>
              <time_slice>36:07</time_slice>
              <text_slice>So V being bigger than that
number is the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>this event, so it's going to be
the area under this curve.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>So the area under that curve
is the height of the curve,</text_slice>
            </slice>
            <slice>
              <time_slice>36:22</time_slice>
              <text_slice>which is 1/3 over 30
times the base.</text_slice>
            </slice>
            <slice>
              <time_slice>36:27</time_slice>
              <text_slice>How big is the base?</text_slice>
            </slice>
            <slice>
              <time_slice>36:28</time_slice>
              <text_slice>Well it's from that point to 60,
so the base has a length</text_slice>
            </slice>
            <slice>
              <time_slice>36:33</time_slice>
              <text_slice>of 60 minus 200/t.</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>And this is a formula which is
valid for those t's for which</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>this picture is correct.</text_slice>
            </slice>
            <slice>
              <time_slice>36:52</time_slice>
              <text_slice>And this picture is correct if
200/T happens to fall in this</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>interval, which is the same as
T falling in that interval,</text_slice>
            </slice>
            <slice>
              <time_slice>37:01</time_slice>
              <text_slice>which are the t's that
are possible.</text_slice>
            </slice>
            <slice>
              <time_slice>37:03</time_slice>
              <text_slice>So finally let's find the
density of T, which is what</text_slice>
            </slice>
            <slice>
              <time_slice>37:07</time_slice>
              <text_slice>we're looking for.</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>We find this by taking the
derivative in this expression</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>with respect to t.</text_slice>
            </slice>
            <slice>
              <time_slice>37:14</time_slice>
              <text_slice>We only get one term
from here.</text_slice>
            </slice>
            <slice>
              <time_slice>37:18</time_slice>
              <text_slice>And this is going to be 200/30,
1 over t squared.</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>And this is the formula for
the density for t's in the</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>allowed to range.</text_slice>
            </slice>
            <slice>
              <time_slice>37:46</time_slice>
              <text_slice>OK, so that's the end of the
solution to this particular</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>problem as well.</text_slice>
            </slice>
            <slice>
              <time_slice>37:52</time_slice>
              <text_slice>I said that there was a little
twist compared to</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>the previous one.</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>What was the twist?</text_slice>
            </slice>
            <slice>
              <time_slice>37:58</time_slice>
              <text_slice>Well the twist was that in the
previous problem we dealt with</text_slice>
            </slice>
            <slice>
              <time_slice>38:01</time_slice>
              <text_slice>the X cubed function, which was
monotonically increasing.</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>Here we dealt with the
function that was</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>monotonically decreasing.</text_slice>
            </slice>
            <slice>
              <time_slice>38:09</time_slice>
              <text_slice>So when we had to find the
probability that T is less</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>than something, that translated
into an event that</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>V was bigger than something.</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>Your time is less than something
if and only if your</text_slice>
            </slice>
            <slice>
              <time_slice>38:22</time_slice>
              <text_slice>velocity is bigger
than something.</text_slice>
            </slice>
            <slice>
              <time_slice>38:25</time_slice>
              <text_slice>So for when you're dealing
with the monotonically</text_slice>
            </slice>
            <slice>
              <time_slice>38:27</time_slice>
              <text_slice>decreasing function, at some
point some inequalities will</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>have to get reversed.</text_slice>
            </slice>
            <slice>
              <time_slice>38:38</time_slice>
              <text_slice>Finally let's look at
a very useful one.</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>Which is the case where we take
a linear function of a</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>38:49</time_slice>
              <text_slice>So X is a random variable with
given distribution, and we can</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>see there is a linear
function.</text_slice>
            </slice>
            <slice>
              <time_slice>38:57</time_slice>
              <text_slice>So in this particular instance,
we take a to be</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>equal to 2 and b equal to 5.</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>And let us first argue
just by picture.</text_slice>
            </slice>
            <slice>
              <time_slice>39:08</time_slice>
              <text_slice>So X is a random variable that
has a given distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>39:13</time_slice>
              <text_slice>Let's say it's this
weird shape here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:16</time_slice>
              <text_slice>And x ranges from -1 to +2.</text_slice>
            </slice>
            <slice>
              <time_slice>39:20</time_slice>
              <text_slice>Let's do things one
step at the time.</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>Let's first find the
distribution of 2X.</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>Why do you think you
know about 2X?</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>Well if x ranges from -1 to 2,
then the random variable X is</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>going to range from -2 to +4.</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>So that's what the range
is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>Now dealing with the random
variable 2X, as opposed to the</text_slice>
            </slice>
            <slice>
              <time_slice>39:48</time_slice>
              <text_slice>random variable X, in some sense
it's just changing the</text_slice>
            </slice>
            <slice>
              <time_slice>39:52</time_slice>
              <text_slice>units in which we measure
that random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>39:55</time_slice>
              <text_slice>It's just changing the
scale on which we</text_slice>
            </slice>
            <slice>
              <time_slice>39:58</time_slice>
              <text_slice>draw and plot things.</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>So if it's just a scale change,
then intuition should</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>tell you that the random
variable X should have a PDF</text_slice>
            </slice>
            <slice>
              <time_slice>40:08</time_slice>
              <text_slice>of the same shape, except that
it's scaled out by a factor of</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>2, because our random variable
of 2X now has a range that's</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>twice as large.</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>So we take the same PDF and
scale it up by stretching the</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>x-axis by a factor of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>So what does scaling
correspond to</text_slice>
            </slice>
            <slice>
              <time_slice>40:30</time_slice>
              <text_slice>in terms of a formula?</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>So the distribution of 2X as a
function, let's say, a generic</text_slice>
            </slice>
            <slice>
              <time_slice>40:39</time_slice>
              <text_slice>argument z, is going to be the
distribution of X, but scaled</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>by a factor of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>40:50</time_slice>
              <text_slice>So taking a function and
replacing its arguments by the</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>argument over 2, what it
does is it stretches it</text_slice>
            </slice>
            <slice>
              <time_slice>40:58</time_slice>
              <text_slice>by a factor of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>You have probably been tortured
ever since middle</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>school to figure out when need
to stretch a function, whether</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>you need to put 2z or z/2.</text_slice>
            </slice>
            <slice>
              <time_slice>41:12</time_slice>
              <text_slice>And the one that actually does
the stretching is to put the</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>z/2 in that place.</text_slice>
            </slice>
            <slice>
              <time_slice>41:18</time_slice>
              <text_slice>So that's what the
stretching does.</text_slice>
            </slice>
            <slice>
              <time_slice>41:21</time_slice>
              <text_slice>Could that to be the
full answer?</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>Well there's a catch.</text_slice>
            </slice>
            <slice>
              <time_slice>41:24</time_slice>
              <text_slice>If you stretch this function by
a factor of 2, what happens</text_slice>
            </slice>
            <slice>
              <time_slice>41:29</time_slice>
              <text_slice>to the area under
the function?</text_slice>
            </slice>
            <slice>
              <time_slice>41:32</time_slice>
              <text_slice>It's going to get doubled.</text_slice>
            </slice>
            <slice>
              <time_slice>41:34</time_slice>
              <text_slice>But the total probability must
add up to 1, so we need to do</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>something else to make sure that
the area under the curve</text_slice>
            </slice>
            <slice>
              <time_slice>41:41</time_slice>
              <text_slice>stays to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>41:44</time_slice>
              <text_slice>So we need to take that function
and scale it down by</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>this factor of 2.</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>So when you're dealing with a
multiple of a random variable,</text_slice>
            </slice>
            <slice>
              <time_slice>41:55</time_slice>
              <text_slice>what happens to the PDF is you
stretch it according to the</text_slice>
            </slice>
            <slice>
              <time_slice>42:00</time_slice>
              <text_slice>multiple, and then scale it
down by the same number so</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>that you preserve the area
under that curve.</text_slice>
            </slice>
            <slice>
              <time_slice>42:07</time_slice>
              <text_slice>So now we found the distribution
of 2X.</text_slice>
            </slice>
            <slice>
              <time_slice>42:10</time_slice>
              <text_slice>How about the distribution
of 2X + 5?</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>Well what does adding 5
to random variable do?</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>You're going to get essentially
the same values</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>with the same probability,
except that those values all</text_slice>
            </slice>
            <slice>
              <time_slice>42:23</time_slice>
              <text_slice>get shifted by 5.</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>So all that you need to do is
to take this PDF here, and</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>shift it by 5 units.</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>So the range used to
be from -2 to 4.</text_slice>
            </slice>
            <slice>
              <time_slice>42:35</time_slice>
              <text_slice>The new range is going
to be from 3 to 9.</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>And that's the final answer.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>This is the distribution of
2X + 5, starting with this</text_slice>
            </slice>
            <slice>
              <time_slice>42:44</time_slice>
              <text_slice>particular distribution of X.</text_slice>
            </slice>
            <slice>
              <time_slice>42:48</time_slice>
              <text_slice>Now shifting to the
right by b, what</text_slice>
            </slice>
            <slice>
              <time_slice>42:53</time_slice>
              <text_slice>does it do to a function?</text_slice>
            </slice>
            <slice>
              <time_slice>42:55</time_slice>
              <text_slice>Shifting to the right to
by a certain amount,</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>mathematically, it corresponds
to putting -b in the argument</text_slice>
            </slice>
            <slice>
              <time_slice>43:04</time_slice>
              <text_slice>of the function.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>So I'm taking the formula that
I had here, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>scaling by a factor of a.</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>The scaling down to keep the
total area equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>And then I need to introduce
this extra</text_slice>
            </slice>
            <slice>
              <time_slice>43:19</time_slice>
              <text_slice>term to do the shifting.</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>So this is a plausible
argument.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>The proof by picture that this
should be the right answer.</text_slice>
            </slice>
            <slice>
              <time_slice>43:31</time_slice>
              <text_slice>But just in order to keep our
skills tuned and refined, let</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>us do this derivation in a
more formal way using our</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>two-step cookbook procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>And I'm going to do it under
the assumption that a is</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>positive, as in the example
that's we just did.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>So what's the two-step
procedure?</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>We want to find the cumulative
of Y, and after that we're</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>going to differentiate.</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>By definition the cumulative
is the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>random variable takes values
less than a certain number.</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>And now we need to take this
event and translate it, and</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>express it in terms of the
original random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>So Y is, by definition,
aX + b, so we're</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>looking at this event.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>And now we want to express this
event in a clean form</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>where X shows up in
a straight way.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>Let's say I'm going to massage
this event and</text_slice>
            </slice>
            <slice>
              <time_slice>44:42</time_slice>
              <text_slice>write it in this form.</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>For this inequality to be true,
x should be less than or</text_slice>
            </slice>
            <slice>
              <time_slice>44:48</time_slice>
              <text_slice>equal to (y minus
b) divided by a.</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>OK, now what is this?</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>This is the cumulative
distribution of X evaluated at</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>the particular point.</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>So we got a formula for the
cumulative Y based on the</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>cumulative of X. What's
the next step?</text_slice>
            </slice>
            <slice>
              <time_slice>45:17</time_slice>
              <text_slice>Next step is to take derivatives
of both sides.</text_slice>
            </slice>
            <slice>
              <time_slice>45:21</time_slice>
              <text_slice>So the density of Y is going to
be the derivative of this</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>expression with respect to y.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>OK, so now here we need
to use the chain rule.</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>It's going to be the derivative
of the F function</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>with respect to its argument.</text_slice>
            </slice>
            <slice>
              <time_slice>45:43</time_slice>
              <text_slice>And then we need to take the
derivative of the argument</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>with respect to y.</text_slice>
            </slice>
            <slice>
              <time_slice>45:48</time_slice>
              <text_slice>What is the derivative
of the cumulative?</text_slice>
            </slice>
            <slice>
              <time_slice>45:51</time_slice>
              <text_slice>The derivative of
the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>45:53</time_slice>
              <text_slice>is the density itself.</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>And we evaluate it at the
point of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>And then the chain rule tells
us that we need to take the</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>derivative of this with
respect to y, and the</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>derivative of this with
respect to y is 1/a.</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>And this gives us the formula
which is consistent with what</text_slice>
            </slice>
            <slice>
              <time_slice>46:18</time_slice>
              <text_slice>I had written down here,
for the case where a</text_slice>
            </slice>
            <slice>
              <time_slice>46:21</time_slice>
              <text_slice>is a positive number.</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>What if a was a negative
number?</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>Could this formula be true?</text_slice>
            </slice>
            <slice>
              <time_slice>46:35</time_slice>
              <text_slice>Of course not.</text_slice>
            </slice>
            <slice>
              <time_slice>46:36</time_slice>
              <text_slice>Densities cannot be
negative, right?</text_slice>
            </slice>
            <slice>
              <time_slice>46:39</time_slice>
              <text_slice>So that formula cannot
be true.</text_slice>
            </slice>
            <slice>
              <time_slice>46:41</time_slice>
              <text_slice>Something needs to change.</text_slice>
            </slice>
            <slice>
              <time_slice>46:43</time_slice>
              <text_slice>What should change?</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>Where does this argument break
down when a is negative?</text_slice>
            </slice>
            <slice>
              <time_slice>46:56</time_slice>
              <text_slice>So when I write this inequality
in this form, I</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>divide by a.</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>But when you divide by a
negative number, the direction</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>of an inequality is
going to change.</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>So when a is negative, this
inequality becomes larger than</text_slice>
            </slice>
            <slice>
              <time_slice>47:14</time_slice>
              <text_slice>or equal to.</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>And in that case, the expression
that I have up</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>there would change when this
is larger than here.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>Instead of getting the
cumulative, I would get 1</text_slice>
            </slice>
            <slice>
              <time_slice>47:27</time_slice>
              <text_slice>minus the cumulative of (y
minus b) divided by a.</text_slice>
            </slice>
            <slice>
              <time_slice>47:35</time_slice>
              <text_slice>So this is the probability that
X is bigger than this</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>particular number.</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>And now when you take the
derivatives, there's going to</text_slice>
            </slice>
            <slice>
              <time_slice>47:44</time_slice>
              <text_slice>be a minus sign that shows up.</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>And that minus sign will
end up being here.</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>And so we're taking the negative
of a negative number,</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>and that basically is equivalent
to taking the</text_slice>
            </slice>
            <slice>
              <time_slice>47:56</time_slice>
              <text_slice>absolute value of that number.</text_slice>
            </slice>
            <slice>
              <time_slice>47:58</time_slice>
              <text_slice>So all that happens when we have
a negative a is that we</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>have to take the absolute value
of the scaling factor</text_slice>
            </slice>
            <slice>
              <time_slice>48:07</time_slice>
              <text_slice>instead of the factor itself.</text_slice>
            </slice>
            <slice>
              <time_slice>48:10</time_slice>
              <text_slice>All right, so this general
formula is quite useful for</text_slice>
            </slice>
            <slice>
              <time_slice>48:14</time_slice>
              <text_slice>dealing with linear functions
of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>And one nice application of it
is to take the formula for a</text_slice>
            </slice>
            <slice>
              <time_slice>48:21</time_slice>
              <text_slice>normal random variable, consider
a linear function of</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>a normal random variable, plug
into this formula, and what</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>you will find is that Y also
has a normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>48:34</time_slice>
              <text_slice>So using this formula, now we
can prove a statement that I</text_slice>
            </slice>
            <slice>
              <time_slice>48:37</time_slice>
              <text_slice>had made a couple of lectures
ago, that a linear function of</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>a normal random variable
is also linear.</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>That's how you would prove it.</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>I think this is it
for today so.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Derived Distributions; Convolution; Covariance and Correlation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Two independent normal r.v.s The sum of independent normal r.v.s
XN(2x,x),2YN(2y,y), XN(0,x),2YN(0,y),
independent independent
fX,Y(x, y)=f X(x)f Y(y)Let
1/braceleftBiggW=X+Y
( )2()2
x = exp x y  y
22/bracerightBigg
fW(w)=/integraldisplay
fX(x)f Y(wx)dx2xy 2x 2y 
12x /22= 2 2 x)x(w /2e eydx
PDF is constant on the ellipse where 2xy
2/integraldisplay

(x2x) ( (algebra) = ce y+22y)2w
x 22y
Conclusion:is constant Wis normal
mean=0, variance=2x+2yEllipse is a circle when x=y
same argument for nonzero mean case
Covariance Correlation coe cient
cov(X, Y )=E/bracketleftbigg
(XE[X])(YE[Y])/bracketrightbigg
Dimensionless version of covariance:
/bracketleftBigg
(XE[X]) (Y E[Y])Zero-mean case: cov( X, Y)=E[XY] =E 
XY/bracketrightBigg
x 
.cov(x X, Y).=
.
....XY
..
.
.
.
.. . .
....
.
...
.
............
.
.....
..... .
..
.. 11. ... ... .....
.. . .
.
..... .
.... .
..
... ..
..
.
...
.. ...
.
.
. .
...
.
. .. .
....
..
... . .y 
...
...
. y .
....
...
.|=1 (XE[X]) = c(Y E[Y]).. .
.
...
.
....
.|   ...
..
..(linearly related)..
...
.
.. Independent =0
cov(X, Y )=E[XY]E[X]E[Y] (converse is not true)
n
varn
 X )
i/summationdisplay
i
=1= var( Xi)+2 cov(X i,Xj
i/summationdisplay
=1 (i,j/summationdisplay
):i=j
independent cov(X, Y )=0
(converse is not true)/negationslash
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 11 A general formula
Derived distributions; convolution; LetY=g(X)
covariance and correlation gstrictly monotonic.
dg
Readings:slope (x)y dx  
Finish Section 4.1;
Section 4.2
g(x)
[y, y+ ?]
Example
y x
f (y,x)=1X,Y[x, x+ d]
1
Event xXx+is the same as
g(x)Yg(x+)
or (approximately)
1 x g(x)Yg(x)+ |(dg/dx )(x)|
Find the PDF of Z=g(X, Y )=Y/XHence,
dgfX(x)=fY(y)
(z z1/vextendsingle/vextendsingle/vextendsingle(x)
F )= dxZ/vextendsingle/vextendsingle/vextendsingle/vextendsingle
where y=g(x)/vextendsingle
FZ(z)= z1
The distribution of X+Y The continuous case
W=X+Y;X, Y independentW=X+Y;X, Y independent
.y y. 
(0,3)
.(1,2) w .(2,1)
.(3,0)
.x w x  
pW(w)=P(X+Y=w)
/summationdisplayx + y = w
= P(X=x)P(Y=wx)
=/summationdisplayx
p(x)p (wx) fW X(w |x)= fY(wx)X Y
x|
fW,X(w, x)=f X(x)fMechanics:W|X(w |x)
=fX(x)f Y(w x)Put the pmfs on top of each other
Flip the pmf of Y f(w)=/integraldisplay
W fX(x)f Y(wx)dx
Shift the ipped pmf by w
(to the right if w&gt;0)
Cross-multiply and add
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-11-derived-distributions-convolution-correlation/</video_url>
          <video_title>Lecture 11: Derived Distributions; Convolution; Covariance and Correlation</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high-quality, educational</text_slice>
            </slice>
            <slice>
              <time_slice>0:08</time_slice>
              <text_slice>resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: Good morning.</text_slice>
            </slice>
            <slice>
              <time_slice>0:23</time_slice>
              <text_slice>So today we're going
to continue the</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>subject from last time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:27</time_slice>
              <text_slice>So we're going to talk about
derived distributions a little</text_slice>
            </slice>
            <slice>
              <time_slice>0:31</time_slice>
              <text_slice>more, how to derive the
distribution of a function of</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>0:36</time_slice>
              <text_slice>So last time we discussed a
couple of examples in which we</text_slice>
            </slice>
            <slice>
              <time_slice>0:40</time_slice>
              <text_slice>had a function of a
single variable.</text_slice>
            </slice>
            <slice>
              <time_slice>0:43</time_slice>
              <text_slice>And we found the distribution
of Y, if we're told the</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>distribution of X.</text_slice>
            </slice>
            <slice>
              <time_slice>0:47</time_slice>
              <text_slice>So today we're going to do an
example where we deal with the</text_slice>
            </slice>
            <slice>
              <time_slice>0:51</time_slice>
              <text_slice>function of two random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>And then we're going to consider
the most interesting</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>example of this kind, in which
we have a random variable of</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>the form W, which is
the sum of two</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>independent, random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>That's a case that shows
up quite often.</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>And so we want to see what
exactly happens in this</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>particular case.</text_slice>
            </slice>
            <slice>
              <time_slice>1:12</time_slice>
              <text_slice>Just one comment that
I should make.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>The material that we're covering
now, chapter four, is</text_slice>
            </slice>
            <slice>
              <time_slice>1:18</time_slice>
              <text_slice>sort of conceptually a little
more difficult than one we</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>have been doing before.</text_slice>
            </slice>
            <slice>
              <time_slice>1:23</time_slice>
              <text_slice>So I would definitely encourage
you to read the text</text_slice>
            </slice>
            <slice>
              <time_slice>1:26</time_slice>
              <text_slice>before you jump and try
to do the problems in</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>your problem sets.</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>OK, so let's start with our
example, in which we're given</text_slice>
            </slice>
            <slice>
              <time_slice>1:40</time_slice>
              <text_slice>two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>They're jointly continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>And their distribution
is pretty simple.</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>They're uniform on
the unit square.</text_slice>
            </slice>
            <slice>
              <time_slice>1:48</time_slice>
              <text_slice>In particular, each one of the
random variables is uniform on</text_slice>
            </slice>
            <slice>
              <time_slice>1:52</time_slice>
              <text_slice>the unit interval.</text_slice>
            </slice>
            <slice>
              <time_slice>1:54</time_slice>
              <text_slice>And the two random variables
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>What we're going to find is the
distribution of the ratio</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>of the two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>How do we go about it? , Well,
the same cookbook procedure</text_slice>
            </slice>
            <slice>
              <time_slice>2:07</time_slice>
              <text_slice>that we used last time
for the case of a</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>single random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:13</time_slice>
              <text_slice>The cookbook procedure that
we used for this case also</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>applies to the case where you
have a function of multiple</text_slice>
            </slice>
            <slice>
              <time_slice>2:20</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>So what was the cookbook
procedure?</text_slice>
            </slice>
            <slice>
              <time_slice>2:23</time_slice>
              <text_slice>The first step is to find the
cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>function of the random variable
of interest and then</text_slice>
            </slice>
            <slice>
              <time_slice>2:30</time_slice>
              <text_slice>take the derivative in order
to find the density.</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>So let's find the cumulative.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>So, by definition, the
cumulative is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>that the random variable is
less than or equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>argument of the cumulative.</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>So if we write this event in
terms of the random variable</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>of interest, this is the
probability that our random</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>variable is less than
or equal to z.</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>So what is that?</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>OK, so the ratio is going to be
less than or equal to z, if</text_slice>
            </slice>
            <slice>
              <time_slice>3:09</time_slice>
              <text_slice>and only if the pair, (x,y),
happens to fall below the line</text_slice>
            </slice>
            <slice>
              <time_slice>3:14</time_slice>
              <text_slice>that has a slope z.</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>OK, so we draw a line
that has a slope z.</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>The ratio is less than this
number, if and only if we get</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>the pair of x and y that falls
inside this triangle.</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>So we're talking about
the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>3:29</time_slice>
              <text_slice>this particular event.</text_slice>
            </slice>
            <slice>
              <time_slice>3:30</time_slice>
              <text_slice>Since this line has a slope of
z, the height at this point is</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>equal to z.</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>And so we can find the
probability of this event.</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>It's just the area
of this triangle.</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>And so the area is 1
times z times 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>And we get the answer, z/2.</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>Now, is this answer
always correct?</text_slice>
            </slice>
            <slice>
              <time_slice>3:56</time_slice>
              <text_slice>Now, this answer is going to be
correct only if the slope</text_slice>
            </slice>
            <slice>
              <time_slice>4:00</time_slice>
              <text_slice>happens to be such that we get
a picture of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>So when do we get a picture
of this kind?</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>When the slope is less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>If I consider a different slope,
a number, little z --</text_slice>
            </slice>
            <slice>
              <time_slice>4:13</time_slice>
              <text_slice>that happens to be a slope
of that kind --</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>then the picture changes.</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>And in that case, we
get a picture of</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>this kind, let's say.</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>So this is a line here
of slope z, again.</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>And this is the second case in
which our number, little z, is</text_slice>
            </slice>
            <slice>
              <time_slice>4:35</time_slice>
              <text_slice>bigger than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>So how do we proceed?</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>Once more, the cumulative is the
probability that the ratio</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>is less than or equal
to that number.</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>So it's the probability that
we fall below the red line.</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>So we're talking about the
event, about this event.</text_slice>
            </slice>
            <slice>
              <time_slice>4:56</time_slice>
              <text_slice>So to find the probability of
this event, we need to find</text_slice>
            </slice>
            <slice>
              <time_slice>4:59</time_slice>
              <text_slice>the area of this red shape.</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>And one way of finding this area
is to consider the whole</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>area and subtract the area
of this triangle.</text_slice>
            </slice>
            <slice>
              <time_slice>5:09</time_slice>
              <text_slice>So let's do it this way.</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>It's going to be 1 minus the
area of the triangle.</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>Now, what's the area
of the triangle?</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>It's 1/2 times this side, which
is 1 times this side.</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>How big is that side?</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>Well, if y and the slope is z,
now z is the ratio y over x.</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>So if y over x--</text_slice>
            </slice>
            <slice>
              <time_slice>5:39</time_slice>
              <text_slice>at this point we have
y/x = z and y =1.</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>This means that z is 1/x.</text_slice>
            </slice>
            <slice>
              <time_slice>5:49</time_slice>
              <text_slice>So the coordinate of
this point is 1/x.</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>And this means that
we're going to--</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>1/z So here we get the
factor of 1/z.</text_slice>
            </slice>
            <slice>
              <time_slice>6:07</time_slice>
              <text_slice>And we're basically done.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>I guess if you want to have a
complete answer, you should</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>also give the formula
for z less than 0.</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>What is the cumulative when
z is less than 0, the</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>probability that you get the
ratio that's negative?</text_slice>
            </slice>
            <slice>
              <time_slice>6:22</time_slice>
              <text_slice>Well, since our random variables
are positive,</text_slice>
            </slice>
            <slice>
              <time_slice>6:25</time_slice>
              <text_slice>there's no way that you can
get a negative ratio.</text_slice>
            </slice>
            <slice>
              <time_slice>6:27</time_slice>
              <text_slice>So the cumulative down
there is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>So we can plot the cumulative.</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>And we can take its derivative
in order to find the density.</text_slice>
            </slice>
            <slice>
              <time_slice>6:45</time_slice>
              <text_slice>So the cumulative that
we got starts at 0,</text_slice>
            </slice>
            <slice>
              <time_slice>6:49</time_slice>
              <text_slice>when z's are negative.</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>Then it starts going up
in proportion to z, at</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>the slope of 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>So this takes us up to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>And then it starts increasing
towards 1,</text_slice>
            </slice>
            <slice>
              <time_slice>7:14</time_slice>
              <text_slice>according to this function.</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>When you let z go to infinity,
the cumulative is</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>going to go to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>And it has a shape of, more
or less, this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>So now to get the density, we
just take the derivative.</text_slice>
            </slice>
            <slice>
              <time_slice>7:36</time_slice>
              <text_slice>And the density is, of
course, 0 down here.</text_slice>
            </slice>
            <slice>
              <time_slice>7:40</time_slice>
              <text_slice>Up here the derivative
is just 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>7:48</time_slice>
              <text_slice>And beyond that point we need to
take the derivative of this</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>expression.</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>And the derivative is going to
be 1/2 times 1 over z-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>So it's going to be a
shape of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>And we're done.</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>So you see that problems
involving functions of</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>multiple random variables are
no harder than problems that</text_slice>
            </slice>
            <slice>
              <time_slice>8:19</time_slice>
              <text_slice>deal with the functional of
a single random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>The general procedure is,
again, exactly the same.</text_slice>
            </slice>
            <slice>
              <time_slice>8:25</time_slice>
              <text_slice>You first find the cumulative,
and then you differentiate.</text_slice>
            </slice>
            <slice>
              <time_slice>8:28</time_slice>
              <text_slice>The only extra difficulty will
be that when you calculate the</text_slice>
            </slice>
            <slice>
              <time_slice>8:31</time_slice>
              <text_slice>cumulative, you need to find
the probability of an event</text_slice>
            </slice>
            <slice>
              <time_slice>8:34</time_slice>
              <text_slice>that involves multiple
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>And sometimes this could be
a little harder to do.</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>By the way, since we dealt
with this example, just a</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>couple of questions.</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>What do you think is going to
be the expected value of the</text_slice>
            </slice>
            <slice>
              <time_slice>8:49</time_slice>
              <text_slice>random variable Z?</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>Let's see, the expected value
of the random variable Z is</text_slice>
            </slice>
            <slice>
              <time_slice>8:55</time_slice>
              <text_slice>going to be the integral
of z times the density.</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>And the density is equal to 1/2
for z going from 0 to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>And then there's another</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>contribution from 1 to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>9:14</time_slice>
              <text_slice>There the density is
1/(2z-squared).</text_slice>
            </slice>
            <slice>
              <time_slice>9:19</time_slice>
              <text_slice>And we get the z, since we're
dealing with expectations, dz.</text_slice>
            </slice>
            <slice>
              <time_slice>9:24</time_slice>
              <text_slice>So what is this integral?</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>Well, if you look here, you're
integrating 1/z, all the way</text_slice>
            </slice>
            <slice>
              <time_slice>9:35</time_slice>
              <text_slice>to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>9:36</time_slice>
              <text_slice>1/z has an integral, which
is the logarithm of z.</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>And since the logarithm goes to
infinity, this means that</text_slice>
            </slice>
            <slice>
              <time_slice>9:44</time_slice>
              <text_slice>this integral is
also infinite.</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>So the expectation of the random
variable Z is actually</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>infinite in this example.</text_slice>
            </slice>
            <slice>
              <time_slice>9:55</time_slice>
              <text_slice>There's nothing wrong
with this.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>Lots of random variables have
infinite expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>10:00</time_slice>
              <text_slice>If the tail of the density falls
kind of slowly, as the</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>argument goes to infinity, then
it may well turn out that</text_slice>
            </slice>
            <slice>
              <time_slice>10:10</time_slice>
              <text_slice>you get an infinite integral.</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>So that's just how
things often are.</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>Nothing strange about it.</text_slice>
            </slice>
            <slice>
              <time_slice>10:19</time_slice>
              <text_slice>And now, since we are still in
this example, let me ask</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>another question.</text_slice>
            </slice>
            <slice>
              <time_slice>10:25</time_slice>
              <text_slice>Would we reason, on the average,
would it be true that</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>the expected value of Z --</text_slice>
            </slice>
            <slice>
              <time_slice>10:31</time_slice>
              <text_slice>remember that Z is the ratio
Y/X -- could it be that the</text_slice>
            </slice>
            <slice>
              <time_slice>10:36</time_slice>
              <text_slice>expected value of Z
is this number?</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>Or could it be that it's
equal to this number?</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>Or could it be that it's
none of the above?</text_slice>
            </slice>
            <slice>
              <time_slice>11:01</time_slice>
              <text_slice>OK, so how many people think
this is correct?</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>Small number.</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>How many people think
this is correct?</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>Slightly bigger, but still
a small number.</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>And how many people think
this is correct?</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>OK, that's--</text_slice>
            </slice>
            <slice>
              <time_slice>11:26</time_slice>
              <text_slice>this one wins the vote.</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>OK, let's see.</text_slice>
            </slice>
            <slice>
              <time_slice>11:32</time_slice>
              <text_slice>This one is not correct, just
because there's no reason it</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>should be correct.</text_slice>
            </slice>
            <slice>
              <time_slice>11:39</time_slice>
              <text_slice>So, in general, you cannot
reason on the average.</text_slice>
            </slice>
            <slice>
              <time_slice>11:44</time_slice>
              <text_slice>The expected value of a function
is not the same as</text_slice>
            </slice>
            <slice>
              <time_slice>11:48</time_slice>
              <text_slice>the same function of the
expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>11:50</time_slice>
              <text_slice>This is only true if you're
dealing with linear functions</text_slice>
            </slice>
            <slice>
              <time_slice>11:53</time_slice>
              <text_slice>of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>So this is not--</text_slice>
            </slice>
            <slice>
              <time_slice>11:56</time_slice>
              <text_slice>this turns out to
not be correct.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>How about this one?</text_slice>
            </slice>
            <slice>
              <time_slice>12:00</time_slice>
              <text_slice>Well, X and Y are independent,
by assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>12:05</time_slice>
              <text_slice>So 1/X and Y are also
independent.</text_slice>
            </slice>
            <slice>
              <time_slice>12:14</time_slice>
              <text_slice>Why is this?</text_slice>
            </slice>
            <slice>
              <time_slice>12:14</time_slice>
              <text_slice>Independence means that one
random variable does not</text_slice>
            </slice>
            <slice>
              <time_slice>12:17</time_slice>
              <text_slice>convey any information
about the other.</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>So Y doesn't give you any
information about X. So Y</text_slice>
            </slice>
            <slice>
              <time_slice>12:24</time_slice>
              <text_slice>doesn't give you any information
about 1/X. Or to</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>put it differently, if two
random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>independent, functions of each
one of those random variables</text_slice>
            </slice>
            <slice>
              <time_slice>12:36</time_slice>
              <text_slice>are also independent.</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>If X is independent from
Y, then g(X) is</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>independent of h(Y).</text_slice>
            </slice>
            <slice>
              <time_slice>12:43</time_slice>
              <text_slice>So this applies to this case.</text_slice>
            </slice>
            <slice>
              <time_slice>12:45</time_slice>
              <text_slice>These two random variables
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>And since they are independent,
this means that</text_slice>
            </slice>
            <slice>
              <time_slice>12:50</time_slice>
              <text_slice>the expected value of their
product is equal to the</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>product of the expected
values.</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>So this relation actually
is true.</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>And therefore, this
is not true.</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>Now, let's move on.</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>We have this general procedure
of finding the derived</text_slice>
            </slice>
            <slice>
              <time_slice>13:22</time_slice>
              <text_slice>distribution by going through
the cumulative.</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>Are there some cases where
we can have a shortcut?</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>Turns out that there is a
special case or a special</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>structure in which we can get
directly from densities to</text_slice>
            </slice>
            <slice>
              <time_slice>13:38</time_slice>
              <text_slice>densities using directly
just a formula.</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>And in that case, we don't
have to go through the</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>cumulative.</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>And this case is also
interesting, because it gives</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>us some insight about how one
density changes to a different</text_slice>
            </slice>
            <slice>
              <time_slice>13:52</time_slice>
              <text_slice>density and what affects the
shape of those densities.</text_slice>
            </slice>
            <slice>
              <time_slice>13:56</time_slice>
              <text_slice>So the case where things easy
is when the transformation</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>from one random variable to
the other is a strictly</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>monotonic one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>So there's a one-to-one relation
between x's and y's.</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>Here we can reason directly in
terms of densities by thinking</text_slice>
            </slice>
            <slice>
              <time_slice>14:14</time_slice>
              <text_slice>in terms of probabilities
of small intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>So let's look at the small
interval on the x-axis, like</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>this one, when X ranges from--</text_slice>
            </slice>
            <slice>
              <time_slice>14:26</time_slice>
              <text_slice>where capital X ranges
from a small x to a</text_slice>
            </slice>
            <slice>
              <time_slice>14:30</time_slice>
              <text_slice>small x plus delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:31</time_slice>
              <text_slice>So this is a small interval
of length delta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>Whenever X happens to fall in
this interval, the random</text_slice>
            </slice>
            <slice>
              <time_slice>14:40</time_slice>
              <text_slice>variable Y is going
to fall in a</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>corresponding interval up there.</text_slice>
            </slice>
            <slice>
              <time_slice>14:45</time_slice>
              <text_slice>So up there we have a
corresponding interval.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>And these two intervals, the
red and the blue interval--</text_slice>
            </slice>
            <slice>
              <time_slice>14:55</time_slice>
              <text_slice>this is the blue interval.</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>And that's the red interval.</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>These two intervals should have
the same probability.</text_slice>
            </slice>
            <slice>
              <time_slice>15:05</time_slice>
              <text_slice>They're exactly the
same event.</text_slice>
            </slice>
            <slice>
              <time_slice>15:08</time_slice>
              <text_slice>When X falls here, g(X) happens
to fall in there.</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>So we can sort of say that the
probability of this little</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>interval is the same
as the probability</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>of that little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>And we know that probabilities
of little intervals have</text_slice>
            </slice>
            <slice>
              <time_slice>15:22</time_slice>
              <text_slice>something to do with
densities.</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>So what is the probability
of this little interval?</text_slice>
            </slice>
            <slice>
              <time_slice>15:28</time_slice>
              <text_slice>It's the density of the random
variable X, at this point,</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>times the length of
the interval.</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>How about the probability
of that interval?</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>It's going to be the density of
the random variable Y times</text_slice>
            </slice>
            <slice>
              <time_slice>15:45</time_slice>
              <text_slice>the length of that
little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>15:48</time_slice>
              <text_slice>Now, this interval
has length delta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>Does that mean that
this interval</text_slice>
            </slice>
            <slice>
              <time_slice>15:51</time_slice>
              <text_slice>also has length delta?</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>Well, not necessarily.</text_slice>
            </slice>
            <slice>
              <time_slice>15:55</time_slice>
              <text_slice>The length of this interval has
something to do with the</text_slice>
            </slice>
            <slice>
              <time_slice>15:58</time_slice>
              <text_slice>slope of your function g.</text_slice>
            </slice>
            <slice>
              <time_slice>16:01</time_slice>
              <text_slice>So slope is dy by dx.</text_slice>
            </slice>
            <slice>
              <time_slice>16:05</time_slice>
              <text_slice>Is how much-- the slope tells
you how big is the y interval</text_slice>
            </slice>
            <slice>
              <time_slice>16:09</time_slice>
              <text_slice>when you take an interval
x of a certain length.</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>So the slope is what multiplies
the length of this</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>interval to give you the length
of that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>So the length of this interval
is delta times the slope of</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>your function.</text_slice>
            </slice>
            <slice>
              <time_slice>16:28</time_slice>
              <text_slice>So the length of the interval
is delta times the slope of</text_slice>
            </slice>
            <slice>
              <time_slice>16:35</time_slice>
              <text_slice>the function, approximately.</text_slice>
            </slice>
            <slice>
              <time_slice>16:37</time_slice>
              <text_slice>So the probability of this
interval is going to be the</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>density of Y times the length
of the interval that we are</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>considering.</text_slice>
            </slice>
            <slice>
              <time_slice>16:47</time_slice>
              <text_slice>So this gives us a relation
between the density of X,</text_slice>
            </slice>
            <slice>
              <time_slice>16:52</time_slice>
              <text_slice>evaluated at this point, to the
density of Y, evaluated at</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>that point.</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>The two densities are
closely related.</text_slice>
            </slice>
            <slice>
              <time_slice>17:00</time_slice>
              <text_slice>If these x's are very likely
to occur, then this is big,</text_slice>
            </slice>
            <slice>
              <time_slice>17:05</time_slice>
              <text_slice>which means that that density
will also be big.</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>If these x's are very likely to
occur, then those y's are</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>also very likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>But there's also another
factor that comes in.</text_slice>
            </slice>
            <slice>
              <time_slice>17:16</time_slice>
              <text_slice>And that's the slope
of the function at</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>this particular point.</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>So we have this relation between
the two densities.</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>Now, in interpreting this
equation, you need to make</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>sure what's the relation between
the two variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>I have both little x's
and little y's.</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>Well, this formula is true for
an (x,y) pair, that they're</text_slice>
            </slice>
            <slice>
              <time_slice>17:39</time_slice>
              <text_slice>related according to this
particular function.</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>So if I fix an x and consider
the corresponding y, then the</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>densities at those x's and
corresponding y's will be</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>related by that formula.</text_slice>
            </slice>
            <slice>
              <time_slice>17:54</time_slice>
              <text_slice>Now, in the end, you want to
come up with a formula that</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>just gives you the density
of Y as a function of y.</text_slice>
            </slice>
            <slice>
              <time_slice>18:01</time_slice>
              <text_slice>And that means that you need to</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>eliminate x from the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>So let's see how that would
go in an example.</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>So suppose that we're dealing
with the function y equal to x</text_slice>
            </slice>
            <slice>
              <time_slice>18:17</time_slice>
              <text_slice>cubed, in which case our
function, g(x), is the</text_slice>
            </slice>
            <slice>
              <time_slice>18:21</time_slice>
              <text_slice>function x cubed.</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>And if x cubed is equal to a
little y, If we have a pair of</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>x's and y's that are related
this way, then this means that</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>x is going to be the
cubic root of y.</text_slice>
            </slice>
            <slice>
              <time_slice>18:41</time_slice>
              <text_slice>So this is the formula that
takes us back from y's to x's.</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>This is the direct function from
x, how to construct y.</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>This is essentially the inverse
function that tells</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>us, from a given y what is
the corresponding x.</text_slice>
            </slice>
            <slice>
              <time_slice>18:59</time_slice>
              <text_slice>Now, if we write this formula,
it tells us that the density</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>at the particular x is going
to be the density at the</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>corresponding y times the slope
of the function at the</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>particular x that we
are considering.</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>The slope of the function
is 3x squared.</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>Now, we want to end up with a
formula for the density of Y.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>So I'm going to take this
factor, send it</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>to the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>19:31</time_slice>
              <text_slice>But since I want it to be a
function of y, I want to</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>eliminate the x's.</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>And I'm going to eliminate
the x's using this</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>correspondence here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:41</time_slice>
              <text_slice>So I'm going to get
the density of X</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>evaluated at y to the 1/3.</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>And then this factor in the
denominator, it's 1/(3y to the</text_slice>
            </slice>
            <slice>
              <time_slice>19:50</time_slice>
              <text_slice>power 2/3).</text_slice>
            </slice>
            <slice>
              <time_slice>19:55</time_slice>
              <text_slice>So we end up finally with the
formula for the density of the</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>random variable Y.</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>And this is the same answer that
you would get if you go</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>through this exercise using the
cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>20:10</time_slice>
              <text_slice>function method.</text_slice>
            </slice>
            <slice>
              <time_slice>20:11</time_slice>
              <text_slice>You end up getting
the same answer.</text_slice>
            </slice>
            <slice>
              <time_slice>20:13</time_slice>
              <text_slice>But here we sort of
get it directly.</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>Just to get a little more
insight as to why</text_slice>
            </slice>
            <slice>
              <time_slice>20:24</time_slice>
              <text_slice>the slope comes in--</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>suppose that we have a function
like this one.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>So the function is sort of flat,
then moves quickly, and</text_slice>
            </slice>
            <slice>
              <time_slice>20:45</time_slice>
              <text_slice>then becomes flat again.</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>What should be --</text_slice>
            </slice>
            <slice>
              <time_slice>20:50</time_slice>
              <text_slice>and suppose that X has some kind
of reasonable density,</text_slice>
            </slice>
            <slice>
              <time_slice>20:55</time_slice>
              <text_slice>some kind of flat density.</text_slice>
            </slice>
            <slice>
              <time_slice>20:57</time_slice>
              <text_slice>Suppose that X is a pretty
uniform random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>What's going to happen to
the random variable Y?</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>What kind of distribution
should it have?</text_slice>
            </slice>
            <slice>
              <time_slice>21:14</time_slice>
              <text_slice>What are the typical values
of the random variable Y?</text_slice>
            </slice>
            <slice>
              <time_slice>21:19</time_slice>
              <text_slice>Either x falls here, and y is
a very small number, or--</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>let's take that number here
to be -- let's say 2 --</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>or x falls in this range, and
y takes a value close to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>And there's a small chance that
x's will be somewhere in</text_slice>
            </slice>
            <slice>
              <time_slice>21:40</time_slice>
              <text_slice>the middle, in which case y
takes intermediate values.</text_slice>
            </slice>
            <slice>
              <time_slice>21:44</time_slice>
              <text_slice>So what kind of shape do
you expect for the</text_slice>
            </slice>
            <slice>
              <time_slice>21:46</time_slice>
              <text_slice>distribution of Y?</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>There's going to be a fair
amount of probability that Y</text_slice>
            </slice>
            <slice>
              <time_slice>21:51</time_slice>
              <text_slice>takes values close to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>21:55</time_slice>
              <text_slice>There's a small probability
that Y takes</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>intermediate values.</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>That corresponds to the case
where x falls in here.</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>That's not a lot
of probability.</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>So the probability that Y takes
values between 0 and 2,</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>that's kind of small.</text_slice>
            </slice>
            <slice>
              <time_slice>22:12</time_slice>
              <text_slice>But then there's a lot of x's
that produces y's that are</text_slice>
            </slice>
            <slice>
              <time_slice>22:16</time_slice>
              <text_slice>close to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>So there's a significant
probability that Y would take</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>values that are close to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:25</time_slice>
              <text_slice>So you--</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>the density of Y would have
a shape of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>By looking at this picture, you
can tell that it's most</text_slice>
            </slice>
            <slice>
              <time_slice>22:35</time_slice>
              <text_slice>likely that either x will fall
here or x will fall there.</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>So the g(x) is most likely
to be close to 0 or</text_slice>
            </slice>
            <slice>
              <time_slice>22:44</time_slice>
              <text_slice>to be close to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:46</time_slice>
              <text_slice>So since y is most likely to be
close to 0 or close to most</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>of the probability
of y is here.</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>And there's a small</text_slice>
            </slice>
            <slice>
              <time_slice>22:54</time_slice>
              <text_slice>probability of being in between.</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>Notice that the y's that get a
lot of probability are those</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>y's associated with flats
regions off your g function.</text_slice>
            </slice>
            <slice>
              <time_slice>23:07</time_slice>
              <text_slice>When the g function is flat,
that gives you big densities</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>for Y.</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>So the density of Y is inversely
proportional to the</text_slice>
            </slice>
            <slice>
              <time_slice>23:16</time_slice>
              <text_slice>slope of the function.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>And that's what you
get from here.</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>The density of Y is--</text_slice>
            </slice>
            <slice>
              <time_slice>23:22</time_slice>
              <text_slice>send that term to the other
side-- is inversely</text_slice>
            </slice>
            <slice>
              <time_slice>23:25</time_slice>
              <text_slice>proportional to the slope of
the function that you're</text_slice>
            </slice>
            <slice>
              <time_slice>23:28</time_slice>
              <text_slice>dealing with.</text_slice>
            </slice>
            <slice>
              <time_slice>23:32</time_slice>
              <text_slice>OK, so this formula works nicely
for the case where the</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>function is one-to-one.</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>So we can have a unique
association between x's and</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>y's and through an inverse
function, from y's to x's.</text_slice>
            </slice>
            <slice>
              <time_slice>23:47</time_slice>
              <text_slice>It works for the monotonically
increasing case.</text_slice>
            </slice>
            <slice>
              <time_slice>23:50</time_slice>
              <text_slice>It also works for the
monotonically decreasing case.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>In the monotonically decreasing
case, the only</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>change that you need to do is to
take the absolute value of</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>the slope, instead of
the slope itself.</text_slice>
            </slice>
            <slice>
              <time_slice>24:16</time_slice>
              <text_slice>OK, now, here's another example
or a special case.</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>Let's talk about the most
interesting case that involves</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>a function of two random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>24:29</time_slice>
              <text_slice>And this is the case where we
have two independent, random</text_slice>
            </slice>
            <slice>
              <time_slice>24:34</time_slice>
              <text_slice>variables, and we want to
find the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>24:38</time_slice>
              <text_slice>the sum of the two.</text_slice>
            </slice>
            <slice>
              <time_slice>24:40</time_slice>
              <text_slice>We're really interested in
the continuous case.</text_slice>
            </slice>
            <slice>
              <time_slice>24:42</time_slice>
              <text_slice>But as a warm-up, it's useful
to look at the discrete case</text_slice>
            </slice>
            <slice>
              <time_slice>24:45</time_slice>
              <text_slice>first of discrete random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>Let's say we want to find the
probability that the sum of X</text_slice>
            </slice>
            <slice>
              <time_slice>24:52</time_slice>
              <text_slice>and Y is equal to a
particular number.</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>And to illustrate this,
let's take that number</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>to be equal to 3.</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>What's the probability that
the sum of the two random</text_slice>
            </slice>
            <slice>
              <time_slice>25:02</time_slice>
              <text_slice>variables is equal to 3?</text_slice>
            </slice>
            <slice>
              <time_slice>25:04</time_slice>
              <text_slice>To find the probability that
the sum is equal to 3, you</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>consider all possible ways that
you can get the sum of 3.</text_slice>
            </slice>
            <slice>
              <time_slice>25:11</time_slice>
              <text_slice>And the different ways are the
points in this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>25:14</time_slice>
              <text_slice>And they correspond to a line
that goes this way.</text_slice>
            </slice>
            <slice>
              <time_slice>25:18</time_slice>
              <text_slice>So the probability that the
sum is equal to a certain</text_slice>
            </slice>
            <slice>
              <time_slice>25:21</time_slice>
              <text_slice>number is the probability
that --</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>is the sum of the
probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>all of those points.</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>What is a typical point
in this picture?</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>In a typical point, the
random variable X</text_slice>
            </slice>
            <slice>
              <time_slice>25:34</time_slice>
              <text_slice>takes a certain value.</text_slice>
            </slice>
            <slice>
              <time_slice>25:36</time_slice>
              <text_slice>And Y takes the value that's
needed so that the sum is</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>equal to W. Any combination of
an x with a w minus x, any</text_slice>
            </slice>
            <slice>
              <time_slice>25:47</time_slice>
              <text_slice>such combination gives
you a sum of w.</text_slice>
            </slice>
            <slice>
              <time_slice>25:51</time_slice>
              <text_slice>So the probability that the sum
is w is the sum over all</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>possible x's.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>That's over all these points of
the probability that we get</text_slice>
            </slice>
            <slice>
              <time_slice>25:59</time_slice>
              <text_slice>a certain x.</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>Let's say x equals 2 times the
corresponding probability that</text_slice>
            </slice>
            <slice>
              <time_slice>26:05</time_slice>
              <text_slice>random variable Y takes
the value 1.</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>And why am I multiplying
probabilities here?</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>That's where we use the
assumption that the two random</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>variables are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>26:16</time_slice>
              <text_slice>So the probability that X takes
a certain value and Y</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>takes the complementary value,
that probability is the</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>product of two probabilities
because of independence.</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>And when we write that into our
usual PMF notation, it's a</text_slice>
            </slice>
            <slice>
              <time_slice>26:29</time_slice>
              <text_slice>formula of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>So this formula is called
the convolution formula.</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>It's an operation that takes
one PMF and another PMF-- p</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>we're given the PMF's
of X and Y --</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>and produces a new PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>26:47</time_slice>
              <text_slice>So think of this formula as
giving you a transformation.</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>You take two PMF's, you do
something with them, and you</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>obtain a new PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>26:56</time_slice>
              <text_slice>This procedure, what this
formula does is --</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>nicely illustrated sort
of by mechanically.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>So let me show you a picture
here and illustrate how the</text_slice>
            </slice>
            <slice>
              <time_slice>27:08</time_slice>
              <text_slice>mechanics go, in general.</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>So you don't have these slides,
but let's just reason</text_slice>
            </slice>
            <slice>
              <time_slice>27:16</time_slice>
              <text_slice>through it.</text_slice>
            </slice>
            <slice>
              <time_slice>27:18</time_slice>
              <text_slice>So suppose that you are
given the PMF of X,</text_slice>
            </slice>
            <slice>
              <time_slice>27:22</time_slice>
              <text_slice>and it has this shape.</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>You're given the PMF of
Y. It has this shape.</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>And somehow we are going
to do this calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>27:28</time_slice>
              <text_slice>Now, we need to do this
calculation for every value of</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>W, in order to get the PMF of
W. Let's start by doing the</text_slice>
            </slice>
            <slice>
              <time_slice>27:37</time_slice>
              <text_slice>calculation just for one case.</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>Suppose the W is equal to 0, in
which case we need to find</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>the sum of Px(x) and Py(-x).</text_slice>
            </slice>
            <slice>
              <time_slice>27:50</time_slice>
              <text_slice>How do you do this calculation
graphically?</text_slice>
            </slice>
            <slice>
              <time_slice>27:53</time_slice>
              <text_slice>It involves the PMF of X. But it
involves the PMF of Y, with</text_slice>
            </slice>
            <slice>
              <time_slice>27:59</time_slice>
              <text_slice>the argument reversed.</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>So how do we plot this?</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>Well, in order to reverse the
argument, what you need is to</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>take this PMF and flip it.</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>So that's where it's handy
to have a pair of</text_slice>
            </slice>
            <slice>
              <time_slice>28:13</time_slice>
              <text_slice>scissors with you.</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>So you cut this down.</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>And so now you take the PMF
of the random variable Y</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>and just flip it.</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>So what you see here is this
function where the argument is</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>being reversed.</text_slice>
            </slice>
            <slice>
              <time_slice>28:35</time_slice>
              <text_slice>And then what do we do?</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>We cross-multiply
the two plots.</text_slice>
            </slice>
            <slice>
              <time_slice>28:39</time_slice>
              <text_slice>Any entry here gets multiplied
with the</text_slice>
            </slice>
            <slice>
              <time_slice>28:41</time_slice>
              <text_slice>corresponding entry there.</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>And we consider all those
products and add them up.</text_slice>
            </slice>
            <slice>
              <time_slice>28:46</time_slice>
              <text_slice>In this particular case, the
flipped PMF doesn't have any</text_slice>
            </slice>
            <slice>
              <time_slice>28:50</time_slice>
              <text_slice>overlap with the PMF of X. So
we're going to get an answer</text_slice>
            </slice>
            <slice>
              <time_slice>28:53</time_slice>
              <text_slice>that's equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>So for w's equal to 0, the Pw is
going to be equal to 0, in</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>this particular plot.</text_slice>
            </slice>
            <slice>
              <time_slice>29:05</time_slice>
              <text_slice>Now if we have a different
value of w --</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>oops.</text_slice>
            </slice>
            <slice>
              <time_slice>29:09</time_slice>
              <text_slice>If we have a different value
of the argument w, then we</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>have here the PMF of Y that's
flipped and shifted by an</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>amount of w.</text_slice>
            </slice>
            <slice>
              <time_slice>29:22</time_slice>
              <text_slice>So the correct picture of what
you do is to take this and</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>displace it by a certain
amount of w.</text_slice>
            </slice>
            <slice>
              <time_slice>29:30</time_slice>
              <text_slice>So here, how much
did I shift it?</text_slice>
            </slice>
            <slice>
              <time_slice>29:33</time_slice>
              <text_slice>I shifted it until one
falls just below 4.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>So I have shifted by a
total amount of 5.</text_slice>
            </slice>
            <slice>
              <time_slice>29:44</time_slice>
              <text_slice>So 0 falls under 5, whereas
0 initially was under 0.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>So I'm shifting it by 5 units.</text_slice>
            </slice>
            <slice>
              <time_slice>29:53</time_slice>
              <text_slice>And I'm now going to
cross-multiply and add.</text_slice>
            </slice>
            <slice>
              <time_slice>29:56</time_slice>
              <text_slice>Does this give us
the correct--</text_slice>
            </slice>
            <slice>
              <time_slice>29:58</time_slice>
              <text_slice>does it do the correct thing?</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>Yes, because a typical term will
be the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>30:03</time_slice>
              <text_slice>this random variable is 3 times
the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>this random variable is 2.</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>That's a particular way that
you can get a sum of 5.</text_slice>
            </slice>
            <slice>
              <time_slice>30:12</time_slice>
              <text_slice>If you see here, the way that
things are aligned, it gives</text_slice>
            </slice>
            <slice>
              <time_slice>30:16</time_slice>
              <text_slice>you all the different ways that
you can get the sum of 5.</text_slice>
            </slice>
            <slice>
              <time_slice>30:19</time_slice>
              <text_slice>You can get the sum of 5 by
having 1 + 4, or 2 + 3, or 3 +</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>2, or 4 + 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>You need to add the
probabilities of all those</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>combinations.</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>So you take this times that.</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>That's one product term.</text_slice>
            </slice>
            <slice>
              <time_slice>30:34</time_slice>
              <text_slice>Then this times 0,
this times that.</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>And so 1--</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>you cross--</text_slice>
            </slice>
            <slice>
              <time_slice>30:40</time_slice>
              <text_slice>you find all the products of the
corresponding terms, and</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>you add them together.</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>So it's a kind of handy
mechanical procedure for doing</text_slice>
            </slice>
            <slice>
              <time_slice>30:50</time_slice>
              <text_slice>this calculation, especially
when the PMF's are given to</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>you in terms of a picture.</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>So the summary of these
mechanics are just what we</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>did, is that you put the PMF's
on top of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>You take the PMF of
Y. You flip it.</text_slice>
            </slice>
            <slice>
              <time_slice>31:06</time_slice>
              <text_slice>And for any particular w that
you're interested in, you take</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>this flipped PMF and shift
it by an amount of w.</text_slice>
            </slice>
            <slice>
              <time_slice>31:14</time_slice>
              <text_slice>Given this particular shift for
a particular value of w,</text_slice>
            </slice>
            <slice>
              <time_slice>31:17</time_slice>
              <text_slice>you cross-multiply terms and
then accumulate them or add</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>them together.</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>What would you expect to happen
in the continuous case?</text_slice>
            </slice>
            <slice>
              <time_slice>31:26</time_slice>
              <text_slice>Well, the story is familiar.</text_slice>
            </slice>
            <slice>
              <time_slice>31:28</time_slice>
              <text_slice>In the continuous case, pretty
much, almost always things</text_slice>
            </slice>
            <slice>
              <time_slice>31:32</time_slice>
              <text_slice>work out the same way,
except that we</text_slice>
            </slice>
            <slice>
              <time_slice>31:34</time_slice>
              <text_slice>replace PMF's by PDF's.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>And we replace sums
by integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>So there shouldn't be any
surprise here that you get a</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>formula of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>31:49</time_slice>
              <text_slice>The density of W can be obtained
from the density of X</text_slice>
            </slice>
            <slice>
              <time_slice>31:54</time_slice>
              <text_slice>and the density of Y by
calculating this integral.</text_slice>
            </slice>
            <slice>
              <time_slice>31:58</time_slice>
              <text_slice>Essentially, what this integral
does is it fits a</text_slice>
            </slice>
            <slice>
              <time_slice>32:03</time_slice>
              <text_slice>particular w of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>32:05</time_slice>
              <text_slice>We're interested in the
probability that the random</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>variable, capital W, takes a
value equal to little w or</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>values close to it.</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>So this corresponds to the
event, which is this</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>particular line on the
two-dimensional space.</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>So we need to find
the sort of odd</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>probabilities along that line.</text_slice>
            </slice>
            <slice>
              <time_slice>32:25</time_slice>
              <text_slice>But since the setting is
continuous, we will not add</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>32:29</time_slice>
              <text_slice>We're going to integrate.</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>And for any typical point in
this picture, the probability</text_slice>
            </slice>
            <slice>
              <time_slice>32:35</time_slice>
              <text_slice>of obtaining an outcome in this
neighborhood is the--</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>has something to do with the
density of that particular x</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>and the density of the
particular y that would</text_slice>
            </slice>
            <slice>
              <time_slice>32:47</time_slice>
              <text_slice>compliment x, in order
to form a sum of w.</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>So this integral that we have
here is really an integral</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>over this particular line.</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>OK, so I'm going to
skip the formal</text_slice>
            </slice>
            <slice>
              <time_slice>33:02</time_slice>
              <text_slice>derivation of this result.</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>There's a couple of derivations
in the text.</text_slice>
            </slice>
            <slice>
              <time_slice>33:06</time_slice>
              <text_slice>And the one which is outlined
here is yet a third</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>derivation.</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>But the easiest way to make
sense of this formula is to</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>consider what happens in
the discrete case.</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>So for the rest of the lecture
we're going to consider a few</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>extra, more miscellaneous
topics, a few remarks, and a</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>few more definitions.</text_slice>
            </slice>
            <slice>
              <time_slice>33:29</time_slice>
              <text_slice>So let's change--</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>flip a page and consider
the next mini topic.</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>There's not going to be anything
deep here, but just</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>something that's worth
being familiar with.</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>If you have two independent,
normal random variables with</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>certain parameters, the question
is, what does the</text_slice>
            </slice>
            <slice>
              <time_slice>33:50</time_slice>
              <text_slice>joined PDF look like?</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>So if they're independent, by
definition the joint PDF is</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>the product of the
individual PDF's.</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>And the PDF's each one
of them involves an</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>exponential of something.</text_slice>
            </slice>
            <slice>
              <time_slice>34:07</time_slice>
              <text_slice>The product of two exponentials
is the</text_slice>
            </slice>
            <slice>
              <time_slice>34:11</time_slice>
              <text_slice>exponential of the sum.</text_slice>
            </slice>
            <slice>
              <time_slice>34:13</time_slice>
              <text_slice>So you just add the exponents.</text_slice>
            </slice>
            <slice>
              <time_slice>34:15</time_slice>
              <text_slice>So this is the formula
for the joint PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>Now, you look at that formula
and you ask, what</text_slice>
            </slice>
            <slice>
              <time_slice>34:20</time_slice>
              <text_slice>does it look like?</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>OK, you can understand it, a
function of two variables by</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>thinking about the contours
of this function.</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>Look at the points at
which the function</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>takes a constant value.</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>Where is it?</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>When is it constant?</text_slice>
            </slice>
            <slice>
              <time_slice>34:37</time_slice>
              <text_slice>What's the shape of
the set of points</text_slice>
            </slice>
            <slice>
              <time_slice>34:40</time_slice>
              <text_slice>where this is a constant?</text_slice>
            </slice>
            <slice>
              <time_slice>34:42</time_slice>
              <text_slice>So consider all x's and y's for
which this expression here</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>is a constant, that this
expression here is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>What kind of shape is this?</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>This is an ellipse.</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>And it's an ellipse that's
centered at--</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>it's centered at mu x, mu y.</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>These are the means of the
two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>35:09</time_slice>
              <text_slice>If those sigmas were equal,
that ellipse would</text_slice>
            </slice>
            <slice>
              <time_slice>35:13</time_slice>
              <text_slice>be actually a circle.</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>And you would get contours
of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>But if, on the other hand, the
sigmas are different, you're</text_slice>
            </slice>
            <slice>
              <time_slice>35:23</time_slice>
              <text_slice>going to get an ellipse that
has contours of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>35:29</time_slice>
              <text_slice>So if my contours are
of this kind, that</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>corresponds to what?</text_slice>
            </slice>
            <slice>
              <time_slice>35:35</time_slice>
              <text_slice>Sigma x being bigger than
sigma y or vice versa.</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>OK, contours of this kind
basically tell you that X is</text_slice>
            </slice>
            <slice>
              <time_slice>35:47</time_slice>
              <text_slice>more likely to be spread out
than Y. So the range of</text_slice>
            </slice>
            <slice>
              <time_slice>35:53</time_slice>
              <text_slice>possible x's is bigger.</text_slice>
            </slice>
            <slice>
              <time_slice>35:55</time_slice>
              <text_slice>And X out here is as likely
as a Y up there.</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>So big X's have roughly the same
probability as certain</text_slice>
            </slice>
            <slice>
              <time_slice>36:08</time_slice>
              <text_slice>smaller y's.</text_slice>
            </slice>
            <slice>
              <time_slice>36:10</time_slice>
              <text_slice>So in a picture of this kind,
the variance of X is going to</text_slice>
            </slice>
            <slice>
              <time_slice>36:14</time_slice>
              <text_slice>be bigger than the
variance of Y.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>So depending on how these
variances compare with each</text_slice>
            </slice>
            <slice>
              <time_slice>36:20</time_slice>
              <text_slice>other, that's going
to determine the</text_slice>
            </slice>
            <slice>
              <time_slice>36:22</time_slice>
              <text_slice>shape of the ellipse.</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>If the variance of Y we're
bigger, then your ellipse</text_slice>
            </slice>
            <slice>
              <time_slice>36:27</time_slice>
              <text_slice>would be the other way.</text_slice>
            </slice>
            <slice>
              <time_slice>36:28</time_slice>
              <text_slice>It would be elongated in
the other dimension.</text_slice>
            </slice>
            <slice>
              <time_slice>36:32</time_slice>
              <text_slice>Just visualize it
a little more.</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>Let me throw at you a
particular picture.</text_slice>
            </slice>
            <slice>
              <time_slice>36:37</time_slice>
              <text_slice>This is one--</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>this is a picture of
one special case.</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>Here, I think, the variances
are equal.</text_slice>
            </slice>
            <slice>
              <time_slice>36:46</time_slice>
              <text_slice>That's the kind of shape
that you get.</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>It looks like a two-dimensional
bell.</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>So remember, for a normal random
variables, for a single</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>random variable you get a
PDF that's bell shaped.</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>That's just a bell-shaped
curve.</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>In the two-dimensional case, we
get the joint PDF, which is</text_slice>
            </slice>
            <slice>
              <time_slice>37:04</time_slice>
              <text_slice>bell shaped again.</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>And now it looks more like a
real bell, the way it would be</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>laid out in ordinary space.</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>And if you look at the contours
of this function, the</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>places where the function is
equal, the typcial contour</text_slice>
            </slice>
            <slice>
              <time_slice>37:18</time_slice>
              <text_slice>would have this shape here.</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>And it would be an ellipse.</text_slice>
            </slice>
            <slice>
              <time_slice>37:23</time_slice>
              <text_slice>And in this case, actually, it
will be more like a circle.</text_slice>
            </slice>
            <slice>
              <time_slice>37:28</time_slice>
              <text_slice>So these would be the different
contours for</text_slice>
            </slice>
            <slice>
              <time_slice>37:32</time_slice>
              <text_slice>different--</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>so the contours are
places where the</text_slice>
            </slice>
            <slice>
              <time_slice>37:38</time_slice>
              <text_slice>joint PDF is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>When you change the value of
that constant, you get the</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>different contours.</text_slice>
            </slice>
            <slice>
              <time_slice>37:44</time_slice>
              <text_slice>And the PDF is, of course,
centered around the mean of</text_slice>
            </slice>
            <slice>
              <time_slice>37:50</time_slice>
              <text_slice>the two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>37:52</time_slice>
              <text_slice>So in this particular case,
since the bell is centered</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>around the (0, 0) vector, this
is a plot of a bivariate</text_slice>
            </slice>
            <slice>
              <time_slice>38:00</time_slice>
              <text_slice>normal with 0 means.</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>OK, there's--</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>bivariate normals are also
interesting when your bell is</text_slice>
            </slice>
            <slice>
              <time_slice>38:14</time_slice>
              <text_slice>oriented differently in space.</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>We talked about ellipses that
are this way, ellipses that</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>are this way.</text_slice>
            </slice>
            <slice>
              <time_slice>38:22</time_slice>
              <text_slice>You could imagine also bells
that you take them, you squash</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>them somehow, so that they
become narrow in one dimension</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>and then maybe rotate them.</text_slice>
            </slice>
            <slice>
              <time_slice>38:32</time_slice>
              <text_slice>So if you had--</text_slice>
            </slice>
            <slice>
              <time_slice>38:33</time_slice>
              <text_slice>we're not going to go into this
subject, but if you had a</text_slice>
            </slice>
            <slice>
              <time_slice>38:37</time_slice>
              <text_slice>joint pdf whose contours were
like this, what would that</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>correspond to?</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>Would your x's and y's
be independent?</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>No.</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>This would indicate that there's
a relation between the</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>x's and the y's.</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>That is, when you have bigger
x's, you would expect to also</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>get bigger y's.</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>So it would be a case of
dependent normals.</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>And we're coming back to
this point in a second.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>Before we get to that point in
a second that has to do with</text_slice>
            </slice>
            <slice>
              <time_slice>39:13</time_slice>
              <text_slice>the dependencies between the
random variables, let's just</text_slice>
            </slice>
            <slice>
              <time_slice>39:16</time_slice>
              <text_slice>do another digression.</text_slice>
            </slice>
            <slice>
              <time_slice>39:18</time_slice>
              <text_slice>If we have our two normals that
are independent, as we</text_slice>
            </slice>
            <slice>
              <time_slice>39:23</time_slice>
              <text_slice>discussed here, we can go and
apply the formula, the</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>convolution formula that we
were just discussing.</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>Suppose you want to find the
distribution of the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>these two independent normals.</text_slice>
            </slice>
            <slice>
              <time_slice>39:37</time_slice>
              <text_slice>How do you do this?</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>There is a closed-form formula
for the density of the sum,</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>which is this one.</text_slice>
            </slice>
            <slice>
              <time_slice>39:44</time_slice>
              <text_slice>We do have formulas for the
density of X and the density</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>of Y, because both of them are
normal, random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>So you need to calculate this
particular integral here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:54</time_slice>
              <text_slice>It's an integral with
respect to x.</text_slice>
            </slice>
            <slice>
              <time_slice>39:57</time_slice>
              <text_slice>And you have to calculate
this integral for any</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>given value of w.</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>So this is an exercise
in integration,</text_slice>
            </slice>
            <slice>
              <time_slice>40:05</time_slice>
              <text_slice>which is not very difficult.</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>And it turns out that after you
do everything, you end up</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>with an answer that
has this form.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>And you look at that,
and you suddenly</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>recognize, oh, this is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>And conclusion from this
exercise, once it's done, is</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>that the sum of two independent
normal random</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>variables is also normal.</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>Now, the mean of W is, of
course, going to be equal to</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>the sum of the means of X and
Y. In this case, in this</text_slice>
            </slice>
            <slice>
              <time_slice>40:35</time_slice>
              <text_slice>formula I took the
means to be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>40:37</time_slice>
              <text_slice>So the mean of W is also
going to be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>In the more general case, the
mean of W is going to be just</text_slice>
            </slice>
            <slice>
              <time_slice>40:43</time_slice>
              <text_slice>the sum of the two means.</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>The variance of W is always the
sum of the variances of X</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>and Y, since we have independent
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>40:53</time_slice>
              <text_slice>So there's no surprise here.</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>The main surprise in this
calculation is this fact here,</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>that the sum of independent
normal random</text_slice>
            </slice>
            <slice>
              <time_slice>41:02</time_slice>
              <text_slice>variables is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>I had mentioned this fact
in a previous lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>Here what we're doing is to
basically outline the argument</text_slice>
            </slice>
            <slice>
              <time_slice>41:12</time_slice>
              <text_slice>that justifies this
particular fact.</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>It's an exercise in integration,
where you realize</text_slice>
            </slice>
            <slice>
              <time_slice>41:17</time_slice>
              <text_slice>that when you convolve two
normal curves, you also get</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>back a normal one once more.</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>So now, let's return to the
comment I was making here,</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>that if you have a contour plot
that has, let's say, a</text_slice>
            </slice>
            <slice>
              <time_slice>41:33</time_slice>
              <text_slice>shape of this kind, this
indicates some kind of</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>dependence between your
two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>So instead of a contour plot,
let me throw in here a</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>scattered diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>41:44</time_slice>
              <text_slice>What does this scattered
diagram correspond to?</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>Suppose you have a discrete
distribution, and each one of</text_slice>
            </slice>
            <slice>
              <time_slice>41:50</time_slice>
              <text_slice>the points in this diagram
has positive probability.</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>When you look at this diagram,
what would you say?</text_slice>
            </slice>
            <slice>
              <time_slice>41:58</time_slice>
              <text_slice>I would say that when
y is big then x</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>also tends to be larger.</text_slice>
            </slice>
            <slice>
              <time_slice>42:09</time_slice>
              <text_slice>So bigger x's are sort of
associated with bigger y's in</text_slice>
            </slice>
            <slice>
              <time_slice>42:15</time_slice>
              <text_slice>some average, statistical
sense.</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>Whereas, if you have a picture
of this kind, it tells you in</text_slice>
            </slice>
            <slice>
              <time_slice>42:21</time_slice>
              <text_slice>association that the positive
y's tend to be associated with</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>negative x's most of the time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>Negative y's tend to be
associated mostly with</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>positive x's.</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>So here there's a relation
that when one variable is</text_slice>
            </slice>
            <slice>
              <time_slice>42:42</time_slice>
              <text_slice>large, the other one is also
expected to be large.</text_slice>
            </slice>
            <slice>
              <time_slice>42:45</time_slice>
              <text_slice>Here there's a relation
of the opposite kind.</text_slice>
            </slice>
            <slice>
              <time_slice>42:48</time_slice>
              <text_slice>How can we capture
this relation</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>between two random variables?</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>The way we capture it is by
defining this concept called</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>the covariance, that looks at
the relation of was X bigger</text_slice>
            </slice>
            <slice>
              <time_slice>43:03</time_slice>
              <text_slice>than usual?</text_slice>
            </slice>
            <slice>
              <time_slice>43:04</time_slice>
              <text_slice>That's the question, whether
this is positive.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>And how does this relate to the
answer-- to the question,</text_slice>
            </slice>
            <slice>
              <time_slice>43:10</time_slice>
              <text_slice>was Y bigger than usual?</text_slice>
            </slice>
            <slice>
              <time_slice>43:13</time_slice>
              <text_slice>We're asking-- by calculating
this quantity, we're sort of</text_slice>
            </slice>
            <slice>
              <time_slice>43:16</time_slice>
              <text_slice>asking the question, is there a
systematic relation between</text_slice>
            </slice>
            <slice>
              <time_slice>43:19</time_slice>
              <text_slice>having a big X with
having a big Y?</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>OK , to understand more
precisely what this does,</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>let's suppose that the random
variable has 0 means, So that</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>we get rid of this--</text_slice>
            </slice>
            <slice>
              <time_slice>43:33</time_slice>
              <text_slice>get rid of some clutter.</text_slice>
            </slice>
            <slice>
              <time_slice>43:35</time_slice>
              <text_slice>So the covariance is defined
just as this product.</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>What does this do?</text_slice>
            </slice>
            <slice>
              <time_slice>43:40</time_slice>
              <text_slice>If positive x's tends to go
together with positive y's,</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>and negative x's tend to go
together with negative y's,</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>this product will always
be positive.</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>And the covariance will
end up being positive.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>In particular, if you sit down
with a scattered diagram and</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>you do the calculations,
you'll find that the</text_slice>
            </slice>
            <slice>
              <time_slice>44:01</time_slice>
              <text_slice>covariance of X and Y in this
diagram would be positive,</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>because here, most of the time,
X times Y is positive.</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>There's going to be a few
negative terms, but there are</text_slice>
            </slice>
            <slice>
              <time_slice>44:12</time_slice>
              <text_slice>fewer than the positive ones.</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>So this is a case of a
positive covariance.</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>It indicates a positive relation
between the two</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>44:20</time_slice>
              <text_slice>When one is big, the other
also tends to be big.</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>This is the opposite
situation.</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>Here, when one variable--</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>here, most of the action happens
in this quadrant and</text_slice>
            </slice>
            <slice>
              <time_slice>44:31</time_slice>
              <text_slice>that quadrant, which means that
X times Y, most of the</text_slice>
            </slice>
            <slice>
              <time_slice>44:35</time_slice>
              <text_slice>time, is negative.</text_slice>
            </slice>
            <slice>
              <time_slice>44:37</time_slice>
              <text_slice>You get a few positive
contributions,</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>but there are few.</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>When you add things up, the
negative terms dominate.</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>And in this case we
have covariance of</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>X and Y being negative.</text_slice>
            </slice>
            <slice>
              <time_slice>44:49</time_slice>
              <text_slice>So a positive covariance
indicates a sort of systematic</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>relation, that there's a
positive association between</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>the two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>When one is large, the other
also tends to be large.</text_slice>
            </slice>
            <slice>
              <time_slice>45:00</time_slice>
              <text_slice>Negative covariance is
sort of the opposite.</text_slice>
            </slice>
            <slice>
              <time_slice>45:03</time_slice>
              <text_slice>When one tends to be
large, the other</text_slice>
            </slice>
            <slice>
              <time_slice>45:05</time_slice>
              <text_slice>variable tends to be small.</text_slice>
            </slice>
            <slice>
              <time_slice>45:09</time_slice>
              <text_slice>OK, so what else is there to
say about the covariance?</text_slice>
            </slice>
            <slice>
              <time_slice>45:15</time_slice>
              <text_slice>One observation to make
is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>What's the covariance
of X with X itself?</text_slice>
            </slice>
            <slice>
              <time_slice>45:23</time_slice>
              <text_slice>If you plug in X here, you see
that what we have is expected</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>value of X minus expected
of X squared.</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>And that's just the
definition of the</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>variance of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>So that's one fact
to keep in mind.</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>We had a shortcut formula for
calculating variances.</text_slice>
            </slice>
            <slice>
              <time_slice>45:44</time_slice>
              <text_slice>There's a similar shortcut
formula for calculating</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>covariances.</text_slice>
            </slice>
            <slice>
              <time_slice>45:48</time_slice>
              <text_slice>In particular, we can calculate
covariances in this</text_slice>
            </slice>
            <slice>
              <time_slice>45:51</time_slice>
              <text_slice>particular way.</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>That's just the convenient way
of doing it whenever you need</text_slice>
            </slice>
            <slice>
              <time_slice>45:56</time_slice>
              <text_slice>to calculate it.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>And finally, covariances are
very useful when you want to</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>calculate the variance of a
sum of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>We know that if two random
variables are independent, the</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>variance of the sum is the
sum of the variances.</text_slice>
            </slice>
            <slice>
              <time_slice>46:16</time_slice>
              <text_slice>When the random variables are
dependent, this is no longer</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>true, and we need to supplement
the formula a</text_slice>
            </slice>
            <slice>
              <time_slice>46:23</time_slice>
              <text_slice>little bit.</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>And there's a typo on
the slides that you</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>have in your hands.</text_slice>
            </slice>
            <slice>
              <time_slice>46:27</time_slice>
              <text_slice>That term of 2 shouldn't
be there.</text_slice>
            </slice>
            <slice>
              <time_slice>46:32</time_slice>
              <text_slice>And let's see where that
formula comes from.</text_slice>
            </slice>
            <slice>
              <time_slice>46:41</time_slice>
              <text_slice>Let's suppose that our
random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>independent of --</text_slice>
            </slice>
            <slice>
              <time_slice>46:46</time_slice>
              <text_slice>not independent --</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>our random variables
have 0 means.</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>And we want to calculate
the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>So the variance is going
to be expected value of</text_slice>
            </slice>
            <slice>
              <time_slice>47:00</time_slice>
              <text_slice>(X1 plus Xn) squared.</text_slice>
            </slice>
            <slice>
              <time_slice>47:04</time_slice>
              <text_slice>What you do is you expand
the square.</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>And you get the expected value
of the sum of the Xi squared.</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>And then you get all
the cross terms.</text_slice>
            </slice>
            <slice>
              <time_slice>47:23</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>And so now, here, let's
assume for simplicity</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>that we have 0 means.</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>The expected value of this is
the sum of the expected values</text_slice>
            </slice>
            <slice>
              <time_slice>47:34</time_slice>
              <text_slice>of the X squared terms.</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>And that gives us
the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:38</time_slice>
              <text_slice>And then we have all the
possible cross terms.</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>And each one of the possible
cross terms is the expected</text_slice>
            </slice>
            <slice>
              <time_slice>47:44</time_slice>
              <text_slice>value of Xi times Xj.</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>This is just the covariance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>So if you can calculate all
the variances and the</text_slice>
            </slice>
            <slice>
              <time_slice>47:52</time_slice>
              <text_slice>covariances, then you're able to
calculate also the variance</text_slice>
            </slice>
            <slice>
              <time_slice>47:56</time_slice>
              <text_slice>of a sum of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>47:58</time_slice>
              <text_slice>Now, if two random variables are
independent, then you look</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>at this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>48:04</time_slice>
              <text_slice>Because of independence,
expected value of the product</text_slice>
            </slice>
            <slice>
              <time_slice>48:07</time_slice>
              <text_slice>is going to be the product
of the expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>48:10</time_slice>
              <text_slice>And the expected value
of just this term is</text_slice>
            </slice>
            <slice>
              <time_slice>48:14</time_slice>
              <text_slice>always equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>You're expected deviation
from the mean is just 0.</text_slice>
            </slice>
            <slice>
              <time_slice>48:19</time_slice>
              <text_slice>So the covariance will
turn out to be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>So independent random
variables lead to 0</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>covariances, although the
opposite fact is not</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>necessarily true.</text_slice>
            </slice>
            <slice>
              <time_slice>48:30</time_slice>
              <text_slice>So covariances give you some
indication of the relation</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>between two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>Something that's not so
convenient conceptually about</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>covariances is that it
has the wrong units.</text_slice>
            </slice>
            <slice>
              <time_slice>48:41</time_slice>
              <text_slice>That's the same comment
that we had</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>made regarding variances.</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>And with variances we got out
of that issue by considering</text_slice>
            </slice>
            <slice>
              <time_slice>48:48</time_slice>
              <text_slice>the standard deviation, which
has the correct units.</text_slice>
            </slice>
            <slice>
              <time_slice>48:52</time_slice>
              <text_slice>So with the same reasoning, we
want to have a concept that</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>captures the relation between
two random variables and, in</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>some sense, that doesn't have
to do with the units that</text_slice>
            </slice>
            <slice>
              <time_slice>49:05</time_slice>
              <text_slice>we're dealing.</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>We want to have a dimensionless
quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>That tells us how strongly two
random variables are related</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>to each other.</text_slice>
            </slice>
            <slice>
              <time_slice>49:16</time_slice>
              <text_slice>So instead of considering the
covariance of just X with Y,</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>we take our random variables
and standardize them by</text_slice>
            </slice>
            <slice>
              <time_slice>49:24</time_slice>
              <text_slice>dividing them by their
individual standard deviations</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>and take the expectation
of this.</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>So what we end up doing is the
covariance of X and Y, which</text_slice>
            </slice>
            <slice>
              <time_slice>49:34</time_slice>
              <text_slice>has units that are the units of
X times the units of Y. But</text_slice>
            </slice>
            <slice>
              <time_slice>49:39</time_slice>
              <text_slice>divide with a standard
deviation, so that we get a</text_slice>
            </slice>
            <slice>
              <time_slice>49:41</time_slice>
              <text_slice>quantity that doesn't
have units.</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>This quantity, we call it the
correlation coefficient.</text_slice>
            </slice>
            <slice>
              <time_slice>49:47</time_slice>
              <text_slice>And it's a very useful quantity,
a very useful</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>measure of the strength
of association</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>between two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>49:55</time_slice>
              <text_slice>It's very informative, because
it falls always</text_slice>
            </slice>
            <slice>
              <time_slice>49:59</time_slice>
              <text_slice>between -1 and +1.</text_slice>
            </slice>
            <slice>
              <time_slice>50:02</time_slice>
              <text_slice>This is an algebraic exercise
that you're going to see in</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>50:07</time_slice>
              <text_slice>And the way that you interpret
it is as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>50:10</time_slice>
              <text_slice>If the two random variables
are independent, the</text_slice>
            </slice>
            <slice>
              <time_slice>50:13</time_slice>
              <text_slice>covariance is going to be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>The correlation coefficient
is going to be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>50:18</time_slice>
              <text_slice>So 0 correlation coefficient
basically indicates a lack of</text_slice>
            </slice>
            <slice>
              <time_slice>50:23</time_slice>
              <text_slice>a systematic relation between
the two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>On the other hand, when rho is
large, either close to 1 or</text_slice>
            </slice>
            <slice>
              <time_slice>50:31</time_slice>
              <text_slice>close to -1, this is an
indication of a strong</text_slice>
            </slice>
            <slice>
              <time_slice>50:34</time_slice>
              <text_slice>association between the
two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>50:37</time_slice>
              <text_slice>And the extreme case is when
rho takes an extreme value.</text_slice>
            </slice>
            <slice>
              <time_slice>50:42</time_slice>
              <text_slice>When rho has a magnitude
equal to 1, it's as</text_slice>
            </slice>
            <slice>
              <time_slice>50:46</time_slice>
              <text_slice>big as it can be.</text_slice>
            </slice>
            <slice>
              <time_slice>50:47</time_slice>
              <text_slice>In that case, the two
random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>very strongly related.</text_slice>
            </slice>
            <slice>
              <time_slice>50:53</time_slice>
              <text_slice>How strongly?</text_slice>
            </slice>
            <slice>
              <time_slice>50:54</time_slice>
              <text_slice>Well, if you know one random
variable, if you know the</text_slice>
            </slice>
            <slice>
              <time_slice>50:58</time_slice>
              <text_slice>value of y, you can recover the
value of x and conversely.</text_slice>
            </slice>
            <slice>
              <time_slice>51:03</time_slice>
              <text_slice>So the case of a complete
correlation is the case where</text_slice>
            </slice>
            <slice>
              <time_slice>51:07</time_slice>
              <text_slice>one random variable is a linear
function of the other</text_slice>
            </slice>
            <slice>
              <time_slice>51:11</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>51:12</time_slice>
              <text_slice>In terms of a scatter plot, this
would mean that there's a</text_slice>
            </slice>
            <slice>
              <time_slice>51:16</time_slice>
              <text_slice>certain line and that the only
possible (x,y) pairs that can</text_slice>
            </slice>
            <slice>
              <time_slice>51:22</time_slice>
              <text_slice>happen would lie on that line.</text_slice>
            </slice>
            <slice>
              <time_slice>51:24</time_slice>
              <text_slice>So if all the possible (x,y)
pairs lie on this line, then</text_slice>
            </slice>
            <slice>
              <time_slice>51:28</time_slice>
              <text_slice>you have this relation, and the
correlation coefficient is</text_slice>
            </slice>
            <slice>
              <time_slice>51:32</time_slice>
              <text_slice>equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>51:33</time_slice>
              <text_slice>A case where the correlation
coefficient is close to 1</text_slice>
            </slice>
            <slice>
              <time_slice>51:36</time_slice>
              <text_slice>would be a scatter plot like
this, where the x's and y's</text_slice>
            </slice>
            <slice>
              <time_slice>51:40</time_slice>
              <text_slice>are quite strongly aligned with
each other, maybe not</text_slice>
            </slice>
            <slice>
              <time_slice>51:44</time_slice>
              <text_slice>exactly, but fairly strongly.</text_slice>
            </slice>
            <slice>
              <time_slice>51:47</time_slice>
              <text_slice>All right, so you're going to
hear a little more about</text_slice>
            </slice>
            <slice>
              <time_slice>51:50</time_slice>
              <text_slice>correlation coefficients
and covariances</text_slice>
            </slice>
            <slice>
              <time_slice>51:52</time_slice>
              <text_slice>in recitation tomorrow.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Markov Chains - III (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 18 Review
Assume a single class of recurrentMarkov Processes  III states,
aperiodic;
plus transient states. Then,
lim r(nReadings: Section 7.4 ij)=jn
where jdoes not depend on the initial
conditions:
Lecture outline limP(Xn=j|X0=i)= jn
Review of steady-state behavior
1, . . . , mcan be found as the unique
Probability of blocked phone calls
solution to the balance equations
Calculating absorption probabilities j=
Calculating expected time to absorption/summationdisplay
kpkj,j =1, . . . , m,
k
together with
/summationdisplay
j=1
j
Example The phone company problem
0.5 0.8Calls originate as a Poisson process,rate 
0.5
Each call duration is exponentially2 1distributed (parameter )
0.2Blines available
1=2/7,2=5/7 Discrete time intervalsof (small) length 
Assume process starts at state 1.
"#
P(X1=1,and X100= 1)= 0 1 i!1 i B-1 B
i#
P(X100= 1 and X101= 2)Balance equations: i1=ii
i Bi
i=0 0=1/ii!i/summationdisplay
ii!=0  
LECTURE 20
Markov Processes  III
Readings: Section 6.4
Lecture outline
Review of steady-state behavior
Probability of blocked phone calls
Calculating absorption probabilities
Calculating expected time to absorption     
Review
Assume a single class of recurrent states,aperiodic. Then,
lim
nrij(n) = j
where jdoes not depend on the initial
conditions
limnP(Xn=j|X0) =j
1, . . . , mcan be found as the unique
solution of the balance equations
j=/summationdisplay
kkpkj
together with
/summationdisplay
jj= 1     
Example
2 10.50.5 0.8
0.2
1= 2/7, 2= 5/7
Assume process starts at state 1.
P(X1= 1, and X100= 1)=
P(X100= 1 and X101= 2)       
The phone company problem
Calls originate as a Poisson process,
rate 
Each call duration is exponentiallydistributed (parameter )
Blines available
Discrete time intervalsof (small) length 
Balance equations: i1=ii
i=0i
ii!0= 1/B/summationdisplay
i=0i
ii!
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Calculating absorption probabilities Expected time to absorption
What is the probability aithat:
process eventually settles in state 4,1
given that the initial state is i? 3 0.5
44 0.50.41 54
0.60.2
0.21 21
3 0.3 0.8
44 0.50.4
Find expected number of transitions 
0.6i,
0.2until reaching the absorbing state,
1 2given that the initial state is i?
0.8
For i= 4, ai=
For i= 5, a= i= 0 for i=i
/summationdisplay For all other i:= for all otheri= 1 + pijjai pijaj, i
j/summationdisplay
j
unique solution
unique solution
Mean rst passage and recurrence
times
Chain with one recurrent class;
xsrecurrent
Mean rst passage time from itos:
ti=E[min{n 0 such that Xn=s}|X0=i]
t1,t2, . . . , t mare the unique solution to
ts=0 ,
ti= 1 +/summationdisplay
pijtj, for all i=s
j
Mean recurrence time of s:
ts=E[min{n 1 such that Xn=s}|X0=s]
ts= 1 +/summationtext
jpsjtj/negationslash
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .
3</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-12-iterated-expectations-sum-of-a-random-number-of-random-variables/</video_url>
          <video_title>Lecture 12: Iterated Expectations; Sum of a Random Number of Random Variables</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Conditioning and Bayes&#8217; Rule (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Models based on conditional Multiplication rule
probabilities
P(A B C )=P(A)P(B A) P(C A B )
Event A: Airplane is ying above   |  | 
Event B: Something registers on radar
screen
AUB P(C | A
U   B)P(B | A)=0.99AUB
UC
P(B | A)cP(B  | A)=0.01
P(A)=0.05A
cP(B  | A)
cAUBUCP(A)
A
UcB cP(A )=0.95cAUc BUCc P(B | A )=0.10
c cP(B  | A )=0.90 cP(A )
cA
P(AB)=
P(B)=
P(A |B)=
Total probability theorem Bayes rule
Divide and conquer Prior probabilities P(Ai)
initial beliefs
Partition of sample space into A1,A2,A3
We know P(B |Ai) for each i
HaveP(B |Ai), for every i
Wish to compute P(Ai|B)
Arevise beliefs, given that Boccurred
1
B
A1
B
A A2 3
One way of computingAP( AB): 2 3
P(B)= P(A1)P(B |A1)
+P(A2)P(B |A2)
P(+ ( ) ( |)AiB)PA3PB A 3 P(Ai|B)=P(B)
P(Ai)P(B=|Ai)
P(B)
P(A)P(B A )=/summationtexti |i
jP(Aj)P(B |Aj)      
Multiplication rule
P(ABC) =P(A)P(B |A)P(C |AB)
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 2 Review of probability models
Readings: Sections 1.3-1.4 Sample space 
Mutually exclusive
Collectively exhaustiveLecture outline
Right granularity
ReviewEvent: Subset of the sample space
Conditional probability
Allocation of probabilities to events
Three important tools:
1.P(A)0
Multiplication rule 2.P()=1
3. If Total probability theorem AB= ,
thenP(AB)=P(A)+ P(B)
Bayes rule
3.IfA1,A2,...are disjoint events, then:
P(A1A2)=P(A1)+P(A2)+ 
Problem solving:
Specify sample space
Dene probability law
Identify event of interest
Calculate...
Conditional probability Die roll example
4 A
3Y = Second B        roll
2
1
P(A |B) = probability of A,
that Boccurred1 2 3given4
is our new universeXB = First roll
Denition: Assuming P(B) = 0, LetBbe the event: min(X, Y )=2
P(AP(A B )=B)|LetM= max( X, Y)
P(B)
P(A |B) undened if P(B)=0 P(M=1| B)=
P(M=2| B)=/negationslash
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-13-bernoulli-process/</video_url>
          <video_title>Lecture 13: Bernoulli Process</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:17</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: So by now you have
seen pretty much every</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>possible trick there is in
basic probability theory,</text_slice>
            </slice>
            <slice>
              <time_slice>0:30</time_slice>
              <text_slice>about how to calculate
distributions, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>You have the basic tools to
do pretty much anything.</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>So what's coming after this?</text_slice>
            </slice>
            <slice>
              <time_slice>0:40</time_slice>
              <text_slice>Well, probability is useful for
developing the science of</text_slice>
            </slice>
            <slice>
              <time_slice>0:45</time_slice>
              <text_slice>inference, and this is a subject
to which we're going</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>to come back at the end
of the semester.</text_slice>
            </slice>
            <slice>
              <time_slice>0:51</time_slice>
              <text_slice>Another chapter, which is what
we will be doing over the next</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>few weeks, is to deal with
phenomena that evolve in time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>So so-called random processes
or stochastic processes.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>So what is this about?</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>So in the real world, you don't
just throw two random</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>variables and go home.</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>Rather the world goes on.</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>So you generate the random
variable, then you get more</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>random variables, and things
evolve in time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:18</time_slice>
              <text_slice>And random processes are
supposed to be models that</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>capture the evolution of random
phenomena over time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>So that's what we
will be doing.</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>Now when we have evolution in
time, mathematically speaking,</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>you can use discrete time
or continuous time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:34</time_slice>
              <text_slice>Of course, discrete
time is easier.</text_slice>
            </slice>
            <slice>
              <time_slice>1:36</time_slice>
              <text_slice>And that's where we're
going to start from.</text_slice>
            </slice>
            <slice>
              <time_slice>1:39</time_slice>
              <text_slice>And we're going to start from
the easiest, simplest random</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>process, which is the so-called
Bernoulli process,</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>which is nothing but just a
sequence of coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>You keep flipping a coin
and keep going forever.</text_slice>
            </slice>
            <slice>
              <time_slice>1:54</time_slice>
              <text_slice>That's what the Bernoulli
process is.</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>So in some sense it's something
that you have</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>already seen.</text_slice>
            </slice>
            <slice>
              <time_slice>1:59</time_slice>
              <text_slice>But we're going to introduce a
few additional ideas here that</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>will be useful and relevant as
we go along and we move on to</text_slice>
            </slice>
            <slice>
              <time_slice>2:07</time_slice>
              <text_slice>continuous time processes.</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>So we're going to define the
Bernoulli process, talk about</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>some basic properties that the
process has, and derive a few</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>formulas, and exploit the
special structure that it has</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>to do a few quite interesting
things.</text_slice>
            </slice>
            <slice>
              <time_slice>2:24</time_slice>
              <text_slice>By the way, where does the
word Bernoulli come from?</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>Well the Bernoulli's were a
family of mathematicians,</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>Swiss mathematicians and
scientists around the 1700s.</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>There were so many of
them that actually--</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>and some of them had the
same first name--</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>historians even have difficulty
of figuring out who</text_slice>
            </slice>
            <slice>
              <time_slice>2:46</time_slice>
              <text_slice>exactly did what.</text_slice>
            </slice>
            <slice>
              <time_slice>2:48</time_slice>
              <text_slice>But in any case, you can imagine
that at the dinner</text_slice>
            </slice>
            <slice>
              <time_slice>2:51</time_slice>
              <text_slice>table they were probably
flipping coins and doing</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>2:55</time_slice>
              <text_slice>So maybe that was
their pass-time.</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>So what is the Bernoulli
process?</text_slice>
            </slice>
            <slice>
              <time_slice>3:02</time_slice>
              <text_slice>The Bernoulli process is nothing
but a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>independent Bernoulli
trials that you can</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>think of as coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>So you can think the result
of each trial</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>being heads or tails.</text_slice>
            </slice>
            <slice>
              <time_slice>3:15</time_slice>
              <text_slice>It's a little more convenient
maybe to talk about successes</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>and failures instead
of heads or tails.</text_slice>
            </slice>
            <slice>
              <time_slice>3:21</time_slice>
              <text_slice>Or if you wish numerical values,
to use a 1 for a</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>success and 0 for a failure.</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>So the model is that each one
of these trials has the same</text_slice>
            </slice>
            <slice>
              <time_slice>3:30</time_slice>
              <text_slice>probability of success, p.</text_slice>
            </slice>
            <slice>
              <time_slice>3:34</time_slice>
              <text_slice>And the other assumption is
that these trials are</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>statistically independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>So what could be some examples
of Bernoulli trials?</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>You buy a lottery ticket every
week and you win or lose.</text_slice>
            </slice>
            <slice>
              <time_slice>3:48</time_slice>
              <text_slice>Presumably, these are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>3:50</time_slice>
              <text_slice>And if it's the same kind of
lottery, the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>3:53</time_slice>
              <text_slice>winning should be the same
during every week.</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>Maybe you want to model
the financial markets.</text_slice>
            </slice>
            <slice>
              <time_slice>3:58</time_slice>
              <text_slice>And a crude model could be that
on any given day the Dow</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>Jones is going to go up
or down with a certain</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>Well that probability must be
somewhere around 0.5, or so.</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>This is a crude model of
financial markets.</text_slice>
            </slice>
            <slice>
              <time_slice>4:14</time_slice>
              <text_slice>You say, probably there
is more into them.</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>Life is not that simple.</text_slice>
            </slice>
            <slice>
              <time_slice>4:19</time_slice>
              <text_slice>But actually it's a pretty
reasonable model.</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>It takes quite a bit of work
to come up with more</text_slice>
            </slice>
            <slice>
              <time_slice>4:26</time_slice>
              <text_slice>sophisticated models that can
do better predictions than</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>just pure heads and tails.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>Now more interesting, perhaps
to the examples we will be</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>dealing with in this class--</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>a Bernoulli process is a good
model for streams of arrivals</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>of any kind to a facility.</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>So it could be a bank, and
you are sitting at</text_slice>
            </slice>
            <slice>
              <time_slice>4:49</time_slice>
              <text_slice>the door of the bank.</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>And at every second, you check
whether a customer came in</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>during that second or not.</text_slice>
            </slice>
            <slice>
              <time_slice>4:56</time_slice>
              <text_slice>Or you can think about arrivals
of jobs to a server.</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>Or any other kind of requests
to a service system.</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>So requests, or jobs, arrive
at random times.</text_slice>
            </slice>
            <slice>
              <time_slice>5:08</time_slice>
              <text_slice>You split the time
into time slots.</text_slice>
            </slice>
            <slice>
              <time_slice>5:12</time_slice>
              <text_slice>And during each time slot
something comes or something</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>does not come.</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>And for many applications, it's
a reasonable assumption</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>to make that arrivals on any
given slot are independent of</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>arrivals in any other
time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>So each time slot can be viewed
as a trial, where</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>either something comes
or doesn't come.</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>And different trials are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>5:35</time_slice>
              <text_slice>Now there's two assumptions
that we're making here.</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>One is the independence
assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>The other is that this number,
p, probability</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>of success, is constant.</text_slice>
            </slice>
            <slice>
              <time_slice>5:44</time_slice>
              <text_slice>Now if you think about the bank
example, if you stand</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>outside the bank at 9:30 in
the morning, you'll see</text_slice>
            </slice>
            <slice>
              <time_slice>5:53</time_slice>
              <text_slice>arrivals happening at
a certain rate.</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>If you stand outside the bank
at 12:00 noon, probably</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>arrivals are more frequent.</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>Which means that the given
time slot has a higher</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>probability of seeing an arrival
around noon time.</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>This means that the assumption
of a constant p is probably</text_slice>
            </slice>
            <slice>
              <time_slice>6:11</time_slice>
              <text_slice>not correct in that setting,
if you're talking</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>about the whole day.</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>So the probability of successes
or arrivals in the</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>morning is going to be smaller
than what it would be at noon.</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>But if you're talking about a
time period, let's say 10:00</text_slice>
            </slice>
            <slice>
              <time_slice>6:27</time_slice>
              <text_slice>to 10:15, probably all slots
have the same probability of</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>seeing an arrival and it's
a good approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>So we're going to stick with
the assumption that p is</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>constant, doesn't change
with time.</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>Now that we have our model
what do we do with it?</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>Well, we start talking about
the statistical properties</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>that it has.</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>And here there's two slightly
different perspectives of</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>thinking about what a
random process is.</text_slice>
            </slice>
            <slice>
              <time_slice>6:55</time_slice>
              <text_slice>The simplest version is to think
about the random process</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>as being just a sequence
of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:04</time_slice>
              <text_slice>We know what random
variables are.</text_slice>
            </slice>
            <slice>
              <time_slice>7:06</time_slice>
              <text_slice>We know what multiple random
variables are.</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>So it's just an experiment that
has associated with it a</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>bunch of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:14</time_slice>
              <text_slice>So once you have random
variables, what do you do</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>instinctively?</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>You talk about the
distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>these random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:21</time_slice>
              <text_slice>We already specified for the
Bernoulli process that each Xi</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>is a Bernoulli random variable,
with probability of</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>success equal to p.</text_slice>
            </slice>
            <slice>
              <time_slice>7:29</time_slice>
              <text_slice>That specifies the distribution
of the random</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>variable X, or Xt, for
general time t.</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>Then you can calculate
expected values and</text_slice>
            </slice>
            <slice>
              <time_slice>7:37</time_slice>
              <text_slice>variances, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>7:39</time_slice>
              <text_slice>So the expected value is, with
probability p, you get a 1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>And with probability
1 - p, you get a 0.</text_slice>
            </slice>
            <slice>
              <time_slice>7:46</time_slice>
              <text_slice>So the expected value
is equal to p.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>And then we have seen before a
formula for the variance of</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>the Bernoulli random variable,
which is p times 1-p.</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>So this way we basically now
have all the statistical</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>properties of the random
variable Xt, and we have those</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>properties for every t.</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>Is this enough of a
probabilistic description of a</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>random process?</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>Well, no.</text_slice>
            </slice>
            <slice>
              <time_slice>8:12</time_slice>
              <text_slice>You need to know how the
different random variables</text_slice>
            </slice>
            <slice>
              <time_slice>8:15</time_slice>
              <text_slice>relate to each other.</text_slice>
            </slice>
            <slice>
              <time_slice>8:16</time_slice>
              <text_slice>If you're talking about a
general random process, you</text_slice>
            </slice>
            <slice>
              <time_slice>8:20</time_slice>
              <text_slice>would like to know things.</text_slice>
            </slice>
            <slice>
              <time_slice>8:23</time_slice>
              <text_slice>For example, the joint
distribution of X2,</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>with X5, and X7.</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>For example, that might be
something that you're</text_slice>
            </slice>
            <slice>
              <time_slice>8:31</time_slice>
              <text_slice>interested in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>And the way you specify it is
by giving the joint PMF of</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>these random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>And you have to do that for
every collection, or any</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>subset, of the random
variables you</text_slice>
            </slice>
            <slice>
              <time_slice>8:46</time_slice>
              <text_slice>are interested in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>So to have a complete
description of a random</text_slice>
            </slice>
            <slice>
              <time_slice>8:49</time_slice>
              <text_slice>processes, you need to specify
for me all the possible joint</text_slice>
            </slice>
            <slice>
              <time_slice>8:54</time_slice>
              <text_slice>distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>8:55</time_slice>
              <text_slice>And once you have all the
possible joint distributions,</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>then you can answer, in
principle, any questions you</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>might be interested in.</text_slice>
            </slice>
            <slice>
              <time_slice>9:03</time_slice>
              <text_slice>How did we get around
this issue for</text_slice>
            </slice>
            <slice>
              <time_slice>9:05</time_slice>
              <text_slice>the Bernoulli process?</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>I didn't give you the joint
distributions explicitly.</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>But I gave them to
you implicitly.</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>And this is because I told you
that the different random</text_slice>
            </slice>
            <slice>
              <time_slice>9:15</time_slice>
              <text_slice>variables are independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>So at least for the Bernoulli
process, where we make the</text_slice>
            </slice>
            <slice>
              <time_slice>9:21</time_slice>
              <text_slice>independence assumption, we know
that this is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>9:24</time_slice>
              <text_slice>the product of the PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>And since I have told you what
the individual PMFs are, this</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>means that you automatically
know all the joint PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>And we can go to business
based on that.</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>So this is one view of what a
random process is, just a</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>collection of random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>There's another view that's a
little more abstract, which is</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>the following.</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>The entire process is to be
thought of as one long</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>9:58</time_slice>
              <text_slice>So we go back to the
chapter one view of</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>probabilistic models.</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>So there must be a sample
space involved.</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>What is the sample space?</text_slice>
            </slice>
            <slice>
              <time_slice>10:07</time_slice>
              <text_slice>If I do my infinite, long
experiment of flipping an</text_slice>
            </slice>
            <slice>
              <time_slice>10:11</time_slice>
              <text_slice>infinite number of coins,
a typical outcome of the</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>experiment would be a sequence
of 0's and 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>So this could be one possible
outcome of the experiment,</text_slice>
            </slice>
            <slice>
              <time_slice>10:25</time_slice>
              <text_slice>just an infinite sequence
of 0's and 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>My sample space is the
set of all possible</text_slice>
            </slice>
            <slice>
              <time_slice>10:33</time_slice>
              <text_slice>outcomes of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>10:35</time_slice>
              <text_slice>Here's another possible
outcome, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>10:40</time_slice>
              <text_slice>And essentially we're dealing
with a sample space, which is</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>the space of all sequences
of 0's and 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>10:46</time_slice>
              <text_slice>And we're making some sort of
probabilistic assumption about</text_slice>
            </slice>
            <slice>
              <time_slice>10:50</time_slice>
              <text_slice>what may happen in
that experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>So one particular sequence that
we may be interested in</text_slice>
            </slice>
            <slice>
              <time_slice>10:56</time_slice>
              <text_slice>is the sequence of obtaining
all 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>So this is the sequence that
gives you 1's forever.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>Once you take the point of view
that this is our sample</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>space-- its the space of all
infinite sequences--</text_slice>
            </slice>
            <slice>
              <time_slice>11:10</time_slice>
              <text_slice>you can start asking questions
that have to do</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>with infinite sequences.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>Such as the question, what's the
probability of obtaining</text_slice>
            </slice>
            <slice>
              <time_slice>11:19</time_slice>
              <text_slice>the infinite sequence that
consists of all 1's?</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>So what is this probability?</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>Let's see how we could
calculate it.</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>So the probability of obtaining
all 1's is certainly</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>less than or equal to the
probability of obtaining 1's,</text_slice>
            </slice>
            <slice>
              <time_slice>11:39</time_slice>
              <text_slice>just in the first 10 tosses.</text_slice>
            </slice>
            <slice>
              <time_slice>11:45</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>This is asking for more things
to happen than this.</text_slice>
            </slice>
            <slice>
              <time_slice>11:50</time_slice>
              <text_slice>If this event is true, then
this is also true.</text_slice>
            </slice>
            <slice>
              <time_slice>11:55</time_slice>
              <text_slice>Therefore the probability of
this is smaller than the</text_slice>
            </slice>
            <slice>
              <time_slice>11:58</time_slice>
              <text_slice>probability of that.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>This event is contained
in that event.</text_slice>
            </slice>
            <slice>
              <time_slice>12:03</time_slice>
              <text_slice>This implies this.</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>So we have this inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>Now what's the probability of
obtaining 1's in 10 trials?</text_slice>
            </slice>
            <slice>
              <time_slice>12:12</time_slice>
              <text_slice>This is just p to the 10th
because the trials are</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>Now of course there's no reason
why I chose 10 here.</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>The same argument goes
through if I use an</text_slice>
            </slice>
            <slice>
              <time_slice>12:26</time_slice>
              <text_slice>arbitrary number, k.</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>And this has to be
true for all k.</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>So this probability is less
than p to the k, no matter</text_slice>
            </slice>
            <slice>
              <time_slice>12:38</time_slice>
              <text_slice>what k I choose.</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>Therefore, this must be less
than or equal to the limit of</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>this, as k goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>12:48</time_slice>
              <text_slice>This is smaller than
that for all k's.</text_slice>
            </slice>
            <slice>
              <time_slice>12:51</time_slice>
              <text_slice>Let k go to infinity, take k
arbitrarily large, this number</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>is going to become arbitrarily
small.</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>It goes to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>And that proves that the
probability of an infinite</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>sequence of 1's is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>So take limits of both sides.</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>It's going to be less than
or equal to the limit--</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>I shouldn't take a limit here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>The probability is less than or
equal to the limit of p to</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>the k, as k goes to infinity,
which is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>So this proves in a formal way
that the sequence of all 1's</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>has 0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>13:32</time_slice>
              <text_slice>If you have an infinite number
of coin flips, what's the</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>probability that all of the coin
flips result in heads?</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>The probability of this
happening is equal to zero.</text_slice>
            </slice>
            <slice>
              <time_slice>13:43</time_slice>
              <text_slice>So this particular sequence
has 0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>Of course, I'm assuming here
that p is less than 1,</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>strictly less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>Now the interesting thing is
that if you look at any other</text_slice>
            </slice>
            <slice>
              <time_slice>13:56</time_slice>
              <text_slice>infinite sequence, and you try
to calculate the probability</text_slice>
            </slice>
            <slice>
              <time_slice>13:59</time_slice>
              <text_slice>of that infinite sequence, you
would get a product of (1-p)</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>times 1, 1-p times 1, 1-p,
times p times p,</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>times 1-p and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>You keep multiplying numbers
that are less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>Again, I'm making the
assumption that p is</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>between 0 and 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>So 1-p is less than 1,
p is less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:21</time_slice>
              <text_slice>You keep multiplying numbers
less than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>If you multiply infinitely
many such numbers, the</text_slice>
            </slice>
            <slice>
              <time_slice>14:26</time_slice>
              <text_slice>infinite product becomes 0.</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>So any individual sequence in
this sample space actually has</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>And that is a little bit
counter-intuitive perhaps.</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>But the situation is more like
the situation where we deal</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>with continuous random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>So if you could draw a
continuous random variable,</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>every possible outcome
has 0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>And that's fine.</text_slice>
            </slice>
            <slice>
              <time_slice>14:52</time_slice>
              <text_slice>But all of the outcomes
collectively still have</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>positive probability.</text_slice>
            </slice>
            <slice>
              <time_slice>14:56</time_slice>
              <text_slice>So the situation here is
very much similar.</text_slice>
            </slice>
            <slice>
              <time_slice>14:59</time_slice>
              <text_slice>So the space of infinite
sequences of 0's and 1's, that</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>sample space is very much
like a continuous space.</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>If you want to push that analogy
further, you could</text_slice>
            </slice>
            <slice>
              <time_slice>15:10</time_slice>
              <text_slice>think of this as the expansion
of a real number.</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>Or the representation of a
real number in binary.</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>Take a real number, write it
down in binary, you are going</text_slice>
            </slice>
            <slice>
              <time_slice>15:22</time_slice>
              <text_slice>to get an infinite sequence
of 0's and 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>So you can think of each
possible outcome here</text_slice>
            </slice>
            <slice>
              <time_slice>15:28</time_slice>
              <text_slice>essentially as a real number.</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>So the experiment of doing an
infinite number of coin flips</text_slice>
            </slice>
            <slice>
              <time_slice>15:36</time_slice>
              <text_slice>is sort of similar to the
experiment of picking a real</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>number at random.</text_slice>
            </slice>
            <slice>
              <time_slice>15:41</time_slice>
              <text_slice>When you pick real numbers at
random, any particular real</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>number has 0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>15:46</time_slice>
              <text_slice>So similarly here, any
particular infinite sequence</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>has 0 probability.</text_slice>
            </slice>
            <slice>
              <time_slice>15:52</time_slice>
              <text_slice>So if we were to push that
analogy further, there would</text_slice>
            </slice>
            <slice>
              <time_slice>15:55</time_slice>
              <text_slice>be a few interesting
things we could do.</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>But we will not push
it further.</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>This is just to give you an
indication that things can get</text_slice>
            </slice>
            <slice>
              <time_slice>16:05</time_slice>
              <text_slice>pretty subtle and interesting
once you start talking about</text_slice>
            </slice>
            <slice>
              <time_slice>16:08</time_slice>
              <text_slice>random processes that involve
forever, over the infinite</text_slice>
            </slice>
            <slice>
              <time_slice>16:12</time_slice>
              <text_slice>time horizon.</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>So things get interesting even
in this context of the simple</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>16:19</time_slice>
              <text_slice>Just to give you a preview of
what's coming further, today</text_slice>
            </slice>
            <slice>
              <time_slice>16:23</time_slice>
              <text_slice>we're going to talk just about
the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>And you should make sure before
the next lecture--</text_slice>
            </slice>
            <slice>
              <time_slice>16:30</time_slice>
              <text_slice>I guess between the exam
and the next lecture--</text_slice>
            </slice>
            <slice>
              <time_slice>16:34</time_slice>
              <text_slice>to understand everything
we do today.</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>Because next time we're going
to do everything once more,</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>but in continuous time.</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>And in continuous time, things
become more subtle and a</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>little more difficult.</text_slice>
            </slice>
            <slice>
              <time_slice>16:47</time_slice>
              <text_slice>But we are going to build on
what we understand for the</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>discrete time case.</text_slice>
            </slice>
            <slice>
              <time_slice>16:52</time_slice>
              <text_slice>Now both the Bernoulli process
and its continuous time analog</text_slice>
            </slice>
            <slice>
              <time_slice>16:55</time_slice>
              <text_slice>has a property that we call
memorylessness, whatever</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>happened in the past does
not affect the future.</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>Later on in this class we're
going to talk about more</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>general random processes,
so-called Markov chains, in</text_slice>
            </slice>
            <slice>
              <time_slice>17:07</time_slice>
              <text_slice>which there are certain
dependences across time.</text_slice>
            </slice>
            <slice>
              <time_slice>17:10</time_slice>
              <text_slice>That is, what has happened in
the past will have some</text_slice>
            </slice>
            <slice>
              <time_slice>17:15</time_slice>
              <text_slice>bearing on what may happen
in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>So it's like having coin flips
where the outcome of the next</text_slice>
            </slice>
            <slice>
              <time_slice>17:22</time_slice>
              <text_slice>coin flip has some dependence
on the previous coin flip.</text_slice>
            </slice>
            <slice>
              <time_slice>17:25</time_slice>
              <text_slice>And that gives us a richer
class of models.</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>And once we get there,
essentially we will have</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>covered all possible models.</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>So for random processes that
are practically useful and</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>which you can manipulate, Markov
chains are a pretty</text_slice>
            </slice>
            <slice>
              <time_slice>17:41</time_slice>
              <text_slice>general class of models.</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>And almost any real world
phenomenon that evolves in</text_slice>
            </slice>
            <slice>
              <time_slice>17:47</time_slice>
              <text_slice>time can be approximately
modeled using Markov chains.</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>So even though this is a first
class in probability, we will</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>get pretty far in
that direction.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>So now let's start doing a few
calculations and answer some</text_slice>
            </slice>
            <slice>
              <time_slice>18:04</time_slice>
              <text_slice>questions about the
Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>So again, the best way to think
in terms of models that</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>correspond to the Bernoulli
process is in terms of</text_slice>
            </slice>
            <slice>
              <time_slice>18:13</time_slice>
              <text_slice>arrivals of jobs
to a facility.</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>And there's two types of
questions that you can ask.</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>In a given amount of time,
how many jobs arrived?</text_slice>
            </slice>
            <slice>
              <time_slice>18:21</time_slice>
              <text_slice>Or conversely, for a given
number of jobs, how much time</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>did it take for them
to arrive?</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>So we're going to deal with
these two questions, starting</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>with the first.</text_slice>
            </slice>
            <slice>
              <time_slice>18:32</time_slice>
              <text_slice>For a given amount of time--</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>that is, for a given number
of time periods--</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>how many arrivals have we had?</text_slice>
            </slice>
            <slice>
              <time_slice>18:40</time_slice>
              <text_slice>How many of those Xi's
happen to be 1's?</text_slice>
            </slice>
            <slice>
              <time_slice>18:44</time_slice>
              <text_slice>We fix the number
of time slots--</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>let's say n time slots--</text_slice>
            </slice>
            <slice>
              <time_slice>18:47</time_slice>
              <text_slice>and you measure the number
of successes.</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>Well this is a very familiar
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>18:54</time_slice>
              <text_slice>The number of successes in n
independent coin flips--</text_slice>
            </slice>
            <slice>
              <time_slice>18:58</time_slice>
              <text_slice>or in n independent trials--</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>is a binomial random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:03</time_slice>
              <text_slice>So we know its distribution is
given by the binomial PMF, and</text_slice>
            </slice>
            <slice>
              <time_slice>19:10</time_slice>
              <text_slice>it's just this, for k going
from 0 up to n.</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>And we know everything by now
about this random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:18</time_slice>
              <text_slice>We know its expected
value is n times p.</text_slice>
            </slice>
            <slice>
              <time_slice>19:21</time_slice>
              <text_slice>And we know the variance, which
is n times p, times 1-p.</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>So there's nothing new here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:31</time_slice>
              <text_slice>That's the easy part.</text_slice>
            </slice>
            <slice>
              <time_slice>19:34</time_slice>
              <text_slice>So now let's look at the
opposite kind of question.</text_slice>
            </slice>
            <slice>
              <time_slice>19:37</time_slice>
              <text_slice>Instead of fixing the time and
asking how many arrivals, now</text_slice>
            </slice>
            <slice>
              <time_slice>19:42</time_slice>
              <text_slice>let us fix the number of
arrivals and ask how much time</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>did it take.</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>And let's start with the time
until the first arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>So the process starts.</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>We got our slots.</text_slice>
            </slice>
            <slice>
              <time_slice>20:04</time_slice>
              <text_slice>And we see, perhaps, a sequence
of 0's and then at</text_slice>
            </slice>
            <slice>
              <time_slice>20:08</time_slice>
              <text_slice>some point we get a 1.</text_slice>
            </slice>
            <slice>
              <time_slice>20:10</time_slice>
              <text_slice>The number of trials it took
until we get a 1, we're going</text_slice>
            </slice>
            <slice>
              <time_slice>20:14</time_slice>
              <text_slice>to call it T1.</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>And it's the time of
the first arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>What is the probability
distribution of T1?</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>What kind of random
variable is it?</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>We've gone through
this before.</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>The event that the first arrival
happens at time little</text_slice>
            </slice>
            <slice>
              <time_slice>20:40</time_slice>
              <text_slice>t is the event that the first
t-1 trials were failures, and</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>the trial number t happens
to be a success.</text_slice>
            </slice>
            <slice>
              <time_slice>20:52</time_slice>
              <text_slice>So for the first success to
happen at time slot number 5,</text_slice>
            </slice>
            <slice>
              <time_slice>20:57</time_slice>
              <text_slice>it means that the first 4 slots
had failures and the 5th</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>slot had a success.</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>So the probability of this
happening is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>21:08</time_slice>
              <text_slice>of having failures in the first
t -1 trials, and having</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>a success at trial number 1.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>And this is the formula for
t equal 1,2, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>So we know what this
distribution is.</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>It's the so-called geometric
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>Let me jump this through
this for a minute.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>In the past, we did calculate
the expected value of the</text_slice>
            </slice>
            <slice>
              <time_slice>21:38</time_slice>
              <text_slice>geometric distribution,
and it's 1/p.</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>Which means that if p is small,
you expect to take a</text_slice>
            </slice>
            <slice>
              <time_slice>21:46</time_slice>
              <text_slice>long time until the
first success.</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>And then there's a formula also
for the variance of T1,</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>which we never formally derived
in class, but it was</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>in your textbook and it just
happens to be this.</text_slice>
            </slice>
            <slice>
              <time_slice>22:01</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>So nothing new until
this point.</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>Now, let's talk about
this property, the</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>memorylessness property.</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>We kind of touched on this
property when we discussed--</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>when we did the derivation
in class of the</text_slice>
            </slice>
            <slice>
              <time_slice>22:15</time_slice>
              <text_slice>expected value of T1.</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>Now what is the memoryless
property?</text_slice>
            </slice>
            <slice>
              <time_slice>22:20</time_slice>
              <text_slice>It's essentially a consequence
of independence.</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>If I tell you the results of my
coin flips up to a certain</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>time, this, because of
independence, doesn't give you</text_slice>
            </slice>
            <slice>
              <time_slice>22:30</time_slice>
              <text_slice>any information about the coin
flips after that time.</text_slice>
            </slice>
            <slice>
              <time_slice>22:37</time_slice>
              <text_slice>So knowing that we had lots of
0's here does not change what</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>I believe about the future coin
flips, because the future</text_slice>
            </slice>
            <slice>
              <time_slice>22:44</time_slice>
              <text_slice>coin flips are going to be just
independent coin flips</text_slice>
            </slice>
            <slice>
              <time_slice>22:47</time_slice>
              <text_slice>with a given probability,
p, for obtaining tails.</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>So this is a statement that I
made about a specific time.</text_slice>
            </slice>
            <slice>
              <time_slice>22:58</time_slice>
              <text_slice>That is, you do coin flips
until 12 o'clock.</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>And then at 12 o'clock,
you start watching.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>No matter what happens before 12
o'clock, after 12:00, what</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>you're going to see is just
a sequence of independent</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>Bernoulli trials with the
same probability, p.</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>Whatever happened in the
past is irrelevant.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>Now instead of talking about
the fixed time at which you</text_slice>
            </slice>
            <slice>
              <time_slice>23:21</time_slice>
              <text_slice>start watching, let's think
about a situation where your</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>sister sits in the next room,
flips the coins until she</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>observes the first success,
and then calls you inside.</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>And you start watching
after this time.</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>What are you're going to see?</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>Well, you're going to see a coin
flip with probability p</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>of success.</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>You're going to see another
trial that has probability p</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>as a success, and these are all
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>So what you're going to see
starting at that time is going</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>to be just a sequence of
independent Bernoulli trials,</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>as if the process was starting
at this time.</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>How long it took for the first
success to occur doesn't have</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>any bearing on what is going
to happen afterwards.</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>What happens afterwards
is still a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>independent coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>And this story is actually
even more general.</text_slice>
            </slice>
            <slice>
              <time_slice>24:24</time_slice>
              <text_slice>So your sister watches the coin
flips and at some point</text_slice>
            </slice>
            <slice>
              <time_slice>24:28</time_slice>
              <text_slice>tells you, oh, something
really interesting is</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>happening here.</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>I got this string of a
hundred 1's in a row.</text_slice>
            </slice>
            <slice>
              <time_slice>24:35</time_slice>
              <text_slice>Come and watch.</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>Now when you go in there and
you start watching, do you</text_slice>
            </slice>
            <slice>
              <time_slice>24:40</time_slice>
              <text_slice>expect to see something
unusual?</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>There were unusual things
that happened before</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>you were called in.</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>Does this means that you're
going to see unusual things</text_slice>
            </slice>
            <slice>
              <time_slice>24:50</time_slice>
              <text_slice>afterwards?</text_slice>
            </slice>
            <slice>
              <time_slice>24:51</time_slice>
              <text_slice>No.</text_slice>
            </slice>
            <slice>
              <time_slice>24:52</time_slice>
              <text_slice>Afterwards, what you're going
to see is, again, just a</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>sequence of independent
coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>The fact that some strange
things happened before doesn't</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>have any bearing as to what is
going to happen in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>So if the roulettes in the
casino are properly made, the</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>fact that there were 3 reds in
a row doesn't affect the odds</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>of whether in the next
roll it's going to</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>be a red or a black.</text_slice>
            </slice>
            <slice>
              <time_slice>25:19</time_slice>
              <text_slice>So whatever happens in
the past-- no matter</text_slice>
            </slice>
            <slice>
              <time_slice>25:22</time_slice>
              <text_slice>how unusual it is--</text_slice>
            </slice>
            <slice>
              <time_slice>25:25</time_slice>
              <text_slice>at the time when you're called
in, what's going to happen in</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>the future is going to be just
independent Bernoulli trials,</text_slice>
            </slice>
            <slice>
              <time_slice>25:32</time_slice>
              <text_slice>with the same probability, p.</text_slice>
            </slice>
            <slice>
              <time_slice>25:36</time_slice>
              <text_slice>The only case where this story
changes is if your sister has</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>a little bit of foresight.</text_slice>
            </slice>
            <slice>
              <time_slice>25:43</time_slice>
              <text_slice>So your sister can look ahead
into the future and knows that</text_slice>
            </slice>
            <slice>
              <time_slice>25:48</time_slice>
              <text_slice>the next 10 coin flips will be
heads, and calls you before</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>those 10 flips will happen.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>If she calls you in, then what
are you going to see?</text_slice>
            </slice>
            <slice>
              <time_slice>25:59</time_slice>
              <text_slice>You're not going to see
independent Bernoulli trials,</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>since she has psychic powers
and she knows that the next</text_slice>
            </slice>
            <slice>
              <time_slice>26:05</time_slice>
              <text_slice>ones would be 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>26:06</time_slice>
              <text_slice>She called you in and you will
see a sequence of 1's.</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>So it's no more independent
Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>So what's the subtle
difference here?</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>The future is independent from
the past, provided that the</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>time that you are called and
asked to start watching is</text_slice>
            </slice>
            <slice>
              <time_slice>26:28</time_slice>
              <text_slice>determined by someone who
doesn't have any foresight,</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>who cannot see the future.</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>If you are called in, just on
the basis of what has happened</text_slice>
            </slice>
            <slice>
              <time_slice>26:36</time_slice>
              <text_slice>so far, then you don't have any</text_slice>
            </slice>
            <slice>
              <time_slice>26:39</time_slice>
              <text_slice>information about the future.</text_slice>
            </slice>
            <slice>
              <time_slice>26:41</time_slice>
              <text_slice>And one special case is
the picture here.</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>You have your coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>26:47</time_slice>
              <text_slice>Once you see a one that happens,
once you see a</text_slice>
            </slice>
            <slice>
              <time_slice>26:51</time_slice>
              <text_slice>success, you are called in.</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>You are called in on the basis
of what happened in the past,</text_slice>
            </slice>
            <slice>
              <time_slice>26:57</time_slice>
              <text_slice>but without any foresight.</text_slice>
            </slice>
            <slice>
              <time_slice>27:02</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>27:03</time_slice>
              <text_slice>And this subtle distinction is
what's going to make our next</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>example interesting
and subtle.</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>So here's the question.</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>You buy a lottery ticket every
day, so we have a Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>27:17</time_slice>
              <text_slice>process that's running
in time.</text_slice>
            </slice>
            <slice>
              <time_slice>27:21</time_slice>
              <text_slice>And you're interested in the
length of the first string of</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>losing days.</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>What does that mean?</text_slice>
            </slice>
            <slice>
              <time_slice>27:28</time_slice>
              <text_slice>So suppose that a typical
sequence of events</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>could be this one.</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>So what are we discussing
here?</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>We're looking at the first
string of losing days, where</text_slice>
            </slice>
            <slice>
              <time_slice>27:47</time_slice>
              <text_slice>losing days means 0's.</text_slice>
            </slice>
            <slice>
              <time_slice>27:51</time_slice>
              <text_slice>So the string of losing days
is this string here.</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>Let's call the length of that
string, L. We're interested in</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>the random variable, which is
the length of this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>28:06</time_slice>
              <text_slice>What kind of random
variable is it?</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>Here's one possible way you
might think about the problem.</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>Starting from this time, and
looking until this time here,</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>what are we looking at?</text_slice>
            </slice>
            <slice>
              <time_slice>28:27</time_slice>
              <text_slice>We're looking at the time,
starting from here, until the</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>first success.</text_slice>
            </slice>
            <slice>
              <time_slice>28:35</time_slice>
              <text_slice>So the past doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>Starting from here we
have coin flips</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>until the first success.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>The time until the
first success</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>in a Bernoulli process--</text_slice>
            </slice>
            <slice>
              <time_slice>28:50</time_slice>
              <text_slice>we just discussed that it's a
geometric random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>So your first conjecture would
be that this random variable</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>here, which is 1 longer than the
one we are interested in,</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>that perhaps is a geometric
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>And if this were so, then you
could say that the random</text_slice>
            </slice>
            <slice>
              <time_slice>29:18</time_slice>
              <text_slice>variable, L, is a geometric,
minus 1.</text_slice>
            </slice>
            <slice>
              <time_slice>29:23</time_slice>
              <text_slice>Can that be the correct
answer?</text_slice>
            </slice>
            <slice>
              <time_slice>29:26</time_slice>
              <text_slice>A geometric random variable,
what values does it take?</text_slice>
            </slice>
            <slice>
              <time_slice>29:29</time_slice>
              <text_slice>It takes values 1,
2, 3, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>29:33</time_slice>
              <text_slice>1 minus a geometric would
take values from 0,</text_slice>
            </slice>
            <slice>
              <time_slice>29:37</time_slice>
              <text_slice>1, 2, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>Can the random variable
L be 0?</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>No.</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>The random variable L
is the length of a</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>string of losing days.</text_slice>
            </slice>
            <slice>
              <time_slice>29:50</time_slice>
              <text_slice>So the shortest that L could
be, would be just 1.</text_slice>
            </slice>
            <slice>
              <time_slice>29:56</time_slice>
              <text_slice>If you get just one losing day
and then you start winning, L</text_slice>
            </slice>
            <slice>
              <time_slice>29:59</time_slice>
              <text_slice>would be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>So L cannot be 0 by definition,
which means that L</text_slice>
            </slice>
            <slice>
              <time_slice>30:05</time_slice>
              <text_slice>+ 1 cannot be 1,
by definition.</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>But if L +1 were geometric,
it could be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:14</time_slice>
              <text_slice>Therefore this random
variable, L</text_slice>
            </slice>
            <slice>
              <time_slice>30:16</time_slice>
              <text_slice>+ 1, is not a geometric.</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>Why is it not geometric?</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>I started watching
at this time.</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>From this time until the first
success, that should be a</text_slice>
            </slice>
            <slice>
              <time_slice>30:33</time_slice>
              <text_slice>geometric random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>Where's the catch?</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>If I'm asked to start watching
at this time, it's because my</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>sister knows that the next
one was a failure.</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>This is the time where the
string of failures starts.</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>In order to know that they
should start watching here,</text_slice>
            </slice>
            <slice>
              <time_slice>30:56</time_slice>
              <text_slice>it's the same as if
I'm told that the</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>next one is a failure.</text_slice>
            </slice>
            <slice>
              <time_slice>31:01</time_slice>
              <text_slice>So to be asked to start watching
at this time requires</text_slice>
            </slice>
            <slice>
              <time_slice>31:05</time_slice>
              <text_slice>that someone looked
in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>And in that case, it's no longer
true that these will be</text_slice>
            </slice>
            <slice>
              <time_slice>31:13</time_slice>
              <text_slice>independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>31:14</time_slice>
              <text_slice>In fact, they're not.</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>If you start watching here,
you're certain that the next</text_slice>
            </slice>
            <slice>
              <time_slice>31:18</time_slice>
              <text_slice>one is a failure.</text_slice>
            </slice>
            <slice>
              <time_slice>31:20</time_slice>
              <text_slice>The next one is not an
independent Bernoulli trial.</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>That's why the argument that
would claim that this L + 1 is</text_slice>
            </slice>
            <slice>
              <time_slice>31:26</time_slice>
              <text_slice>geometric would be incorrect.</text_slice>
            </slice>
            <slice>
              <time_slice>31:30</time_slice>
              <text_slice>So if this is not the correct
answer, which</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>is the correct answer?</text_slice>
            </slice>
            <slice>
              <time_slice>31:35</time_slice>
              <text_slice>The correct answer
goes as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>Your sister is watching.</text_slice>
            </slice>
            <slice>
              <time_slice>31:39</time_slice>
              <text_slice>Your sister sees the first
failure, and then tells you,</text_slice>
            </slice>
            <slice>
              <time_slice>31:44</time_slice>
              <text_slice>OK, the failures--</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>or losing days--</text_slice>
            </slice>
            <slice>
              <time_slice>31:46</time_slice>
              <text_slice>have started.</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>Come in and watch.</text_slice>
            </slice>
            <slice>
              <time_slice>31:49</time_slice>
              <text_slice>So you start to watching
at this time.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>And you start watching until
the first success comes.</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>This will be a geometric
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>31:59</time_slice>
              <text_slice>So from here to here, this
will be geometric.</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>So things happen.</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>You are asked to
start watching.</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>After you start watching, the
future is just a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>32:18</time_slice>
              <text_slice>independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>32:20</time_slice>
              <text_slice>And the time until the first
failure occurs, this is going</text_slice>
            </slice>
            <slice>
              <time_slice>32:23</time_slice>
              <text_slice>to be a geometric random
variable with parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>And then you notice that the
interval of interest is</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>exactly the same as the length
of this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:35</time_slice>
              <text_slice>This starts one time step
later, and ends</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>one time step later.</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>So conclusion is that L is
actually geometric, with</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>OK, it looks like I'm
missing one slide.</text_slice>
            </slice>
            <slice>
              <time_slice>33:36</time_slice>
              <text_slice>Can I cheat a little
from here?</text_slice>
            </slice>
            <slice>
              <time_slice>33:46</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>33:48</time_slice>
              <text_slice>So now that we dealt with the
time until the first arrival,</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>we can start talking about
the time until the second</text_slice>
            </slice>
            <slice>
              <time_slice>33:56</time_slice>
              <text_slice>arrival, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>How do we define these?</text_slice>
            </slice>
            <slice>
              <time_slice>33:59</time_slice>
              <text_slice>After the first arrival happens,
we're going to have a</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>sequence of time slots with no
arrivals, and then the next</text_slice>
            </slice>
            <slice>
              <time_slice>34:06</time_slice>
              <text_slice>arrival is going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>So we call this time
that elapses--</text_slice>
            </slice>
            <slice>
              <time_slice>34:11</time_slice>
              <text_slice>or number of time slots after
the first arrival</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>until the next one--</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>we call it T2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>This is the second inter-arrival
time, that is,</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>time between arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>Once this arrival has happened,
then we wait and see</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>how many more it takes until
the third arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>34:31</time_slice>
              <text_slice>And we call this
time here, T3.</text_slice>
            </slice>
            <slice>
              <time_slice>34:37</time_slice>
              <text_slice>We're interested in the time of
the k-th arrival, which is</text_slice>
            </slice>
            <slice>
              <time_slice>34:42</time_slice>
              <text_slice>going to be just the
sum of the first k</text_slice>
            </slice>
            <slice>
              <time_slice>34:45</time_slice>
              <text_slice>inter-arrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>So for example, let's say Y3
is the time that the third</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>arrival comes.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>Y3 is just the sum of T1,
plus T2, plus T3.</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>So we're interested in this
random variable, Y3, and it's</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>the sum of inter-arrival
times.</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>To understand what kind of
random variable it is, I guess</text_slice>
            </slice>
            <slice>
              <time_slice>35:12</time_slice>
              <text_slice>we should understand what kind
of random variables these are</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>35:18</time_slice>
              <text_slice>So what kind of random
variable is T2?</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>Your sister is doing her coin
flips until a success is</text_slice>
            </slice>
            <slice>
              <time_slice>35:27</time_slice>
              <text_slice>observed for the first time.</text_slice>
            </slice>
            <slice>
              <time_slice>35:29</time_slice>
              <text_slice>Based on that information about
what has happened so</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>far, you are called
into the room.</text_slice>
            </slice>
            <slice>
              <time_slice>35:34</time_slice>
              <text_slice>And you start watching until a
success is observed again.</text_slice>
            </slice>
            <slice>
              <time_slice>35:39</time_slice>
              <text_slice>So after you start watching,
what you have is just a</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>sequence of independent
Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>35:45</time_slice>
              <text_slice>So each one of these
has probability</text_slice>
            </slice>
            <slice>
              <time_slice>35:47</time_slice>
              <text_slice>p of being a success.</text_slice>
            </slice>
            <slice>
              <time_slice>35:49</time_slice>
              <text_slice>The time it's going to take
until the first success, this</text_slice>
            </slice>
            <slice>
              <time_slice>35:52</time_slice>
              <text_slice>number, T2, is going to be again
just another geometric</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>It's as if the process
just started.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>After you are called into the
room, you have no foresight,</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>you don't have any information
about the future, other than</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>the fact that these
are going to be</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>36:13</time_slice>
              <text_slice>So T2 itself is going to be
geometric with the same</text_slice>
            </slice>
            <slice>
              <time_slice>36:18</time_slice>
              <text_slice>parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>36:20</time_slice>
              <text_slice>And then you can continue the
arguments and argue that T3 is</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>also geometric with the
same parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>36:27</time_slice>
              <text_slice>Furthermore, whatever happened,
how long it took</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>until you were called in, it
doesn't change the statistics</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>about what's going to happen
in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>36:36</time_slice>
              <text_slice>So whatever happens
in the future is</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>independent from the past.</text_slice>
            </slice>
            <slice>
              <time_slice>36:41</time_slice>
              <text_slice>So T1, T2, and T3 are
independent random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>So conclusion is that the time
until the third arrival is the</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>sum of 3 independent geometric
random variables, with the</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>same parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>And this is true
more generally.</text_slice>
            </slice>
            <slice>
              <time_slice>37:04</time_slice>
              <text_slice>The time until the k-th arrival
is going to be the sum</text_slice>
            </slice>
            <slice>
              <time_slice>37:08</time_slice>
              <text_slice>of k independent random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>37:14</time_slice>
              <text_slice>So in general, Yk is going to be
T1 plus Tk, where the Ti's</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>are geometric, with the same
parameter p, and independent.</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>So now what's more natural
than trying to find the</text_slice>
            </slice>
            <slice>
              <time_slice>37:33</time_slice>
              <text_slice>distribution of the random
variable Yk?</text_slice>
            </slice>
            <slice>
              <time_slice>37:37</time_slice>
              <text_slice>How can we find it?</text_slice>
            </slice>
            <slice>
              <time_slice>37:38</time_slice>
              <text_slice>So I fixed k for you.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>Let's say k is 100.</text_slice>
            </slice>
            <slice>
              <time_slice>37:41</time_slice>
              <text_slice>I'm interested in how
long it takes until</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>100 customers arrive.</text_slice>
            </slice>
            <slice>
              <time_slice>37:46</time_slice>
              <text_slice>How can we find the distribution
of Yk?</text_slice>
            </slice>
            <slice>
              <time_slice>37:48</time_slice>
              <text_slice>Well one way of doing
it is to use this</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>lovely convolution formula.</text_slice>
            </slice>
            <slice>
              <time_slice>37:54</time_slice>
              <text_slice>Take a geometric, convolve it
with another geometric, you</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>get something.</text_slice>
            </slice>
            <slice>
              <time_slice>37:59</time_slice>
              <text_slice>Take that something that you
got, convolve it with a</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>geometric once more, do this 99
times, and this gives you</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>the distribution of Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>38:09</time_slice>
              <text_slice>So that's definitely doable,
and it's extremely tedious.</text_slice>
            </slice>
            <slice>
              <time_slice>38:14</time_slice>
              <text_slice>Let's try to find
the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>38:16</time_slice>
              <text_slice>of Yk using a shortcut.</text_slice>
            </slice>
            <slice>
              <time_slice>38:22</time_slice>
              <text_slice>So the probability that
Yk is equal to t.</text_slice>
            </slice>
            <slice>
              <time_slice>38:28</time_slice>
              <text_slice>So we're trying to find
the PMF of Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>k has been fixed for us.</text_slice>
            </slice>
            <slice>
              <time_slice>38:34</time_slice>
              <text_slice>And we want to calculate this
probability for the various</text_slice>
            </slice>
            <slice>
              <time_slice>38:36</time_slice>
              <text_slice>values of t, because this
is going to give</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>us the PMF of Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>38:45</time_slice>
              <text_slice>What is this event?</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>What does it take for the k-th
arrival to be at time t?</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>For that to happen, we
need two things.</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>In the first t -1 slots,
how many arrivals</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>should we have gotten?</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>k - 1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>And then in the last slot, we
get one more arrival, and</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>that's the k-th one.</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>So this is the probability that
we have k - 1 arrivals in</text_slice>
            </slice>
            <slice>
              <time_slice>39:20</time_slice>
              <text_slice>the time interval
from 1 up to t.</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>And then, an arrival
at time t.</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>That's the only way that it
can happen, that the k-th</text_slice>
            </slice>
            <slice>
              <time_slice>39:43</time_slice>
              <text_slice>arrival happens at time t.</text_slice>
            </slice>
            <slice>
              <time_slice>39:45</time_slice>
              <text_slice>We need to have an arrival
at time t.</text_slice>
            </slice>
            <slice>
              <time_slice>39:48</time_slice>
              <text_slice>And before that time,
we need to have</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>exactly k - 1 arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>Now this is an event
that refers--</text_slice>
            </slice>
            <slice>
              <time_slice>39:58</time_slice>
              <text_slice>t-1.</text_slice>
            </slice>
            <slice>
              <time_slice>40:02</time_slice>
              <text_slice>In the previous time slots we
had exactly k -1 arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>And then at the last time slot
we get one more arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>Now the interesting thing is
that this event here has to do</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>with what happened from time
1 up to time t -1.</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>This event has to do with
what happened at time t.</text_slice>
            </slice>
            <slice>
              <time_slice>40:22</time_slice>
              <text_slice>Different time slots are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>40:25</time_slice>
              <text_slice>So this event and that event
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>So this means that we can
multiply their probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>So take the probability
of this.</text_slice>
            </slice>
            <slice>
              <time_slice>40:37</time_slice>
              <text_slice>What is that?</text_slice>
            </slice>
            <slice>
              <time_slice>40:38</time_slice>
              <text_slice>Well probability of having a
certain number of arrivals in</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>a certain number of time slots,
these are just the</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>binomial probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>40:46</time_slice>
              <text_slice>So this is, out of t - 1 slots,
to get exactly k - 1</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>arrivals, p to the k-1, (1-p)
to the t-1 - (k-1),</text_slice>
            </slice>
            <slice>
              <time_slice>41:02</time_slice>
              <text_slice>this gives us t-k.</text_slice>
            </slice>
            <slice>
              <time_slice>41:05</time_slice>
              <text_slice>And then we multiply with this
probability, the probability</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>of an arrival, at time
t is equal to p.</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>And so this is the formula for
the PMF of the number--</text_slice>
            </slice>
            <slice>
              <time_slice>41:21</time_slice>
              <text_slice>of the time it takes until
the k-th arrival happens.</text_slice>
            </slice>
            <slice>
              <time_slice>41:32</time_slice>
              <text_slice>Does it agree with the formula
in your handout?</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>Or its not there?</text_slice>
            </slice>
            <slice>
              <time_slice>41:37</time_slice>
              <text_slice>It's not there.</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:48</time_slice>
              <text_slice>Yeah.</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:50</time_slice>
              <text_slice>So that's the formula and it is
true for what values of t?</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>[INAUDIBLE].</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>It takes at least k time slots
in order to get k arrivals, so</text_slice>
            </slice>
            <slice>
              <time_slice>42:08</time_slice>
              <text_slice>this formula should be
true for k larger</text_slice>
            </slice>
            <slice>
              <time_slice>42:12</time_slice>
              <text_slice>than or equal to t.</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>For t larger than
or equal to k.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>42:31</time_slice>
              <text_slice>So this gives us the PMF of
the random variable Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>Of course, we may also be
interested in the mean and</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>variance of Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>42:39</time_slice>
              <text_slice>But this is a lot easier.</text_slice>
            </slice>
            <slice>
              <time_slice>42:42</time_slice>
              <text_slice>Since Yk is the sum of
independent random variables,</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>the expected value of Yk is
going to be just k times the</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>expected value of
your typical t.</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>So the expected value of Yk is
going to be just k times 1/p,</text_slice>
            </slice>
            <slice>
              <time_slice>43:03</time_slice>
              <text_slice>which is the mean of
the geometric.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>And similarly for the variance,
it's going to be k</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>times the variance
of a geometric.</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>So we have everything there is
to know about the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>43:16</time_slice>
              <text_slice>of how long it takes until
the first arrival comes.</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>Finally, let's do a few
more things about</text_slice>
            </slice>
            <slice>
              <time_slice>43:27</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>It's interesting to talk about
several processes at the time.</text_slice>
            </slice>
            <slice>
              <time_slice>43:34</time_slice>
              <text_slice>So in the situation here of
splitting a Bernoulli process</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>is where you have arrivals
that come to a server.</text_slice>
            </slice>
            <slice>
              <time_slice>43:43</time_slice>
              <text_slice>And that's a picture of which
slots get arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>But actually maybe you
have two servers.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>And whenever an arrival comes to
the system, you flip a coin</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>and with some probability, q,
you send it to one server.</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>And with probability 1-q, you
send it to another server.</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>So there is a single
arrival stream, but</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>two possible servers.</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>And whenever there's an arrival,
you either send it</text_slice>
            </slice>
            <slice>
              <time_slice>44:07</time_slice>
              <text_slice>here or you send it there.</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>And each time you decide where
you send it by flipping an</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>independent coin that
has its own bias q.</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>The coin flips that decide
where do you send it are</text_slice>
            </slice>
            <slice>
              <time_slice>44:22</time_slice>
              <text_slice>assumed to be independent from
the arrival process itself.</text_slice>
            </slice>
            <slice>
              <time_slice>44:27</time_slice>
              <text_slice>So there's two coin flips
that are happening.</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>At each time slot, there's a
coin flip that decides whether</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>you have an arrival in this
process here, and that coin</text_slice>
            </slice>
            <slice>
              <time_slice>44:37</time_slice>
              <text_slice>flip is with parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>And if you have something that
arrives, you flip another coin</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>with probabilities q, and 1-q,
that decides whether you send</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>it up there or you send
it down there.</text_slice>
            </slice>
            <slice>
              <time_slice>44:49</time_slice>
              <text_slice>So what kind of arrival process
does this server see?</text_slice>
            </slice>
            <slice>
              <time_slice>44:55</time_slice>
              <text_slice>At any given time slot, there's
probability p that</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>there's an arrival here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>And there's a further
probability q that this</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>arrival gets sent up there.</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>So the probability that this
server sees an arrival at any</text_slice>
            </slice>
            <slice>
              <time_slice>45:10</time_slice>
              <text_slice>given time is p times q.</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>So this process here is going to
be a Bernoulli process, but</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>with a different parameter,
p times q.</text_slice>
            </slice>
            <slice>
              <time_slice>45:21</time_slice>
              <text_slice>And this one down here, with the
same argument, is going to</text_slice>
            </slice>
            <slice>
              <time_slice>45:24</time_slice>
              <text_slice>be Bernoulli with parameter
p times (1-q).</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>So by taking a Bernoulli
stream of arrivals and</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>splitting it into
two, you get two</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>separate Bernoulli processes.</text_slice>
            </slice>
            <slice>
              <time_slice>45:39</time_slice>
              <text_slice>This is going to be a Bernoulli
process, that's</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>going to be a Bernoulli
process.</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>Well actually, I'm running
a little too fast.</text_slice>
            </slice>
            <slice>
              <time_slice>45:45</time_slice>
              <text_slice>What does it take to verify that
it's a Bernoulli process?</text_slice>
            </slice>
            <slice>
              <time_slice>45:49</time_slice>
              <text_slice>At each time slot,
it's a 0 or 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>And it's going to be a 1, you're
going to see an arrival</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>with probability p times q.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>What else do we need to verify,
to be able to tell--</text_slice>
            </slice>
            <slice>
              <time_slice>46:00</time_slice>
              <text_slice>to say that it's a Bernoulli
process?</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>We need to make sure that
whatever happens in this</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>process, in different time
slots, are statistically</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>independent from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>46:11</time_slice>
              <text_slice>Is that property true?</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>For example, what happens in
this time slot whether you got</text_slice>
            </slice>
            <slice>
              <time_slice>46:16</time_slice>
              <text_slice>an arrival or not, is it
independent from what happened</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>at that time slot?</text_slice>
            </slice>
            <slice>
              <time_slice>46:22</time_slice>
              <text_slice>The answer is yes for the
following reason.</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>What happens in this time slot
has to do with the coin flip</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>associated with the original
process at this time, and the</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>coin flip that decides
where to send things.</text_slice>
            </slice>
            <slice>
              <time_slice>46:38</time_slice>
              <text_slice>What happens at that time slot
has to do with the coin flip</text_slice>
            </slice>
            <slice>
              <time_slice>46:41</time_slice>
              <text_slice>here, and the additional coin
flip that decides where to</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>send it if something came.</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>Now all these coin flips are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>46:50</time_slice>
              <text_slice>The coin flips that determine
whether we have an arrival</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>here is independent from the
coin flips that determined</text_slice>
            </slice>
            <slice>
              <time_slice>46:56</time_slice>
              <text_slice>whether we had an
arrival there.</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>And you can generalize this
argument and conclude that,</text_slice>
            </slice>
            <slice>
              <time_slice>47:02</time_slice>
              <text_slice>indeed, every time slot here
is independent from</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>any other time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>47:09</time_slice>
              <text_slice>And this does make it
a Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>And the reason is that, in the
original process, every time</text_slice>
            </slice>
            <slice>
              <time_slice>47:15</time_slice>
              <text_slice>slot is independent from
every other time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>And the additional assumption
that the coin flips that we're</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>using to decide where to send
things, these are also</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>47:26</time_slice>
              <text_slice>So we're using here the basic
property that functions of</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>independent things remain
independent.</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>There's a converse
picture of this.</text_slice>
            </slice>
            <slice>
              <time_slice>47:38</time_slice>
              <text_slice>Instead of taking one stream
and splitting it into two</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>streams, you can do
the opposite.</text_slice>
            </slice>
            <slice>
              <time_slice>47:44</time_slice>
              <text_slice>You could start from two
streams of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>47:48</time_slice>
              <text_slice>Let's say you have arrivals of
men and you have arrivals of</text_slice>
            </slice>
            <slice>
              <time_slice>47:51</time_slice>
              <text_slice>women, but you don't
care about gender.</text_slice>
            </slice>
            <slice>
              <time_slice>47:54</time_slice>
              <text_slice>And the only thing you record
is whether, in a given time</text_slice>
            </slice>
            <slice>
              <time_slice>47:57</time_slice>
              <text_slice>slot, you had an
arrival or not.</text_slice>
            </slice>
            <slice>
              <time_slice>48:00</time_slice>
              <text_slice>Notice that here we may have
an arrival of a man and the</text_slice>
            </slice>
            <slice>
              <time_slice>48:04</time_slice>
              <text_slice>arrival of a woman.</text_slice>
            </slice>
            <slice>
              <time_slice>48:05</time_slice>
              <text_slice>We just record it with a 1, by
saying there was an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>48:11</time_slice>
              <text_slice>So in the merged process, we're
not keeping track of how</text_slice>
            </slice>
            <slice>
              <time_slice>48:14</time_slice>
              <text_slice>many arrivals we had total.</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>We just record whether
there was an</text_slice>
            </slice>
            <slice>
              <time_slice>48:18</time_slice>
              <text_slice>arrival or not an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>48:21</time_slice>
              <text_slice>So an arrival gets recorded here
if, and only if, one or</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>both of these streams
had an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>So that we call a merging
of two Bernoull-- of two</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>processes, of two arrival
processes.</text_slice>
            </slice>
            <slice>
              <time_slice>48:34</time_slice>
              <text_slice>So let's make the assumption
that this arrival process is</text_slice>
            </slice>
            <slice>
              <time_slice>48:37</time_slice>
              <text_slice>independent from that
arrival process.</text_slice>
            </slice>
            <slice>
              <time_slice>48:41</time_slice>
              <text_slice>So what happens at the
typical slot here?</text_slice>
            </slice>
            <slice>
              <time_slice>48:44</time_slice>
              <text_slice>I'm going to see an arrival,
unless none of</text_slice>
            </slice>
            <slice>
              <time_slice>48:49</time_slice>
              <text_slice>these had an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>So the probability of an arrival
in a typical time slot</text_slice>
            </slice>
            <slice>
              <time_slice>48:56</time_slice>
              <text_slice>is going to be 1 minus the
probability of no arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>And the event of no arrival
corresponds to the first</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>process having no arrival,
and the second</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>process having no arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>So there's no arrival in the
merged process if, and only</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>if, there's no arrival in the
first process and no arrival</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>in the second process.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>We're assuming that the two
processes are independent and</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>that's why we can multiply
probabilities here.</text_slice>
            </slice>
            <slice>
              <time_slice>49:29</time_slice>
              <text_slice>And then you can take this
formula and it simplifies to p</text_slice>
            </slice>
            <slice>
              <time_slice>49:34</time_slice>
              <text_slice>+ q, minus p times q.</text_slice>
            </slice>
            <slice>
              <time_slice>49:38</time_slice>
              <text_slice>So each time slot of the merged
process has a certain</text_slice>
            </slice>
            <slice>
              <time_slice>49:41</time_slice>
              <text_slice>probability of seeing
an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>Is the merged process
a Bernoulli process?</text_slice>
            </slice>
            <slice>
              <time_slice>49:47</time_slice>
              <text_slice>Yes, it is after you verify the
additional property that</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>different slots are independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>Why are they independent?</text_slice>
            </slice>
            <slice>
              <time_slice>49:56</time_slice>
              <text_slice>What happens in this slot has to
do with that slot, and that</text_slice>
            </slice>
            <slice>
              <time_slice>50:01</time_slice>
              <text_slice>slot down here.</text_slice>
            </slice>
            <slice>
              <time_slice>50:03</time_slice>
              <text_slice>These two slots--</text_slice>
            </slice>
            <slice>
              <time_slice>50:05</time_slice>
              <text_slice>so what happens here,
has to do with what</text_slice>
            </slice>
            <slice>
              <time_slice>50:08</time_slice>
              <text_slice>happens here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>What happens in this slot has
to do with whatever happened</text_slice>
            </slice>
            <slice>
              <time_slice>50:16</time_slice>
              <text_slice>here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>50:19</time_slice>
              <text_slice>Now, whatever happens here and
there is independent from</text_slice>
            </slice>
            <slice>
              <time_slice>50:23</time_slice>
              <text_slice>whatever happens
here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>50:25</time_slice>
              <text_slice>Therefore, what happens here
is independent from what</text_slice>
            </slice>
            <slice>
              <time_slice>50:29</time_slice>
              <text_slice>happens there.</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>So the independence property
is preserved.</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>The different slots of this
merged process are independent</text_slice>
            </slice>
            <slice>
              <time_slice>50:36</time_slice>
              <text_slice>of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>50:37</time_slice>
              <text_slice>So the merged process is itself
a Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>So please digest these two
pictures of merging and</text_slice>
            </slice>
            <slice>
              <time_slice>50:45</time_slice>
              <text_slice>splitting, because we're going
to revisit them in continuous</text_slice>
            </slice>
            <slice>
              <time_slice>50:48</time_slice>
              <text_slice>time where things are little
subtler than that.</text_slice>
            </slice>
            <slice>
              <time_slice>50:52</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>50:53</time_slice>
              <text_slice>Good luck on the exam and
see you in a week.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Independence (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 3 Models based on conditional
probabilities
Readings: Section 1.5
3 tosses of a biased coin:
Review P(H)=p,P(T)=1 p
Independence of two events p HHH
Independence of a collection of events p
HHT1 - p
p HTHReview p 1 - p
1 - p HTT
P(AB)) 0pP(A|B)= , assuming P(B &gt;THH
P(B) 1 - pp
1 - p THT
Multiplication rule: p TTH
1 - p
P(AB)=P(B)P(A|B)=P(A) P(B|A)1 - p TTT
Total probability theorem:
P()=P( )P(|)+P(c)P(|c P(T HT )=B A B A A B A )
Bayes rule:P(1 head) =
P(Ai)P(B A i)P(Ai|B)=|
P(B)P(rst toss is H |1 head) =
Independence of two events Conditioning may a ect independence
Defn: P(B|A)=P(B) Conditional independence, given C,
is dened as independenceoccurrence of A
under probability law P(provides no information|C)
about Bs occurrence
Assume Aand Bare independent
Recall that P(AB)=P(A) P(B|A)
Defn: P(AB)=P(A) P(B)CA
Symmetric with respect to Aand B
Bapplies even if P(A)=0
implies P(A|B)=P(A)If we are told that Coccurred,
areAand Bindependent?
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Conditioning may a ect independence Independence of a collection of events
Two unfair coins, Aand B:Intuitive denition:
P(H |coin A)=0 .9,P(H |coin B)=0 .1Information on some of the events tellschoose either coin with equal probabilityus nothing about probabilities related to
0.9 the remaining events
0.1 0.9 E.g.:
Coin AP((c)|c)=P((c
0.9 A1 A2A3A5A6 A1 A2A3))
0.5 0.10.1
Mathematical denition:
0.1 Events A1,A2, . . . , A n
0.50.1
0.9are called independent if:
Coin B 0.1
0.9P(AiAjAq)=P(Ai)P(Aj)P(Aq)
0.9 for any distinct indices i, j, . . . , q ,
(chosen from {1, . . . , n )
Once we know it is coin A, are tosses}
independent?
If we do not know which coin it is, are
tosses independent?
Compare:
P(toss 11 = H)
P(toss 11 = H|rst 10 tosses are heads)
Independence vs. pairwise The kings sibling
independence
The king comes from a family of two
Two independent fair coin tosses children. What is the probability that
A: First toss is H his sibling is female?
B: Second toss is H
P(A)= P(B)=1 /2
HH HT
TH TT
C: First and second toss give same
result
P(C)=
P(CA)=
P(ABC)=
P(C|AB)=
Pairwise independence does not
imply independence
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-14-poisson-process-i/</video_url>
          <video_title>Lecture 14: Poisson Process I</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:17</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: So last time
we started talking</text_slice>
            </slice>
            <slice>
              <time_slice>0:24</time_slice>
              <text_slice>about random processes.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>A random process is a random
experiment that</text_slice>
            </slice>
            <slice>
              <time_slice>0:30</time_slice>
              <text_slice>evolves over time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>And conceptually, it's important
to realize that it's</text_slice>
            </slice>
            <slice>
              <time_slice>0:35</time_slice>
              <text_slice>a single probabilistic
experiment</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>that has many stages.</text_slice>
            </slice>
            <slice>
              <time_slice>0:39</time_slice>
              <text_slice>Actually, it has an infinite
number of stages.</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>And we discussed the simplest
random process there is, the</text_slice>
            </slice>
            <slice>
              <time_slice>0:47</time_slice>
              <text_slice>Bernoulli process, which is
nothing but the sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>Bernoulli trials--</text_slice>
            </slice>
            <slice>
              <time_slice>0:51</time_slice>
              <text_slice>an infinite sequence of
Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>0:54</time_slice>
              <text_slice>For example, flipping a
coin over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>Once we understand what's going
on with that process,</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>then what we want is to move
into a continuous time version</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>of the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>And this is what we will call
the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>And for the Poisson process,
we're going to do exactly the</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>same things that we did for
the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>That is, talk about the number
of arrivals during a given</text_slice>
            </slice>
            <slice>
              <time_slice>1:18</time_slice>
              <text_slice>time period, and talk also
about the time between</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>consecutive arrivals, and
for the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>inter-arrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>So let's start with a quick
review of what we</text_slice>
            </slice>
            <slice>
              <time_slice>1:30</time_slice>
              <text_slice>discussed last time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>First, a note about language.</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>If you think of coin tosses,
we then talk</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>about heads and tails.</text_slice>
            </slice>
            <slice>
              <time_slice>1:40</time_slice>
              <text_slice>If you think of these as a
sequence of trials, you can</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>talk about successes
and failures.</text_slice>
            </slice>
            <slice>
              <time_slice>1:47</time_slice>
              <text_slice>The language that we will be
using will be more the</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>language of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>1:51</time_slice>
              <text_slice>That is, if in a given slot you
have a success, you say</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>that something arrived.</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>If you have a failure,
nothing arrived.</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>And that language is a little
more convenient and more</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>natural, especially when we talk
about continuous time--</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>to talk about arrivals
instead of successes.</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>But in any case, for the
Bernoulli process let's keep,</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>for a little bit, the language
of successes.</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>Whereas working in discrete
time, we have time slots.</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>During each time slot,
we have an</text_slice>
            </slice>
            <slice>
              <time_slice>2:20</time_slice>
              <text_slice>independent Bernoulli trial.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>There is probability p
of having a success.</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>Different slots are independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>And this probability p is the
same for any given time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>So for this process we will
discuss the one random</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>variable of interest, which
is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>If we have n time slots,
or n trials, how many</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>arrivals will there be?</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>Or how many successes
will there be?</text_slice>
            </slice>
            <slice>
              <time_slice>2:46</time_slice>
              <text_slice>Well, this is just given
by the binomial PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>Number of successes in n trials
is a random variable</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>that has a binomial PMF, and
we know what this is.</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>Then we talked about
inter-arrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>The time until the first
arrival happens has a</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>geometric distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>3:07</time_slice>
              <text_slice>And we have seen that
from some time ago.</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>Now if you start thinking
about the time until k</text_slice>
            </slice>
            <slice>
              <time_slice>3:14</time_slice>
              <text_slice>arrivals happen, and we denote
that by Yk, this is the time</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>until the first arrival
happens.</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>And then after the first arrival
happens, you have to</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>wait some time until
the second arrival</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>happens, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>And then the time from the
(k -1)th arrival, until</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>arrival number k.</text_slice>
            </slice>
            <slice>
              <time_slice>3:34</time_slice>
              <text_slice>The important thing to realize
here is that because the</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>process has a memorylessness
property, once the first</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>arrival comes, it's as if we're
starting from scratch</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>and we will be flipping
our coins until the</text_slice>
            </slice>
            <slice>
              <time_slice>3:48</time_slice>
              <text_slice>next arrival comes.</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>So the time it will take until
the next arrival comes will</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>also be a geometric
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>3:54</time_slice>
              <text_slice>And because different slots
are independent, whatever</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>happens after the first arrival
is independent from</text_slice>
            </slice>
            <slice>
              <time_slice>4:00</time_slice>
              <text_slice>whatever happened before.</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>So T1 and T2 will be independent
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>4:06</time_slice>
              <text_slice>And similarly, all
the way up to Tk.</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>So the time until the k-th
arrival is a sum of</text_slice>
            </slice>
            <slice>
              <time_slice>4:13</time_slice>
              <text_slice>independent geometric random
variables, with the same</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>parameter p.</text_slice>
            </slice>
            <slice>
              <time_slice>4:19</time_slice>
              <text_slice>And we saw last time that we
can find the probability</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>distribution of Yk.</text_slice>
            </slice>
            <slice>
              <time_slice>4:25</time_slice>
              <text_slice>The probability that Yk takes
a value of t is equal to--</text_slice>
            </slice>
            <slice>
              <time_slice>4:30</time_slice>
              <text_slice>there's this combinatorial
factor here, and then you get</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>p to the k, (1-p) to the (t-k),
and this formula is</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>true for t equal to
k, k+1, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>And this distribution
has a name.</text_slice>
            </slice>
            <slice>
              <time_slice>4:49</time_slice>
              <text_slice>It's called the Pascal PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>So this is all there
is to know about</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>4:59</time_slice>
              <text_slice>One important comment is to
realize what exactly this</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>memorylessness property
is saying.</text_slice>
            </slice>
            <slice>
              <time_slice>5:05</time_slice>
              <text_slice>So I discussed it a little
bit last time.</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>Let me reiterate it.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>So we have a Bernoulli process,
which is a sequence</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>of Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>And these are (0,1) random
variables that</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>keep going on forever.</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>So someone is watching this
movie of Bernoulli trials B_t.</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>And at some point, they say
they think, or something</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>interesting has happened,
why don't you come</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>in and start watching?</text_slice>
            </slice>
            <slice>
              <time_slice>5:36</time_slice>
              <text_slice>So at some time t, they
tell you to come</text_slice>
            </slice>
            <slice>
              <time_slice>5:39</time_slice>
              <text_slice>in and start watching.</text_slice>
            </slice>
            <slice>
              <time_slice>5:41</time_slice>
              <text_slice>So what you will see once
you come in will</text_slice>
            </slice>
            <slice>
              <time_slice>5:44</time_slice>
              <text_slice>be this future trials.</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>So actually what you will see
is a random process, whose</text_slice>
            </slice>
            <slice>
              <time_slice>5:52</time_slice>
              <text_slice>first random variable is going
to be the first one that you</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>see, B_(t +1).</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>The second one is going
to be this, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>So this is the process that's
seen by the person who's asked</text_slice>
            </slice>
            <slice>
              <time_slice>6:06</time_slice>
              <text_slice>to come in and start watching
at that time.</text_slice>
            </slice>
            <slice>
              <time_slice>6:10</time_slice>
              <text_slice>And the claim is that this
process is itself a Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>process, provided that the
person who calls you into the</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>room does not look
into the future.</text_slice>
            </slice>
            <slice>
              <time_slice>6:23</time_slice>
              <text_slice>The person who calls you into
the room decides to call you</text_slice>
            </slice>
            <slice>
              <time_slice>6:27</time_slice>
              <text_slice>in only on the basis of what
they have seen so far.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>So for example, who calls you
into the room might have a</text_slice>
            </slice>
            <slice>
              <time_slice>6:33</time_slice>
              <text_slice>rule that says, as soon as I see
a sequence of 3 heads, I</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>ask the other person
to come in.</text_slice>
            </slice>
            <slice>
              <time_slice>6:43</time_slice>
              <text_slice>So if they use that particular
rule, it means that when</text_slice>
            </slice>
            <slice>
              <time_slice>6:46</time_slice>
              <text_slice>you're called in, the previous
3 were heads.</text_slice>
            </slice>
            <slice>
              <time_slice>6:49</time_slice>
              <text_slice>But this doesn't give you any
information about the future.</text_slice>
            </slice>
            <slice>
              <time_slice>6:53</time_slice>
              <text_slice>And so the future ones
will be just</text_slice>
            </slice>
            <slice>
              <time_slice>6:55</time_slice>
              <text_slice>independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>6:57</time_slice>
              <text_slice>If on the other hand, the person
who calls you in has</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>seen the movie before and they
use a rule, such as, for</text_slice>
            </slice>
            <slice>
              <time_slice>7:04</time_slice>
              <text_slice>example, I call you in just
before 3 heads show up for the</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>first time.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>So the person calls you in based
on knowledge that these</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>two would be three heads.</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>If they have such foresight--</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>if they can look into
the future--</text_slice>
            </slice>
            <slice>
              <time_slice>7:19</time_slice>
              <text_slice>then X1, X2, X3, they're certain
to be three heads, so</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>they do not correspond
to random</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>7:29</time_slice>
              <text_slice>So to rephrase this, the
process is memoryless.</text_slice>
            </slice>
            <slice>
              <time_slice>7:33</time_slice>
              <text_slice>It does not matter what has
happened in the past.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>And that's true even if you are
called into the room and</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>start watching at a random time,
as long as that random</text_slice>
            </slice>
            <slice>
              <time_slice>7:45</time_slice>
              <text_slice>time is determined in a causal
way on the basis of what has</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>happened so far.</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>So you are called into the room
in a causal manner, just</text_slice>
            </slice>
            <slice>
              <time_slice>7:55</time_slice>
              <text_slice>based on what's happened
so far.</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>What you're going to see
starting from that time will</text_slice>
            </slice>
            <slice>
              <time_slice>8:00</time_slice>
              <text_slice>still be a sequence of
independent Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>And this is the argument that we
used here, essentially, to</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>argue that this T2 is an
independent random</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>variable from T1.</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>So a person is watching the
movie, sees the first success.</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>And on the basis of what
they have seen--</text_slice>
            </slice>
            <slice>
              <time_slice>8:19</time_slice>
              <text_slice>they have just seen the
first success--</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>they ask you to come in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:23</time_slice>
              <text_slice>You come in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>What you're going to see is a
sequence of Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>8:27</time_slice>
              <text_slice>And you wait this long until
the next success comes in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>What you see is a Bernoulli
process, as if the process was</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>just starting right now.</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>And that convinces us that this
should be a geometric</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>random variable of the same
kind as this one, as</text_slice>
            </slice>
            <slice>
              <time_slice>8:43</time_slice>
              <text_slice>independent from what
happened before.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>So this is pretty much all there
is to know about the</text_slice>
            </slice>
            <slice>
              <time_slice>8:49</time_slice>
              <text_slice>Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>8:50</time_slice>
              <text_slice>Plus the two things that we
did at the end of the last</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>lecture where we merge two
independent Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>8:55</time_slice>
              <text_slice>processes, we get a
Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>If we have a Bernoulli process
and we split it by flipping a</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>coin and sending things one way
or the other, then we get</text_slice>
            </slice>
            <slice>
              <time_slice>9:05</time_slice>
              <text_slice>two separate Bernoulli
processes.</text_slice>
            </slice>
            <slice>
              <time_slice>9:07</time_slice>
              <text_slice>And we see that all of
these carry over to</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>the continuous time.</text_slice>
            </slice>
            <slice>
              <time_slice>9:11</time_slice>
              <text_slice>And our task for today is
basically to work these</text_slice>
            </slice>
            <slice>
              <time_slice>9:14</time_slice>
              <text_slice>continuous time variations.</text_slice>
            </slice>
            <slice>
              <time_slice>9:18</time_slice>
              <text_slice>So the Poisson process is a
continuous time version of the</text_slice>
            </slice>
            <slice>
              <time_slice>9:21</time_slice>
              <text_slice>Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>Here's the motivation
for considering</text_slice>
            </slice>
            <slice>
              <time_slice>9:25</time_slice>
              <text_slice>it a Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>9:26</time_slice>
              <text_slice>So you have that person whose
job is to sit outside</text_slice>
            </slice>
            <slice>
              <time_slice>9:29</time_slice>
              <text_slice>the door of a bank.</text_slice>
            </slice>
            <slice>
              <time_slice>9:32</time_slice>
              <text_slice>And they have this long sheet,
and for every one second slot,</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>they mark an X if a person
came in, or they mark</text_slice>
            </slice>
            <slice>
              <time_slice>9:42</time_slice>
              <text_slice>something else if no one came
in during that slot.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>Now the bank manager is a really
scientifically trained</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>person and wants very
accurate results.</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>So they tell you, don't use
one second slots, use</text_slice>
            </slice>
            <slice>
              <time_slice>9:53</time_slice>
              <text_slice>milliseconds slots.</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>So you have all those slots
and you keep filling if</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>someone arrived or not
during that slot.</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>Well then you come
up with an idea.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>Why use millisecond slots and
keep putting crosses or zero's</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>into each slot?</text_slice>
            </slice>
            <slice>
              <time_slice>10:08</time_slice>
              <text_slice>It's much simpler if I just
record the exact times when</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>people came in.</text_slice>
            </slice>
            <slice>
              <time_slice>10:14</time_slice>
              <text_slice>So time is continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>I don't keep doing something
at every time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>But instead of the time axis,
I mark the times at which</text_slice>
            </slice>
            <slice>
              <time_slice>10:24</time_slice>
              <text_slice>customers arrive.</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>So there's no real
need for slots.</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>The only information that you
want is when did we have</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>arrivals of people.</text_slice>
            </slice>
            <slice>
              <time_slice>10:34</time_slice>
              <text_slice>And we want to now model a
process of this kind happening</text_slice>
            </slice>
            <slice>
              <time_slice>10:37</time_slice>
              <text_slice>in continuous time, that has the
same flavor, however, as</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>So that's the model we
want to develop.</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>So what are the properties
that we're going to have?</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>First, we're going to assume
that intervals over the same</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>length behave probabilistically
in an</text_slice>
            </slice>
            <slice>
              <time_slice>11:01</time_slice>
              <text_slice>identical fashion.</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>So what does that mean?</text_slice>
            </slice>
            <slice>
              <time_slice>11:06</time_slice>
              <text_slice>Think of an interval of
some given length.</text_slice>
            </slice>
            <slice>
              <time_slice>11:09</time_slice>
              <text_slice>During the interval of that
length, there's going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>11:12</time_slice>
              <text_slice>random number of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>And that random number of
arrivals is going to have a</text_slice>
            </slice>
            <slice>
              <time_slice>11:17</time_slice>
              <text_slice>probability distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>11:19</time_slice>
              <text_slice>So that probability
distribution--</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>let's denote it by
this notation.</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>We fix t, we fix the duration.</text_slice>
            </slice>
            <slice>
              <time_slice>11:29</time_slice>
              <text_slice>So this is fixed.</text_slice>
            </slice>
            <slice>
              <time_slice>11:31</time_slice>
              <text_slice>And we look at the
different k's.</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>The probability of having 0
arrivals, the probability of 1</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>arrival, the probability of
2 arrivals, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>So this thing is essentially
a PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>11:42</time_slice>
              <text_slice>So it should have the property
that the sum over all k's of</text_slice>
            </slice>
            <slice>
              <time_slice>11:46</time_slice>
              <text_slice>this P_(k, tau) should
be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>Now, hidden inside this notation
is an assumption of</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>time homogeneity.</text_slice>
            </slice>
            <slice>
              <time_slice>11:59</time_slice>
              <text_slice>That is, this probability
distribution for the number of</text_slice>
            </slice>
            <slice>
              <time_slice>12:03</time_slice>
              <text_slice>arrivals only depends on the
length of the interval, but</text_slice>
            </slice>
            <slice>
              <time_slice>12:09</time_slice>
              <text_slice>not the exact location of the
interval on the time axis.</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>That is, if I take an interval
of length tau, and I ask about</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>the number of arrivals
in this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>12:21</time_slice>
              <text_slice>And I take another interval of
length tau, and I ask about</text_slice>
            </slice>
            <slice>
              <time_slice>12:24</time_slice>
              <text_slice>the number of arrivals
during that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>Number of arrivals here, and
number of arrivals there have</text_slice>
            </slice>
            <slice>
              <time_slice>12:31</time_slice>
              <text_slice>the same probability
distribution, which is</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>denoted this way.</text_slice>
            </slice>
            <slice>
              <time_slice>12:36</time_slice>
              <text_slice>So the statistical behavior of
arrivals here is the same as</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>the statistical behavioral
of arrivals there.</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>What's the relation with
the Bernoulli process?</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>It's very much like
the assumption--</text_slice>
            </slice>
            <slice>
              <time_slice>12:48</time_slice>
              <text_slice>the Bernoulli process--</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>that in different slots,
we have the same</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>probability of success.</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>Every slot looks
probabilistically</text_slice>
            </slice>
            <slice>
              <time_slice>12:56</time_slice>
              <text_slice>as any other slot.</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>So similarly here, any interval
of length tau looks</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>probabilistically as any other
interval of length tau.</text_slice>
            </slice>
            <slice>
              <time_slice>13:06</time_slice>
              <text_slice>And the number of arrivals
during that interval is a</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>random variable described
by these probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>Number of arrivals here is a
random variable described by</text_slice>
            </slice>
            <slice>
              <time_slice>13:15</time_slice>
              <text_slice>these same probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>So that's our first
assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>Then what else?</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>In the Bernoulli process we
had the assumption that</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>different time slots were
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>13:27</time_slice>
              <text_slice>Here we do not have time slots,
but we can still think</text_slice>
            </slice>
            <slice>
              <time_slice>13:32</time_slice>
              <text_slice>in a similar way and impose the
following assumption, that</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>these joint time intervals are
statistically independent.</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>What does that mean?</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>Does a random number of arrivals
during this interval,</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>and the random number of
arrivals during this interval,</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>and the random number of</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>arrivals during this interval--</text_slice>
            </slice>
            <slice>
              <time_slice>13:51</time_slice>
              <text_slice>so these are three different
random variables--</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>these three random variables are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>How many arrivals we got here
is independent from how many</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>arrivals we got there.</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>So this is similar to saying
that different time slots were</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>That's what we did
in discrete time.</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>The continuous time analog is
this independence assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>So for example, in particular,
number of arrivals here is</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>independent from the number
of arrivals there.</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>So these are two basic
assumptions about the process.</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>Now in order to write down a
formula, eventually, about</text_slice>
            </slice>
            <slice>
              <time_slice>14:30</time_slice>
              <text_slice>this probability
distribution--</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>which is our next objective, we
would like to say something</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>specific about this
distribution</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>of number of arrivals--</text_slice>
            </slice>
            <slice>
              <time_slice>14:40</time_slice>
              <text_slice>we need to add a little more
structure into the problem.</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>And we're going to make the
following assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>If we look at the time interval
of length delta--</text_slice>
            </slice>
            <slice>
              <time_slice>14:51</time_slice>
              <text_slice>and delta now is supposed to
be a small number, so a</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>picture like this--</text_slice>
            </slice>
            <slice>
              <time_slice>14:55</time_slice>
              <text_slice>during a very small time
interval, there is a</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>probability that we get exactly
one arrival, which is</text_slice>
            </slice>
            <slice>
              <time_slice>15:06</time_slice>
              <text_slice>lambda times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>Delta is the length of the
interval and lambda is a</text_slice>
            </slice>
            <slice>
              <time_slice>15:10</time_slice>
              <text_slice>proportionality factor, which
is sort of the intensity of</text_slice>
            </slice>
            <slice>
              <time_slice>15:14</time_slice>
              <text_slice>the arrival process.</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>Bigger lambda means that a
little interval is more likely</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>to get an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>15:24</time_slice>
              <text_slice>So there's a probability lambda</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>times delta of 1 arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>The remaining probability
goes to 0 arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>15:31</time_slice>
              <text_slice>And when delta is small, the
probability of 2 arrivals can</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>be approximated by 0.</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>So this is a description
of what happens during</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>a small, tiny slot.</text_slice>
            </slice>
            <slice>
              <time_slice>15:45</time_slice>
              <text_slice>Now this is something that's
supposed to be true in some</text_slice>
            </slice>
            <slice>
              <time_slice>15:48</time_slice>
              <text_slice>limiting sense, when delta
is very small.</text_slice>
            </slice>
            <slice>
              <time_slice>15:51</time_slice>
              <text_slice>So the exact version of this
statement would be that this</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>is an equality, plus order
of delta squared terms.</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>So this is an approximate
equality.</text_slice>
            </slice>
            <slice>
              <time_slice>16:04</time_slice>
              <text_slice>And what approximation means is
that in the limit of small</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>deltas, the dominant terms--</text_slice>
            </slice>
            <slice>
              <time_slice>16:11</time_slice>
              <text_slice>the constant and the first order
term are given by this.</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>Now when delta is very small,
second order terms in delta do</text_slice>
            </slice>
            <slice>
              <time_slice>16:19</time_slice>
              <text_slice>not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>They are small compared
to first order terms.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>So we ignore this.</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>So you can either think in terms
of an exact relation,</text_slice>
            </slice>
            <slice>
              <time_slice>16:30</time_slice>
              <text_slice>which is the probabilities are
given by this, plus delta</text_slice>
            </slice>
            <slice>
              <time_slice>16:34</time_slice>
              <text_slice>squared terms.</text_slice>
            </slice>
            <slice>
              <time_slice>16:35</time_slice>
              <text_slice>Or if you want to be a little
more loose, you just write</text_slice>
            </slice>
            <slice>
              <time_slice>16:38</time_slice>
              <text_slice>here, as an approximate
equality.</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>And the understanding is that
this equality holds--</text_slice>
            </slice>
            <slice>
              <time_slice>16:44</time_slice>
              <text_slice>approximately becomes more
and more correct as</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>delta goes to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>16:53</time_slice>
              <text_slice>So another version of that
statement would be that if you</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>take the limit as delta goes to
0, of p, the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>having 1 arrival in an interval
of length delta,</text_slice>
            </slice>
            <slice>
              <time_slice>17:06</time_slice>
              <text_slice>divided by delta, this
is equal to lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>17:10</time_slice>
              <text_slice>So that would be one version of
an exact statement of what</text_slice>
            </slice>
            <slice>
              <time_slice>17:16</time_slice>
              <text_slice>we are assuming here.</text_slice>
            </slice>
            <slice>
              <time_slice>17:19</time_slice>
              <text_slice>So this lambda, we call it the
arrival rate, or the intensity</text_slice>
            </slice>
            <slice>
              <time_slice>17:22</time_slice>
              <text_slice>of the process.</text_slice>
            </slice>
            <slice>
              <time_slice>17:23</time_slice>
              <text_slice>And clearly, if you double
lambda, then a little interval</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>is likely --</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>you expect to get --</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>the probability of obtaining
an arrival during that</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>interval has doubled.</text_slice>
            </slice>
            <slice>
              <time_slice>17:35</time_slice>
              <text_slice>So in some sense we have twice
as intense arrival process.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>If you look at the number
of arrivals during delta</text_slice>
            </slice>
            <slice>
              <time_slice>17:46</time_slice>
              <text_slice>interval, what is the expected
value of that random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>17:54</time_slice>
              <text_slice>Well with probability lambda
delta we get 1 arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>And with the remaining</text_slice>
            </slice>
            <slice>
              <time_slice>18:01</time_slice>
              <text_slice>probability, we get 0 arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>So it's just lambda
times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>So expected number of arrivals
during a little interval is</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>lambda times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>So expected number of arrivals
is proportional to lambda, and</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>that's again why we call lambda
the arrival rate.</text_slice>
            </slice>
            <slice>
              <time_slice>18:19</time_slice>
              <text_slice>If you send delta to the
denominator in this equality,</text_slice>
            </slice>
            <slice>
              <time_slice>18:22</time_slice>
              <text_slice>it tells you that lambda is
the expected number of</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>arrivals per unit time.</text_slice>
            </slice>
            <slice>
              <time_slice>18:30</time_slice>
              <text_slice>So the arrival rate is expected
number of arrivals</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>per unit time.</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>And again, that justifies why
we call lambda the intensity</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>of this process.</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>So where are we now?</text_slice>
            </slice>
            <slice>
              <time_slice>18:49</time_slice>
              <text_slice>For the Bernoulli process, the
number of arrivals during a</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>given interval of length n had
the PMF that we knew it was</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>the binomial PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>What is the formula for the
corresponding PMF for the</text_slice>
            </slice>
            <slice>
              <time_slice>19:07</time_slice>
              <text_slice>continuous time process?</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>Somehow we would like to use
our assumptions and come up</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>with the formula for
this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>So this tells us about the
distribution of number of</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>arrivals during an interval
of some general length.</text_slice>
            </slice>
            <slice>
              <time_slice>19:23</time_slice>
              <text_slice>We have made assumptions about
the number of arrivals during</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>an interval of small length.</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>An interval of big length is
composed of many intervals of</text_slice>
            </slice>
            <slice>
              <time_slice>19:34</time_slice>
              <text_slice>small length, so maybe this
is the way to go.</text_slice>
            </slice>
            <slice>
              <time_slice>19:37</time_slice>
              <text_slice>Take a big interval, and split
it into many intervals of</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>small length.</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>So we have here our time axis.</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>And we have an interval
of length tau.</text_slice>
            </slice>
            <slice>
              <time_slice>19:51</time_slice>
              <text_slice>And I'm going to split it into
lots of little intervals of</text_slice>
            </slice>
            <slice>
              <time_slice>19:55</time_slice>
              <text_slice>length delta.</text_slice>
            </slice>
            <slice>
              <time_slice>19:56</time_slice>
              <text_slice>So how many intervals are
we going to have?</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>The number of intervals is going
to be the total time,</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>divided by delta.</text_slice>
            </slice>
            <slice>
              <time_slice>20:07</time_slice>
              <text_slice>Now what happens during each one
of these little intervals?</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>As long as the intervals are
small, what you have is that</text_slice>
            </slice>
            <slice>
              <time_slice>20:22</time_slice>
              <text_slice>during an interval, you're
going to have</text_slice>
            </slice>
            <slice>
              <time_slice>20:24</time_slice>
              <text_slice>either 0 or 1 arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>The probability of more than
1 arrival during a little</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>interval is negligible.</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>So with this picture, you have
essentially a Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>20:35</time_slice>
              <text_slice>process that consists
of so many trials.</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>And during each one of those
trials, we have a probability</text_slice>
            </slice>
            <slice>
              <time_slice>20:43</time_slice>
              <text_slice>of success, which is
lambda times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>Different little intervals
here are</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>20:56</time_slice>
              <text_slice>That's one of our assumptions,
that these joint time</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>intervals are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>21:00</time_slice>
              <text_slice>So approximately, what we have
is a Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>We have independence.</text_slice>
            </slice>
            <slice>
              <time_slice>21:06</time_slice>
              <text_slice>We have the number of
slots of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>And during each one of the
slots we have a certain</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>probability of success.</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>So if we think of this as
another good approximation of</text_slice>
            </slice>
            <slice>
              <time_slice>21:17</time_slice>
              <text_slice>the Poisson process--</text_slice>
            </slice>
            <slice>
              <time_slice>21:18</time_slice>
              <text_slice>with the approximation becoming
more and more</text_slice>
            </slice>
            <slice>
              <time_slice>21:21</time_slice>
              <text_slice>accurate as delta goes to 0 --</text_slice>
            </slice>
            <slice>
              <time_slice>21:23</time_slice>
              <text_slice>what we should do would be to
take the formula for the PMF</text_slice>
            </slice>
            <slice>
              <time_slice>21:28</time_slice>
              <text_slice>of number of arrivals in a
Bernoulli process, and then</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>take the limit as
delta goes to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>So in the Bernoulli process, the
probability of k arrivals</text_slice>
            </slice>
            <slice>
              <time_slice>21:45</time_slice>
              <text_slice>is n choose k, and then
you have p to the k.</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>Now in our case, we have here
lambda times delta, delta is</text_slice>
            </slice>
            <slice>
              <time_slice>21:57</time_slice>
              <text_slice>tau over n.</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>Delta is tau over n, so p is
lambda times tau divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>So here's our p --</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>Lambda tau over n --</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>to the power k, and then times
one minus this-- this is our</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>one minus p--</text_slice>
            </slice>
            <slice>
              <time_slice>22:24</time_slice>
              <text_slice>to the power n-k.</text_slice>
            </slice>
            <slice>
              <time_slice>22:30</time_slice>
              <text_slice>So this is the exact formula
for the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>22:35</time_slice>
              <text_slice>For the Poisson process, what we
do is we take that formula</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>and we let delta go to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>As delta goes to 0, n
goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>22:48</time_slice>
              <text_slice>So that's the limit
that we're taking.</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>On the other hand, this
expression lambda times tau--</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>lambda times tau, what
is it going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>Lambda times tau is equal
to n times p.</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>n times p, is that
what I want?</text_slice>
            </slice>
            <slice>
              <time_slice>23:21</time_slice>
              <text_slice>No, let's see.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>Lambda tau is np.</text_slice>
            </slice>
            <slice>
              <time_slice>23:28</time_slice>
              <text_slice>Yeah.</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>So lambda tau is np.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>23:54</time_slice>
              <text_slice>So we have this relation,
lambda tau equals np.</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>These two numbers being equal
kind of makes sense. np is the</text_slice>
            </slice>
            <slice>
              <time_slice>24:03</time_slice>
              <text_slice>expected number of successes
you're going to get in the</text_slice>
            </slice>
            <slice>
              <time_slice>24:05</time_slice>
              <text_slice>Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>Lambda tau--</text_slice>
            </slice>
            <slice>
              <time_slice>24:08</time_slice>
              <text_slice>since lambda is the arrival rate
and you have a total time</text_slice>
            </slice>
            <slice>
              <time_slice>24:11</time_slice>
              <text_slice>of tau, lambda tau you can think
of it as the number of</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>expected arrivals in the
Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>We're doing a Bernoulli
approximation</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>to the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>24:23</time_slice>
              <text_slice>We take the formula for the
Bernoulli, and now take the</text_slice>
            </slice>
            <slice>
              <time_slice>24:26</time_slice>
              <text_slice>limit as n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>Now lambda tau over n is equal
to p, so it's clear what this</text_slice>
            </slice>
            <slice>
              <time_slice>24:35</time_slice>
              <text_slice>term is going to give us.</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>This is just p to the power k.</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>It will actually take a little
more work than that.</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>Now I'm not going to do the
algebra, but I'm just telling</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>you that one can take the limit
in this formula here, as</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>And that will give you another
formula, the final formula for</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>the Poisson PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>25:10</time_slice>
              <text_slice>One thing to notice is that here
you have something like 1</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>minus a constant over
n, to the power n.</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>And you may recall from calculus
a formula of this</text_slice>
            </slice>
            <slice>
              <time_slice>25:21</time_slice>
              <text_slice>kind, that this converges
to e to the minus c.</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>If you remember that formula
from calculus, then you will</text_slice>
            </slice>
            <slice>
              <time_slice>25:29</time_slice>
              <text_slice>expect that here, in the limit,
you are going to get</text_slice>
            </slice>
            <slice>
              <time_slice>25:32</time_slice>
              <text_slice>something like an e to
the minus lambda tau.</text_slice>
            </slice>
            <slice>
              <time_slice>25:36</time_slice>
              <text_slice>So indeed, we will
get such a term.</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>There is some work that needs
to be done to find the limit</text_slice>
            </slice>
            <slice>
              <time_slice>25:42</time_slice>
              <text_slice>of this expression, times
that expression.</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>The algebra is not hard,
it's in the text.</text_slice>
            </slice>
            <slice>
              <time_slice>25:48</time_slice>
              <text_slice>Let's not spend more
time doing this.</text_slice>
            </slice>
            <slice>
              <time_slice>25:51</time_slice>
              <text_slice>But let me just give you
the formula of what</text_slice>
            </slice>
            <slice>
              <time_slice>25:53</time_slice>
              <text_slice>comes at the end.</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>And the formula that comes at
the end is of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>25:59</time_slice>
              <text_slice>So what matters here is not so
much the specific algebra that</text_slice>
            </slice>
            <slice>
              <time_slice>26:03</time_slice>
              <text_slice>you will do to go from this
formula to that one.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>It's kind of straightforward.</text_slice>
            </slice>
            <slice>
              <time_slice>26:09</time_slice>
              <text_slice>What's important is the idea
that the Poisson process, by</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>definition, can be approximated
by a Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>process in which we have a very
large number of slots--</text_slice>
            </slice>
            <slice>
              <time_slice>26:25</time_slice>
              <text_slice>n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>Whereas we have a very small
probability of success during</text_slice>
            </slice>
            <slice>
              <time_slice>26:32</time_slice>
              <text_slice>each time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>So a large number of slots,
but tiny probability of</text_slice>
            </slice>
            <slice>
              <time_slice>26:38</time_slice>
              <text_slice>success during each slot.</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>And we take the limit
as the slots</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>become smaller and smaller.</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>So with this approximation
we end up with</text_slice>
            </slice>
            <slice>
              <time_slice>26:47</time_slice>
              <text_slice>this particular formula.</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>And this is the so-called
Poisson PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>26:51</time_slice>
              <text_slice>Now this function P here --</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>has two arguments.</text_slice>
            </slice>
            <slice>
              <time_slice>26:55</time_slice>
              <text_slice>The important thing to realize
is that when you think of this</text_slice>
            </slice>
            <slice>
              <time_slice>26:58</time_slice>
              <text_slice>as a PMF, you fix t to tau.</text_slice>
            </slice>
            <slice>
              <time_slice>27:02</time_slice>
              <text_slice>And for a fixed tau,
now this is a PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>27:06</time_slice>
              <text_slice>As I said before, the sum over
k has to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>27:11</time_slice>
              <text_slice>So for a given tau, these
probabilities add up to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>The formula is moderately messy,
but not too messy.</text_slice>
            </slice>
            <slice>
              <time_slice>27:20</time_slice>
              <text_slice>One can work with it without
too much pain.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>And what's the mean and
variance of this PMF?</text_slice>
            </slice>
            <slice>
              <time_slice>27:28</time_slice>
              <text_slice>Well what's the expected
number of arrivals?</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>If you think of this Bernoulli
analogy, we know that the</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>expected number of arrivals
in the Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>27:37</time_slice>
              <text_slice>process is n times p.</text_slice>
            </slice>
            <slice>
              <time_slice>27:41</time_slice>
              <text_slice>In the approximation that
we're using in these</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>procedure, n times p is the
same as lambda tau.</text_slice>
            </slice>
            <slice>
              <time_slice>27:48</time_slice>
              <text_slice>And that's why we get lambda tau
to be the expected number</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>27:53</time_slice>
              <text_slice>Here I'm using t
instead of tau.</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>The expected number of
arrivals is lambda t.</text_slice>
            </slice>
            <slice>
              <time_slice>28:01</time_slice>
              <text_slice>So if you double the time,
you expect to get</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>twice as many arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>If you double the arrival rate,
you expect to get twice</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>as many arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>28:12</time_slice>
              <text_slice>How about the formula
for the variance?</text_slice>
            </slice>
            <slice>
              <time_slice>28:15</time_slice>
              <text_slice>The variance of the Bernoulli
process is np,</text_slice>
            </slice>
            <slice>
              <time_slice>28:18</time_slice>
              <text_slice>times one minus p.</text_slice>
            </slice>
            <slice>
              <time_slice>28:22</time_slice>
              <text_slice>What does this go
to in the limit?</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>In the limit that we're taking,
as delta goes to zero,</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>then p also goes to zero.</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>The probability of success in
any given slot goes to zero.</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>So this term becomes
insignificant.</text_slice>
            </slice>
            <slice>
              <time_slice>28:39</time_slice>
              <text_slice>So this becomes n times p, which
is again lambda t, or</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>lambda tau.</text_slice>
            </slice>
            <slice>
              <time_slice>28:47</time_slice>
              <text_slice>So the variance, instead of
having this more complicated</text_slice>
            </slice>
            <slice>
              <time_slice>28:50</time_slice>
              <text_slice>formula of the variance is the
Bernoulli process, here it</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>gets simplified and
it's lambda t.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>So interestingly, the variance
in the Poisson process is</text_slice>
            </slice>
            <slice>
              <time_slice>29:00</time_slice>
              <text_slice>exactly the same as the
expected value.</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>So you can look at this as
just some interesting</text_slice>
            </slice>
            <slice>
              <time_slice>29:06</time_slice>
              <text_slice>coincidence.</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>So now we're going to take
this formula and</text_slice>
            </slice>
            <slice>
              <time_slice>29:10</time_slice>
              <text_slice>see how to use it.</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>First we're going to do
a completely trivial,</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>straightforward example.</text_slice>
            </slice>
            <slice>
              <time_slice>29:16</time_slice>
              <text_slice>So 15 years ago when that
example was made, email was</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>coming at a rate of five
messages per hour.</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>I wish that was the
case today.</text_slice>
            </slice>
            <slice>
              <time_slice>29:30</time_slice>
              <text_slice>And now emails that are coming
in, let's say during the day--</text_slice>
            </slice>
            <slice>
              <time_slice>29:38</time_slice>
              <text_slice>the arrival rates of emails
are probably different in</text_slice>
            </slice>
            <slice>
              <time_slice>29:41</time_slice>
              <text_slice>different times of the day.</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>But if you fix a time slot,
let's say 1:00 to 2:00 in the</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>afternoon, there's probably
a constant rate.</text_slice>
            </slice>
            <slice>
              <time_slice>29:49</time_slice>
              <text_slice>And email arrivals are
reasonably well modeled by a</text_slice>
            </slice>
            <slice>
              <time_slice>29:53</time_slice>
              <text_slice>Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>Speaking of modeling, it's
not just email arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>29:58</time_slice>
              <text_slice>Whenever arrivals happen in a
completely random way, without</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>any additional structure, the
Poisson process is a good</text_slice>
            </slice>
            <slice>
              <time_slice>30:05</time_slice>
              <text_slice>model of these arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>30:07</time_slice>
              <text_slice>So the times at which car
accidents will happen, that's</text_slice>
            </slice>
            <slice>
              <time_slice>30:10</time_slice>
              <text_slice>a Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>If you have a very, very weak
light source that's shooting</text_slice>
            </slice>
            <slice>
              <time_slice>30:19</time_slice>
              <text_slice>out photons, just one at a time,
the times at which these</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>photons will go out is
well modeled again</text_slice>
            </slice>
            <slice>
              <time_slice>30:27</time_slice>
              <text_slice>by a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>So it's completely random.</text_slice>
            </slice>
            <slice>
              <time_slice>30:30</time_slice>
              <text_slice>Or if you have a radioactive
material where one atom at a</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>time changes at random times.</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>So it's a very slow
radioactive decay.</text_slice>
            </slice>
            <slice>
              <time_slice>30:45</time_slice>
              <text_slice>The time at which these alpha
particles, or whatever we get</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>emitted, again is going
to be described</text_slice>
            </slice>
            <slice>
              <time_slice>30:51</time_slice>
              <text_slice>by a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>So if you have arrivals, or
emissions, that happen at</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>completely random times, and
once in a while you get an</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>arrival or an event, then the
Poisson process is a very good</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>model for these events.</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>So back to emails.</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>Get them at a rate of five
messages per day, per hour.</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>In 30 minutes this
is half an hour.</text_slice>
            </slice>
            <slice>
              <time_slice>31:19</time_slice>
              <text_slice>So what we have is that
lambda t, total</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>number of arrivals is--</text_slice>
            </slice>
            <slice>
              <time_slice>31:26</time_slice>
              <text_slice>the expected number
of arrivals is--</text_slice>
            </slice>
            <slice>
              <time_slice>31:29</time_slice>
              <text_slice>lambda is five, t is one-half,
if we talk about hours.</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>So lambda t is two to the 0.5.</text_slice>
            </slice>
            <slice>
              <time_slice>31:36</time_slice>
              <text_slice>The probability of no new
messages is the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>31:41</time_slice>
              <text_slice>zero, in time interval of length
t, which, in our case,</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>is one-half.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>And then we look back into the
formula from the previous</text_slice>
            </slice>
            <slice>
              <time_slice>31:55</time_slice>
              <text_slice>slide, and the probability of
zero arrivals is lambda t to</text_slice>
            </slice>
            <slice>
              <time_slice>31:59</time_slice>
              <text_slice>the power zero, divided by zero
factorial, and then an e</text_slice>
            </slice>
            <slice>
              <time_slice>32:03</time_slice>
              <text_slice>to the lambda t.</text_slice>
            </slice>
            <slice>
              <time_slice>32:05</time_slice>
              <text_slice>And you plug in the numbers
that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>Lambda t to the zero
power is one.</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>Zero factorial is one.</text_slice>
            </slice>
            <slice>
              <time_slice>32:12</time_slice>
              <text_slice>So we're left with e
to the minus 2.5.</text_slice>
            </slice>
            <slice>
              <time_slice>32:15</time_slice>
              <text_slice>And that number is 0.08.</text_slice>
            </slice>
            <slice>
              <time_slice>32:18</time_slice>
              <text_slice>Similarly, you can ask for the
probability that you get</text_slice>
            </slice>
            <slice>
              <time_slice>32:22</time_slice>
              <text_slice>exactly one message
in half an hour.</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>And that would be-- the
probability of one message in</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>one-half an hour--</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>is going to be lambda t to the
first power, divided by 1</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>factorial, e to the minus
lambda t, which--</text_slice>
            </slice>
            <slice>
              <time_slice>32:38</time_slice>
              <text_slice>as we now get the extra lambda t
factor-- is going to be 2.5,</text_slice>
            </slice>
            <slice>
              <time_slice>32:41</time_slice>
              <text_slice>e to the minus 2.5.</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>And the numerical
answer is 0.20.</text_slice>
            </slice>
            <slice>
              <time_slice>32:46</time_slice>
              <text_slice>So this is how you use the PMF
formula for the Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>distribution that we had
in the previous slide.</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>So this was all about
the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>the number of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>What else did we do last time?</text_slice>
            </slice>
            <slice>
              <time_slice>33:03</time_slice>
              <text_slice>Last time we also talked about
the time it takes until the</text_slice>
            </slice>
            <slice>
              <time_slice>33:08</time_slice>
              <text_slice>k-th arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>33:12</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>33:12</time_slice>
              <text_slice>So let's try to figure out
something about this</text_slice>
            </slice>
            <slice>
              <time_slice>33:16</time_slice>
              <text_slice>particular distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>We can derive the distribution
of the time of the k-th</text_slice>
            </slice>
            <slice>
              <time_slice>33:21</time_slice>
              <text_slice>arrival by using the
exact same argument</text_slice>
            </slice>
            <slice>
              <time_slice>33:24</time_slice>
              <text_slice>as we did last time.</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>So now the time of the
k-th arrival is a</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>continuous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>So it has a PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>33:36</time_slice>
              <text_slice>Since we are in continuous
time, arrivals can</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>happen at any time.</text_slice>
            </slice>
            <slice>
              <time_slice>33:39</time_slice>
              <text_slice>So Yk is a continuous
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>But now let's think of
a time interval of</text_slice>
            </slice>
            <slice>
              <time_slice>33:48</time_slice>
              <text_slice>length little delta.</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>And use our usual interpretation
of PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>The PDF of a random variable
evaluated at a certain time</text_slice>
            </slice>
            <slice>
              <time_slice>34:03</time_slice>
              <text_slice>times delta, this is the
probability that the Yk falls</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>in this little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>34:13</time_slice>
              <text_slice>So as I've said before, this
is the best way of thinking</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>about PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>PDFs give you probabilities
of little intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>So now let's try to calculate
this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>34:25</time_slice>
              <text_slice>For the k-th arrival to happen
inside this little interval,</text_slice>
            </slice>
            <slice>
              <time_slice>34:29</time_slice>
              <text_slice>we need two things.</text_slice>
            </slice>
            <slice>
              <time_slice>34:31</time_slice>
              <text_slice>We need an arrival to happen in
this interval, and we need</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>k minus one arrivals to happen
during that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>You'll tell me, but it's
possible that we might have</text_slice>
            </slice>
            <slice>
              <time_slice>34:45</time_slice>
              <text_slice>the k minus one arrival happen
here, and the k-th arrival to</text_slice>
            </slice>
            <slice>
              <time_slice>34:50</time_slice>
              <text_slice>happen here.</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>In principle, that's possible.</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>But in the limit, when we take
delta very small, the</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>probability of having two
arrivals in the same little</text_slice>
            </slice>
            <slice>
              <time_slice>34:59</time_slice>
              <text_slice>slot is negligible.</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>So assuming that no two arrivals
can happen in the</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>same mini slot, then for the
k-th one to happen here, we</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>must have k minus one during
this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>35:15</time_slice>
              <text_slice>Now because we have assumed that
these joint intervals are</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>independent of each other,
this breaks down into the</text_slice>
            </slice>
            <slice>
              <time_slice>35:23</time_slice>
              <text_slice>probability that we have exactly
k minus one arrivals,</text_slice>
            </slice>
            <slice>
              <time_slice>35:33</time_slice>
              <text_slice>during the interval from zero to
t, times the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>exactly one arrival during that
little interval, which is</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>lambda delta.</text_slice>
            </slice>
            <slice>
              <time_slice>35:43</time_slice>
              <text_slice>We do have a formula for this
from the previous slide, which</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>is lambda t, to the k minus 1,
over k minus one factorial,</text_slice>
            </slice>
            <slice>
              <time_slice>35:59</time_slice>
              <text_slice>times e to minus lambda t.</text_slice>
            </slice>
            <slice>
              <time_slice>36:07</time_slice>
              <text_slice>And then lambda times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:14</time_slice>
              <text_slice>Did I miss something?</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>Yeah, OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>And now you cancel this
delta with that delta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>And that gives us a formula for
the PDF of the time until</text_slice>
            </slice>
            <slice>
              <time_slice>36:36</time_slice>
              <text_slice>the k-th arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>This PDF, of course, depends
on the number k.</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>The first arrival is going
to happen somewhere in</text_slice>
            </slice>
            <slice>
              <time_slice>36:46</time_slice>
              <text_slice>this range of time.</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>So this is the PDF
that it has.</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>The second arrival, of course,
is going to happen later.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>And the PDF is this.</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>So it's more likely to happen
around these times.</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>The third arrival has this PDF,
so it's more likely to</text_slice>
            </slice>
            <slice>
              <time_slice>37:01</time_slice>
              <text_slice>happen around those times.</text_slice>
            </slice>
            <slice>
              <time_slice>37:03</time_slice>
              <text_slice>And if you were to take
k equal to 100,</text_slice>
            </slice>
            <slice>
              <time_slice>37:08</time_slice>
              <text_slice>you might get a PDF--</text_slice>
            </slice>
            <slice>
              <time_slice>37:10</time_slice>
              <text_slice>it's extremely unlikely that
the k-th arrival happens in</text_slice>
            </slice>
            <slice>
              <time_slice>37:13</time_slice>
              <text_slice>the beginning, and it might
happen somewhere down there,</text_slice>
            </slice>
            <slice>
              <time_slice>37:18</time_slice>
              <text_slice>far into the future.</text_slice>
            </slice>
            <slice>
              <time_slice>37:20</time_slice>
              <text_slice>So depending on which particular
arrival we're</text_slice>
            </slice>
            <slice>
              <time_slice>37:22</time_slice>
              <text_slice>talking about, it has a
different probability</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>37:26</time_slice>
              <text_slice>The time of the 100th arrival,
of course, is expected to be a</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>lot larger than the time
of the first arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>Incidentally, the time of the
first arrival has a PDF whose</text_slice>
            </slice>
            <slice>
              <time_slice>37:38</time_slice>
              <text_slice>form is quite simple.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>If you let k equal to one here,
this term disappears.</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>That term becomes a one.</text_slice>
            </slice>
            <slice>
              <time_slice>37:46</time_slice>
              <text_slice>You're left with just lambda,
e to the minus lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>And you recognize it, it's the
exponential distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>37:53</time_slice>
              <text_slice>So the time until the first
arrival in a Poisson process</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>is an exponential
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>38:00</time_slice>
              <text_slice>What was the time of the
first arrival in</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>the Bernoulli process?</text_slice>
            </slice>
            <slice>
              <time_slice>38:03</time_slice>
              <text_slice>It was a geometric
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>Well, not coincidentally, these
two look quite a bit</text_slice>
            </slice>
            <slice>
              <time_slice>38:11</time_slice>
              <text_slice>like the other.</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>A geometric distribution
has this kind of shape.</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>The exponential distribution
has that kind of shape.</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>The geometric is just a discrete
version of the</text_slice>
            </slice>
            <slice>
              <time_slice>38:25</time_slice>
              <text_slice>exponential.</text_slice>
            </slice>
            <slice>
              <time_slice>38:27</time_slice>
              <text_slice>In the Bernoulli case, we
are in discrete time.</text_slice>
            </slice>
            <slice>
              <time_slice>38:29</time_slice>
              <text_slice>We have a PMF for the
time of the first</text_slice>
            </slice>
            <slice>
              <time_slice>38:32</time_slice>
              <text_slice>arrival, which is geometric.</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>In the Poisson case, what we
get is the limit of the</text_slice>
            </slice>
            <slice>
              <time_slice>38:38</time_slice>
              <text_slice>geometric as you let those
lines become closer and</text_slice>
            </slice>
            <slice>
              <time_slice>38:41</time_slice>
              <text_slice>closer, which gives you the
exponential distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>Now the Poisson process shares
all the memorylessness</text_slice>
            </slice>
            <slice>
              <time_slice>38:50</time_slice>
              <text_slice>properties of the Bernoulli
process.</text_slice>
            </slice>
            <slice>
              <time_slice>38:52</time_slice>
              <text_slice>And the way one can argue is
just in terms of this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>Since the Poisson process is
the limit of Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>processes, whatever qualitative
processes you have</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>in the Bernoulli process
remain valid</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>for the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>39:08</time_slice>
              <text_slice>In particular we have this
memorylessness property.</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>You let the Poisson process run
for some time, and then</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>you start watching it.</text_slice>
            </slice>
            <slice>
              <time_slice>39:16</time_slice>
              <text_slice>What ever happened in
the past has no</text_slice>
            </slice>
            <slice>
              <time_slice>39:18</time_slice>
              <text_slice>bearing about the future.</text_slice>
            </slice>
            <slice>
              <time_slice>39:20</time_slice>
              <text_slice>Starting from right now, what's
going to happen in the</text_slice>
            </slice>
            <slice>
              <time_slice>39:23</time_slice>
              <text_slice>future is described again by a
Poisson process, in the sense</text_slice>
            </slice>
            <slice>
              <time_slice>39:27</time_slice>
              <text_slice>that during every little slot of
length delta, there's going</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>to be a probability of lambda
delta of having an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>39:33</time_slice>
              <text_slice>And that probably lambda
delta is the same-- is</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>always lambda delta--</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>no matter what happened in
the past of the process.</text_slice>
            </slice>
            <slice>
              <time_slice>39:41</time_slice>
              <text_slice>And in particular, we could use
this argument to say that</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>the time until the k-th arrival
is the time that it</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>takes for the first
arrival to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>OK, let me do it for
k equal to two.</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>And then after the first arrival
happens, you wait a</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>certain amount of time until
the second arrival happens.</text_slice>
            </slice>
            <slice>
              <time_slice>40:02</time_slice>
              <text_slice>Now once the first arrival
happened, that's in the past.</text_slice>
            </slice>
            <slice>
              <time_slice>40:06</time_slice>
              <text_slice>You start watching.</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>From now on you have mini slots
of length delta, each</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>one having a probability of
success lambda delta.</text_slice>
            </slice>
            <slice>
              <time_slice>40:13</time_slice>
              <text_slice>It's as if we started the
Poisson process from scratch.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>So starting from that time,
the time until the next</text_slice>
            </slice>
            <slice>
              <time_slice>40:19</time_slice>
              <text_slice>arrival is going to be again an
exponential distribution,</text_slice>
            </slice>
            <slice>
              <time_slice>40:22</time_slice>
              <text_slice>which doesn't care about what
happened in the past, how long</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>it took you for the
first arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>40:28</time_slice>
              <text_slice>So these two random variables
are going to be independent</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>and exponential, with the
same parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>40:38</time_slice>
              <text_slice>So among other things, what we
have done here is we have</text_slice>
            </slice>
            <slice>
              <time_slice>40:42</time_slice>
              <text_slice>essentially derived the PDF of
the sum of k independent</text_slice>
            </slice>
            <slice>
              <time_slice>40:48</time_slice>
              <text_slice>exponentials.</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>The time of the k-th arrival
is the sum of k</text_slice>
            </slice>
            <slice>
              <time_slice>40:53</time_slice>
              <text_slice>inter-arrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>The inter-arrival times are all
independent of each other</text_slice>
            </slice>
            <slice>
              <time_slice>40:59</time_slice>
              <text_slice>because of memorylessness.</text_slice>
            </slice>
            <slice>
              <time_slice>41:01</time_slice>
              <text_slice>And they all have the same
exponential distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>And by the way, this
gives you a way to</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>simulate the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>If you wanted to simulate it
on your computer, you would</text_slice>
            </slice>
            <slice>
              <time_slice>41:14</time_slice>
              <text_slice>have one option to break time
into tiny, tiny slots.</text_slice>
            </slice>
            <slice>
              <time_slice>41:20</time_slice>
              <text_slice>And for every tiny slot, use
your random number generator</text_slice>
            </slice>
            <slice>
              <time_slice>41:24</time_slice>
              <text_slice>to decide whether there
was an arrival or not.</text_slice>
            </slice>
            <slice>
              <time_slice>41:27</time_slice>
              <text_slice>To get it very accurate,
you would have to</text_slice>
            </slice>
            <slice>
              <time_slice>41:29</time_slice>
              <text_slice>use tiny, tiny slots.</text_slice>
            </slice>
            <slice>
              <time_slice>41:32</time_slice>
              <text_slice>So that would be a lot
of computation.</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>The more clever way of
simulating the Poisson process</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>is you use your random number
generator to generate a sample</text_slice>
            </slice>
            <slice>
              <time_slice>41:42</time_slice>
              <text_slice>from an exponential distribution
and call that</text_slice>
            </slice>
            <slice>
              <time_slice>41:45</time_slice>
              <text_slice>your first arrival time.</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>Then go back to the random
number generator, generate</text_slice>
            </slice>
            <slice>
              <time_slice>41:50</time_slice>
              <text_slice>another independent sample,
again from the same</text_slice>
            </slice>
            <slice>
              <time_slice>41:53</time_slice>
              <text_slice>exponential distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>That's the time between the
first and the second arrival,</text_slice>
            </slice>
            <slice>
              <time_slice>41:58</time_slice>
              <text_slice>and you keep going that way.</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>So as a sort of a
quick summary,</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>this is the big picture.</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>This table doesn't tell
you anything new.</text_slice>
            </slice>
            <slice>
              <time_slice>42:08</time_slice>
              <text_slice>But it's good to have it as a
reference, and to look at it,</text_slice>
            </slice>
            <slice>
              <time_slice>42:12</time_slice>
              <text_slice>and to make sure you understand
what all the</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>different boxes are.</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>Basically the Bernoulli process
runs in discrete time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>The Poisson process runs
in continuous time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>There's an analogy of arrival
rates, p per trial, or</text_slice>
            </slice>
            <slice>
              <time_slice>42:25</time_slice>
              <text_slice>intensity per unit time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:27</time_slice>
              <text_slice>We did derive, or sketched the
derivation for the PMF of the</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>number of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>42:33</time_slice>
              <text_slice>And the Poisson distribution,
which is the distribution that</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>we get, this Pk of t.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>Pk and t is the limit of the
binomial when we take the</text_slice>
            </slice>
            <slice>
              <time_slice>42:44</time_slice>
              <text_slice>limit in this particular way,
as delta goes to zero, and n</text_slice>
            </slice>
            <slice>
              <time_slice>42:49</time_slice>
              <text_slice>goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>42:51</time_slice>
              <text_slice>The geometric becomes an
exponential in the limit.</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>And the distribution of the
time of the k-th arrival--</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>we had a closed form formula
last time for</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>43:01</time_slice>
              <text_slice>We got the closed form
formula this time</text_slice>
            </slice>
            <slice>
              <time_slice>43:03</time_slice>
              <text_slice>for the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>And we actually used exactly the
same argument to get these</text_slice>
            </slice>
            <slice>
              <time_slice>43:08</time_slice>
              <text_slice>two closed form formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>So now let's talk about adding
or merging Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>43:18</time_slice>
              <text_slice>And there's two statements
that we can make here.</text_slice>
            </slice>
            <slice>
              <time_slice>43:21</time_slice>
              <text_slice>One has to do with adding
Poisson random variables, just</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>There's another statement about</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>adding Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>And the second is a bigger
statement than the first.</text_slice>
            </slice>
            <slice>
              <time_slice>43:34</time_slice>
              <text_slice>But this is a warm up.</text_slice>
            </slice>
            <slice>
              <time_slice>43:36</time_slice>
              <text_slice>Let's work with the
first statement.</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>So the claim is that the sum of
independent Poisson random</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>variables is Poisson.</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>So suppose that we have a
Poisson process with rate--</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>just for simplicity--</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>lambda one.</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>And I take the interval
from zero to two.</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>And that take then the interval
from two until five.</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>The number of arrivals during
this interval--</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>let's call it n from
zero to two--</text_slice>
            </slice>
            <slice>
              <time_slice>44:06</time_slice>
              <text_slice>is going to be a Poisson
random variable, with</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>parameter, or with mean, two.</text_slice>
            </slice>
            <slice>
              <time_slice>44:18</time_slice>
              <text_slice>The number of arrivals during
this interval is n from time</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>two until five.</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>This is again a Poisson random
variable with mean equal to</text_slice>
            </slice>
            <slice>
              <time_slice>44:31</time_slice>
              <text_slice>three, because the arrival rate
is 1 and the duration of</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>the interval is three.</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>These two random variables
are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>44:41</time_slice>
              <text_slice>They obey the Poisson
distribution</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>that we derived before.</text_slice>
            </slice>
            <slice>
              <time_slice>44:45</time_slice>
              <text_slice>If you add them, what you get
is the number of arrivals</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>during the interval
from zero to five.</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>Now what kind of distribution
does this</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>random variable have?</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>Well this is the number of
arrivals over an interval of a</text_slice>
            </slice>
            <slice>
              <time_slice>45:00</time_slice>
              <text_slice>certain length in a
Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>45:03</time_slice>
              <text_slice>Therefore, this is also Poisson
with mean five.</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>Because for the Poisson process
we know that this</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>number of arrivals is Poisson,
this is Poisson, but also the</text_slice>
            </slice>
            <slice>
              <time_slice>45:23</time_slice>
              <text_slice>number of overall arrivals
is also Poisson.</text_slice>
            </slice>
            <slice>
              <time_slice>45:26</time_slice>
              <text_slice>This establishes that the sum
of a Poisson plus a Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>45:30</time_slice>
              <text_slice>random variable gives
us another</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>Poisson random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>So adding Poisson random
variables gives us a Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:38</time_slice>
              <text_slice>But now I'm going to make a more
general statement that</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>it's not just number
of arrivals during</text_slice>
            </slice>
            <slice>
              <time_slice>45:44</time_slice>
              <text_slice>a fixed time interval--</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>it's not just numbers of
arrivals for given time</text_slice>
            </slice>
            <slice>
              <time_slice>45:53</time_slice>
              <text_slice>intervals--</text_slice>
            </slice>
            <slice>
              <time_slice>45:54</time_slice>
              <text_slice>but rather if you take two
different Poisson processes</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>and add them up, the process
itself is Poisson in the sense</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>that this process is going to
satisfy all the assumptions of</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>So the story is that you have
a red bulb that flashes at</text_slice>
            </slice>
            <slice>
              <time_slice>46:11</time_slice>
              <text_slice>random times at the rate
of lambda one.</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>It's a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>You have an independent process
where a green bulb</text_slice>
            </slice>
            <slice>
              <time_slice>46:19</time_slice>
              <text_slice>flashes at random times.</text_slice>
            </slice>
            <slice>
              <time_slice>46:21</time_slice>
              <text_slice>And you happen to be color
blind, so you just see when</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>something is flashing.</text_slice>
            </slice>
            <slice>
              <time_slice>46:26</time_slice>
              <text_slice>So these two are assumed to be
independent Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>46:29</time_slice>
              <text_slice>What can we say about the
process that you observe?</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>So in the processes that you
observe, if you take a typical</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>time interval of length little
delta, what can happen during</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>that little time interval?</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>The red process may have
something flashing.</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>So red flashes.</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>Or the red does not.</text_slice>
            </slice>
            <slice>
              <time_slice>47:06</time_slice>
              <text_slice>And for the other bulb, the
green bulb, there's two</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>possibilities.</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>The green one flashes.</text_slice>
            </slice>
            <slice>
              <time_slice>47:17</time_slice>
              <text_slice>And the other possibility is
that the green does not.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>47:25</time_slice>
              <text_slice>So there's four possibilities
about what can happen during a</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>little slot.</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>The probability that the red one
flashes and the green one</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>flashes, what is this
probability?</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>It's lambda one delta that the
first one flashes, and lambda</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>two delta that the
second one does.</text_slice>
            </slice>
            <slice>
              <time_slice>47:47</time_slice>
              <text_slice>I'm multiplying probabilities
here because I'm making the</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>assumption that the two
processes are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>47:55</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>47:57</time_slice>
              <text_slice>Now the probability that
the red one flashes</text_slice>
            </slice>
            <slice>
              <time_slice>48:00</time_slice>
              <text_slice>is lambda one delta.</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>But the green one doesn't is
one, minus lambda two delta.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>Here the probability would be
that the red one does not,</text_slice>
            </slice>
            <slice>
              <time_slice>48:12</time_slice>
              <text_slice>times the probability that
the green one does.</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>And then here we have the
probability that none of them</text_slice>
            </slice>
            <slice>
              <time_slice>48:20</time_slice>
              <text_slice>flash, which is whatever
is left.</text_slice>
            </slice>
            <slice>
              <time_slice>48:26</time_slice>
              <text_slice>But it's one minus lambda
one delta, times one</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>minus lambda two delta.</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>Now we're thinking about
delta as small.</text_slice>
            </slice>
            <slice>
              <time_slice>48:36</time_slice>
              <text_slice>So think of the case where delta
goes to zero, but in a</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>way that we keep the
first order terms.</text_slice>
            </slice>
            <slice>
              <time_slice>48:49</time_slice>
              <text_slice>We keep the delta terms, but
we throw away the delta</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>squared terms.</text_slice>
            </slice>
            <slice>
              <time_slice>48:55</time_slice>
              <text_slice>Delta squared terms are much
smaller than the delta terms</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>when delta becomes small.</text_slice>
            </slice>
            <slice>
              <time_slice>49:00</time_slice>
              <text_slice>If we do that--</text_slice>
            </slice>
            <slice>
              <time_slice>49:01</time_slice>
              <text_slice>if we only keep the order
of delta terms--</text_slice>
            </slice>
            <slice>
              <time_slice>49:05</time_slice>
              <text_slice>this term effectively
disappears.</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>This is delta squared.</text_slice>
            </slice>
            <slice>
              <time_slice>49:09</time_slice>
              <text_slice>So we make it zero.</text_slice>
            </slice>
            <slice>
              <time_slice>49:11</time_slice>
              <text_slice>So the probability of having
simultaneously a red and a</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>green flash during a little
interval is negligible.</text_slice>
            </slice>
            <slice>
              <time_slice>49:17</time_slice>
              <text_slice>What do we get here?</text_slice>
            </slice>
            <slice>
              <time_slice>49:20</time_slice>
              <text_slice>Lambda delta times
one survives, but</text_slice>
            </slice>
            <slice>
              <time_slice>49:23</time_slice>
              <text_slice>this times that doesn't.</text_slice>
            </slice>
            <slice>
              <time_slice>49:24</time_slice>
              <text_slice>So we can throw that away.</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>So the approximation that we
get is lambda one delta.</text_slice>
            </slice>
            <slice>
              <time_slice>49:32</time_slice>
              <text_slice>Similarly here, this
goes away.</text_slice>
            </slice>
            <slice>
              <time_slice>49:34</time_slice>
              <text_slice>We're left with a lambda
two delta.</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>And this is whatever remains,
whatever is left.</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>So what do we have?</text_slice>
            </slice>
            <slice>
              <time_slice>49:45</time_slice>
              <text_slice>That there is a probability of
seeing a flash, either a red</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>or a green, which is
lambda one delta,</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>plus lambda two delta.</text_slice>
            </slice>
            <slice>
              <time_slice>49:57</time_slice>
              <text_slice>So if we take a little interval
of length delta here,</text_slice>
            </slice>
            <slice>
              <time_slice>50:03</time_slice>
              <text_slice>it's going to see an arrival
with probability approximately</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>lambda one, plus lambda
two, delta.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>So every slot in this merged
process has an arrival</text_slice>
            </slice>
            <slice>
              <time_slice>50:20</time_slice>
              <text_slice>probability with a rate which
is the sum of the rates of</text_slice>
            </slice>
            <slice>
              <time_slice>50:25</time_slice>
              <text_slice>these two processes.</text_slice>
            </slice>
            <slice>
              <time_slice>50:27</time_slice>
              <text_slice>So this is one part
of the definition</text_slice>
            </slice>
            <slice>
              <time_slice>50:29</time_slice>
              <text_slice>of the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>50:31</time_slice>
              <text_slice>There's a few more things that
one would need to verify.</text_slice>
            </slice>
            <slice>
              <time_slice>50:34</time_slice>
              <text_slice>Namely, that intervals of the
same length have the same</text_slice>
            </slice>
            <slice>
              <time_slice>50:37</time_slice>
              <text_slice>probability distribution and
that different slots are</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>50:42</time_slice>
              <text_slice>This can be argued by starting
from here because different</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>intervals in this process are
independent from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>50:53</time_slice>
              <text_slice>Different intervals here are
independent from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>50:56</time_slice>
              <text_slice>It's not hard to argue that
different intervals in the</text_slice>
            </slice>
            <slice>
              <time_slice>50:59</time_slice>
              <text_slice>merged process will also be
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>51:03</time_slice>
              <text_slice>So the conclusion that comes
at the end is that this</text_slice>
            </slice>
            <slice>
              <time_slice>51:06</time_slice>
              <text_slice>process is a Poisson process,
with a total rate which is</text_slice>
            </slice>
            <slice>
              <time_slice>51:10</time_slice>
              <text_slice>equal to the sum of the rate
of the two processes.</text_slice>
            </slice>
            <slice>
              <time_slice>51:13</time_slice>
              <text_slice>And now if I tell you that an
arrival happened in the merged</text_slice>
            </slice>
            <slice>
              <time_slice>51:17</time_slice>
              <text_slice>process at a certain time,
how likely is it that</text_slice>
            </slice>
            <slice>
              <time_slice>51:20</time_slice>
              <text_slice>it came from here?</text_slice>
            </slice>
            <slice>
              <time_slice>51:23</time_slice>
              <text_slice>How likely is it?</text_slice>
            </slice>
            <slice>
              <time_slice>51:24</time_slice>
              <text_slice>We go to this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>51:26</time_slice>
              <text_slice>Given that an arrival
occurred--</text_slice>
            </slice>
            <slice>
              <time_slice>51:30</time_slice>
              <text_slice>which is the event that this
or that happened--</text_slice>
            </slice>
            <slice>
              <time_slice>51:36</time_slice>
              <text_slice>what is the probability that
it came from the first</text_slice>
            </slice>
            <slice>
              <time_slice>51:39</time_slice>
              <text_slice>process, the red one?</text_slice>
            </slice>
            <slice>
              <time_slice>51:42</time_slice>
              <text_slice>Well it's the probability
of this divided by the</text_slice>
            </slice>
            <slice>
              <time_slice>51:45</time_slice>
              <text_slice>probability of this,
times that.</text_slice>
            </slice>
            <slice>
              <time_slice>51:48</time_slice>
              <text_slice>Given that this event occurred,
you want to find the</text_slice>
            </slice>
            <slice>
              <time_slice>51:52</time_slice>
              <text_slice>conditional probability
of that sub event.</text_slice>
            </slice>
            <slice>
              <time_slice>51:56</time_slice>
              <text_slice>So we're asking the question,
out of the total probability</text_slice>
            </slice>
            <slice>
              <time_slice>51:58</time_slice>
              <text_slice>of these two, what
fraction of that</text_slice>
            </slice>
            <slice>
              <time_slice>52:00</time_slice>
              <text_slice>probability is assigned here?</text_slice>
            </slice>
            <slice>
              <time_slice>52:02</time_slice>
              <text_slice>And this is lambda one
delta, after we</text_slice>
            </slice>
            <slice>
              <time_slice>52:05</time_slice>
              <text_slice>ignore the other terms.</text_slice>
            </slice>
            <slice>
              <time_slice>52:07</time_slice>
              <text_slice>This is lambda two delta.</text_slice>
            </slice>
            <slice>
              <time_slice>52:09</time_slice>
              <text_slice>So that fraction is going to be
lambda one, over lambda one</text_slice>
            </slice>
            <slice>
              <time_slice>52:15</time_slice>
              <text_slice>plus lambda two.</text_slice>
            </slice>
            <slice>
              <time_slice>52:16</time_slice>
              <text_slice>What does this tell you?</text_slice>
            </slice>
            <slice>
              <time_slice>52:17</time_slice>
              <text_slice>If lambda one and lambda two are
equal, given that I saw an</text_slice>
            </slice>
            <slice>
              <time_slice>52:21</time_slice>
              <text_slice>arrival here, it's equally
likely to be red or green.</text_slice>
            </slice>
            <slice>
              <time_slice>52:25</time_slice>
              <text_slice>But if the reds have a much
higher arrival rate, when I</text_slice>
            </slice>
            <slice>
              <time_slice>52:29</time_slice>
              <text_slice>see an arrival here, it's
more likely this</text_slice>
            </slice>
            <slice>
              <time_slice>52:32</time_slice>
              <text_slice>number will be large.</text_slice>
            </slice>
            <slice>
              <time_slice>52:34</time_slice>
              <text_slice>So it's more likely to have
come from the red process.</text_slice>
            </slice>
            <slice>
              <time_slice>52:38</time_slice>
              <text_slice>OK so we'll continue with
this story and do some</text_slice>
            </slice>
            <slice>
              <time_slice>52:40</time_slice>
              <text_slice>applications next time.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Poisson Process - II (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Light bulb example Splitting of Poisson processes
Each light bulb has independent, Assume that email tra c through a server
exponential()l i f e t i m e is a Poisson process.
Destinations of di erent messages are
Install three light bulbs.independent.
Find expected time until last light bulbdies out.
USA
Email Traffic
leaving  MIT p!MIT
Server
! (1 - p)!
Foreign
Each output stream is Poisson.
Random incidence for Poisson Random incidence in
renewal processes
Poisson process that has been running
forever Series of successive arrivals
i.i.d. interarrival timesShow up at some random time
(but not necessarily exponential)(really means arbitrary time)
Example:Bus interarrival times are equally likely to
x x xx x
Time be 5 or 10 minutes
Chosen 
time instant If you arrive at a random time:
what is the probability that you selected
a 5 minute interarrival interval?
what is the expected time
What is the distribution of the length ofto next arrival?
the chosen interarrival interval?     
Splitting of Poisson processes
Each message is routed along the rst
stream with probability p
Routings of dierent messages are in-
dependent
Each output stream is Poisson   
Random incidence for Poisson
Poisson process that has been running
forever
Show up at some random time
(random incidence)
x x
Time
Chosen time instantx x x
What is the distribution of the length of
the chosen interarrival interval?  
Renewal processes
Series of successive arrivals
i.i.d. interarrival times
(but not necessarily exponential)
Example:Bus interarrival times are equally likely tobe 5 or 10 minutes
If you arrive at a random time:
what is the probability that you selected
a 5 minute interarrival interval?
what is the expected time to next ar-
rival?
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 15 Review
Dening characteristics
Poisson process  II
T i m e h o m o g e n e i t y :P(k,)
Readings: Finish Section 6.2. I n d e p e n d e n c e
S m a l l i n t e r v a l p r o b a b i l i t i e s (small ):
Review of Poisson process
1,ifk=0,
P(k,), ifk=1,
Merging and splitting

0, ifk&gt;1.
Examples Nis a Poisson
r.v., with parameter :
()ke
Random incidence P(k,)= ,k =0,1,...k!
E[N]=v a r ( N)=
Interarrival times (k = 1): exponential:
fT(t)= e1t,t0, E[T1]=1 /
Time Yktokth arrival: Erlang(k ):
kyk1ey
fY(y)= ,yk(k 1)!0
Poisson shing Merging Poisson Processes (again)
Assume: Poisson, =0.6/hour. Merging of independent Poisson processes
is PoissonFish for two hours.
Red bulb flashes
if no catch, continue until rst catch.  (Poisson)
All  flashes
a)P(sh for more than two hours)="1  (Poisson)
"2
b)P(sh for more than two and less than
ve hours)=Green bulb flashes
 (Poisson)
c)P(catch at least two sh)= What is the probability that the next
arrival comes from the rst process?
d)E[number of sh]=
e)E[future shing time |shed for four hours]=
f)E[total shing time]=     
Interarrival Times
Yktime of kth arrival
Erlang distribution:
fYk(y) =kyk1ey
(k1)!, y 0
k=1
k=2
k=3
yfY (y)
k
First-order interarrival times (k = 1):
exponentialf
Y1(y) =ey, y 0
Memoryless property: The time to the
next arrival is independent of the past       
Bernoulli/Poisson Relation
Time 0x x x!
Arrivalsp ="!! ! ! ! ! ! !n = t /!
np  ="t
POISSON BERNOULLI
Times of Arrival Continuous Discrete
Arrival Rate /unit time p/per trial
PMF of # of Arrivals Poisson Binomial
Interarrival Time Distr. Exponential Geometric
Time to k-th arrival Erlang Pascal     
Adding Poisson Processes
Sum of independent Poisson random vari-
ables is Poisson
Sum of independent Poisson processes
is Poisson
What is the probability that the next
arrival comes from the rst process?
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-15-poisson-process-ii/</video_url>
          <video_title>Lecture 15: Poisson Process II</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>JOHN TSITSIKLIS: Today we're
going to finish our discussion</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>of the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>0:27</time_slice>
              <text_slice>We're going to see a few of
its properties, do a few</text_slice>
            </slice>
            <slice>
              <time_slice>0:31</time_slice>
              <text_slice>interesting problems, some more
interesting than others.</text_slice>
            </slice>
            <slice>
              <time_slice>0:35</time_slice>
              <text_slice>So go through a few examples and
then we're going to talk</text_slice>
            </slice>
            <slice>
              <time_slice>0:39</time_slice>
              <text_slice>about some quite strange things
that happen with the</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>0:43</time_slice>
              <text_slice>So the first thing is to
remember what the Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>processes is.</text_slice>
            </slice>
            <slice>
              <time_slice>0:48</time_slice>
              <text_slice>It's a model, let's say, of
arrivals of customers that</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>are, in some sense, quote
unquote, completely random,</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>that is a customer can arrive
at any point in time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>All points in time are
equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>And different points in time
are sort of independent of</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>other points in time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>So the fact that I got an
arrival now doesn't tell me</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>anything about whether there's
going to be an arrival at some</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>other time.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>In some sense, it's a continuous
time version of the</text_slice>
            </slice>
            <slice>
              <time_slice>1:18</time_slice>
              <text_slice>Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>1:19</time_slice>
              <text_slice>So the best way to think about
the Poisson process is that we</text_slice>
            </slice>
            <slice>
              <time_slice>1:23</time_slice>
              <text_slice>divide time into extremely
tiny slots.</text_slice>
            </slice>
            <slice>
              <time_slice>1:26</time_slice>
              <text_slice>And in each time slot, there's
an independent possibility of</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>having an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>Different time slots are
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>1:33</time_slice>
              <text_slice>On the other hand, when the slot
is tiny, the probability</text_slice>
            </slice>
            <slice>
              <time_slice>1:36</time_slice>
              <text_slice>for obtaining an arrival during
that tiny slot is</text_slice>
            </slice>
            <slice>
              <time_slice>1:39</time_slice>
              <text_slice>itself going to be tiny.</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>So we capture these properties
into a formal definition what</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>the Poisson process is.</text_slice>
            </slice>
            <slice>
              <time_slice>1:48</time_slice>
              <text_slice>We have a probability mass
function for the number of</text_slice>
            </slice>
            <slice>
              <time_slice>1:51</time_slice>
              <text_slice>arrivals, k, during an interval
of a given length.</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>So this is the sort of basic
description of the</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>distribution of the number
of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>So tau is fixed.</text_slice>
            </slice>
            <slice>
              <time_slice>2:07</time_slice>
              <text_slice>And k is the parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>So when we add over all k's, the
sum of these probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>2:13</time_slice>
              <text_slice>has to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>2:15</time_slice>
              <text_slice>There's a time homogeneity
assumption, which is hidden in</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>this, namely, the only thing
that matters is the duration</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>of the time interval, not where
the time interval sits</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>on the real axis.</text_slice>
            </slice>
            <slice>
              <time_slice>2:28</time_slice>
              <text_slice>Then we have an independence
assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>Intervals that are disjoint are
statistically independent</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>2:35</time_slice>
              <text_slice>So any information you give me
about arrivals during this</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>time interval doesn't change my
beliefs about what's going</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>to happen during another
time interval.</text_slice>
            </slice>
            <slice>
              <time_slice>2:45</time_slice>
              <text_slice>So this is a generalization
of the idea that we had in</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>Bernoulli processes that
different time slots are</text_slice>
            </slice>
            <slice>
              <time_slice>2:51</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>And then to specify this
function, the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>the number of arrivals, we
sort of go in stages.</text_slice>
            </slice>
            <slice>
              <time_slice>3:00</time_slice>
              <text_slice>We first specify this function
for the case where the time</text_slice>
            </slice>
            <slice>
              <time_slice>3:03</time_slice>
              <text_slice>interval is very small.</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>And I'm telling you what those
probabilities will be.</text_slice>
            </slice>
            <slice>
              <time_slice>3:09</time_slice>
              <text_slice>And based on these then, we do
some calculations and to find</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>the formula for the distribution
of the number of</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>arrivals for intervals of
a general duration.</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>So for a small duration, delta,
the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>obtaining 1 arrival
is lambda delta.</text_slice>
            </slice>
            <slice>
              <time_slice>3:26</time_slice>
              <text_slice>The remaining probability is
assigned to the event that we</text_slice>
            </slice>
            <slice>
              <time_slice>3:30</time_slice>
              <text_slice>get to no arrivals during
that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>The probability of obtaining
more than 1 arrival in a tiny</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>interval is essentially 0.</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>And when we say essentially,
it's means modular, terms that</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>of order delta squared.</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>And when delta is very small,
anything which is delta</text_slice>
            </slice>
            <slice>
              <time_slice>3:50</time_slice>
              <text_slice>squared can be ignored.</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>So up to delta squared terms,
that's what happened during a</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>Now if we know the probability
distribution for the number of</text_slice>
            </slice>
            <slice>
              <time_slice>4:01</time_slice>
              <text_slice>arrivals in a little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>We can use this to get the
distribution for the number of</text_slice>
            </slice>
            <slice>
              <time_slice>4:06</time_slice>
              <text_slice>arrivals over several
intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>How do we do that?</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>The big interval is composed
of many little intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>4:13</time_slice>
              <text_slice>Each little interval is
independent from any other</text_slice>
            </slice>
            <slice>
              <time_slice>4:16</time_slice>
              <text_slice>little interval, so is it is
as if we have a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>Bernoulli trials.</text_slice>
            </slice>
            <slice>
              <time_slice>4:22</time_slice>
              <text_slice>Each Bernoulli trial is
associated with a little</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>interval and has a small
probability of obtaining a</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>success or an arrival during
that mini-slot.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>On the other hand, when delta
is small, and you take a big</text_slice>
            </slice>
            <slice>
              <time_slice>4:35</time_slice>
              <text_slice>interval and chop it
up, you get a large</text_slice>
            </slice>
            <slice>
              <time_slice>4:38</time_slice>
              <text_slice>number of little intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>So what we essentially have here
is a Bernoulli process,</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>in which is the number of
trials is huge but the</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>probability of success during
any given trial is tiny.</text_slice>
            </slice>
            <slice>
              <time_slice>4:53</time_slice>
              <text_slice>The average number of trials
ends up being proportional to</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>the length of the interval.</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>If you have twice as large an
interval, it's as if you're</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>having twice as many over these
mini-trials, so the</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>expected number of arrivals will
increase proportionately.</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>There's also this parameter
lambda, which we interpret as</text_slice>
            </slice>
            <slice>
              <time_slice>5:18</time_slice>
              <text_slice>expected number of arrivals
per unit time.</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>And it comes in those
probabilities here.</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>When you double lambda, this
means that a little interval</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>is twice as likely to
get an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>So you would expect
to get twice as</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>many arrivals as well.</text_slice>
            </slice>
            <slice>
              <time_slice>5:34</time_slice>
              <text_slice>That's why the expected number
of arrivals during an interval</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>of length tau also scales
proportional to</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>this parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>Somewhat unexpectedly, it turns
out that the variance of</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>the number of arrivals is also
the same as the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>This is a peculiarity
that happens</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>in the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>5:52</time_slice>
              <text_slice>So this is one way of thinking
about Poisson process, in</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>terms of little intervals, each
one of which has a tiny</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>probability of success.</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>And we think of the distribution
associated with</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>that process as being
described by</text_slice>
            </slice>
            <slice>
              <time_slice>6:07</time_slice>
              <text_slice>this particular PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>So this is the PMF for the
number of arrivals during an</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>interval of a fixed
duration, tau.</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>It's a PMF that extends all
over the entire range of</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>non-negative integers.</text_slice>
            </slice>
            <slice>
              <time_slice>6:22</time_slice>
              <text_slice>So the number of arrivals you
can get during an interval for</text_slice>
            </slice>
            <slice>
              <time_slice>6:25</time_slice>
              <text_slice>certain length can
be anything.</text_slice>
            </slice>
            <slice>
              <time_slice>6:27</time_slice>
              <text_slice>You can get as many arrivals
as you want.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>Of course the probability of
getting a zillion arrivals is</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>going to be tiny.</text_slice>
            </slice>
            <slice>
              <time_slice>6:35</time_slice>
              <text_slice>But in principle, this
is possible.</text_slice>
            </slice>
            <slice>
              <time_slice>6:38</time_slice>
              <text_slice>And that's because an interval,
even if it's a fixed</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>length, consists of an infinite
number of mini-slots</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>in some sense.</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>You can divide, chop
it up, into as many</text_slice>
            </slice>
            <slice>
              <time_slice>6:50</time_slice>
              <text_slice>mini-slots as you want.</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>So in principle, it's
possible that every</text_slice>
            </slice>
            <slice>
              <time_slice>6:54</time_slice>
              <text_slice>mini-slot gets an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>6:55</time_slice>
              <text_slice>In principle, it's possible to
get an arbitrarily large</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>number of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>7:01</time_slice>
              <text_slice>So this particular formula here
is not very intuitive</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>when you look at it.</text_slice>
            </slice>
            <slice>
              <time_slice>7:06</time_slice>
              <text_slice>But it's a legitimate PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>And it's called the
Poisson PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>It's the PMF that describes
the number of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>So that's one way of thinking
about the Poisson process,</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>where the basic object of
interest would be this PMF and</text_slice>
            </slice>
            <slice>
              <time_slice>7:21</time_slice>
              <text_slice>you try to work with it.</text_slice>
            </slice>
            <slice>
              <time_slice>7:23</time_slice>
              <text_slice>There's another way of thinking
about what happens in</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>And this has to do with letting
things evolve in time.</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>You start at time 0.</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>There's going to be a time at
which the first arrival</text_slice>
            </slice>
            <slice>
              <time_slice>7:37</time_slice>
              <text_slice>occurs, and call that time T1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:40</time_slice>
              <text_slice>This time turns out to have an
exponential distribution with</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>7:46</time_slice>
              <text_slice>Once you get an arrival,
it's as if the</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>process starts fresh.</text_slice>
            </slice>
            <slice>
              <time_slice>7:53</time_slice>
              <text_slice>The best way to understand why
this is the case is by</text_slice>
            </slice>
            <slice>
              <time_slice>7:55</time_slice>
              <text_slice>thinking in terms of
the analogy with</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>If you believe that statement
for the Bernoulli process,</text_slice>
            </slice>
            <slice>
              <time_slice>8:01</time_slice>
              <text_slice>since this is a limiting case,
it should also be true.</text_slice>
            </slice>
            <slice>
              <time_slice>8:05</time_slice>
              <text_slice>So starting from this time,
we're going to wait a random</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>amount of time until we get the
second arrival This random</text_slice>
            </slice>
            <slice>
              <time_slice>8:12</time_slice>
              <text_slice>amount of time, let's
call it T2.</text_slice>
            </slice>
            <slice>
              <time_slice>8:15</time_slice>
              <text_slice>This time, T2 is also going
to have an exponential</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>distribution with the same
parameter, lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>And these two are going to be
independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>OK?</text_slice>
            </slice>
            <slice>
              <time_slice>8:27</time_slice>
              <text_slice>So the Poisson process has all
the same memorylessness</text_slice>
            </slice>
            <slice>
              <time_slice>8:31</time_slice>
              <text_slice>properties that the Bernoulli
process has.</text_slice>
            </slice>
            <slice>
              <time_slice>8:34</time_slice>
              <text_slice>What's another way of thinking
of this property?</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>So think of a process where
you have a light bulb.</text_slice>
            </slice>
            <slice>
              <time_slice>8:43</time_slice>
              <text_slice>The time at the light bulb burns
out, you can model it by</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>an exponential random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>And suppose that they tell you
that so far, we're are sitting</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>at some time, T. And I tell you
that the light bulb has</text_slice>
            </slice>
            <slice>
              <time_slice>9:01</time_slice>
              <text_slice>not yet burned out.</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>What does this tell you about
the future of the light bulb?</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>Is the fact that they didn't
burn out, so far, is it good</text_slice>
            </slice>
            <slice>
              <time_slice>9:11</time_slice>
              <text_slice>news or is it bad news?</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>Would you rather keep this light
bulb that has worked for</text_slice>
            </slice>
            <slice>
              <time_slice>9:17</time_slice>
              <text_slice>t times steps and is still OK?</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>Or would you rather use a new
light bulb that starts new at</text_slice>
            </slice>
            <slice>
              <time_slice>9:25</time_slice>
              <text_slice>that point in time?</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>Because of the memorylessness
property, the past of that</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>light bulb doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>9:33</time_slice>
              <text_slice>So the future of this light bulb
is statistically the same</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>as the future of a
new light bulb.</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>For both of them, the time until
they burn out is going</text_slice>
            </slice>
            <slice>
              <time_slice>9:43</time_slice>
              <text_slice>to be described an exponential
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>9:46</time_slice>
              <text_slice>So one way that people described
the situation is to</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>say that used is exactly
as good as a new.</text_slice>
            </slice>
            <slice>
              <time_slice>9:55</time_slice>
              <text_slice>So a used on is no worse
than a new one.</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>A used one is no better
than a new one.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>So a used light bulb that
hasn't yet burnt out is</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>exactly as good as
a new light bulb.</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>So that's another way of
thinking about the</text_slice>
            </slice>
            <slice>
              <time_slice>10:11</time_slice>
              <text_slice>memorylessness that we have
in the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>Back to this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>10:19</time_slice>
              <text_slice>The time until the second
arrival is the sum of two</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>independent exponential
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>10:24</time_slice>
              <text_slice>So, in principle, you can use
the convolution formula to</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>find the distribution of T1
plus T2, and that would be</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>what we call Y2, the time until
the second arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>10:36</time_slice>
              <text_slice>But there's also a direct
way of obtaining to the</text_slice>
            </slice>
            <slice>
              <time_slice>10:39</time_slice>
              <text_slice>distribution of Y2, and this is
the calculation that we did</text_slice>
            </slice>
            <slice>
              <time_slice>10:42</time_slice>
              <text_slice>last time on the blackboard.</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>And actually, we did
it more generally.</text_slice>
            </slice>
            <slice>
              <time_slice>10:46</time_slice>
              <text_slice>We found the time until the
case arrival occurs.</text_slice>
            </slice>
            <slice>
              <time_slice>10:49</time_slice>
              <text_slice>It has a closed form formula,
which is called the Erlang</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>distribution with k degrees
of freedom.</text_slice>
            </slice>
            <slice>
              <time_slice>10:56</time_slice>
              <text_slice>So let's see what's
going on here.</text_slice>
            </slice>
            <slice>
              <time_slice>11:00</time_slice>
              <text_slice>It's a distribution
Of what kind?</text_slice>
            </slice>
            <slice>
              <time_slice>11:03</time_slice>
              <text_slice>It's a continuous
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>It's a probability
density function.</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>This is because the time is a
continuous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>11:10</time_slice>
              <text_slice>Time is continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>Arrivals can happen
at any time.</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>So we're talking
about the PDF.</text_slice>
            </slice>
            <slice>
              <time_slice>11:17</time_slice>
              <text_slice>This k is just the parameter
of the distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>We're talking about the
k-th arrival, so</text_slice>
            </slice>
            <slice>
              <time_slice>11:22</time_slice>
              <text_slice>k is a fixed number.</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>Lambda is another parameter of
the distribution, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>arrival rate So it's a PDF over
the Y's, whereas lambda</text_slice>
            </slice>
            <slice>
              <time_slice>11:32</time_slice>
              <text_slice>and k are parameters of
the distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>So this was what we knew
from last time.</text_slice>
            </slice>
            <slice>
              <time_slice>11:45</time_slice>
              <text_slice>Just to get some practice, let
us do a problem that's not too</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>difficult, but just to see how
we use the various formulas</text_slice>
            </slice>
            <slice>
              <time_slice>11:55</time_slice>
              <text_slice>that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>So Poisson was a mathematician,
but Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>12:01</time_slice>
              <text_slice>also means fish in French.</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>So Poisson goes fishing.</text_slice>
            </slice>
            <slice>
              <time_slice>12:07</time_slice>
              <text_slice>And let's assume that fish
are caught according</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>to a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>That's not too bad
an assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>At any given point in time, you
have a little probability</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>that a fish would be caught.</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>And whether you catch one now
is sort of independent about</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>whether at some later time a
fish will be caught or not.</text_slice>
            </slice>
            <slice>
              <time_slice>12:28</time_slice>
              <text_slice>So let's just make
this assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>And suppose that the rules of
the game are that you--</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>Fish are being called it the
certain rate of 0.6 per hour.</text_slice>
            </slice>
            <slice>
              <time_slice>12:40</time_slice>
              <text_slice>You fish for 2 hours,
no matter what.</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>And then there are two
possibilities.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>If I have caught a fish,
I stop and go home.</text_slice>
            </slice>
            <slice>
              <time_slice>12:50</time_slice>
              <text_slice>So if some fish have been
caught, so there's at least 1</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>arrival during this interval,
I go home.</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>Or if nothing has being caught,
I continue fishing</text_slice>
            </slice>
            <slice>
              <time_slice>13:01</time_slice>
              <text_slice>until I catch something.</text_slice>
            </slice>
            <slice>
              <time_slice>13:03</time_slice>
              <text_slice>And then I go home.</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>So that's the description of
what is going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>And now let's starts asking
questions of all sorts.</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>What is the probability that
I'm going to be fishing for</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>more than 2 hours?</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>I will be fishing for more than
2 hours, if and only if</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>no fish were caught during those
2 hours, in which case,</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>I will have to continue.</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>Therefore, this is just
this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>13:33</time_slice>
              <text_slice>The probability of catching
2 fish in--</text_slice>
            </slice>
            <slice>
              <time_slice>13:38</time_slice>
              <text_slice>of catching 0 fish in the next
2 hours, and according to the</text_slice>
            </slice>
            <slice>
              <time_slice>13:43</time_slice>
              <text_slice>formula that we have, this is
going to be e to the minus</text_slice>
            </slice>
            <slice>
              <time_slice>13:47</time_slice>
              <text_slice>lambda times how much
time we have.</text_slice>
            </slice>
            <slice>
              <time_slice>13:50</time_slice>
              <text_slice>There's another way of
thinking about this.</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>The probability that I fish for
more than 2 hours is the</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>probability that the first catch
happens after time 2,</text_slice>
            </slice>
            <slice>
              <time_slice>14:01</time_slice>
              <text_slice>which would be the integral
from 2 to infinity of the</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>density of the first
arrival time.</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>And that density is
an exponential.</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>So you do the integral of an
exponential, and, of course,</text_slice>
            </slice>
            <slice>
              <time_slice>14:14</time_slice>
              <text_slice>you would get the same answer.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>That's easy.</text_slice>
            </slice>
            <slice>
              <time_slice>14:18</time_slice>
              <text_slice>So what's the probability of
fishing for more than 2 but</text_slice>
            </slice>
            <slice>
              <time_slice>14:22</time_slice>
              <text_slice>less than 5 hours?</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>What does it take for
this to happen?</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>For this to happen, we need to
catch 0 fish from time 0 to 2</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>and catch the first fish
sometime between 2 and 5.</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>So if you--</text_slice>
            </slice>
            <slice>
              <time_slice>14:44</time_slice>
              <text_slice>one way of thinking about what's
happening here might be</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>to say that there's a
Poisson process that</text_slice>
            </slice>
            <slice>
              <time_slice>14:49</time_slice>
              <text_slice>keeps going on forever.</text_slice>
            </slice>
            <slice>
              <time_slice>14:52</time_slice>
              <text_slice>But as soon as I catch the
first fish, instead of</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>continuing fishing and obtaining
those other fish I</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>just go home right now.</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>Now the fact that I go home
before time 5 means that, if I</text_slice>
            </slice>
            <slice>
              <time_slice>15:11</time_slice>
              <text_slice>were to stay until time
5, I would have</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>caught at least 1 fish.</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>I might have caught
more than 1.</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>So the event of interest here
is that the first catch</text_slice>
            </slice>
            <slice>
              <time_slice>15:22</time_slice>
              <text_slice>happens between times 2 and 5.</text_slice>
            </slice>
            <slice>
              <time_slice>15:26</time_slice>
              <text_slice>So one way of calculating
this quantity would be--</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>Its the probability that the
first catch happens between</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>times 2 and 5.</text_slice>
            </slice>
            <slice>
              <time_slice>15:37</time_slice>
              <text_slice>Another way to deal with it
is to say, this is the</text_slice>
            </slice>
            <slice>
              <time_slice>15:40</time_slice>
              <text_slice>probability that I caught 0 fish
in the first 2 hours and</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>then the probability that I
catch at least 1 fish during</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>the next 3 hours.</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>This.</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>What is this?</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>The probability of 0 fish in
the next 3 hours is the</text_slice>
            </slice>
            <slice>
              <time_slice>15:59</time_slice>
              <text_slice>probability of 0 fish
during this time.</text_slice>
            </slice>
            <slice>
              <time_slice>16:01</time_slice>
              <text_slice>1 minus this is the probability
of catching at</text_slice>
            </slice>
            <slice>
              <time_slice>16:04</time_slice>
              <text_slice>least 1 fish, of having
at least 1 arrival,</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>between times 2 and 5.</text_slice>
            </slice>
            <slice>
              <time_slice>16:09</time_slice>
              <text_slice>If there's at least 1 arrival
between times 2 and 5, then I</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>would have gone home
by time 5.</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>So both of these, if you plug-in
numbers and all that,</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>of course, are going to give
you the same answer.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>Now next, what's the probability
that I catch at</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>least 2 fish?</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>In which scenario are we?</text_slice>
            </slice>
            <slice>
              <time_slice>16:32</time_slice>
              <text_slice>Under this scenario, I go home
when I catch my first fish.</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>So in order to catch
at least 2 fish, it</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>must be in this case.</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>So this is the same as the event
that I catch at least 2</text_slice>
            </slice>
            <slice>
              <time_slice>16:44</time_slice>
              <text_slice>fish during the first
2 time steps.</text_slice>
            </slice>
            <slice>
              <time_slice>16:49</time_slice>
              <text_slice>So it's going to be the
probability from 2 to</text_slice>
            </slice>
            <slice>
              <time_slice>16:52</time_slice>
              <text_slice>infinity, the probability that
I catch 2 fish, or that I</text_slice>
            </slice>
            <slice>
              <time_slice>16:56</time_slice>
              <text_slice>catch 3 fish, or I catch
more than that.</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>So it's this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>k is the number of fish
that I catch.</text_slice>
            </slice>
            <slice>
              <time_slice>17:06</time_slice>
              <text_slice>At least 2, so k goes
from 2 to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:09</time_slice>
              <text_slice>These are the probabilities of
catching a number k of fish</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>during this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>And if you want a simpler form
without an infinite sum, this</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>would be 1 minus the probability
of catching 0</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>fish, minus the probability of
catching 1 fish, during a time</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>interval of length 2.</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>Another way to think of it.</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>I'm going to catch 2 fish, at
least 2 fish, if and only if</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>the second fish caught in this
process happens before time 2.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>So that's another way of
thinking about the same event.</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>So it's going to be the
probability that the random</text_slice>
            </slice>
            <slice>
              <time_slice>17:46</time_slice>
              <text_slice>variable Y2, the arrival time
over the second fish, is less</text_slice>
            </slice>
            <slice>
              <time_slice>17:51</time_slice>
              <text_slice>than or equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>The next one is a
little trickier.</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>Here we need to do a little
bit of divide and conquer.</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>Overall, in this expedition,
what the expected number of</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>fish to be caught?</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>One way to think about it is
to try to use the total</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>expectations theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>18:13</time_slice>
              <text_slice>And think of expected number of
fish, given this scenario,</text_slice>
            </slice>
            <slice>
              <time_slice>18:17</time_slice>
              <text_slice>or expected number of fish,
given this scenario.</text_slice>
            </slice>
            <slice>
              <time_slice>18:21</time_slice>
              <text_slice>That's a little more complicated
than the way I'm</text_slice>
            </slice>
            <slice>
              <time_slice>18:24</time_slice>
              <text_slice>going to do it.</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>The way I'm going to do is
to think as follows--</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>Expected number of fish is the
expected number of fish caught</text_slice>
            </slice>
            <slice>
              <time_slice>18:32</time_slice>
              <text_slice>between times 0 and 2 plus
expected number of fish caught</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>after time 2.</text_slice>
            </slice>
            <slice>
              <time_slice>18:39</time_slice>
              <text_slice>So what's the expected number
caught between time 0 and 2?</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>This is lambda t.</text_slice>
            </slice>
            <slice>
              <time_slice>18:47</time_slice>
              <text_slice>So lambda is 0.6 times 2.</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>This is the expected number of
fish that are caught between</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>times 0 and 2.</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>Now let's think about the
expected number of fish caught</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>afterwards.</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>How many fish are being
caught afterwards?</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>Well it depends on
the scenario.</text_slice>
            </slice>
            <slice>
              <time_slice>19:06</time_slice>
              <text_slice>If we're in this scenario,
we've gone home</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>and we catch 0.</text_slice>
            </slice>
            <slice>
              <time_slice>19:10</time_slice>
              <text_slice>If we're in this scenario, then
we continue fishing until</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>we catch one.</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>So the expected number of fish
to be caught after time 2 is</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>going to be the probability
of this scenario times 1.</text_slice>
            </slice>
            <slice>
              <time_slice>19:24</time_slice>
              <text_slice>And the probability of that
scenario is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>that they call it's 0 fish
during the first 2 time steps</text_slice>
            </slice>
            <slice>
              <time_slice>19:33</time_slice>
              <text_slice>times 1, which is the number of
fish I'm going to catch if</text_slice>
            </slice>
            <slice>
              <time_slice>19:37</time_slice>
              <text_slice>I continue.</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>The expected total fishing time
we can calculate exactly</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>the same way.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>I'm jumping to the last one.</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>My total fishing time has a
period of 2 time steps.</text_slice>
            </slice>
            <slice>
              <time_slice>19:51</time_slice>
              <text_slice>I'm going to fish for 2 time
steps no matter what.</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>And then if I caught 0 fish,
which happens with this</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>probability, my expected time
is going to be the expected</text_slice>
            </slice>
            <slice>
              <time_slice>20:04</time_slice>
              <text_slice>time from here onwards, which is
the expected value of this</text_slice>
            </slice>
            <slice>
              <time_slice>20:08</time_slice>
              <text_slice>geometric random variable
with parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>So the expected time
is 1 over lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>20:15</time_slice>
              <text_slice>And in our case this,
is 1/0.6.</text_slice>
            </slice>
            <slice>
              <time_slice>20:22</time_slice>
              <text_slice>Finally, if I tell you that I
have been fishing for 4 hours</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>and nothing has been caught so
far, how much do you expect</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>this quantity to be?</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>Here is the story that, again,
that for the Poisson process</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>used is as good as new.</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>The process does not
have any memory.</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>Given what happens in the past
doesn't matter for the future.</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>It's as if the process starts
new at this point in time.</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>So this one is going to be,
again, the same exponentially</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>distributed random
variable with the</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>same parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>21:08</time_slice>
              <text_slice>So expected time until an
arrival comes is an</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>exponential distribut --</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>has an exponential distribution
with parameter</text_slice>
            </slice>
            <slice>
              <time_slice>21:15</time_slice>
              <text_slice>lambda, no matter what has
happened in the past.</text_slice>
            </slice>
            <slice>
              <time_slice>21:19</time_slice>
              <text_slice>Starting from now and looking
into the future, it's as if</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>the process has just started.</text_slice>
            </slice>
            <slice>
              <time_slice>21:24</time_slice>
              <text_slice>So it's going to be 1 over
lambda, which is 1/0.6.</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>Now our next example is going
to be a little more</text_slice>
            </slice>
            <slice>
              <time_slice>21:41</time_slice>
              <text_slice>complicated or subtle.</text_slice>
            </slice>
            <slice>
              <time_slice>21:43</time_slice>
              <text_slice>But before we get to the
example, let's refresh our</text_slice>
            </slice>
            <slice>
              <time_slice>21:46</time_slice>
              <text_slice>memory about what we discussed
last time about merging</text_slice>
            </slice>
            <slice>
              <time_slice>21:50</time_slice>
              <text_slice>Poisson independent
Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>21:53</time_slice>
              <text_slice>Instead of drawing the picture
that way, another way we could</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>draw it could be this.</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>We have a Poisson process with
rate lambda1, and a Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>22:01</time_slice>
              <text_slice>process with rate lambda2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>They have, each one of these,
have their arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>22:07</time_slice>
              <text_slice>And then we form the
merged process.</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>And the merged process records
an arrival whenever there's an</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>arrival in either of
the two processes.</text_slice>
            </slice>
            <slice>
              <time_slice>22:19</time_slice>
              <text_slice>This process in that process are
assumed to be independent</text_slice>
            </slice>
            <slice>
              <time_slice>22:23</time_slice>
              <text_slice>of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>Now different times in this
process and that process are</text_slice>
            </slice>
            <slice>
              <time_slice>22:32</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>So what happens in these two
time intervals is independent</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>from what happens in these
two time intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>These two time intervals to
determine what happens here.</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>These two time intervals
determine what happens there.</text_slice>
            </slice>
            <slice>
              <time_slice>22:48</time_slice>
              <text_slice>So because these are independent
from these, this</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>means that this is also
independent from that.</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>So the independence assumption
is satisfied</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>for the merged process.</text_slice>
            </slice>
            <slice>
              <time_slice>23:01</time_slice>
              <text_slice>And the merged process turns out
to be a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>And if you want to find the
arrival rate for that process,</text_slice>
            </slice>
            <slice>
              <time_slice>23:10</time_slice>
              <text_slice>you argue as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>During a little interval of
length delta, we have</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>probability lambda1
delta of having an</text_slice>
            </slice>
            <slice>
              <time_slice>23:17</time_slice>
              <text_slice>arrival in this process.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>We have probability lambda2
delta of an arrival in this</text_slice>
            </slice>
            <slice>
              <time_slice>23:21</time_slice>
              <text_slice>process, plus second
order terms in</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>delta, which we're ignoring.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>And then you do the calculation
and you find that</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>in this process, you're going
to have an arrival</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>probability, which is lambda1
plus lambda2, again ignoring</text_slice>
            </slice>
            <slice>
              <time_slice>23:37</time_slice>
              <text_slice>second order in delta--</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>terms that are second
order in delta.</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>So the merged process is a
Poisson process whose arrival</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>rate is the sum of the
arrival rates of</text_slice>
            </slice>
            <slice>
              <time_slice>23:48</time_slice>
              <text_slice>the individual processes.</text_slice>
            </slice>
            <slice>
              <time_slice>23:52</time_slice>
              <text_slice>And the calculation we did at
the end of the last lecture--</text_slice>
            </slice>
            <slice>
              <time_slice>23:55</time_slice>
              <text_slice>If I tell you that the new
arrival happened here, where</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>did that arrival come from?</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>Did it come from here
or from there?</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>If the lambda1 is equal to
lambda2, then by symmetry you</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>would say that it's equally
likely to have come from here</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>or to come from there.</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>But if this lambda is much
bigger than that lambda, the</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>fact that they saw an arrival
is more likely to have come</text_slice>
            </slice>
            <slice>
              <time_slice>24:16</time_slice>
              <text_slice>from there.</text_slice>
            </slice>
            <slice>
              <time_slice>24:17</time_slice>
              <text_slice>And the formula that captures
this is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>This is the probability that my
arrival has come from this</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>particular stream rather than
that particular stream.</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>So when an arrival comes and you
ask, what is the origin of</text_slice>
            </slice>
            <slice>
              <time_slice>24:38</time_slice>
              <text_slice>that arrival?</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>It's as if I'm flipping a
coin with these odds.</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>And depending on outcome of that
coin, I'm going to tell</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>you came from there or
it came from there.</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>So the origin of an arrival
is either this</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>stream or that stream.</text_slice>
            </slice>
            <slice>
              <time_slice>24:55</time_slice>
              <text_slice>And this is the probability that
the origin of the arrival</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>is that one.</text_slice>
            </slice>
            <slice>
              <time_slice>24:59</time_slice>
              <text_slice>Now if we look at 2 different
arrivals, and we ask about</text_slice>
            </slice>
            <slice>
              <time_slice>25:04</time_slice>
              <text_slice>their origins--</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>So let's think about the origin
of this arrival and</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>compare it with the origin
that arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>The origin of this arrival
is random.</text_slice>
            </slice>
            <slice>
              <time_slice>25:14</time_slice>
              <text_slice>It could be right be either
this or that.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>And this is the relevant
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>25:18</time_slice>
              <text_slice>The origin of that arrival
is random.</text_slice>
            </slice>
            <slice>
              <time_slice>25:20</time_slice>
              <text_slice>It could be either here or is
there, and again, with the</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>same relevant probability.</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>Question.</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>The origin of this arrival, is
it dependent or independent</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>from the origin that arrival?</text_slice>
            </slice>
            <slice>
              <time_slice>25:34</time_slice>
              <text_slice>And here's how the
argument goes.</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>Separate times are
independent.</text_slice>
            </slice>
            <slice>
              <time_slice>25:40</time_slice>
              <text_slice>Whatever has happened in the
process during this set of</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>times is independent from
whatever happened in the</text_slice>
            </slice>
            <slice>
              <time_slice>25:48</time_slice>
              <text_slice>process during that
set of times.</text_slice>
            </slice>
            <slice>
              <time_slice>25:50</time_slice>
              <text_slice>Because different times have
nothing to do with each other,</text_slice>
            </slice>
            <slice>
              <time_slice>25:55</time_slice>
              <text_slice>the origin of this, of an
arrival here, has nothing to</text_slice>
            </slice>
            <slice>
              <time_slice>25:59</time_slice>
              <text_slice>do with the origin of
an arrival there.</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>So the origins of different
arrivals are also independent</text_slice>
            </slice>
            <slice>
              <time_slice>26:06</time_slice>
              <text_slice>random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>So if I tell you that--</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>yeah.</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>So it as if that each time that
you have an arrival in</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>the merge process, it's as if
you're flipping a coin to</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>determine where did that arrival
came from and these</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>coins are independent
of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>Now we're going to use this--</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>what we know about merged
processes to solve the problem</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>that would be harder to do, if
you were not using ideas from</text_slice>
            </slice>
            <slice>
              <time_slice>26:48</time_slice>
              <text_slice>Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>So the formulation of the
problem has nothing to do with</text_slice>
            </slice>
            <slice>
              <time_slice>26:52</time_slice>
              <text_slice>the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>The formulation is
the following.</text_slice>
            </slice>
            <slice>
              <time_slice>26:57</time_slice>
              <text_slice>We have 3 light-bulbs.</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>And each light bulb is
independent and is going to</text_slice>
            </slice>
            <slice>
              <time_slice>27:03</time_slice>
              <text_slice>die out at the time that's
exponentially distributed.</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>So 3 light bulbs.</text_slice>
            </slice>
            <slice>
              <time_slice>27:11</time_slice>
              <text_slice>They start their lives and
then at some point</text_slice>
            </slice>
            <slice>
              <time_slice>27:16</time_slice>
              <text_slice>they die or burn out.</text_slice>
            </slice>
            <slice>
              <time_slice>27:21</time_slice>
              <text_slice>So let's think of this as X,
this as Y, and this as Z.</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>And we're interested in the
time until the last</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>light-bulb burns out.</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>So we're interested in the
maximum of the 3 random</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>variables, X, Y, and Z. And in
particular, we want to find</text_slice>
            </slice>
            <slice>
              <time_slice>27:41</time_slice>
              <text_slice>the expected value
of this maximum.</text_slice>
            </slice>
            <slice>
              <time_slice>27:45</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>27:47</time_slice>
              <text_slice>So you can do derived
distribution, use the expected</text_slice>
            </slice>
            <slice>
              <time_slice>27:50</time_slice>
              <text_slice>value rule, anything you want.</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>You can get this answer using
the tools that you already</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>have in your hands.</text_slice>
            </slice>
            <slice>
              <time_slice>27:58</time_slice>
              <text_slice>But now let us see how we can
connect to this picture with a</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>Poisson picture and come up
with the answer in a very</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>simple way.</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>What is an exponential
random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>28:09</time_slice>
              <text_slice>An exponential random variable
is the first act in the long</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>play that involves a whole
Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>28:19</time_slice>
              <text_slice>So an exponential random
variable is the first act of a</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>Poisson movie.</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>Same thing here.</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>You can think of this random
variable as being part of some</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>Poisson process that
has been running.</text_slice>
            </slice>
            <slice>
              <time_slice>28:35</time_slice>
              <text_slice>So it's part of this
bigger picture.</text_slice>
            </slice>
            <slice>
              <time_slice>28:38</time_slice>
              <text_slice>We're still interested in
the maximum of the 3.</text_slice>
            </slice>
            <slice>
              <time_slice>28:42</time_slice>
              <text_slice>The other arrivals are not going
to affect our answers.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>It's just, conceptually
speaking, we can think of the</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>exponential random variable as
being embedded in a bigger</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>Poisson picture.</text_slice>
            </slice>
            <slice>
              <time_slice>28:55</time_slice>
              <text_slice>So we have 3 Poisson process
that are running in parallel.</text_slice>
            </slice>
            <slice>
              <time_slice>29:00</time_slice>
              <text_slice>Let us split the expected time
until the last burnout into</text_slice>
            </slice>
            <slice>
              <time_slice>29:06</time_slice>
              <text_slice>pieces, which is time until the
first burnout, time from</text_slice>
            </slice>
            <slice>
              <time_slice>29:09</time_slice>
              <text_slice>the first until the second,
and time from the</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>second until the third.</text_slice>
            </slice>
            <slice>
              <time_slice>29:16</time_slice>
              <text_slice>And find the expected values of
each one of these pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>What can we say about the
expected value of this?</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>This is the first arrival
out of all of</text_slice>
            </slice>
            <slice>
              <time_slice>29:29</time_slice>
              <text_slice>these 3 Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>29:31</time_slice>
              <text_slice>It's the first event that
happens when you look at all</text_slice>
            </slice>
            <slice>
              <time_slice>29:34</time_slice>
              <text_slice>of these processes
simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>So 3 Poisson processes
running in parallel.</text_slice>
            </slice>
            <slice>
              <time_slice>29:39</time_slice>
              <text_slice>We're interested in the time
until one of them, any one of</text_slice>
            </slice>
            <slice>
              <time_slice>29:43</time_slice>
              <text_slice>them, gets in arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>Rephrase.</text_slice>
            </slice>
            <slice>
              <time_slice>29:47</time_slice>
              <text_slice>We merged the 3 Poisson
processes, and we ask for the</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>time until we observe an arrival
in the merged process.</text_slice>
            </slice>
            <slice>
              <time_slice>29:56</time_slice>
              <text_slice>When 1 of the 3 gets an arrival
for the first time,</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>the merged process gets
its first arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>30:03</time_slice>
              <text_slice>So what's the expected
value of this time</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>until the first burnout?</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>It's going to be the
expected value of a</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>Poisson random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>So the first burnout is going
to have an expected</text_slice>
            </slice>
            <slice>
              <time_slice>30:17</time_slice>
              <text_slice>value, which is--</text_slice>
            </slice>
            <slice>
              <time_slice>30:20</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>It's a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>The merged process of the 3 has
a collective arrival rate,</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>which is 3 times lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>So this is the parameter over
the exponential distribution</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>that describes the time until
the first arrival in the</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>merged process.</text_slice>
            </slice>
            <slice>
              <time_slice>30:41</time_slice>
              <text_slice>And the expected value
of this random</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>variable is 1 over that.</text_slice>
            </slice>
            <slice>
              <time_slice>30:45</time_slice>
              <text_slice>When you have an exponential
random variable with parameter</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>lambda, the expected value
of that random</text_slice>
            </slice>
            <slice>
              <time_slice>30:50</time_slice>
              <text_slice>variable is 1 over lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>Here we're talking about the
first arrival time in a</text_slice>
            </slice>
            <slice>
              <time_slice>30:56</time_slice>
              <text_slice>process with rate 3 lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>The expected time until
the first arrival</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>is 1 over (3 lambda).</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>Alright.</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>So at this time, this bulb, this
arrival happened, this</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>bulb has been burned.</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>So we don't care about
that bulb anymore.</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>We start at this time,
and we look forward.</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>This bulb has been burned.</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>So let's just look forward
from now on.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>What have we got?</text_slice>
            </slice>
            <slice>
              <time_slice>31:28</time_slice>
              <text_slice>We have two bulbs that
are burning.</text_slice>
            </slice>
            <slice>
              <time_slice>31:34</time_slice>
              <text_slice>We have a Poisson process that's
the bigger picture of</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>what could happen to that light
bulb, if we were to keep</text_slice>
            </slice>
            <slice>
              <time_slice>31:40</time_slice>
              <text_slice>replacing it.</text_slice>
            </slice>
            <slice>
              <time_slice>31:41</time_slice>
              <text_slice>Another Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>These two processes are,
again, independent.</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>From this time until that time,
how long does it take?</text_slice>
            </slice>
            <slice>
              <time_slice>31:50</time_slice>
              <text_slice>It's the time until either
this process records an</text_slice>
            </slice>
            <slice>
              <time_slice>31:53</time_slice>
              <text_slice>arrival or that process
records and arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>31:57</time_slice>
              <text_slice>That's the same as the time
that the merged process of</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>these two records an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>32:03</time_slice>
              <text_slice>So we're talking about the
expected time until the first</text_slice>
            </slice>
            <slice>
              <time_slice>32:06</time_slice>
              <text_slice>arrival in a merged process.</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>The merged process is Poisson.</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>It's Poisson with
rate 2 lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>So that extra time is
going to take--</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>the expected value is going to
be 1 over the (rate of that</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>Poisson process).</text_slice>
            </slice>
            <slice>
              <time_slice>32:22</time_slice>
              <text_slice>So 1 over (2 lambda) is
the expected value</text_slice>
            </slice>
            <slice>
              <time_slice>32:25</time_slice>
              <text_slice>of this random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>So at this point, this bulb
now is also burned.</text_slice>
            </slice>
            <slice>
              <time_slice>32:30</time_slice>
              <text_slice>So we start looking
from this time on.</text_slice>
            </slice>
            <slice>
              <time_slice>32:33</time_slice>
              <text_slice>That part of the picture
disappears.</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>Starting from this time, what's
the expected value</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>until that remaining light-bulb
burns out?</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>Well, as we said before, in
a Poisson process or with</text_slice>
            </slice>
            <slice>
              <time_slice>32:47</time_slice>
              <text_slice>exponential random variables,
we have memorylessness.</text_slice>
            </slice>
            <slice>
              <time_slice>32:50</time_slice>
              <text_slice>A used bulb is as good
as a new one.</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>So it's as if we're starting
from scratch here.</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>So this is going to be an
exponential random variable</text_slice>
            </slice>
            <slice>
              <time_slice>32:58</time_slice>
              <text_slice>with parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>And the expected value of it is
going to be 1 over lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>33:05</time_slice>
              <text_slice>So the beauty of approaching
this problem in this</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>particular way is, of course,
that we manage to do</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>everything without any calculus
at all, without</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>striking an integral, without
trying to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>33:16</time_slice>
              <text_slice>expectations in any form.</text_slice>
            </slice>
            <slice>
              <time_slice>33:19</time_slice>
              <text_slice>Most of the non-trivial problems
that you encounter in</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>the Poisson world basically
involve tricks of these kind.</text_slice>
            </slice>
            <slice>
              <time_slice>33:28</time_slice>
              <text_slice>You have a question and you try
to rephrase it, trying to</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>think in terms of what might
happen in the Poisson setting,</text_slice>
            </slice>
            <slice>
              <time_slice>33:35</time_slice>
              <text_slice>use memorylessness, use merging,
et cetera, et cetera.</text_slice>
            </slice>
            <slice>
              <time_slice>33:43</time_slice>
              <text_slice>Now we talked about merging.</text_slice>
            </slice>
            <slice>
              <time_slice>33:46</time_slice>
              <text_slice>It turns out that the splitting
of Poisson processes</text_slice>
            </slice>
            <slice>
              <time_slice>33:49</time_slice>
              <text_slice>also works in a nice way.</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>The story here is exactly
the same as for</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>So I'm having a Poisson
process.</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>And each time, with some rate
lambda, and each time that an</text_slice>
            </slice>
            <slice>
              <time_slice>34:06</time_slice>
              <text_slice>arrival comes, I'm going to send
it to that stream and the</text_slice>
            </slice>
            <slice>
              <time_slice>34:09</time_slice>
              <text_slice>record an arrival here with some
probability P. And I'm</text_slice>
            </slice>
            <slice>
              <time_slice>34:13</time_slice>
              <text_slice>going to send it to the other
stream with some probability 1</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>minus P. So either of this
will happen or that will</text_slice>
            </slice>
            <slice>
              <time_slice>34:19</time_slice>
              <text_slice>happen, depending on
the outcome of the</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>coin flip that I do.</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>Each time that then arrival
occurs, I flip a coin and I</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>decide whether to record
it here or there.</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>This is called splitting
a Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>process into two pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>What kind of process
do we get here?</text_slice>
            </slice>
            <slice>
              <time_slice>34:37</time_slice>
              <text_slice>If you look at the little
interval for length delta,</text_slice>
            </slice>
            <slice>
              <time_slice>34:40</time_slice>
              <text_slice>what's the probability
that this little</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>interval gets an arrival?</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>It's the probability that this
one gets an arrival, which is</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>lambda delta times the
probability that after I get</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>an arrival my coin flip came out
to be that way, so that it</text_slice>
            </slice>
            <slice>
              <time_slice>34:55</time_slice>
              <text_slice>sends me there.</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>So this means that this little
interval is going to have</text_slice>
            </slice>
            <slice>
              <time_slice>34:58</time_slice>
              <text_slice>probability lambda delta P. Or
maybe more suggestively, I</text_slice>
            </slice>
            <slice>
              <time_slice>35:03</time_slice>
              <text_slice>should write it as lambda
P times delta.</text_slice>
            </slice>
            <slice>
              <time_slice>35:09</time_slice>
              <text_slice>So every little interval has
a probability of an arrival</text_slice>
            </slice>
            <slice>
              <time_slice>35:12</time_slice>
              <text_slice>proportional to delta.</text_slice>
            </slice>
            <slice>
              <time_slice>35:13</time_slice>
              <text_slice>The proportionality factor is
lambda P. So lambda P is the</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>rate of that process.</text_slice>
            </slice>
            <slice>
              <time_slice>35:18</time_slice>
              <text_slice>And then you go through the
mental exercise that you went</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>through for the Bernoulli
process to argue that a</text_slice>
            </slice>
            <slice>
              <time_slice>35:25</time_slice>
              <text_slice>different intervals here are
independent and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>And that completes checking that
this process is going to</text_slice>
            </slice>
            <slice>
              <time_slice>35:31</time_slice>
              <text_slice>be a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>35:33</time_slice>
              <text_slice>So when you split a Poisson
process by doing independent</text_slice>
            </slice>
            <slice>
              <time_slice>35:38</time_slice>
              <text_slice>coin flips each time that
something happens, the</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>processes that you get is again
a Poisson process, but</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>of course with a reduced rate.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>So instead of the word
splitting, sometimes people</text_slice>
            </slice>
            <slice>
              <time_slice>35:50</time_slice>
              <text_slice>also use the words
thinning-out.</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>That is, out of the arrivals
that came, you keep a few but</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>throw away a few.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:02</time_slice>
              <text_slice>So now the last topic over
this lecture is a quite</text_slice>
            </slice>
            <slice>
              <time_slice>36:08</time_slice>
              <text_slice>curious phenomenon that
goes under the</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>name of random incidents.</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>So here's the story.</text_slice>
            </slice>
            <slice>
              <time_slice>36:18</time_slice>
              <text_slice>Buses have been running
on Mass Ave. from time</text_slice>
            </slice>
            <slice>
              <time_slice>36:22</time_slice>
              <text_slice>immemorial.</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>And the bus company that runs
the buses claims that they</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>come as a Poisson process with
some rate, let's say, of 4</text_slice>
            </slice>
            <slice>
              <time_slice>36:33</time_slice>
              <text_slice>buses per hour.</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>So that the expected time
between bus arrivals is going</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>to be 15 minutes.</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>Alright.</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>So people have been complaining
that they have</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>been showing up there.</text_slice>
            </slice>
            <slice>
              <time_slice>36:49</time_slice>
              <text_slice>They think the buses are
taking too long.</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>So you are asked
to investigate.</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>Is the company--</text_slice>
            </slice>
            <slice>
              <time_slice>36:56</time_slice>
              <text_slice>Does it operate according
to its promises or not.</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>So you send an undercover agent
to go and check the</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>interarrival times
of the buses.</text_slice>
            </slice>
            <slice>
              <time_slice>37:07</time_slice>
              <text_slice>Are they 15 minutes?</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>Or are they longer?</text_slice>
            </slice>
            <slice>
              <time_slice>37:11</time_slice>
              <text_slice>So you put your dark glasses
and you show up at the bus</text_slice>
            </slice>
            <slice>
              <time_slice>37:17</time_slice>
              <text_slice>stop at some random time.</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>And you go and ask the guy in
the falafel truck, how long</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>has it been since the
last arrival?</text_slice>
            </slice>
            <slice>
              <time_slice>37:28</time_slice>
              <text_slice>So of course that guy works
for the FBI, right?</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>So they tell you, well, it's
been, let's say, 12 minutes</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>since the last bus arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>And then you say,
"Oh, 12 minutes.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>Average time is 15.</text_slice>
            </slice>
            <slice>
              <time_slice>37:42</time_slice>
              <text_slice>So a bus should be coming
any time now."</text_slice>
            </slice>
            <slice>
              <time_slice>37:47</time_slice>
              <text_slice>Is that correct?</text_slice>
            </slice>
            <slice>
              <time_slice>37:48</time_slice>
              <text_slice>No, you wouldn't
think that way.</text_slice>
            </slice>
            <slice>
              <time_slice>37:49</time_slice>
              <text_slice>It's a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>It doesn't matter how long
it has been since</text_slice>
            </slice>
            <slice>
              <time_slice>37:53</time_slice>
              <text_slice>the last bus arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>So you don't go through
that fallacy.</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>Instead of predicting how long
it's going to be, you just sit</text_slice>
            </slice>
            <slice>
              <time_slice>37:59</time_slice>
              <text_slice>down there and wait and
measure the time.</text_slice>
            </slice>
            <slice>
              <time_slice>38:03</time_slice>
              <text_slice>And you find that this is,
let's say, 11 minutes.</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>And you go to your boss and
report, "Well, it took--</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>I went there and the time from
the previous bus to the next</text_slice>
            </slice>
            <slice>
              <time_slice>38:16</time_slice>
              <text_slice>one was 23 minutes.</text_slice>
            </slice>
            <slice>
              <time_slice>38:18</time_slice>
              <text_slice>It's more than the 15
that they said."</text_slice>
            </slice>
            <slice>
              <time_slice>38:20</time_slice>
              <text_slice>So go and do that again.</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>You go day after day.</text_slice>
            </slice>
            <slice>
              <time_slice>38:23</time_slice>
              <text_slice>You keep these statistics of the
length of this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>38:28</time_slice>
              <text_slice>And you tell your boss it's
a lot more than 15.</text_slice>
            </slice>
            <slice>
              <time_slice>38:32</time_slice>
              <text_slice>It tends to be more
like 30 or so.</text_slice>
            </slice>
            <slice>
              <time_slice>38:36</time_slice>
              <text_slice>So the bus company
is cheating us.</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>Does the bus company really run
Poisson buses at the rate</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>that they have promised?</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>Well let's analyze the situation
here and figure out</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>what the length of
this interval</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>should be, on the average.</text_slice>
            </slice>
            <slice>
              <time_slice>38:57</time_slice>
              <text_slice>The naive argument is that
this interval is an</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>interarrival time.</text_slice>
            </slice>
            <slice>
              <time_slice>39:02</time_slice>
              <text_slice>And interarrival times, on the
average, are 15 minutes, if</text_slice>
            </slice>
            <slice>
              <time_slice>39:06</time_slice>
              <text_slice>the company runs indeed Poisson
processes with these</text_slice>
            </slice>
            <slice>
              <time_slice>39:10</time_slice>
              <text_slice>interarrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>39:11</time_slice>
              <text_slice>But actually the situation is
a little more subtle because</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>this is not a typical
interarrival interval.</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>This interarrival interval
consists of two pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>39:23</time_slice>
              <text_slice>Let's call them T1
and T1 prime.</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>What can you tell me about those
two random variables?</text_slice>
            </slice>
            <slice>
              <time_slice>39:32</time_slice>
              <text_slice>What kind of random
variable is T1?</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>Starting from this time, with
the Poisson process, the past</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>39:41</time_slice>
              <text_slice>It's the time until an
arrival happens.</text_slice>
            </slice>
            <slice>
              <time_slice>39:43</time_slice>
              <text_slice>So T1 is going to be an
exponential random variable</text_slice>
            </slice>
            <slice>
              <time_slice>39:49</time_slice>
              <text_slice>with parameter lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>So in particular, the expected
value of T1 is</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>going to be 15 by itself.</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>How about the random
variable T1 prime.</text_slice>
            </slice>
            <slice>
              <time_slice>40:02</time_slice>
              <text_slice>What kind of random
variable is it?</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>This is like the first arrival
in a Poisson process that runs</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>backwards in time.</text_slice>
            </slice>
            <slice>
              <time_slice>40:17</time_slice>
              <text_slice>What kind of process is a
Poisson process running</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>backwards in time?</text_slice>
            </slice>
            <slice>
              <time_slice>40:21</time_slice>
              <text_slice>Let's think of coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>Suppose you have a movie
of coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>And for some accident, that
fascinating movie, you happen</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>to watch it backwards.</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>Will it look any different
statistically?</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>No.</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>It's going to be just the
sequence of random coin flips.</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>So a Bernoulli process that's
runs in reverse time is</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>statistically identical
to a Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>40:42</time_slice>
              <text_slice>process in forward time.</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>The Poisson process is a
limit of the Bernoulli.</text_slice>
            </slice>
            <slice>
              <time_slice>40:46</time_slice>
              <text_slice>So, same story with the
Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>40:48</time_slice>
              <text_slice>If you run it backwards in
time it looks the same.</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>So looking backwards in time,
this is a Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>And T1 prime is the time until
the first arrival in this</text_slice>
            </slice>
            <slice>
              <time_slice>40:58</time_slice>
              <text_slice>backward process.</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>So T1 prime is also going to
be an exponential random</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>variable with the same
parameter, lambda.</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>And the expected value
of T1 prime is 15.</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>Conclusion is that the expected
length of this</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>interval is going to
be 30 minutes.</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>And the fact that this agent
found the average to be</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>something like 30 does not
contradict the claims of the</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>bus company that they're running
Poisson buses with a</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>rate of lambda equal to 4.</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>So maybe the company can this
way-- they can defend</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>themselves in court.</text_slice>
            </slice>
            <slice>
              <time_slice>41:44</time_slice>
              <text_slice>But there's something
puzzling here.</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>How long is the interarrival
time?</text_slice>
            </slice>
            <slice>
              <time_slice>41:50</time_slice>
              <text_slice>Is it 15?</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>Or is it 30?</text_slice>
            </slice>
            <slice>
              <time_slice>41:53</time_slice>
              <text_slice>On the average.</text_slice>
            </slice>
            <slice>
              <time_slice>41:55</time_slice>
              <text_slice>The issue is what do we
mean by a typical</text_slice>
            </slice>
            <slice>
              <time_slice>41:59</time_slice>
              <text_slice>interarrival time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>When we say typical, we mean
some kind of average.</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>But average over what?</text_slice>
            </slice>
            <slice>
              <time_slice>42:08</time_slice>
              <text_slice>And here's two different ways
of thinking about averages.</text_slice>
            </slice>
            <slice>
              <time_slice>42:13</time_slice>
              <text_slice>You number the buses.</text_slice>
            </slice>
            <slice>
              <time_slice>42:15</time_slice>
              <text_slice>And you have bus number 100.</text_slice>
            </slice>
            <slice>
              <time_slice>42:17</time_slice>
              <text_slice>You have bus number 101,
bus number 102, bus</text_slice>
            </slice>
            <slice>
              <time_slice>42:21</time_slice>
              <text_slice>number 110, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>One way of thinking about
averages is that you pick a</text_slice>
            </slice>
            <slice>
              <time_slice>42:29</time_slice>
              <text_slice>bus number at random.</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>I pick, let's say, that bus,
all buses being sort of</text_slice>
            </slice>
            <slice>
              <time_slice>42:36</time_slice>
              <text_slice>equally likely to be picked.</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>And I measure this interarrival
time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:41</time_slice>
              <text_slice>So for a typical bus.</text_slice>
            </slice>
            <slice>
              <time_slice>42:45</time_slice>
              <text_slice>Then, starting from here until
there, the expected time has</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>to be 1 over lambda, for
the Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>But what we did in
this experiment</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>was something different.</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>We didn't pick a
bus at random.</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>We picked a time at random.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>And if the picture is, let's
say, this way, I'm much more</text_slice>
            </slice>
            <slice>
              <time_slice>43:08</time_slice>
              <text_slice>likely to pick this interval
and therefore this</text_slice>
            </slice>
            <slice>
              <time_slice>43:12</time_slice>
              <text_slice>interarrival time, rather
than that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:16</time_slice>
              <text_slice>Because, this interval
corresponds to very few times.</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>So if I'm picking a time at
random and, in some sense,</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>let's say, uniform, so that all
times are equally likely,</text_slice>
            </slice>
            <slice>
              <time_slice>43:27</time_slice>
              <text_slice>I'm much more likely to fall
inside a big interval rather</text_slice>
            </slice>
            <slice>
              <time_slice>43:31</time_slice>
              <text_slice>than a small interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>So a person who shows up at the
bus stop at a random time.</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>They're selecting an interval in
a biased way, with the bias</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>favor of longer intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>43:44</time_slice>
              <text_slice>And that's why what they observe
is a random variable</text_slice>
            </slice>
            <slice>
              <time_slice>43:47</time_slice>
              <text_slice>that has a larger expected
value then the ordinary</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>expected value.</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>So the subtlety here is to
realize that we're talking</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>between two different kinds
of experiments.</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>Picking a bus number at random
verses picking an interval at</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>random with a bias in favor
of longer intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>Lots of paradoxes that one
can cook up using Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>processes and random processes
in general often have to do</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>with the story of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>The phenomenon that we had in
this particular example also</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>shows up in general, whenever
you have other kinds of</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>arrival processes.</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>So the Poisson process is the
simplest arrival process there</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>is, where the interarrival
times are</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>exponential random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>44:38</time_slice>
              <text_slice>There's a larger class
of models.</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>They're called renewal
processes, in which, again, we</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>have a sequence of successive
arrivals, interarrival times</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>are identically distributed and
independent, but they may</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>come from a general
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>44:52</time_slice>
              <text_slice>So to make the same point of the
previous example but in a</text_slice>
            </slice>
            <slice>
              <time_slice>44:55</time_slice>
              <text_slice>much simpler setting, suppose
that bus interarrival times</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>are either 5 or 10
minutes apart.</text_slice>
            </slice>
            <slice>
              <time_slice>45:02</time_slice>
              <text_slice>So you get some intervals
that are of length 5.</text_slice>
            </slice>
            <slice>
              <time_slice>45:05</time_slice>
              <text_slice>You get some that are
of length 10.</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>And suppose that these
are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>45:12</time_slice>
              <text_slice>So we have -- not exactly --</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>In the long run, we have as many
5 minute intervals as we</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>have 10 minute intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>So the average interarrival
time is 7 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>45:30</time_slice>
              <text_slice>But if a person shows up at a
random time, what are they</text_slice>
            </slice>
            <slice>
              <time_slice>45:35</time_slice>
              <text_slice>going to see?</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>Do we have as many 5s as 10s?</text_slice>
            </slice>
            <slice>
              <time_slice>45:43</time_slice>
              <text_slice>But every 10 covers twice
as much space.</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>So if I show up at a random
time, I have probability 2/3</text_slice>
            </slice>
            <slice>
              <time_slice>45:52</time_slice>
              <text_slice>falling inside an interval
of duration 10.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>And I have one 1/3 probability
of falling inside an interval</text_slice>
            </slice>
            <slice>
              <time_slice>46:00</time_slice>
              <text_slice>of duration 5.</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>That's because, out of the whole
real line, 2/3 of it is</text_slice>
            </slice>
            <slice>
              <time_slice>46:06</time_slice>
              <text_slice>covered by intervals
of length 10, just</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>because they're longer.</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>1/3 is covered by the
smaller intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>Now if I fall inside an interval
of length 10 and I</text_slice>
            </slice>
            <slice>
              <time_slice>46:19</time_slice>
              <text_slice>measure the length of the
interval that I fell into,</text_slice>
            </slice>
            <slice>
              <time_slice>46:23</time_slice>
              <text_slice>that's going to be 10.</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>But if I fall inside an interval
of length 5 and I</text_slice>
            </slice>
            <slice>
              <time_slice>46:27</time_slice>
              <text_slice>measure how long it is,
I'm going to get a 5.</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>And that, of course, is going
to be different than 7.5.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>46:38</time_slice>
              <text_slice>And which number should
be bigger?</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>It's the second number that's
bigger because this one is</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>biased in favor of the
longer intervals.</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>So that's, again, another
illustration of the different</text_slice>
            </slice>
            <slice>
              <time_slice>46:51</time_slice>
              <text_slice>results that you get when you
have this random incidence</text_slice>
            </slice>
            <slice>
              <time_slice>46:54</time_slice>
              <text_slice>phenomenon.</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>So the bottom line, again, is
that if you talk about a</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>typical interarrival time, one
must be very precise in</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>specifying what we
mean typical.</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>So typical means
sort of random.</text_slice>
            </slice>
            <slice>
              <time_slice>47:08</time_slice>
              <text_slice>But to use the word random,
you must specify very</text_slice>
            </slice>
            <slice>
              <time_slice>47:11</time_slice>
              <text_slice>precisely what is the random
experiment that you are using.</text_slice>
            </slice>
            <slice>
              <time_slice>47:15</time_slice>
              <text_slice>And if you're not careful, you
can get into apparent puzzles,</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>such as the following.</text_slice>
            </slice>
            <slice>
              <time_slice>47:20</time_slice>
              <text_slice>Suppose somebody tells you the
average family size is 4, but</text_slice>
            </slice>
            <slice>
              <time_slice>47:25</time_slice>
              <text_slice>the average person lives
in a family of size 6.</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>Is that compatible?</text_slice>
            </slice>
            <slice>
              <time_slice>47:33</time_slice>
              <text_slice>Family size is 4 on the average,
but typical people</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>live, on the average, in
families of size 6.</text_slice>
            </slice>
            <slice>
              <time_slice>47:40</time_slice>
              <text_slice>Well yes.</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>There's no contradiction here.</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>We're talking about two
different experiments.</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>In one experiment, I pick a
family at random, and I tell</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>you the average family is 4.</text_slice>
            </slice>
            <slice>
              <time_slice>47:51</time_slice>
              <text_slice>In another experiment, I pick a
person at random and I tell</text_slice>
            </slice>
            <slice>
              <time_slice>47:55</time_slice>
              <text_slice>you that this person, on the
average, will be in their</text_slice>
            </slice>
            <slice>
              <time_slice>47:58</time_slice>
              <text_slice>family of size 6.</text_slice>
            </slice>
            <slice>
              <time_slice>48:00</time_slice>
              <text_slice>And what is the catch here?</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>That if I pick a person at
random, large families are</text_slice>
            </slice>
            <slice>
              <time_slice>48:05</time_slice>
              <text_slice>more likely to be picked.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>So there's a bias in favor
of large families.</text_slice>
            </slice>
            <slice>
              <time_slice>48:11</time_slice>
              <text_slice>Or if you want to survey, let's
say, are trains crowded</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>in your city?</text_slice>
            </slice>
            <slice>
              <time_slice>48:16</time_slice>
              <text_slice>Or are buses crowded?</text_slice>
            </slice>
            <slice>
              <time_slice>48:19</time_slice>
              <text_slice>One choice is to pick a bus
at random and inspect</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>how crowded it is.</text_slice>
            </slice>
            <slice>
              <time_slice>48:23</time_slice>
              <text_slice>Another choice is to pick a
typical person and ask them,</text_slice>
            </slice>
            <slice>
              <time_slice>48:27</time_slice>
              <text_slice>"Did you ride the bus today?</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>Was it's crowded?" Well suppose
that in this city</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>there's one bus that's extremely
crowded and all the</text_slice>
            </slice>
            <slice>
              <time_slice>48:36</time_slice>
              <text_slice>other buses are completely
empty.</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>If you ask a person. "Was your
bus crowded?" They will tell</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>you, "Yes, my bus was crowded."
There's no witness</text_slice>
            </slice>
            <slice>
              <time_slice>48:46</time_slice>
              <text_slice>from the empty buses to testify
in their favor.</text_slice>
            </slice>
            <slice>
              <time_slice>48:49</time_slice>
              <text_slice>So by sampling people instead
of sampling buses, you're</text_slice>
            </slice>
            <slice>
              <time_slice>48:52</time_slice>
              <text_slice>going to get different result.</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>And in the process industry, if
your job is to inspect and</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>check cookies, you will be
faced with a big dilemma.</text_slice>
            </slice>
            <slice>
              <time_slice>49:01</time_slice>
              <text_slice>Do you want to find out how many
chocolate chips there are</text_slice>
            </slice>
            <slice>
              <time_slice>49:05</time_slice>
              <text_slice>on a typical cookie?</text_slice>
            </slice>
            <slice>
              <time_slice>49:06</time_slice>
              <text_slice>Are you going to interview
cookies or are you going to</text_slice>
            </slice>
            <slice>
              <time_slice>49:09</time_slice>
              <text_slice>interview chocolate chips and
ask them how many other chips</text_slice>
            </slice>
            <slice>
              <time_slice>49:13</time_slice>
              <text_slice>where there on your cookie?</text_slice>
            </slice>
            <slice>
              <time_slice>49:16</time_slice>
              <text_slice>And you're going to
get different</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>answers in these cases.</text_slice>
            </slice>
            <slice>
              <time_slice>49:19</time_slice>
              <text_slice>So moral is, one has to be
very precise on how you</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>formulate the sampling procedure
that you have.</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>And you'll get different
answers.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Probability Models and Axioms (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>Sample space: Discrete example Sample space: Continuous example 
 Two rolls of a tetrahedral die  = {(x, y ) | 0  x, y  1} 
 Sample space vs. sequential description 
y 
1,1
1 1,2 
1,31 
1,4 
4 2 
Y3  = Second 
roll 32  1 x 
1 
1 2 3 4 4 
X = First roll 4,4 
Probability axioms Probability law: Example with nite sample space 
 Event: a subset of the sample space 4 
 Probability is assigned to events 
Y3 = Second 
roll 
2 Axioms: 
11. Nonnegativity: P(A)  0 
1 2 3 42. Normalization: P()=1 
X = First roll 3. Additivity: If A B = , then P(A B)= P(A)+ P(B) 
 Let every possible outcome have probability 1/16 
 P((X, Y )   is (1,1) or (1,2)) = P({s1,s2,...,s k})=P({s1})+ + P({sk}) 
= P(X=1)=P(s)+ { 1  + P(sk) }
 P(X + Y is odd) =  Axiom 3 needs strengthening 
 Do weird sets have probabilities?  P(min(X, Y ) = 2) = 
2</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Discrete uniform law Continuous uniform law 
 Let all outcomes be equally likely  Two random numbers in [0,1]. 
y 
 Then, 1 
number of elements of A P(A)= total number of sample points 
 x 
 Computing probabilities 1 counting 
 Uniform law: Probability = Area  Denes fair coins, fair dice, well-shued decks 
 P(X + Y  1/2) = ? 
 P((X, Y)=( 0 .5,0.3) ) 
Probability law: Ex. w/countably innite sample space 
 Sample space: {1,2,... }
    n We are given P(n)=2 , n =1,2,... 
 Find P(outcome is even) Remember! 
p 
1/2Turn   in recitation/tutorial scheduling form now 
1/4      
1/8 1/16 ..       
1 2 3 4 Tutorials start next week
1 1 11     
  P({2,4,6,... })= P (2) + P(4) +  = + + +  = 
22 24 26 3 
 Countable additivity axiom (needed for this calculation): 
If A1,A2,... are disjoint events, then: 
P(A1  A2  )= P(A1)+ P(A2)+  
3</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.041 Probabilistic Systems Analysis	 Coursework 
6.431 Applied Probability 
	 Quiz 1 (October 12, 12:05-12:55pm) 17% 
	 Sta: 	 Quiz 2 (November 2, 7:30-9:30pm) 30% 
	 Lecturer: John Tsitsikli  	 Final exam (scheduled by registrar) 40% 
	 Weekly homework (best 9 of 10) 10%
	 Attendance/participation/enthusiasm in 3%
recitations/tutorials Pick up and
	         	        read course information handout
	 Turn in recitation and tutorial scheduling form
  Collab
oration policy  de scribed in course info handout (last sheet of  course information handout) 
	 Text: Intro duc tion to Probability, 2nd Edition, 	 Pick up  copy of slides 

D. P.
 Bertsekas and J. N. Tsitsiklis, Athena Scientic, 2008 	  
Read the text! 
LECTU
RE 1	 Sample
 space  
	 Readings: Sections 1.1, 1.2  List (set) of possible outcomes 
 List must be: 
Lecture outline  Mutually exclusive 
	 Collectively exhaustive 	 Probability as a mathematical framework 
for reasoning about uncertainty  Art: to be at the right granularity 
	 Probabilistic models 
	 sample space 
	 probability law 
	 Axioms of probability 
	 Simple examples 
1s</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-16-markov-chains-i/</video_url>
          <video_title>Lecture 16: Markov Chains I</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: So we're going to
start now with a new chapter.</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>We're going to talk about
Markov processes.</text_slice>
            </slice>
            <slice>
              <time_slice>0:28</time_slice>
              <text_slice>The good news is that this is
a subject that is a lot more</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>intuitive and simple in many
ways than, let's say, the</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>Poisson processes.</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>So hopefully this will
be enjoyable.</text_slice>
            </slice>
            <slice>
              <time_slice>0:40</time_slice>
              <text_slice>So Markov processes
is, a general</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>class of random processes.</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>In some sense, it's more
elaborate than the Bernoulli</text_slice>
            </slice>
            <slice>
              <time_slice>0:49</time_slice>
              <text_slice>and Poisson processes, because
now we're going to have</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>dependencies between difference
times, instead of</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>having memoryless processes.</text_slice>
            </slice>
            <slice>
              <time_slice>0:57</time_slice>
              <text_slice>So the basic idea is
the following.</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>In physics, for example, you
write down equations for how a</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>system evolves that has
the general form.</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>The new state of a system one
second later is some function</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>of old state.</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>So Newton's equations and all
that in physics allow you to</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>write equations of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>And so if that a particle is
moving at a certain velocity</text_slice>
            </slice>
            <slice>
              <time_slice>1:26</time_slice>
              <text_slice>and it's at some location, you
can predict when it's going to</text_slice>
            </slice>
            <slice>
              <time_slice>1:28</time_slice>
              <text_slice>be a little later.</text_slice>
            </slice>
            <slice>
              <time_slice>1:30</time_slice>
              <text_slice>Markov processes have the same
flavor, except that there's</text_slice>
            </slice>
            <slice>
              <time_slice>1:34</time_slice>
              <text_slice>also some randomness thrown
inside the equation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>So that's what Markov process
essentially is.</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>It describes the evolution of
the system, or some variables,</text_slice>
            </slice>
            <slice>
              <time_slice>1:47</time_slice>
              <text_slice>but in the presence of some
noise so that the motion</text_slice>
            </slice>
            <slice>
              <time_slice>1:51</time_slice>
              <text_slice>itself is a bit random.</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>So this is a pretty
general framework.</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>So pretty much any useful or
interesting random process</text_slice>
            </slice>
            <slice>
              <time_slice>2:02</time_slice>
              <text_slice>that you can think about, you
can always described it as a</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>Markov process if you
define properly the</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>notion of the state.</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>So what we're going to do is
we're going to introduce the</text_slice>
            </slice>
            <slice>
              <time_slice>2:13</time_slice>
              <text_slice>class of Markov processes by,
example, by talking about the</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>checkout counter in
a supermarket.</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>Then we're going to abstract
from our example so that we</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>get a more general definition.</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>And then we're going to do a
few things, such as how to</text_slice>
            </slice>
            <slice>
              <time_slice>2:28</time_slice>
              <text_slice>predict what's going to happen
n time steps later, if we</text_slice>
            </slice>
            <slice>
              <time_slice>2:32</time_slice>
              <text_slice>start at the particular state.</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>And then talk a little bit
about some structural</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>properties of Markov processes
or Markov chains.</text_slice>
            </slice>
            <slice>
              <time_slice>2:40</time_slice>
              <text_slice>So here's our example.</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>You go to the checkout counter
at the supermarket, and you</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>stand there and watch the
customers who come.</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>So customers come, they get in
queue, and customers get</text_slice>
            </slice>
            <slice>
              <time_slice>2:59</time_slice>
              <text_slice>served one at a time.</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>So the discussion is going to
be in terms of supermarket</text_slice>
            </slice>
            <slice>
              <time_slice>3:03</time_slice>
              <text_slice>checkout counters, but the
same story applies to any</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>service system.</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>You may have a server, jobs
arrive to that server, they</text_slice>
            </slice>
            <slice>
              <time_slice>3:12</time_slice>
              <text_slice>get put into the queue, and
the server processes those</text_slice>
            </slice>
            <slice>
              <time_slice>3:15</time_slice>
              <text_slice>jobs one at a time.</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>Now to make a probabilistic
model, we need to make some</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>assumption about the customer
arrivals and the customer</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>departures.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>And we want to keep things
as simple as</text_slice>
            </slice>
            <slice>
              <time_slice>3:26</time_slice>
              <text_slice>possible to get started.</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>So let's assume that customers
arrive according to a</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>Bernoulli process with
some parameter b.</text_slice>
            </slice>
            <slice>
              <time_slice>3:34</time_slice>
              <text_slice>So essentially, that's the same
as the assumption that</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>the time between consecutive
customer arrivals is a</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>geometric random variable
with parameter b.</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>Another way of thinking about
the arrival process--</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>that's not how it happens, but
it's helpful, mathematically,</text_slice>
            </slice>
            <slice>
              <time_slice>3:54</time_slice>
              <text_slice>is to think of someone who's
flipping a coin with bias</text_slice>
            </slice>
            <slice>
              <time_slice>3:58</time_slice>
              <text_slice>equal to b.</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>And whenever the coin
lands heads,</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>then a customer arrives.</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>So it's as if there's a coin
flip being done by nature that</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>decides the arrivals
of the customers.</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>So we know that coin flipping
to determine the customer</text_slice>
            </slice>
            <slice>
              <time_slice>4:16</time_slice>
              <text_slice>arrivals is the same as having
geometric inter-arrival times.</text_slice>
            </slice>
            <slice>
              <time_slice>4:19</time_slice>
              <text_slice>We know that from our study
of the Bernoulli process.</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>And now how about the customer
service times.</text_slice>
            </slice>
            <slice>
              <time_slice>4:27</time_slice>
              <text_slice>We're going to assume that--</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>4:30</time_slice>
              <text_slice>If there is no customer in
queue, no one being served,</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>then of course, no
one is going to</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>depart from the queue.</text_slice>
            </slice>
            <slice>
              <time_slice>4:38</time_slice>
              <text_slice>But if there a customer in
queue, then that customer</text_slice>
            </slice>
            <slice>
              <time_slice>4:42</time_slice>
              <text_slice>starts being served, and is
going to be served for a</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>random amount of time.</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>And we make the assumption that
the time it takes for the</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>clerk to serve the customer has
a geometric distribution</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>with some known parameter q.</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>So the time it takes to serve a
customer is random, because</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>it's random how many items they
got in their cart, and</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>how many coupons they have
to unload and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>So it's random.</text_slice>
            </slice>
            <slice>
              <time_slice>5:09</time_slice>
              <text_slice>In the real world, it has some
probability distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>Let's not care exactly about
what it would be in the real</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>world, but as a modeling
approximation or just to get</text_slice>
            </slice>
            <slice>
              <time_slice>5:18</time_slice>
              <text_slice>started, let's pretend that
customer service time are well</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>described by a geometric
distribution,</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>with a parameter q.</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>An equivalent way of thinking
about the customer service,</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>mathematically, would
be, again, in</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>terms of coin flipping.</text_slice>
            </slice>
            <slice>
              <time_slice>5:34</time_slice>
              <text_slice>That is, the clerk has a coin
with a bias, and at each time</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>slot the clerk flips the coin.</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>With probability q,
service is over.</text_slice>
            </slice>
            <slice>
              <time_slice>5:43</time_slice>
              <text_slice>With probability 1-q, you
continue the service process.</text_slice>
            </slice>
            <slice>
              <time_slice>5:49</time_slice>
              <text_slice>An assumption that we're going
to make is that the coin flips</text_slice>
            </slice>
            <slice>
              <time_slice>5:52</time_slice>
              <text_slice>that happen here to determine
the arrivals, they're all</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>The coin flips that determine
the end of service are also</text_slice>
            </slice>
            <slice>
              <time_slice>6:02</time_slice>
              <text_slice>independent from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>But also the coin flips involved
here are independent</text_slice>
            </slice>
            <slice>
              <time_slice>6:07</time_slice>
              <text_slice>from the coin flips that
happened there.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>So how arrivals happen is
independent with what happens</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>at the service process.</text_slice>
            </slice>
            <slice>
              <time_slice>6:17</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>So suppose now you
want to answer a</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>question such as the following.</text_slice>
            </slice>
            <slice>
              <time_slice>6:23</time_slice>
              <text_slice>The time is 7:00 PM.</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>What's the probability that the
customer will be departing</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>at this particular time?</text_slice>
            </slice>
            <slice>
              <time_slice>6:30</time_slice>
              <text_slice>Well, you say, it depends.</text_slice>
            </slice>
            <slice>
              <time_slice>6:33</time_slice>
              <text_slice>If the queue is empty at that
time, then you're certain that</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>you're not going to have
a customer departure.</text_slice>
            </slice>
            <slice>
              <time_slice>6:40</time_slice>
              <text_slice>But if the queue is not empty,
then there is probability q</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>that a departure will
happen at that time.</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>So the answer to a question like
this has something to do</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>with the state of the
system at that time.</text_slice>
            </slice>
            <slice>
              <time_slice>6:54</time_slice>
              <text_slice>It depends what the queue is.</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>And if I ask you, will the
queue be empty at 7:10?</text_slice>
            </slice>
            <slice>
              <time_slice>7:02</time_slice>
              <text_slice>Well, the answer to that
question depends on whether at</text_slice>
            </slice>
            <slice>
              <time_slice>7:06</time_slice>
              <text_slice>7 o'clock whether the queue
was huge or not.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>So knowing something about the
state of the queue right now</text_slice>
            </slice>
            <slice>
              <time_slice>7:14</time_slice>
              <text_slice>gives me relevant information
about what may</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>happen in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>7:19</time_slice>
              <text_slice>So what is the state
of the system?</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>Therefore we're brought to
start using this term.</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>So the state basically
corresponds to</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>anything that's relevant.</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>Anything that's happening
right now that's kind of</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>relevant to what may happen
in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>Knowing the size of the queue
right now, is useful</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>information for me to make
predictions about what may</text_slice>
            </slice>
            <slice>
              <time_slice>7:45</time_slice>
              <text_slice>happen 2 minutes
later from now.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>So in this particular example,
a reasonable choice for the</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>state is to just count
how many customers</text_slice>
            </slice>
            <slice>
              <time_slice>7:56</time_slice>
              <text_slice>we have in the queue.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>And let's assume that our
supermarket building is not</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>too big, so it can only
hold 10 people.</text_slice>
            </slice>
            <slice>
              <time_slice>8:05</time_slice>
              <text_slice>So we're going to limit
the states.</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>Instead of going from 0 to
infinity, we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>truncate our model at ten.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>So we have 11 possible states,
corresponding to 0 customers</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>in queue, 1 customer in queue,
2 customers, and so on, all</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>the way up to 10.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>So these are the different
possible states of the system,</text_slice>
            </slice>
            <slice>
              <time_slice>8:27</time_slice>
              <text_slice>assuming that the store cannot
handle more than 10 customers.</text_slice>
            </slice>
            <slice>
              <time_slice>8:33</time_slice>
              <text_slice>So this is the first step, to
write down the set of possible</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>states for our system.</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>Then the next thing to do is
to start describing the</text_slice>
            </slice>
            <slice>
              <time_slice>8:41</time_slice>
              <text_slice>possible transitions
between the states.</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>At any given time step,
what are the</text_slice>
            </slice>
            <slice>
              <time_slice>8:48</time_slice>
              <text_slice>things that can happen?</text_slice>
            </slice>
            <slice>
              <time_slice>8:50</time_slice>
              <text_slice>We can have a customer
arrival, which</text_slice>
            </slice>
            <slice>
              <time_slice>8:53</time_slice>
              <text_slice>moves the state 1 higher.</text_slice>
            </slice>
            <slice>
              <time_slice>8:55</time_slice>
              <text_slice>We can have a customer
departure, which moves the</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>state 1 lower.</text_slice>
            </slice>
            <slice>
              <time_slice>9:00</time_slice>
              <text_slice>There's a possibility that
nothing happens, in which case</text_slice>
            </slice>
            <slice>
              <time_slice>9:03</time_slice>
              <text_slice>the state stays the same.</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>And there's also the possibility
of having</text_slice>
            </slice>
            <slice>
              <time_slice>9:06</time_slice>
              <text_slice>simultaneously an arrival and a
departure, in which case the</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>state again stays the same.</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>So let's write some
representative probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>If we have 2 customers, the
probability that during this</text_slice>
            </slice>
            <slice>
              <time_slice>9:19</time_slice>
              <text_slice>step we go down, this is the
probability that we have a</text_slice>
            </slice>
            <slice>
              <time_slice>9:22</time_slice>
              <text_slice>service completion, but to
no customer arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>9:26</time_slice>
              <text_slice>So this is the probability
associated with this</text_slice>
            </slice>
            <slice>
              <time_slice>9:30</time_slice>
              <text_slice>transition.</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>The other possibility is that
there's a customer arrival,</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>which happens with probability
p, and we do not have a</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>customer departure, and so the
probability of that particular</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>transition is this number.</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>And then finally, the
probability that we stay in</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>the same state, this can happen
in 2 possible ways.</text_slice>
            </slice>
            <slice>
              <time_slice>9:55</time_slice>
              <text_slice>One way is that we have an
arrival and a departure</text_slice>
            </slice>
            <slice>
              <time_slice>10:00</time_slice>
              <text_slice>simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>And the other possibility is
that we have no arrival and no</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>departure, so that the
state stays the same.</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>So these transition
probabilities would be the</text_slice>
            </slice>
            <slice>
              <time_slice>10:11</time_slice>
              <text_slice>same starting from any other
states, state 3, or</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>state 9, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>Transition probabilities become
a little different at</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>the borders, at the boundaries
of this diagram, because if</text_slice>
            </slice>
            <slice>
              <time_slice>10:23</time_slice>
              <text_slice>you're in a state 0, then you
cannot have any customer</text_slice>
            </slice>
            <slice>
              <time_slice>10:27</time_slice>
              <text_slice>departures.</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>There's no one to be served, but
there is a probability p</text_slice>
            </slice>
            <slice>
              <time_slice>10:31</time_slice>
              <text_slice>that the customer arrives, in
which case the number of</text_slice>
            </slice>
            <slice>
              <time_slice>10:36</time_slice>
              <text_slice>customers in the system
goes to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>10:38</time_slice>
              <text_slice>Then probability 1-p,
nothing happens.</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>Similarly with departures, if
the system is full, there's no</text_slice>
            </slice>
            <slice>
              <time_slice>10:46</time_slice>
              <text_slice>room for another arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>But we may have a departure that
happens with probability</text_slice>
            </slice>
            <slice>
              <time_slice>10:50</time_slice>
              <text_slice>q, and nothing happens
with probability 1-q.</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>So this is the full transition
diagram annotated with</text_slice>
            </slice>
            <slice>
              <time_slice>11:00</time_slice>
              <text_slice>transition probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>11:02</time_slice>
              <text_slice>And this is a complete
description of a discrete</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>time, finite state
Markov chain.</text_slice>
            </slice>
            <slice>
              <time_slice>11:10</time_slice>
              <text_slice>So this is a complete
probabilistic model.</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>Once you have all of these
pieces of information, you can</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>start calculating things, and
trying to predict what's going</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>to happen in the future.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>Now let us abstract from this
example and come up with a</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>more general definition.</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>So we have this concept of the
state which describes the</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>current situation in the system
that we're looking at.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>The current state is random, so
we're going to think of it</text_slice>
            </slice>
            <slice>
              <time_slice>11:44</time_slice>
              <text_slice>as a random variable Xn is the
state, and transitions after</text_slice>
            </slice>
            <slice>
              <time_slice>11:50</time_slice>
              <text_slice>the system started operating.</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>So the system starts operating
at some initial state X0, and</text_slice>
            </slice>
            <slice>
              <time_slice>11:56</time_slice>
              <text_slice>after n transitions, it
moves to state Xn.</text_slice>
            </slice>
            <slice>
              <time_slice>12:00</time_slice>
              <text_slice>Now we have a set of
possible states.</text_slice>
            </slice>
            <slice>
              <time_slice>12:03</time_slice>
              <text_slice>State 1 state 2, state
3, and in general,</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>state i and state j.</text_slice>
            </slice>
            <slice>
              <time_slice>12:10</time_slice>
              <text_slice>To keep things simple, we
assume that the set of</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>possible states is
a finite set.</text_slice>
            </slice>
            <slice>
              <time_slice>12:16</time_slice>
              <text_slice>As you can imagine, we can
have systems in which the</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>state space is going
to be infinite.</text_slice>
            </slice>
            <slice>
              <time_slice>12:21</time_slice>
              <text_slice>It could be discrete,
or continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>But all that is more difficult
and more complicated.</text_slice>
            </slice>
            <slice>
              <time_slice>12:25</time_slice>
              <text_slice>It makes sense to start from the
simplest possible setting</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>where we just deal with the
finite state space.</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>And time is discrete, so we can
think of this state in the</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>beginning, after 1 transition,
2 transitions, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:42</time_slice>
              <text_slice>So we're in discrete time and we
have finite in many states.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>So the system starts somewhere,
and at every time</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>step, the state is,
let's say, here.</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>A whistle blows, and the state
jumps to a random next state.</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>So it may move here, or it may
move there, or it may move</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>here, or it might stay
in the place.</text_slice>
            </slice>
            <slice>
              <time_slice>13:08</time_slice>
              <text_slice>So one possible transition is
the transition before you</text_slice>
            </slice>
            <slice>
              <time_slice>13:11</time_slice>
              <text_slice>jump, and just land
in the same place</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>where you started from.</text_slice>
            </slice>
            <slice>
              <time_slice>13:15</time_slice>
              <text_slice>Now we want to describe the
statistics of these</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>13:20</time_slice>
              <text_slice>If I am at that state, how
likely is it to that, next</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>time, I'm going to find
myself at that state?</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>Well, we describe the statistics
of this transition</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>by writing down a transition
probability, the transition</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>probability of going from
state 3 to state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>13:41</time_slice>
              <text_slice>So this transition probability
is to be thought of as a</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>conditional probability.</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>Given that right now I am
at state i what is the</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>probability that next time
I find myself at state j?</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>So given that right now I am
at state 3, P31 is the</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>probability that the next
time I'm going to find</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>myself at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>Similarly here, we would have
a probability P3i, which is</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>the probability that given that
right now I'm at state 3,</text_slice>
            </slice>
            <slice>
              <time_slice>14:12</time_slice>
              <text_slice>next time I'm going to find
myself at state i.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>Now one can write such
conditional probabilities down</text_slice>
            </slice>
            <slice>
              <time_slice>14:21</time_slice>
              <text_slice>in principle, but we
need to make--</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>so you might think of this as a
definition here, but we need</text_slice>
            </slice>
            <slice>
              <time_slice>14:29</time_slice>
              <text_slice>to make one additional big
assumption, and this is the</text_slice>
            </slice>
            <slice>
              <time_slice>14:34</time_slice>
              <text_slice>assumption that to
make a process</text_slice>
            </slice>
            <slice>
              <time_slice>14:36</time_slice>
              <text_slice>to be a Markov process.</text_slice>
            </slice>
            <slice>
              <time_slice>14:38</time_slice>
              <text_slice>This is the so-called
Markov property, and</text_slice>
            </slice>
            <slice>
              <time_slice>14:41</time_slice>
              <text_slice>here's what it says.</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>Let me describe it first
in words here.</text_slice>
            </slice>
            <slice>
              <time_slice>14:46</time_slice>
              <text_slice>Every time that I find myself
at state 3, the probability</text_slice>
            </slice>
            <slice>
              <time_slice>14:52</time_slice>
              <text_slice>that next time I'm going to find
myself at state 1 is this</text_slice>
            </slice>
            <slice>
              <time_slice>14:56</time_slice>
              <text_slice>particular number, no matter
how I got there.</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>That is, this transition
probability is not affected by</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>the past of the process.</text_slice>
            </slice>
            <slice>
              <time_slice>15:06</time_slice>
              <text_slice>It doesn't care about what
path I used to find</text_slice>
            </slice>
            <slice>
              <time_slice>15:11</time_slice>
              <text_slice>myself at state 3.</text_slice>
            </slice>
            <slice>
              <time_slice>15:14</time_slice>
              <text_slice>Mathematically, it means
the following.</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>You have this transition
probability that from state i</text_slice>
            </slice>
            <slice>
              <time_slice>15:19</time_slice>
              <text_slice>jump to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>Suppose that I gave you some
additional information, that I</text_slice>
            </slice>
            <slice>
              <time_slice>15:24</time_slice>
              <text_slice>told you everything else that
happened in the past of the</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>process, everything that
happened, how did you</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>get to state i?</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>The assumption we're making is
that this information about</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>the past has no bearing in
making predictions about the</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>future, as long as you know
where you are right now.</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>So if I tell you, right now, you
are at state i, and by the</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>way, you got there by following
a particular path,</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>you can ignore the extra
information of the particular</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>path that you followed.</text_slice>
            </slice>
            <slice>
              <time_slice>15:58</time_slice>
              <text_slice>You only take into account
where you are right now.</text_slice>
            </slice>
            <slice>
              <time_slice>16:01</time_slice>
              <text_slice>So every time you find yourself
at that state, no</text_slice>
            </slice>
            <slice>
              <time_slice>16:05</time_slice>
              <text_slice>matter how you got there, you
will find yourself next time</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>at state 1 with probability
P31.</text_slice>
            </slice>
            <slice>
              <time_slice>16:12</time_slice>
              <text_slice>So the past has no bearing into
the future, as long as</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>you know where you are
sitting right now.</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>For this property to happen, you
need to choose your state</text_slice>
            </slice>
            <slice>
              <time_slice>16:27</time_slice>
              <text_slice>carefully in the right way.</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>In that sense, the states
needs to include any</text_slice>
            </slice>
            <slice>
              <time_slice>16:32</time_slice>
              <text_slice>information that's relevant
about the</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>future of the system.</text_slice>
            </slice>
            <slice>
              <time_slice>16:38</time_slice>
              <text_slice>Anything that's not in the state
is not going to play a</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>role, but the state needs to
have all the information</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>that's relevant in determining
what kind of transitions are</text_slice>
            </slice>
            <slice>
              <time_slice>16:48</time_slice>
              <text_slice>going to happen next.</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>So to take an example, before
you go to Markov process, just</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>from the deterministic world,
if you have a ball that's</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>flying up in the air, and you
want to make predictions about</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>the future.</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>If I tell you that the state of
the ball is the position of</text_slice>
            </slice>
            <slice>
              <time_slice>17:06</time_slice>
              <text_slice>the ball at the particular time,
is that enough for you</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>to make predictions where the
ball is going to go next?</text_slice>
            </slice>
            <slice>
              <time_slice>17:15</time_slice>
              <text_slice>No.</text_slice>
            </slice>
            <slice>
              <time_slice>17:15</time_slice>
              <text_slice>You need to know both the
position and the velocity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:19</time_slice>
              <text_slice>If you know position and
velocity, you can make</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>predictions about the future.</text_slice>
            </slice>
            <slice>
              <time_slice>17:23</time_slice>
              <text_slice>So the state of a ball that's
flying is position together</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>with velocity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>If you were to just take
position, that would not be</text_slice>
            </slice>
            <slice>
              <time_slice>17:32</time_slice>
              <text_slice>enough information, because if
I tell you current position,</text_slice>
            </slice>
            <slice>
              <time_slice>17:36</time_slice>
              <text_slice>and then I tell you past
position, you could use the</text_slice>
            </slice>
            <slice>
              <time_slice>17:39</time_slice>
              <text_slice>information from the past
position to complete the</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>trajectory and to make
the prediction.</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>So information from the past
is useful if you don't know</text_slice>
            </slice>
            <slice>
              <time_slice>17:47</time_slice>
              <text_slice>the velocity.</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>But if both position and
velocity, you don't care how</text_slice>
            </slice>
            <slice>
              <time_slice>17:53</time_slice>
              <text_slice>you got there, or what
time you started.</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>From position and velocity, you
can make predictions about</text_slice>
            </slice>
            <slice>
              <time_slice>17:58</time_slice>
              <text_slice>the future.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>So there's a certain art, or a
certain element of thinking, a</text_slice>
            </slice>
            <slice>
              <time_slice>18:04</time_slice>
              <text_slice>non-mechanical aspect into
problems of this kind, to</text_slice>
            </slice>
            <slice>
              <time_slice>18:07</time_slice>
              <text_slice>figure out which is the
right state variable.</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>When you define the state of
your system, you need to</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>define it in such a way that
includes all information that</text_slice>
            </slice>
            <slice>
              <time_slice>18:19</time_slice>
              <text_slice>has been accumulated that has
some relevance for the future.</text_slice>
            </slice>
            <slice>
              <time_slice>18:27</time_slice>
              <text_slice>So the general process for
coming up with a Markov model</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>is to first make this big
decision of what your state</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>variable is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>Then you write down if it
may be a picture of</text_slice>
            </slice>
            <slice>
              <time_slice>18:41</time_slice>
              <text_slice>the different states.</text_slice>
            </slice>
            <slice>
              <time_slice>18:43</time_slice>
              <text_slice>Then you identify the possible
transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>So sometimes the diagram that
you're going to have will not</text_slice>
            </slice>
            <slice>
              <time_slice>18:48</time_slice>
              <text_slice>include all the possible arcs.</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>You would only show those arcs
that correspond to transitions</text_slice>
            </slice>
            <slice>
              <time_slice>18:54</time_slice>
              <text_slice>that are possible.</text_slice>
            </slice>
            <slice>
              <time_slice>18:54</time_slice>
              <text_slice>For example, in the supermarket
example, we did</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>not have a transition from state
2 to state 5, because</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>that cannot happen.</text_slice>
            </slice>
            <slice>
              <time_slice>19:02</time_slice>
              <text_slice>You can only have 1 arrival
at any time.</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>So in the diagram, we only
showed the possible</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>And for each of the possible
transitions, then you work</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>with the description of the
model to figure out the</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>correct transition
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>19:17</time_slice>
              <text_slice>So you got the diagram by
writing down transition</text_slice>
            </slice>
            <slice>
              <time_slice>19:21</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>OK, so suppose you got
your Markov model.</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>What will you do with it?</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>Well, what do we need
models for?</text_slice>
            </slice>
            <slice>
              <time_slice>19:34</time_slice>
              <text_slice>We need models in order to
make predictions, to make</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>probabilistic predictions.</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>So for example, I tell you that
the process started in</text_slice>
            </slice>
            <slice>
              <time_slice>19:42</time_slice>
              <text_slice>that state.</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>You let it run for some time.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>Where do you think it's going to
be 10 time steps from now?</text_slice>
            </slice>
            <slice>
              <time_slice>19:49</time_slice>
              <text_slice>That's a question that you
might want to answer.</text_slice>
            </slice>
            <slice>
              <time_slice>19:52</time_slice>
              <text_slice>Since the process is random,
there's no way for you to tell</text_slice>
            </slice>
            <slice>
              <time_slice>19:55</time_slice>
              <text_slice>me exactly where it's
going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>But maybe you can give
me probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>20:00</time_slice>
              <text_slice>You can tell me, with so
much probability, the</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>state would be there.</text_slice>
            </slice>
            <slice>
              <time_slice>20:04</time_slice>
              <text_slice>With so much probability,
the state would be</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>there, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>20:07</time_slice>
              <text_slice>So our first exercise is to
calculate those probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>about what may happen to the
process a number of steps in</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>the future.</text_slice>
            </slice>
            <slice>
              <time_slice>20:18</time_slice>
              <text_slice>It's handy to have some
notation in here.</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>So somebody tells us that this
process starts at the</text_slice>
            </slice>
            <slice>
              <time_slice>20:25</time_slice>
              <text_slice>particular state i.</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>We let the process run
for n transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>It may land at some state j, but
that state j at which it's</text_slice>
            </slice>
            <slice>
              <time_slice>20:36</time_slice>
              <text_slice>going to land is going
to be random.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>So we want to give
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>20:40</time_slice>
              <text_slice>Tell me, with what probability
the state, n times steps</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>later, is going to be that
particular state j?</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>The shorthand notation is to use
this symbol here for the</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>n-step transition probabilities
that you find</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>yourself at state j given that
you started at state i.</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>So the way these two indices are
ordered, the way to think</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>about them is that from
i, you go to j.</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>So the probability that from
i you go to j if you have n</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>steps in front of you.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>Some of these transition
probabilities are, of course</text_slice>
            </slice>
            <slice>
              <time_slice>21:18</time_slice>
              <text_slice>easy to write.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>For example, in 0 transitions,
you're going to be exactly</text_slice>
            </slice>
            <slice>
              <time_slice>21:29</time_slice>
              <text_slice>where you started.</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>So this probability is going to
be equal to 1 if i is equal</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>to j, And 0 if i is
different than j.</text_slice>
            </slice>
            <slice>
              <time_slice>21:40</time_slice>
              <text_slice>That's an easy one
to write down.</text_slice>
            </slice>
            <slice>
              <time_slice>21:43</time_slice>
              <text_slice>If you have only 1 transition,
what's the probability that 1</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>step later you find yourself
in state j given that you</text_slice>
            </slice>
            <slice>
              <time_slice>21:51</time_slice>
              <text_slice>started at state i?</text_slice>
            </slice>
            <slice>
              <time_slice>21:54</time_slice>
              <text_slice>What is this?</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>These are just the ordinary
1-step transition</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>probabilities that we are given
in the description of</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>the problem.</text_slice>
            </slice>
            <slice>
              <time_slice>22:04</time_slice>
              <text_slice>So by definition, the 1-step
transition probabilities are</text_slice>
            </slice>
            <slice>
              <time_slice>22:08</time_slice>
              <text_slice>of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>22:14</time_slice>
              <text_slice>This equality is correct just
because of the way that we</text_slice>
            </slice>
            <slice>
              <time_slice>22:17</time_slice>
              <text_slice>defined those two quantities.</text_slice>
            </slice>
            <slice>
              <time_slice>22:20</time_slice>
              <text_slice>Now we want to say something
about the n-step transition</text_slice>
            </slice>
            <slice>
              <time_slice>22:24</time_slice>
              <text_slice>probabilities when n
is a bigger number.</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>22:31</time_slice>
              <text_slice>So here, we're going to use the
total probability theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>So we're going to condition in
two different scenarios, and</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>break up the calculation of this
quantity, by considering</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>the different ways that
this event can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>22:46</time_slice>
              <text_slice>So what is the event
of interest?</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>The event of interest
is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>At time 0 we start i.</text_slice>
            </slice>
            <slice>
              <time_slice>22:54</time_slice>
              <text_slice>We are interested in landing
at time n at the</text_slice>
            </slice>
            <slice>
              <time_slice>22:57</time_slice>
              <text_slice>particular state j.</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>Now this event can happen in
several different ways, in</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>lots of different ways.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>But let us group them
into subgroups.</text_slice>
            </slice>
            <slice>
              <time_slice>23:08</time_slice>
              <text_slice>One group, or one sort of
scenario, is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>During the first n-1 time steps,
things happen, and</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>somehow you end up at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>And then from state 1, in the
next time step you make a</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>transition to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>23:27</time_slice>
              <text_slice>This particular arc here
actually corresponds to lots</text_slice>
            </slice>
            <slice>
              <time_slice>23:32</time_slice>
              <text_slice>and lots of different possible
scenarios, or different spots,</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>or different transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>In n-1 time steps, there's lots
of possible ways by which</text_slice>
            </slice>
            <slice>
              <time_slice>23:43</time_slice>
              <text_slice>you could end up at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>Different paths through
the state space.</text_slice>
            </slice>
            <slice>
              <time_slice>23:48</time_slice>
              <text_slice>But all of them together
collectively have a</text_slice>
            </slice>
            <slice>
              <time_slice>23:51</time_slice>
              <text_slice>probability, which is the
(n-1)-step transition</text_slice>
            </slice>
            <slice>
              <time_slice>23:55</time_slice>
              <text_slice>probability, that from state
i, you end up at state 1</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>And then there's other
possible scenarios.</text_slice>
            </slice>
            <slice>
              <time_slice>24:05</time_slice>
              <text_slice>Perhaps in the first n-1 time
steps, you follow the</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>trajectory that took
you at state m.</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>And then from state m, you did
this transition, and you ended</text_slice>
            </slice>
            <slice>
              <time_slice>24:17</time_slice>
              <text_slice>up at state j.</text_slice>
            </slice>
            <slice>
              <time_slice>24:18</time_slice>
              <text_slice>So this diagram breaks up
the set of all possible</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>trajectories from i to j into
different collections, where</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>each collection has to do with
which one happens to be the</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>state just before the last time
step, just before time n.</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>And we're going to condition
on the state at time n-1.</text_slice>
            </slice>
            <slice>
              <time_slice>24:42</time_slice>
              <text_slice>So the total probability of
ending up at state j is the</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>sum of the probabilities of
the different scenarios --</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>the different ways that you
can get to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>24:56</time_slice>
              <text_slice>If we look at that type of
scenario, what's the</text_slice>
            </slice>
            <slice>
              <time_slice>25:00</time_slice>
              <text_slice>probability of that scenario
happening?</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>With probability Ri1(n-1),
I find myself at</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>state 1 at time n-1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:10</time_slice>
              <text_slice>This is just by the definition
of these multi-step transition</text_slice>
            </slice>
            <slice>
              <time_slice>25:15</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>This is the number
of transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>The probability that from state
i, I end up at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:22</time_slice>
              <text_slice>And then given that I found
myself at state 1, with</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>probability P1j, that's the
transition probability, next</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>time I'm going to find
myself at state j.</text_slice>
            </slice>
            <slice>
              <time_slice>25:34</time_slice>
              <text_slice>So the product of these two is
the total probability of my</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>getting from state i to
state j through state</text_slice>
            </slice>
            <slice>
              <time_slice>25:43</time_slice>
              <text_slice>1 at the time before.</text_slice>
            </slice>
            <slice>
              <time_slice>25:47</time_slice>
              <text_slice>Now where exactly did we use
the Markov assumption here?</text_slice>
            </slice>
            <slice>
              <time_slice>25:53</time_slice>
              <text_slice>No matter which particular path
we used to get from i to</text_slice>
            </slice>
            <slice>
              <time_slice>25:57</time_slice>
              <text_slice>state 1, the probability that
next I'm going to make this</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>transition is that
same number, P1j.</text_slice>
            </slice>
            <slice>
              <time_slice>26:05</time_slice>
              <text_slice>So that number does not depend
on the particular path that I</text_slice>
            </slice>
            <slice>
              <time_slice>26:09</time_slice>
              <text_slice>followed in order
to get there.</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>If we didn't have the Markov
assumption, we should have</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>considered all possible
individual trajectories here,</text_slice>
            </slice>
            <slice>
              <time_slice>26:18</time_slice>
              <text_slice>and then we would need to use
the transition probability</text_slice>
            </slice>
            <slice>
              <time_slice>26:21</time_slice>
              <text_slice>that corresponds to that
particular trajectory.</text_slice>
            </slice>
            <slice>
              <time_slice>26:23</time_slice>
              <text_slice>But because of the Markov
assumption, the only thing</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>that matters is that right
now we are at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>26:29</time_slice>
              <text_slice>It does not matter
how we got there.</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>So now once you see this
scenario, then this scenario,</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>and that scenario, and you add
the probabilities of these</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>different scenarios, you end
up with this formula here,</text_slice>
            </slice>
            <slice>
              <time_slice>26:43</time_slice>
              <text_slice>which is a recursion.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>It tells us that once you have
computed the (n-1)-step</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>transition probabilities, then
you can compute also the</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>n-step transition
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>26:55</time_slice>
              <text_slice>This is a recursion that you
execute or you run for all i's</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>and j's simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>27:03</time_slice>
              <text_slice>That is fixed.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>And for a particular n, you
calculate this quantity for</text_slice>
            </slice>
            <slice>
              <time_slice>27:08</time_slice>
              <text_slice>all possible i's, j's, k's.</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>You have all of those
quantities, and then you use</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>this equation to find those
numbers again for all the</text_slice>
            </slice>
            <slice>
              <time_slice>27:16</time_slice>
              <text_slice>possible i's and j's.</text_slice>
            </slice>
            <slice>
              <time_slice>27:20</time_slice>
              <text_slice>Now this is formula which is
always true, and there's a big</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>idea behind the formula.</text_slice>
            </slice>
            <slice>
              <time_slice>27:28</time_slice>
              <text_slice>And now there's variations of
this formula, depending on</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>whether you're interested
in something</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>that's slightly different.</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>So for example, if you were to
have a random initial state,</text_slice>
            </slice>
            <slice>
              <time_slice>27:42</time_slice>
              <text_slice>somebody gives you the
probability distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>the initial state, so you're
told that with probability</text_slice>
            </slice>
            <slice>
              <time_slice>27:48</time_slice>
              <text_slice>such and such, you're going
to start at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>27:51</time_slice>
              <text_slice>With that probability, you're
going to start at</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>state 2, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>27:54</time_slice>
              <text_slice>And you want to find the
probability at the time n you</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>find yourself at state j.</text_slice>
            </slice>
            <slice>
              <time_slice>27:58</time_slice>
              <text_slice>Well again, total probability
theorem, you condition on the</text_slice>
            </slice>
            <slice>
              <time_slice>28:01</time_slice>
              <text_slice>initial state.</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>With this probability you find
yourself at that particular</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>initial state, and given that
this is your initial state,</text_slice>
            </slice>
            <slice>
              <time_slice>28:08</time_slice>
              <text_slice>this is the probability that
n time steps later you find</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>yourself at state j.</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>Now building again on the same
idea, you can run every</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>recursion of this kind
by conditioning</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>at different times.</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>So here's a variation.</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>You start at state i.</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>After 1 time step, you find
yourself at state 1, with</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>probability pi1, and you find
yourself at state m with</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>probability Pim.</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>And once that happens, then
you're going to follow some</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>trajectories.</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>And there is a possibility that
you're going to end up at</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>state j after n-1 time steps.</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>This scenario can happen
in many possible ways.</text_slice>
            </slice>
            <slice>
              <time_slice>29:05</time_slice>
              <text_slice>There's lots of possible paths
from state 1 to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>There's many paths from
state 1 to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>What is the collective
probability of all these</text_slice>
            </slice>
            <slice>
              <time_slice>29:15</time_slice>
              <text_slice>transitions?</text_slice>
            </slice>
            <slice>
              <time_slice>29:19</time_slice>
              <text_slice>This is the event that, starting
from state 1, I end</text_slice>
            </slice>
            <slice>
              <time_slice>29:23</time_slice>
              <text_slice>up at state j in
n-1 time steps.</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>So this one has here probability
R1j of n-1.</text_slice>
            </slice>
            <slice>
              <time_slice>29:34</time_slice>
              <text_slice>And similarly down here.</text_slice>
            </slice>
            <slice>
              <time_slice>29:37</time_slice>
              <text_slice>And then by using the same way
of thinking as before, we get</text_slice>
            </slice>
            <slice>
              <time_slice>29:41</time_slice>
              <text_slice>the formula that Rij(n) is the
sum over all k's of Pik, and</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>then the Rkj(n-1).</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>So this formula looks almost the
same as this one, but it's</text_slice>
            </slice>
            <slice>
              <time_slice>29:59</time_slice>
              <text_slice>actually different.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>The indices and the way things
work out are a bit different,</text_slice>
            </slice>
            <slice>
              <time_slice>30:05</time_slice>
              <text_slice>but the basic idea
is the same.</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>Here we use the total
probability theory by</text_slice>
            </slice>
            <slice>
              <time_slice>30:10</time_slice>
              <text_slice>conditioning on the state just
1 step before the end of our</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>time horizon.</text_slice>
            </slice>
            <slice>
              <time_slice>30:17</time_slice>
              <text_slice>Here we use total probability
theorem by conditioning on the</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>state right after the
first transition.</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>So this generally idea has
different variations.</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>They're all valid, and depending
on the context that</text_slice>
            </slice>
            <slice>
              <time_slice>30:30</time_slice>
              <text_slice>you're dealing with, you might
want to work with one of these</text_slice>
            </slice>
            <slice>
              <time_slice>30:34</time_slice>
              <text_slice>or another.</text_slice>
            </slice>
            <slice>
              <time_slice>30:37</time_slice>
              <text_slice>So let's illustrate
these calculations</text_slice>
            </slice>
            <slice>
              <time_slice>30:40</time_slice>
              <text_slice>in terms of an example.</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>So in this example, we just have
2 states, and somebody</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>gives us transition
probabilities to be those</text_slice>
            </slice>
            <slice>
              <time_slice>30:49</time_slice>
              <text_slice>particular numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>30:51</time_slice>
              <text_slice>Let's write down
the equations.</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>So the probability that starting
from state 1, I find</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>myself at state 1 n
time steps later.</text_slice>
            </slice>
            <slice>
              <time_slice>31:06</time_slice>
              <text_slice>This can happen in 2 ways.</text_slice>
            </slice>
            <slice>
              <time_slice>31:09</time_slice>
              <text_slice>At time n-1, I might find
myself at state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>And then from state 2, I make a
transition back to state 1,</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>which happens with
probability--</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>why'd I put 2 there --</text_slice>
            </slice>
            <slice>
              <time_slice>31:25</time_slice>
              <text_slice>anyway, 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>And another way is that from
state 1, I go to state 1 in</text_slice>
            </slice>
            <slice>
              <time_slice>31:32</time_slice>
              <text_slice>n-1 steps, and then from state
1 I stay where I am, which</text_slice>
            </slice>
            <slice>
              <time_slice>31:38</time_slice>
              <text_slice>happens with probability 0.5.</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>So this is for R11(n).</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>Now R12(n), we can
write a similar</text_slice>
            </slice>
            <slice>
              <time_slice>31:54</time_slice>
              <text_slice>recursion for this one.</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>On the other hand, seems these
are probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>32:00</time_slice>
              <text_slice>The state at time n is
going to be either</text_slice>
            </slice>
            <slice>
              <time_slice>32:02</time_slice>
              <text_slice>state 1 or state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>32:04</time_slice>
              <text_slice>So these 2 numbers need to add
to 1, so we can just write</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>this as 1 - R11(n).</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>And this is an enough of a
recursion to propagate R11 and</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>R12 as time goes on.</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>So after n-1 transitions, either
I find myself in state</text_slice>
            </slice>
            <slice>
              <time_slice>32:29</time_slice>
              <text_slice>2, and then there's a point to
transition that I go to 1, or</text_slice>
            </slice>
            <slice>
              <time_slice>32:33</time_slice>
              <text_slice>I find myself in state 1, which
with that probability,</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>and from there, I have
probability 0.5 of staying</text_slice>
            </slice>
            <slice>
              <time_slice>32:41</time_slice>
              <text_slice>where I am.</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>Now let's start calculating.</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>As we discussed before, if I
start at state 1, after 0</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>transitions I'm certain to be at
state , and I'm certain not</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>to be at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>If I start from state 1, I'm
certain to not to be at state</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>at that time, and I'm certain
that I am right</text_slice>
            </slice>
            <slice>
              <time_slice>33:01</time_slice>
              <text_slice>now, it's state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>33:03</time_slice>
              <text_slice>After I make transition,
starting from state 1, there's</text_slice>
            </slice>
            <slice>
              <time_slice>33:09</time_slice>
              <text_slice>probability 0.5 that
I stay at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>33:13</time_slice>
              <text_slice>And there's probability 0.5
that I stay at state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>33:17</time_slice>
              <text_slice>If I were to start from state
2, the probability that I go</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>to 1 in 1 time step is this
transition that has</text_slice>
            </slice>
            <slice>
              <time_slice>33:25</time_slice>
              <text_slice>probability 0.2, and
the other 0.8.</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>So the calculation now becomes
more interesting, if we want</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>to calculate the next term.</text_slice>
            </slice>
            <slice>
              <time_slice>33:36</time_slice>
              <text_slice>How likely is that at time 2,
I find myself at state 1?</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>In order to be here at state 1,
this can happen in 2 ways.</text_slice>
            </slice>
            <slice>
              <time_slice>33:50</time_slice>
              <text_slice>Either the first transition left
me there, and the second</text_slice>
            </slice>
            <slice>
              <time_slice>33:54</time_slice>
              <text_slice>transition is the same.</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>So these correspond to this 0.5,
that the first transition</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>took me there, and the
next transition was</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>also of the same kind.</text_slice>
            </slice>
            <slice>
              <time_slice>34:07</time_slice>
              <text_slice>That's one possibility.</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>But there's another scenario.</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>In order to be at state 1
at time 2 -- this can</text_slice>
            </slice>
            <slice>
              <time_slice>34:15</time_slice>
              <text_slice>also happen this way.</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>So that's the event
that, after 1</text_slice>
            </slice>
            <slice>
              <time_slice>34:19</time_slice>
              <text_slice>transition, I got there.</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>And the next transition happened
to be this one.</text_slice>
            </slice>
            <slice>
              <time_slice>34:26</time_slice>
              <text_slice>So this corresponds
to 0.5 times 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:31</time_slice>
              <text_slice>It corresponds to taking the
1-step transition probability</text_slice>
            </slice>
            <slice>
              <time_slice>34:34</time_slice>
              <text_slice>of getting there, times the
probability that from state 2</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>I move to state 1, which
in this case, is 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>34:43</time_slice>
              <text_slice>So basically we take this
number, multiplied with 0.2,</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>and then add those 2 numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>34:50</time_slice>
              <text_slice>And after you add them,
you get 0.35.</text_slice>
            </slice>
            <slice>
              <time_slice>34:54</time_slice>
              <text_slice>And similarly here, you're
going to get 0.65.</text_slice>
            </slice>
            <slice>
              <time_slice>34:59</time_slice>
              <text_slice>And now to continue with the
recursion, we keep doing the</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>same thing.</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>We take this number times 0.5
plus this number times 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>Add them up, you get
the next entry.</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>Keep doing that, keep doing
that, and eventually you will</text_slice>
            </slice>
            <slice>
              <time_slice>35:15</time_slice>
              <text_slice>notice that the numbers
start settling into a</text_slice>
            </slice>
            <slice>
              <time_slice>35:19</time_slice>
              <text_slice>limiting value at 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>35:23</time_slice>
              <text_slice>And let's verify this.</text_slice>
            </slice>
            <slice>
              <time_slice>35:25</time_slice>
              <text_slice>If this number is 2/7, what is
the next number going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>35:33</time_slice>
              <text_slice>The next number is going to
be 2/7 -- (not 2.7) --</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>it's going to be 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>35:42</time_slice>
              <text_slice>That's the probability that I
find myself at that state,</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>times 0.5--</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>that's the next transition that
takes me to state 1 --</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>plus 5/7--</text_slice>
            </slice>
            <slice>
              <time_slice>35:53</time_slice>
              <text_slice>that would be the remaining
probability that I find myself</text_slice>
            </slice>
            <slice>
              <time_slice>35:56</time_slice>
              <text_slice>in state 2 --</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>times 1/5.</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>And so that gives
me, again, 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>So this calculation basically
illustrates, if this number</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>has become 2/7, then
the next number is</text_slice>
            </slice>
            <slice>
              <time_slice>36:19</time_slice>
              <text_slice>also going to be 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>And of course this number here
is going to have to be 5/7.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>And this one would have to
be again, the same, 5/7.</text_slice>
            </slice>
            <slice>
              <time_slice>36:32</time_slice>
              <text_slice>So the probability that I find
myself at state 1, after a</text_slice>
            </slice>
            <slice>
              <time_slice>36:36</time_slice>
              <text_slice>long time has elapsed, settles
into some steady state value.</text_slice>
            </slice>
            <slice>
              <time_slice>36:41</time_slice>
              <text_slice>So that's an interesting
phenomenon.</text_slice>
            </slice>
            <slice>
              <time_slice>36:44</time_slice>
              <text_slice>We just make this observation.</text_slice>
            </slice>
            <slice>
              <time_slice>36:46</time_slice>
              <text_slice>Now we can also do the
calculation about the</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>probability, starting
from state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>And here, you do the
calculations --</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>I'm not going to do them.</text_slice>
            </slice>
            <slice>
              <time_slice>36:58</time_slice>
              <text_slice>But after you do them, you find
this probability also</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>settles to 2/7 and this one
also settles to 5/7.</text_slice>
            </slice>
            <slice>
              <time_slice>37:11</time_slice>
              <text_slice>So these numbers here are the
same as those numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>What's the difference
between these?</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>This is the probability that I
find myself at state 1 given</text_slice>
            </slice>
            <slice>
              <time_slice>37:24</time_slice>
              <text_slice>that I started at 1.</text_slice>
            </slice>
            <slice>
              <time_slice>37:27</time_slice>
              <text_slice>This is the probability that I
find myself at state 1 given</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>that I started at state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>These probabilities are the
same, no matter where I</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>started from.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>So this numerical example sort
of illustrates the idea that</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>after the chain has run for a
long time, what the state of</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>the chain is, does not care
about the initial</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>state of the chain.</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>So if you start here, you know
that you're going to stay here</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>for some time, a few
transitions, because this</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>probability is kind of small.</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>So the initial state does that's
tell you something.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>But in the very long run,
transitions of this kind are</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>38:14</time_slice>
              <text_slice>Transitions of that kind
are going to happen.</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>There's a lot of randomness
that comes in, and that</text_slice>
            </slice>
            <slice>
              <time_slice>38:20</time_slice>
              <text_slice>randomness washes out any
information that could come</text_slice>
            </slice>
            <slice>
              <time_slice>38:24</time_slice>
              <text_slice>from the initial state
of the system.</text_slice>
            </slice>
            <slice>
              <time_slice>38:28</time_slice>
              <text_slice>We describe this situation by
saying that the Markov chain</text_slice>
            </slice>
            <slice>
              <time_slice>38:33</time_slice>
              <text_slice>eventually enters
a steady state.</text_slice>
            </slice>
            <slice>
              <time_slice>38:37</time_slice>
              <text_slice>Where a steady state, what
does it mean it?</text_slice>
            </slice>
            <slice>
              <time_slice>38:41</time_slice>
              <text_slice>Does it mean the state itself
becomes steady and</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>stops at one place?</text_slice>
            </slice>
            <slice>
              <time_slice>38:48</time_slice>
              <text_slice>No, the state of the chain
keeps jumping forever.</text_slice>
            </slice>
            <slice>
              <time_slice>38:52</time_slice>
              <text_slice>The state of the chain will keep
making transitions, will</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>keep going back and forth
between 1 and 2.</text_slice>
            </slice>
            <slice>
              <time_slice>38:58</time_slice>
              <text_slice>So the state itself, the
Xn, does not become</text_slice>
            </slice>
            <slice>
              <time_slice>39:02</time_slice>
              <text_slice>steady in any sense.</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>What becomes steady are
the probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>39:07</time_slice>
              <text_slice>that describe Xn.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>That is, after a long time
elapses, the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>39:12</time_slice>
              <text_slice>you find yourself at state 1
becomes a constant 2/7, and</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>the probability that you
find yourself in</text_slice>
            </slice>
            <slice>
              <time_slice>39:21</time_slice>
              <text_slice>state 2 becomes a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>39:23</time_slice>
              <text_slice>So jumps will keep happening,
but at any given time, if you</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>ask what's the probability that
right now I am at state</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>1, the answer is going
to be 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>Incidentally, do the numbers
sort of makes sense?</text_slice>
            </slice>
            <slice>
              <time_slice>39:37</time_slice>
              <text_slice>Why is this number bigger
than that number?</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>Well, this state is a little
more sticky than that state.</text_slice>
            </slice>
            <slice>
              <time_slice>39:46</time_slice>
              <text_slice>Once you enter here, it's kind
of harder to get out.</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>So when you enter here, you
spend a lot of time here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>This one is easier to get out,
because the probability is</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>0.5, so when you enter there,
you tend to get out faster.</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>So you keep moving from one to
the other, but you tend to</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>spend more time on that state,
and this is reflected in this</text_slice>
            </slice>
            <slice>
              <time_slice>40:08</time_slice>
              <text_slice>probability being bigger
than that one.</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>So no matter where you start,
there's 5/7 probability of</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>being here, 2/7 probability
being there.</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>So there were some really
nice things that</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>happened in this example.</text_slice>
            </slice>
            <slice>
              <time_slice>40:24</time_slice>
              <text_slice>The question is, whether things
are always as nice for</text_slice>
            </slice>
            <slice>
              <time_slice>40:28</time_slice>
              <text_slice>general Markov chains.</text_slice>
            </slice>
            <slice>
              <time_slice>40:30</time_slice>
              <text_slice>The two nice things that
happened where the following--</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>as we keep doing this
calculation, this number</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>settles to something.</text_slice>
            </slice>
            <slice>
              <time_slice>40:37</time_slice>
              <text_slice>The limit exists.</text_slice>
            </slice>
            <slice>
              <time_slice>40:39</time_slice>
              <text_slice>The other thing that happens
is that this number is the</text_slice>
            </slice>
            <slice>
              <time_slice>40:42</time_slice>
              <text_slice>same as that number, which means
that the initial state</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>does not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>40:47</time_slice>
              <text_slice>Is this always the case?</text_slice>
            </slice>
            <slice>
              <time_slice>40:50</time_slice>
              <text_slice>Is it always the case that as
n goes to infinity, the</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>transition probabilities
converge to something?</text_slice>
            </slice>
            <slice>
              <time_slice>40:58</time_slice>
              <text_slice>And if they do converge to
something, is it the case that</text_slice>
            </slice>
            <slice>
              <time_slice>41:02</time_slice>
              <text_slice>the limit is not affected by the
initial state i at which</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>the chain started?</text_slice>
            </slice>
            <slice>
              <time_slice>41:09</time_slice>
              <text_slice>So mathematically speaking, the
question we are raising is</text_slice>
            </slice>
            <slice>
              <time_slice>41:12</time_slice>
              <text_slice>whether Rij(n) converges
to something.</text_slice>
            </slice>
            <slice>
              <time_slice>41:19</time_slice>
              <text_slice>And whether that something to
which it converges to has only</text_slice>
            </slice>
            <slice>
              <time_slice>41:25</time_slice>
              <text_slice>to do with j.</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>It's the probability that you
find yourself at state j, and</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>that probability doesn't care
about the initial state.</text_slice>
            </slice>
            <slice>
              <time_slice>41:34</time_slice>
              <text_slice>So it's the question of whether
the initial state gets</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>forgotten in the long run.</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>So the answer is that usually,
or for nice chains, both of</text_slice>
            </slice>
            <slice>
              <time_slice>41:46</time_slice>
              <text_slice>these things will be true.</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>You get the limit which
does not depend</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>on the initial state.</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>But if your chain has some
peculiar or unique structure,</text_slice>
            </slice>
            <slice>
              <time_slice>41:59</time_slice>
              <text_slice>this might not happen.</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>So let's think first about
the issue of convergence.</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>So convergence, as n goes to
infinity at a steady value,</text_slice>
            </slice>
            <slice>
              <time_slice>42:12</time_slice>
              <text_slice>really means the following.</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>If I tell you a lot of time has
passed, then you tell me,</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>OK, the state of the
probabilities are equal to</text_slice>
            </slice>
            <slice>
              <time_slice>42:21</time_slice>
              <text_slice>that value without having
to consult your clock.</text_slice>
            </slice>
            <slice>
              <time_slice>42:25</time_slice>
              <text_slice>If you don't have convergence,
it means that Rij can keep</text_slice>
            </slice>
            <slice>
              <time_slice>42:29</time_slice>
              <text_slice>going up and down, without
settling to something.</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>So in order for you to tell me
the value of Rij, you need to</text_slice>
            </slice>
            <slice>
              <time_slice>42:35</time_slice>
              <text_slice>consult your clock to
check if, right now,</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>it's up or is it down.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>So there's some kind of periodic
behavior that you</text_slice>
            </slice>
            <slice>
              <time_slice>42:43</time_slice>
              <text_slice>might get when you do not get
convergence, and this example</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>here illustrates it.</text_slice>
            </slice>
            <slice>
              <time_slice>42:47</time_slice>
              <text_slice>So what's happened
in this example?</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>Starting from state 2, next time
you go here, or there,</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>with probability half.</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>And then next time, no matter
where you are, you move back</text_slice>
            </slice>
            <slice>
              <time_slice>43:00</time_slice>
              <text_slice>to state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>So this chain has some
randomness, but the randomness</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>is kind of limited type.</text_slice>
            </slice>
            <slice>
              <time_slice>43:08</time_slice>
              <text_slice>You go out, you come in.</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>You go out, you come in.</text_slice>
            </slice>
            <slice>
              <time_slice>43:10</time_slice>
              <text_slice>So there's a periodic pattern
that gets repeated.</text_slice>
            </slice>
            <slice>
              <time_slice>43:14</time_slice>
              <text_slice>It means that if you start at
state 2 after an even number</text_slice>
            </slice>
            <slice>
              <time_slice>43:19</time_slice>
              <text_slice>of steps, you are certain
to be back at state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>43:24</time_slice>
              <text_slice>So this probability here is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>On the other hand, if the number
of transitions is odd,</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>there's no way that you can
be at your initial state.</text_slice>
            </slice>
            <slice>
              <time_slice>43:33</time_slice>
              <text_slice>If you start here, at even times
you would be here, at</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>odd times you would
be there or there.</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>So this probability is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>As n goes to infinity, these
probabilities, the n-step</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>transition probability does
not converge to anything.</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>It keeps alternating
between 0 and 1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>So convergence fails.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>This is the main mechanism by
which convergence can fail if</text_slice>
            </slice>
            <slice>
              <time_slice>43:57</time_slice>
              <text_slice>your chain has a periodic
structure.</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>And we're going to discuss next
time that, if periodicity</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>absent, then we don't have an
issue with convergence.</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>The second question if we have
convergence, whether the</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>initial state matters or not.</text_slice>
            </slice>
            <slice>
              <time_slice>44:16</time_slice>
              <text_slice>In the previous chain, where you
could keep going back and</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>forth between states 1 and 2
numerically, one finds that</text_slice>
            </slice>
            <slice>
              <time_slice>44:22</time_slice>
              <text_slice>the initial state
does not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>44:25</time_slice>
              <text_slice>But you can think of situations
where the initial</text_slice>
            </slice>
            <slice>
              <time_slice>44:27</time_slice>
              <text_slice>state does matter.</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>Look at this chain here.</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>If you start at state 1, you
stay at state 1 forever.</text_slice>
            </slice>
            <slice>
              <time_slice>44:37</time_slice>
              <text_slice>There's no way to escape.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>So this means that R11(n)
is 1 for all n.</text_slice>
            </slice>
            <slice>
              <time_slice>44:46</time_slice>
              <text_slice>If you start at state 3, you
will be moving between stage 3</text_slice>
            </slice>
            <slice>
              <time_slice>44:50</time_slice>
              <text_slice>and 4, but there's no way to
go in that direction, so</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>there's no way that
you go to state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>And for that reason,
R31 is 0 for all n.</text_slice>
            </slice>
            <slice>
              <time_slice>45:06</time_slice>
              <text_slice>OK So this is a case where the
initial state matters.</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>R11 goes to a limit, as
n goes to infinity,</text_slice>
            </slice>
            <slice>
              <time_slice>45:21</time_slice>
              <text_slice>because it's constant.</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>It's always 1 so
the limit is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>R31 also has a limit.</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>It's 0 for all times.</text_slice>
            </slice>
            <slice>
              <time_slice>45:30</time_slice>
              <text_slice>So these are the long term
probabilities of finding</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>yourself at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>But those long-term
probabilities are affected by</text_slice>
            </slice>
            <slice>
              <time_slice>45:37</time_slice>
              <text_slice>where you started.</text_slice>
            </slice>
            <slice>
              <time_slice>45:39</time_slice>
              <text_slice>If you start here, you're sure
that's, in the long term,</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>you'll be here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>If you start here, you're sure
that, in the long term, you</text_slice>
            </slice>
            <slice>
              <time_slice>45:45</time_slice>
              <text_slice>will not be there.</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>So the initial state
does matter here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>And this is a situation where
certain states are not</text_slice>
            </slice>
            <slice>
              <time_slice>45:53</time_slice>
              <text_slice>accessible from certain other
states, so it has something to</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>do with the graph structure
of our Markov chain.</text_slice>
            </slice>
            <slice>
              <time_slice>46:00</time_slice>
              <text_slice>Finally let's answer this
question here, at</text_slice>
            </slice>
            <slice>
              <time_slice>46:04</time_slice>
              <text_slice>least for large n's.</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>What do you think is going to
happen in the long term if you</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>start at state 2?</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>If you start at state 2, you
may stay at state 2 for a</text_slice>
            </slice>
            <slice>
              <time_slice>46:18</time_slice>
              <text_slice>random amount of time, but
eventually this transition</text_slice>
            </slice>
            <slice>
              <time_slice>46:22</time_slice>
              <text_slice>will happen, or that transition
would happen.</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>Because of the symmetry, you are
as likely to escape from</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>state 2 in this direction, or in
that direction, so there's</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>probability 1/2 that, when the
transition happens, the</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>transition happens in
that direction.</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>So for large N, you're
certain that the</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>transition does happen.</text_slice>
            </slice>
            <slice>
              <time_slice>46:50</time_slice>
              <text_slice>And given that the transition
has happened, it has</text_slice>
            </slice>
            <slice>
              <time_slice>46:54</time_slice>
              <text_slice>probability 1/2 that it has
gone that particular way.</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>So clearly here, you see that
the probability of finding</text_slice>
            </slice>
            <slice>
              <time_slice>47:00</time_slice>
              <text_slice>yourself in a particular state
is very much affected by where</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>you started from.</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>So what we want to do next is
to abstract from these two</text_slice>
            </slice>
            <slice>
              <time_slice>47:09</time_slice>
              <text_slice>examples and describe the
general structural properties</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>that have to do with
periodicity, and that have to</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>do with what happened here with
certain states, not being</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>accessible from the others.</text_slice>
            </slice>
            <slice>
              <time_slice>47:20</time_slice>
              <text_slice>We're going to leave periodicity
for next time.</text_slice>
            </slice>
            <slice>
              <time_slice>47:23</time_slice>
              <text_slice>But let's talk about
the second kind of</text_slice>
            </slice>
            <slice>
              <time_slice>47:25</time_slice>
              <text_slice>phenomenon that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>47:28</time_slice>
              <text_slice>So here, what we're going to do
is to classify the states</text_slice>
            </slice>
            <slice>
              <time_slice>47:32</time_slice>
              <text_slice>in a transition diagram
into two types,</text_slice>
            </slice>
            <slice>
              <time_slice>47:35</time_slice>
              <text_slice>recurrent and transient.</text_slice>
            </slice>
            <slice>
              <time_slice>47:38</time_slice>
              <text_slice>So a state is said to
be recurrent if the</text_slice>
            </slice>
            <slice>
              <time_slice>47:41</time_slice>
              <text_slice>following is true.</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>If you start from the state i,
you can go to some places, but</text_slice>
            </slice>
            <slice>
              <time_slice>47:49</time_slice>
              <text_slice>no matter where you go, there
is a way of coming back.</text_slice>
            </slice>
            <slice>
              <time_slice>47:55</time_slice>
              <text_slice>So what's an example for
the recurrent state?</text_slice>
            </slice>
            <slice>
              <time_slice>47:59</time_slice>
              <text_slice>This one.</text_slice>
            </slice>
            <slice>
              <time_slice>48:02</time_slice>
              <text_slice>Starting from here, you
can go elsewhere.</text_slice>
            </slice>
            <slice>
              <time_slice>48:04</time_slice>
              <text_slice>You can go to state 7.</text_slice>
            </slice>
            <slice>
              <time_slice>48:06</time_slice>
              <text_slice>You can go to state 6.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>That's all where
you can go to.</text_slice>
            </slice>
            <slice>
              <time_slice>48:11</time_slice>
              <text_slice>But no matter where you go,
there is a path that can take</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>you back there.</text_slice>
            </slice>
            <slice>
              <time_slice>48:17</time_slice>
              <text_slice>So no matter where you go, there
is a chance, and there</text_slice>
            </slice>
            <slice>
              <time_slice>48:20</time_slice>
              <text_slice>is a way for returning
where you started.</text_slice>
            </slice>
            <slice>
              <time_slice>48:23</time_slice>
              <text_slice>Those states we call
recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>And by this, 8 is recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>All of these are recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>So this is recurrent,
this is recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:34</time_slice>
              <text_slice>And this state 5 is
also recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:36</time_slice>
              <text_slice>You cannot go anywhere from
5 except to 5 itself.</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>Wherever you can go, you can
go back to where you start.</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>So this is recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>If it is not the recurrent, we
say that it is transient.</text_slice>
            </slice>
            <slice>
              <time_slice>48:49</time_slice>
              <text_slice>So what does transient mean?</text_slice>
            </slice>
            <slice>
              <time_slice>48:50</time_slice>
              <text_slice>You need to take this
definition, and reverse it.</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>Transient means that, starting
from i, there is a place to</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>which you could go, and from
which you cannot return.</text_slice>
            </slice>
            <slice>
              <time_slice>49:05</time_slice>
              <text_slice>If it's recurrent, anywhere
you go, you</text_slice>
            </slice>
            <slice>
              <time_slice>49:07</time_slice>
              <text_slice>can always come back.</text_slice>
            </slice>
            <slice>
              <time_slice>49:09</time_slice>
              <text_slice>Transient means there are places
where you can go from</text_slice>
            </slice>
            <slice>
              <time_slice>49:12</time_slice>
              <text_slice>which you cannot come back.</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>So state 1 is recurrent -
because starting from here,</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>there's a possibility that
you get there, and then</text_slice>
            </slice>
            <slice>
              <time_slice>49:20</time_slice>
              <text_slice>there's no way back.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>State 4 is recurrent, starting
from 4, there's somewhere you</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>can go and--</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>sorry, transient, correct.</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>State 4 is transient starting
from here, there are places</text_slice>
            </slice>
            <slice>
              <time_slice>49:33</time_slice>
              <text_slice>where you could go, and from
which you cannot come back.</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>And in this particular diagram,
all these 4 states</text_slice>
            </slice>
            <slice>
              <time_slice>49:40</time_slice>
              <text_slice>are transients.</text_slice>
            </slice>
            <slice>
              <time_slice>49:43</time_slice>
              <text_slice>Now if the state is transient,
it means that there is a way</text_slice>
            </slice>
            <slice>
              <time_slice>49:49</time_slice>
              <text_slice>to go somewhere where you're
going to get stuck and not to</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>be able to come.</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>As long as your state keeps
circulating around here,</text_slice>
            </slice>
            <slice>
              <time_slice>49:59</time_slice>
              <text_slice>eventually one of these
transitions is going to</text_slice>
            </slice>
            <slice>
              <time_slice>50:02</time_slice>
              <text_slice>happen, and once that happens,
then there's no way that you</text_slice>
            </slice>
            <slice>
              <time_slice>50:05</time_slice>
              <text_slice>can come back.</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>So that transient state will
be visited only a finite</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>number of times.</text_slice>
            </slice>
            <slice>
              <time_slice>50:12</time_slice>
              <text_slice>You will not be able
to return to it.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>And in the long run, you're
certain that you're going to</text_slice>
            </slice>
            <slice>
              <time_slice>50:17</time_slice>
              <text_slice>get out of the transient states,
and get to some class</text_slice>
            </slice>
            <slice>
              <time_slice>50:22</time_slice>
              <text_slice>of recurrent states, and
get stuck forever.</text_slice>
            </slice>
            <slice>
              <time_slice>50:25</time_slice>
              <text_slice>So, let's see, in this diagram,
if I start here,</text_slice>
            </slice>
            <slice>
              <time_slice>50:29</time_slice>
              <text_slice>could I stay in this lump
of states forever?</text_slice>
            </slice>
            <slice>
              <time_slice>50:32</time_slice>
              <text_slice>Well as long as I'm staying in
this type of states, I would</text_slice>
            </slice>
            <slice>
              <time_slice>50:35</time_slice>
              <text_slice>keep visiting states 1 and 2
Each time that I visit state</text_slice>
            </slice>
            <slice>
              <time_slice>50:40</time_slice>
              <text_slice>2, there's going to be positive</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>probability that I escape.</text_slice>
            </slice>
            <slice>
              <time_slice>50:43</time_slice>
              <text_slice>So in the long run, if I were
to stay here, I would visit</text_slice>
            </slice>
            <slice>
              <time_slice>50:47</time_slice>
              <text_slice>state 2 an infinite number
of times, and I would get</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>infinite chances to escape.</text_slice>
            </slice>
            <slice>
              <time_slice>50:52</time_slice>
              <text_slice>But if you have infinite chances
to escape, eventually</text_slice>
            </slice>
            <slice>
              <time_slice>50:56</time_slice>
              <text_slice>you will escape.</text_slice>
            </slice>
            <slice>
              <time_slice>50:57</time_slice>
              <text_slice>So you are certain that with
probability 1, starting from</text_slice>
            </slice>
            <slice>
              <time_slice>51:01</time_slice>
              <text_slice>here, you're going to move
either to those states, or to</text_slice>
            </slice>
            <slice>
              <time_slice>51:05</time_slice>
              <text_slice>those states.</text_slice>
            </slice>
            <slice>
              <time_slice>51:06</time_slice>
              <text_slice>So starting from transient
states, you only stay at the</text_slice>
            </slice>
            <slice>
              <time_slice>51:09</time_slice>
              <text_slice>transient states for random
but finite amount of time.</text_slice>
            </slice>
            <slice>
              <time_slice>51:14</time_slice>
              <text_slice>And after that happens,
you end up in a class</text_slice>
            </slice>
            <slice>
              <time_slice>51:19</time_slice>
              <text_slice>of recurrent states.</text_slice>
            </slice>
            <slice>
              <time_slice>51:20</time_slice>
              <text_slice>And when I say class, what they
mean is that, in this</text_slice>
            </slice>
            <slice>
              <time_slice>51:23</time_slice>
              <text_slice>picture, I divide the recurrent
states into 2</text_slice>
            </slice>
            <slice>
              <time_slice>51:26</time_slice>
              <text_slice>classes, or categories.</text_slice>
            </slice>
            <slice>
              <time_slice>51:28</time_slice>
              <text_slice>What's special about them?</text_slice>
            </slice>
            <slice>
              <time_slice>51:30</time_slice>
              <text_slice>These states are recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>51:31</time_slice>
              <text_slice>These states are recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>51:33</time_slice>
              <text_slice>But there's no communication
between the 2.</text_slice>
            </slice>
            <slice>
              <time_slice>51:35</time_slice>
              <text_slice>If you start here, you're
stuck here.</text_slice>
            </slice>
            <slice>
              <time_slice>51:36</time_slice>
              <text_slice>If you start here, you
are stuck there.</text_slice>
            </slice>
            <slice>
              <time_slice>51:39</time_slice>
              <text_slice>And this is a case where the
initial state does matter,</text_slice>
            </slice>
            <slice>
              <time_slice>51:42</time_slice>
              <text_slice>because if you start here,
you get stuck here.</text_slice>
            </slice>
            <slice>
              <time_slice>51:45</time_slice>
              <text_slice>You start here, you
get stuck there.</text_slice>
            </slice>
            <slice>
              <time_slice>51:47</time_slice>
              <text_slice>So depending on the initial
state, that's going to affect</text_slice>
            </slice>
            <slice>
              <time_slice>51:49</time_slice>
              <text_slice>the long term behavior
of your chain.</text_slice>
            </slice>
            <slice>
              <time_slice>51:52</time_slice>
              <text_slice>So the guess you can make at
this point is that, for the</text_slice>
            </slice>
            <slice>
              <time_slice>51:55</time_slice>
              <text_slice>initial state to not matter,
we should not have multiple</text_slice>
            </slice>
            <slice>
              <time_slice>51:59</time_slice>
              <text_slice>recurrent classes.</text_slice>
            </slice>
            <slice>
              <time_slice>52:00</time_slice>
              <text_slice>We should have only 1.</text_slice>
            </slice>
            <slice>
              <time_slice>52:01</time_slice>
              <text_slice>But we're going to get back
to this point next time.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Continuous Bayes&#8217; Rule; Derived Distributions (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>How to nd them The continuous case
Discrete case Two-step procedure:
Obtain probability mass for eachGet CDF of Y:FY(y)=P(Y y )
possible value of Y=g(X)
Dierentiate to getpY(y)=P(g(X)=y )
=dF p YX(x)fY(y)= (y)
x:g(x)=y dy
x y
.g(x).Example
. .X: uniform on [0,2]
. .Find PDF of =3Y X
. .Solution:
. .3FY(y)= P(Yy)=P(Xy)
. .=1 3 11 3P(/Xy)=/y2
. .dFfY(Y 1y)= (y)=dy 62y/3
Example The pdf of Y=aX+b
Joan is driving from Boston to New York.
Her speed is uniformly distributed be- Y=2X+ 5:
tween 30 and 60 mph. What is the dis-
tribution of the duration of the trip? fXfaX faX+b
200Let T(V)= .V
Find fT(t) -2 -1 2 3 4 9
f(v )v 0
1/301 y bfY(y)= f|a|X
a
30 60 v0 Use this to check that if Xis normal,
then Y=aX+bis also normal.
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 10 The Bayes variations
Continuous Bayes rule; pX,Y(x, y) pX(x)pY y x)
(x y )= =|X(
p|
Derived distributions X|Y |pY(y) pY(y)
Readings: pY(y)= pX(x)pY X(y|x)
x|
Section 3.6; start Section 4.1
Example:
ReviewX=1,0: airplane present/not present
Y=1,0: something did/did not register
p ( on adaX(x) f r rXx)
pX,Y(x, y) fX,Y(x, y)
pX,Y(x, y) fX,Y(x, y) Continuous counterpartpX(x y )= f(x y )=|Y |pY(y)X|Y |fY(y)
f(x, y) fX(x)fp(x)= p(x, y) f(x)= f(x, y)dy|X,Y Y|X(y )
X X,Y X X, f(x =y  X|Y|x
Y y)=fY(y) fY(y)
fY(y)= fX(x)fY X(y x) dx
x| |
FX(x)= P( )
Xx
Example: X: some signal; prior fX(x)
E[X],var(X) Y: noisy version of X
fY X(y|x): model of the noise|
Discrete X,C o n t i n u o u s Y What is a derived distribution
It is a PMF or PDF of a function of onepX(x)f (y x)
pY(x|Y
X y)=|X |
or more random variables with known|fY(y)probability law. E.g.:
)=y fY(y
pX(x)fY|X(y|x) f (y,x)=1X,Yx
1Example:
X: a discrete signal; prior pX(x)
Y: noisy version of X
fY X(y|x): continuous noise model|
1 x 
Continuous X,D i s c r e t eYObtaining the PDF for
fX(x)pYf x X, Y)=XY/X|Y(y)=|X(y|x)
| g(pY(y)
 involves deriving a distribution.
pY(y)= fX(x)pY X(y|x)dx Note: g(X, Y ) is a random variable
x|
Example:When not to nd themX: a continuous signal; prior fX(x)
(e.g., intensity of light beam); Dont need PDF for g(X, Y ) if only want
Y: discrete r.v. aected by X to compute expected value:
(e.g., photon count)E[g(X, Y )] = g(x, y)f (x, y)dx dy s tX,
X(y|x): model of the di cre e r. .
YpY v|
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-17-markov-chains-ii/</video_url>
          <video_title>Lecture 17: Markov Chains II</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high-quality, educational</text_slice>
            </slice>
            <slice>
              <time_slice>0:08</time_slice>
              <text_slice>resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:21</time_slice>
              <text_slice>PROFESSOR: All right.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>So today, we're going to start
by taking stock of what we</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>discussed last time, review the</text_slice>
            </slice>
            <slice>
              <time_slice>0:27</time_slice>
              <text_slice>definition of Markov chains.</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>And then most of the lecture,
we're going to concentrate on</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>their steady-state behavior.</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>Meaning, we're going to look at
what does a Markov chain do</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>if it has run for a long time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:40</time_slice>
              <text_slice>What can we say about
the probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>the different states?</text_slice>
            </slice>
            <slice>
              <time_slice>0:44</time_slice>
              <text_slice>So what I would like to repeat
is a statement I made last</text_slice>
            </slice>
            <slice>
              <time_slice>0:47</time_slice>
              <text_slice>time that Markov chains
is a very, very</text_slice>
            </slice>
            <slice>
              <time_slice>0:49</time_slice>
              <text_slice>useful class of models.</text_slice>
            </slice>
            <slice>
              <time_slice>0:51</time_slice>
              <text_slice>Pretty much anything in
the real world can be</text_slice>
            </slice>
            <slice>
              <time_slice>0:56</time_slice>
              <text_slice>approximately modeled by a
Markov chain provided that you</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>set your states in
the proper way.</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>So we're going to see
some examples.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>You're going to see more
examples in the problems</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>you're going to do in homework
and recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>On the other hand, we're
not going to go</text_slice>
            </slice>
            <slice>
              <time_slice>1:15</time_slice>
              <text_slice>too deep into examples.</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>Rather, we're going to develop
the general methodology.</text_slice>
            </slice>
            <slice>
              <time_slice>2:11</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>Markov models can be
pretty general.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>They can run in continuous
or discrete time.</text_slice>
            </slice>
            <slice>
              <time_slice>2:24</time_slice>
              <text_slice>They can have continuous or
discrete state spaces.</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>In this class, we're going to
stick just to the case where</text_slice>
            </slice>
            <slice>
              <time_slice>2:30</time_slice>
              <text_slice>the state space is discrete and
time is discrete because</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>this is the simplest case.</text_slice>
            </slice>
            <slice>
              <time_slice>2:35</time_slice>
              <text_slice>And also, it's the one where
you build your intuition</text_slice>
            </slice>
            <slice>
              <time_slice>2:38</time_slice>
              <text_slice>before going to more
general cases</text_slice>
            </slice>
            <slice>
              <time_slice>2:40</time_slice>
              <text_slice>perhaps in other classes.</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>So the state is discrete
and finite.</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>There's a finite number
of states.</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>At any point in time, the
process is sitting on one of</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>those states.</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>Time is discrete, so at each
unit of time, somebody</text_slice>
            </slice>
            <slice>
              <time_slice>3:00</time_slice>
              <text_slice>whistles and then
the state jumps.</text_slice>
            </slice>
            <slice>
              <time_slice>3:03</time_slice>
              <text_slice>And when it jumps, it can either
land in the same place,</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>or it can land somewhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>And the evolution of the
process is described by</text_slice>
            </slice>
            <slice>
              <time_slice>3:10</time_slice>
              <text_slice>transition probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>Pij is the probability that the
next state is j given that</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>the current state is i.</text_slice>
            </slice>
            <slice>
              <time_slice>3:18</time_slice>
              <text_slice>And the most important property
that the Markov chain</text_slice>
            </slice>
            <slice>
              <time_slice>3:21</time_slice>
              <text_slice>has, the definition of a
Markov chain or Markov</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>process, is that this
probability, Pij, is the same</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>every time that you
land at state i --</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>no matter how you got there
and also no matter</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>what time it is.</text_slice>
            </slice>
            <slice>
              <time_slice>3:38</time_slice>
              <text_slice>So the model we have is time
homogeneous, which basically</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>means that those transition
probabilities are the same at</text_slice>
            </slice>
            <slice>
              <time_slice>3:44</time_slice>
              <text_slice>every time.</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>So the model is time invariant
in that sense.</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>So we're interested in what the
chain or the process is</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>going to do in the longer run.</text_slice>
            </slice>
            <slice>
              <time_slice>3:54</time_slice>
              <text_slice>So we're interested, let's say,
in the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>starting at a certain state, n
times steps later, we find</text_slice>
            </slice>
            <slice>
              <time_slice>4:00</time_slice>
              <text_slice>ourselves at some particular
state j.</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>Fortunately, we can calculate
those probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>4:06</time_slice>
              <text_slice>recursively.</text_slice>
            </slice>
            <slice>
              <time_slice>4:07</time_slice>
              <text_slice>Of course, at the first time 1,
the probability of being 1</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>time later at state j given that
we are right now at state</text_slice>
            </slice>
            <slice>
              <time_slice>4:16</time_slice>
              <text_slice>i, by definition, this is just
the transition probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>So by knowing these, we can
start a recursion that tells</text_slice>
            </slice>
            <slice>
              <time_slice>4:26</time_slice>
              <text_slice>us the transition probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>4:27</time_slice>
              <text_slice>for more than n steps.</text_slice>
            </slice>
            <slice>
              <time_slice>4:31</time_slice>
              <text_slice>This recursion, it's
a formula.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>It's always true.</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>You can copy it or
memorize it.</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>But there is a big idea behind
that formula that you should</text_slice>
            </slice>
            <slice>
              <time_slice>4:40</time_slice>
              <text_slice>keep in mind.</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>And basically, the divide
and conquer idea.</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>It's an application of the
total probability law.</text_slice>
            </slice>
            <slice>
              <time_slice>4:47</time_slice>
              <text_slice>So let's fix i.</text_slice>
            </slice>
            <slice>
              <time_slice>4:49</time_slice>
              <text_slice>The probability that you find
yourself at state j, you break</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>it up into the probabilities of
the different ways that you</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>can get to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>4:57</time_slice>
              <text_slice>What are those different ways?</text_slice>
            </slice>
            <slice>
              <time_slice>4:59</time_slice>
              <text_slice>The different ways are the
different states k at which</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>you might find yourself
the previous time.</text_slice>
            </slice>
            <slice>
              <time_slice>5:05</time_slice>
              <text_slice>So with some probability, with
this probability, you find</text_slice>
            </slice>
            <slice>
              <time_slice>5:08</time_slice>
              <text_slice>yourself at state k
the previous time.</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>And then with probability
Pkj, you make a</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>transition to state j.</text_slice>
            </slice>
            <slice>
              <time_slice>5:15</time_slice>
              <text_slice>So this is a possible scenario
that takes you to state j</text_slice>
            </slice>
            <slice>
              <time_slice>5:19</time_slice>
              <text_slice>after n transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>And by summing over all the k's,
then we have considered</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>all the possible scenarios.</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>Now, before we move to the more
serious stuff, let's do a</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>little bit of warm up to get
a handle on how we use</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>transition probabilities to
calculate more general</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>probabilities, then talk about
some structural properties of</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>Markov chains, and then
eventually get to the main</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>business of today, which is
a steady-state behavior.</text_slice>
            </slice>
            <slice>
              <time_slice>5:51</time_slice>
              <text_slice>So somebody gives you this
chain, and our convention is</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>that those arcs that are not
shown here corresponds to 0</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>And each one of the arcs that's
shown has a non-zero</text_slice>
            </slice>
            <slice>
              <time_slice>6:05</time_slice>
              <text_slice>probability, and somebody
gives it to us.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>Suppose that the chain
starts at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>6:11</time_slice>
              <text_slice>We want to calculate the
probability that it follows</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>this particular path.</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>That is, it goes to 2,
then to 6, then to 7.</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>How do we calculate the
probability of a particular</text_slice>
            </slice>
            <slice>
              <time_slice>6:22</time_slice>
              <text_slice>trajectory?</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>Well, this is the
probability--</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>so it's the probability of the
trajectory from 1 that you go</text_slice>
            </slice>
            <slice>
              <time_slice>6:30</time_slice>
              <text_slice>to 2, then to 6, then to 7.</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>So the probability of this
trajectory is we use the</text_slice>
            </slice>
            <slice>
              <time_slice>6:38</time_slice>
              <text_slice>multiplication rule.</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>The probability of several
things happening is the</text_slice>
            </slice>
            <slice>
              <time_slice>6:42</time_slice>
              <text_slice>probability that the first
thing happens, which is a</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>transition from 1 to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>And then given that we are at
state 2, we multiply with a</text_slice>
            </slice>
            <slice>
              <time_slice>6:53</time_slice>
              <text_slice>conditional probability that
the next event happens.</text_slice>
            </slice>
            <slice>
              <time_slice>6:57</time_slice>
              <text_slice>That is, that X2 is equal to 6
given that right now, we are</text_slice>
            </slice>
            <slice>
              <time_slice>7:02</time_slice>
              <text_slice>at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>And that conditional probability
is just P26.</text_slice>
            </slice>
            <slice>
              <time_slice>7:07</time_slice>
              <text_slice>And notice that this conditional
probability</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>applies no matter how
we got to state 2.</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>This is the Markov assumption.</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>So we don't care about the
fact that we came in in a</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>particular way.</text_slice>
            </slice>
            <slice>
              <time_slice>7:19</time_slice>
              <text_slice>Given that we came in here, this
probability P26, that the</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>next transition takes us to 6.</text_slice>
            </slice>
            <slice>
              <time_slice>7:24</time_slice>
              <text_slice>And then given that all that
stuff happened, so given that</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>right now, we are at state 6,
we need to multiply with a</text_slice>
            </slice>
            <slice>
              <time_slice>7:32</time_slice>
              <text_slice>conditional probability that the
next transition takes us</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>to state 7.</text_slice>
            </slice>
            <slice>
              <time_slice>7:36</time_slice>
              <text_slice>And this is just the P67.</text_slice>
            </slice>
            <slice>
              <time_slice>7:39</time_slice>
              <text_slice>So to find the probability
of following a specific</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>trajectory, you just multiply
the transition probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>along the particular
trajectory.</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>Now, if you want to calculate
something else, such as for</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>example, the probability that
4 time steps later, I find</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>myself at state 7 given that
they started, let's say, at</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>this state.</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>How do you calculate
this probability?</text_slice>
            </slice>
            <slice>
              <time_slice>8:09</time_slice>
              <text_slice>One way is to use the recursion
for the Rijs that we</text_slice>
            </slice>
            <slice>
              <time_slice>8:15</time_slice>
              <text_slice>know that it is always valid.</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>But for short and simple
examples, and with a small</text_slice>
            </slice>
            <slice>
              <time_slice>8:20</time_slice>
              <text_slice>time horizon, perhaps you can do
this in a brute force way.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>What would be the
brute force way?</text_slice>
            </slice>
            <slice>
              <time_slice>8:27</time_slice>
              <text_slice>This is the event that 4 time
steps later, I find</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>myself at state 7.</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>This event can happen
in various ways.</text_slice>
            </slice>
            <slice>
              <time_slice>8:36</time_slice>
              <text_slice>So we can take stock of all the
different ways, and write</text_slice>
            </slice>
            <slice>
              <time_slice>8:41</time_slice>
              <text_slice>down their probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>8:42</time_slice>
              <text_slice>So starting from 2.</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>One possibility is to follow
this trajectory, 1 transition,</text_slice>
            </slice>
            <slice>
              <time_slice>8:49</time_slice>
              <text_slice>2 transitions, 3 transitions,
4 transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>8:53</time_slice>
              <text_slice>And that takes me to state 7.</text_slice>
            </slice>
            <slice>
              <time_slice>8:55</time_slice>
              <text_slice>What's the probability
of this trajectory?</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>It's P26 times P67 times
P76 and then times P67.</text_slice>
            </slice>
            <slice>
              <time_slice>9:05</time_slice>
              <text_slice>So this is a probability of a
particular trajectory that</text_slice>
            </slice>
            <slice>
              <time_slice>9:08</time_slice>
              <text_slice>takes you to state 7
after 4 time steps.</text_slice>
            </slice>
            <slice>
              <time_slice>9:11</time_slice>
              <text_slice>But there's other trajectories
as well.</text_slice>
            </slice>
            <slice>
              <time_slice>9:14</time_slice>
              <text_slice>What could be it?</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>I might start from state 2, go
to state 6, stay at state 6,</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>stay at state 6 once more.</text_slice>
            </slice>
            <slice>
              <time_slice>9:26</time_slice>
              <text_slice>And then from state
6, go to state 7.</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>And so there must be one more.</text_slice>
            </slice>
            <slice>
              <time_slice>9:36</time_slice>
              <text_slice>What's the other one?</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>I guess I could go 1, 2, 6, 7.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>That's the other trajectory.</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>Plus P21 times P12 times
P26 and times P67.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>So the transition probability,
the overall probability of</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>finding ourselves at state 7,
is broken down as the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>the probabilities of all the
different ways that I can get</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>to state 7 in exactly 4 steps.</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>So we could always do that
without knowing much about</text_slice>
            </slice>
            <slice>
              <time_slice>10:19</time_slice>
              <text_slice>Markov chains or the general
formula for the</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>Rij's that we had.</text_slice>
            </slice>
            <slice>
              <time_slice>10:24</time_slice>
              <text_slice>What's the trouble with
this procedure?</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>The trouble with this procedure
is that the number</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>of possible trajectories becomes
quite large if this</text_slice>
            </slice>
            <slice>
              <time_slice>10:34</time_slice>
              <text_slice>index is a little bigger.</text_slice>
            </slice>
            <slice>
              <time_slice>10:37</time_slice>
              <text_slice>If this 4 was 100, and you
ask how many different</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>trajectories of length 100 are
there to take me from here to</text_slice>
            </slice>
            <slice>
              <time_slice>10:45</time_slice>
              <text_slice>there, that number of
trajectories would be huge.</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>It grows exponentially with
the time horizon.</text_slice>
            </slice>
            <slice>
              <time_slice>10:51</time_slice>
              <text_slice>And this kind of calculation
would be impossible.</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>The basic equation, the
recursion that have for the</text_slice>
            </slice>
            <slice>
              <time_slice>10:58</time_slice>
              <text_slice>Rij's is basically a clever
way of organizing this</text_slice>
            </slice>
            <slice>
              <time_slice>11:01</time_slice>
              <text_slice>computation so that the amount
of computation that you do is</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>not exponential in
the time horizon.</text_slice>
            </slice>
            <slice>
              <time_slice>11:06</time_slice>
              <text_slice>Rather, it's sort of linear
with the time horizon.</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>For each time step you need in
the time horizon, you just</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>keep repeating the same
iteration over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>Now, the other thing that we
discussed last time, briefly,</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>was a classification of the
different states of the Markov</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>chain in two different types.</text_slice>
            </slice>
            <slice>
              <time_slice>11:31</time_slice>
              <text_slice>A Markov chain, in general, has
states that are recurrent,</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>which means that from a
recurrent state, I can go</text_slice>
            </slice>
            <slice>
              <time_slice>11:39</time_slice>
              <text_slice>somewhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>But from that somewhere else,
there's always some way of</text_slice>
            </slice>
            <slice>
              <time_slice>11:44</time_slice>
              <text_slice>coming back.</text_slice>
            </slice>
            <slice>
              <time_slice>11:46</time_slice>
              <text_slice>So if you have a chain of this
form, no matter where you go,</text_slice>
            </slice>
            <slice>
              <time_slice>11:49</time_slice>
              <text_slice>no matter where you start,
you can always come</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>back where you started.</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>States of this kind are
called recurrent.</text_slice>
            </slice>
            <slice>
              <time_slice>11:56</time_slice>
              <text_slice>On the other hand, if you have
a few states all this kind, a</text_slice>
            </slice>
            <slice>
              <time_slice>12:00</time_slice>
              <text_slice>transition of this type, then
these states are transient in</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>the sense that from those
states, it's possible to go</text_slice>
            </slice>
            <slice>
              <time_slice>12:07</time_slice>
              <text_slice>somewhere else from which place
there's no way to come</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>back where you started.</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>The general structure of a
Markov chain is basically a</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>collection of transient
states.</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>You're certain that you are
going to leave the transient</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>states eventually.</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>And after you leave the
transient states, you enter</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>into a class of states in
which you are trapped.</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>You are trapped if you
get inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>You are trapped if you
get inside there.</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>This is a recurrent
class of states.</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>From any state, you can
get to any other</text_slice>
            </slice>
            <slice>
              <time_slice>12:43</time_slice>
              <text_slice>state within this class.</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>That's another recurrent
class.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>From any state inside here,
you can get anywhere else</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>inside that class.</text_slice>
            </slice>
            <slice>
              <time_slice>12:50</time_slice>
              <text_slice>But these 2 classes, you
do not communicate.</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>If you start here, there's
no way to get there.</text_slice>
            </slice>
            <slice>
              <time_slice>12:56</time_slice>
              <text_slice>If you have 2 recurrent classes,
then it's clear that</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>the initial conditions
of your Markov chain</text_slice>
            </slice>
            <slice>
              <time_slice>13:02</time_slice>
              <text_slice>matter in the long run.</text_slice>
            </slice>
            <slice>
              <time_slice>13:04</time_slice>
              <text_slice>If you start here, you will be
stuck inside here for the long</text_slice>
            </slice>
            <slice>
              <time_slice>13:07</time_slice>
              <text_slice>run and similarly about here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>So the initial conditions
do make a difference.</text_slice>
            </slice>
            <slice>
              <time_slice>13:11</time_slice>
              <text_slice>On the other hand, if this class
was not here and you</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>only had that class, what would
happen to the chain?</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>Let's say you start here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>You move around.</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>At some point, you make
that transition.</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>You get stuck in here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>And inside here, you keep
circulating, because of the</text_slice>
            </slice>
            <slice>
              <time_slice>13:26</time_slice>
              <text_slice>randomness, you keep visiting
all states over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>13:30</time_slice>
              <text_slice>And hopefully or possibly, in
the long run, it doesn't</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>matter exactly what time it is
or where you started, but the</text_slice>
            </slice>
            <slice>
              <time_slice>13:39</time_slice>
              <text_slice>probability of being at that
particular state is the same</text_slice>
            </slice>
            <slice>
              <time_slice>13:43</time_slice>
              <text_slice>no matter what the initial
condition was.</text_slice>
            </slice>
            <slice>
              <time_slice>13:46</time_slice>
              <text_slice>So with a single recurrent
class, we hope that the</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>initial conditions
do not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>13:50</time_slice>
              <text_slice>With 2 or more recurrent
classes, initial conditions</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>will definitely matter.</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>So how many recurrent classes we
have is something that has</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>to do with the long-term
behavior of the chain and the</text_slice>
            </slice>
            <slice>
              <time_slice>14:06</time_slice>
              <text_slice>extent to which initial
conditions matter.</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>Another way that initial
conditions may matter is if a</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>chain has a periodic
structure.</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>There are many ways of
defining periodicity.</text_slice>
            </slice>
            <slice>
              <time_slice>14:21</time_slice>
              <text_slice>The one that I find sort of the
most intuitive and with</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>the least amount
of mathematical</text_slice>
            </slice>
            <slice>
              <time_slice>14:27</time_slice>
              <text_slice>symbols is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>14:29</time_slice>
              <text_slice>The state space of a chain is
said to be periodic if you can</text_slice>
            </slice>
            <slice>
              <time_slice>14:34</time_slice>
              <text_slice>lump the states into a number
of clusters called</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>d clusters or groups.</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>And the transition diagram has
the property that from a</text_slice>
            </slice>
            <slice>
              <time_slice>14:45</time_slice>
              <text_slice>cluster, you always
make a transition</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>into the next cluster.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>So here d is equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>14:52</time_slice>
              <text_slice>We have two subsets of
the state space.</text_slice>
            </slice>
            <slice>
              <time_slice>14:55</time_slice>
              <text_slice>Whenever we're here, next
time we'll be there.</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>Whenever we're here, next
time we will be there.</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>So this chain has a periodic
structure.</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>There may be still
some randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>15:05</time_slice>
              <text_slice>When I jump from here to here,
the state to which I jump may</text_slice>
            </slice>
            <slice>
              <time_slice>15:10</time_slice>
              <text_slice>be random, but I'm sure that I'm
going to be inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:14</time_slice>
              <text_slice>And then next time, I will be
sure that I'm inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>This would be a structure of a
diagram in which we have a</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>period of 3.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>If you start in this lump, you
know that the next time, you</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>would be in a state
inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>Next time, you'll be in a state
inside here, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>So these chains certainly have
a periodic structure.</text_slice>
            </slice>
            <slice>
              <time_slice>15:35</time_slice>
              <text_slice>And that periodicity
gets maintained.</text_slice>
            </slice>
            <slice>
              <time_slice>15:37</time_slice>
              <text_slice>If I start, let's say, at this
lump, at even times,</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>I'm sure I'm here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>At odd times, I'm
sure I am here.</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>So the exact time does matter
in determining the</text_slice>
            </slice>
            <slice>
              <time_slice>15:51</time_slice>
              <text_slice>probabilities of the
different states.</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>And in particular, the
probability of being at the</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>particular state cannot convert
to a state value.</text_slice>
            </slice>
            <slice>
              <time_slice>16:00</time_slice>
              <text_slice>The probability of being at the
state inside here is going</text_slice>
            </slice>
            <slice>
              <time_slice>16:03</time_slice>
              <text_slice>to be 0 for all times.</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>In general, it's going
to be some positive</text_slice>
            </slice>
            <slice>
              <time_slice>16:08</time_slice>
              <text_slice>number for even times.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>So it goes 0 positive, zero,
positive, 0 positive.</text_slice>
            </slice>
            <slice>
              <time_slice>16:13</time_slice>
              <text_slice>Doesn't settle to anything.</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>So when we have periodicity,
we do not expect the states</text_slice>
            </slice>
            <slice>
              <time_slice>16:19</time_slice>
              <text_slice>probabilities to converge to
something, but rather, we</text_slice>
            </slice>
            <slice>
              <time_slice>16:22</time_slice>
              <text_slice>expect them to oscillate.</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>Now, how can we tell whether
a Markov chain</text_slice>
            </slice>
            <slice>
              <time_slice>16:26</time_slice>
              <text_slice>is periodic or not?</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>There are systematic ways of
doing it, but usually with the</text_slice>
            </slice>
            <slice>
              <time_slice>16:33</time_slice>
              <text_slice>types of examples we see in this
class, we just eyeball</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>the chain, and we tell whether
it's periodic or not.</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>So is this chain down here, is
it the periodic one or not?</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>How many people think
it's periodic?</text_slice>
            </slice>
            <slice>
              <time_slice>16:48</time_slice>
              <text_slice>No one.</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>One.</text_slice>
            </slice>
            <slice>
              <time_slice>16:51</time_slice>
              <text_slice>How many people think
it's not periodic?</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>Not periodic?</text_slice>
            </slice>
            <slice>
              <time_slice>16:56</time_slice>
              <text_slice>Let's see.</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>Let me do some drawing here.</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>Is it periodic?</text_slice>
            </slice>
            <slice>
              <time_slice>17:07</time_slice>
              <text_slice>It is.</text_slice>
            </slice>
            <slice>
              <time_slice>17:09</time_slice>
              <text_slice>From a red state, you can only
get to a white state.</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>And from a white state, you can
only get to a red state.</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>So this chain, even though it's
not apparent from the</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>picture, actually has
this structure.</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>We can group the states into red
states and white states.</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>And from reds, we always go to
a white, and from a white, we</text_slice>
            </slice>
            <slice>
              <time_slice>17:32</time_slice>
              <text_slice>always go to a red.</text_slice>
            </slice>
            <slice>
              <time_slice>17:34</time_slice>
              <text_slice>So this tells you
that sometimes</text_slice>
            </slice>
            <slice>
              <time_slice>17:36</time_slice>
              <text_slice>eyeballing is not as easy.</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>If you have lots and lots of
states, you might have some</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>trouble doing this exercise.</text_slice>
            </slice>
            <slice>
              <time_slice>17:43</time_slice>
              <text_slice>On the other hand, something
very useful to know.</text_slice>
            </slice>
            <slice>
              <time_slice>17:47</time_slice>
              <text_slice>Sometimes it's extremely
easy to tell that the</text_slice>
            </slice>
            <slice>
              <time_slice>17:50</time_slice>
              <text_slice>chain is not periodic.</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>What's that case?</text_slice>
            </slice>
            <slice>
              <time_slice>17:53</time_slice>
              <text_slice>Suppose that your chain has a
self-transition somewhere.</text_slice>
            </slice>
            <slice>
              <time_slice>17:58</time_slice>
              <text_slice>Then automatically,
you know that your</text_slice>
            </slice>
            <slice>
              <time_slice>18:01</time_slice>
              <text_slice>chain is not periodic.</text_slice>
            </slice>
            <slice>
              <time_slice>18:04</time_slice>
              <text_slice>So remember, the definition of
periodicity requires that if</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>you are in a certain group of
states, next time, you will be</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>in a different group.</text_slice>
            </slice>
            <slice>
              <time_slice>18:14</time_slice>
              <text_slice>But if you have
self-transitions, that</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>property is not true.</text_slice>
            </slice>
            <slice>
              <time_slice>18:17</time_slice>
              <text_slice>If you have a possible
self-transition, it's possible</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>that you stay inside your own
group for the next time step.</text_slice>
            </slice>
            <slice>
              <time_slice>18:24</time_slice>
              <text_slice>So whenever you have a
self-transition, this implies</text_slice>
            </slice>
            <slice>
              <time_slice>18:29</time_slice>
              <text_slice>that the chain is
not periodic.</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>And usually that's the simplest
and easy way that we</text_slice>
            </slice>
            <slice>
              <time_slice>18:39</time_slice>
              <text_slice>can tell most of the time that
the chain is not periodic.</text_slice>
            </slice>
            <slice>
              <time_slice>18:44</time_slice>
              <text_slice>So now, we come to the big topic
of today, the central</text_slice>
            </slice>
            <slice>
              <time_slice>18:49</time_slice>
              <text_slice>topic, which is the question
about what does the chain do</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>in the long run.</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>The question we are asking and
which we motivated last time</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>by looking at an example.</text_slice>
            </slice>
            <slice>
              <time_slice>19:02</time_slice>
              <text_slice>It's something that did happen
in our example of last time.</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>So we're asking whether
this happens for</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>every Markov chain.</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>We're asking the question
whether the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>being at state j at some
time n settles to a</text_slice>
            </slice>
            <slice>
              <time_slice>19:18</time_slice>
              <text_slice>steady-state value.</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>Let's call it pi sub j.</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>That these were asking whether
this quantity has a limit as n</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>goes to infinity, so that
we can talk about the</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>steady-state probability
of state j.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>And furthermore, we asked
whether the steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>probability of that state
does not depend</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>on the initial state.</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>In other words, after the chain
runs for a long, long</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>time, it doesn't matter exactly
what time it is, and</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>it doesn't matter where the
chain started from.</text_slice>
            </slice>
            <slice>
              <time_slice>19:51</time_slice>
              <text_slice>You can tell me the probability
that the state is</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>a particular j is approximately
the steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>19:58</time_slice>
              <text_slice>probability pi sub j.</text_slice>
            </slice>
            <slice>
              <time_slice>20:00</time_slice>
              <text_slice>It doesn't matter exactly what
time it is as long as you tell</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>me that a lot of time
has elapsed so</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>that n is a big number.</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>So this is the question.</text_slice>
            </slice>
            <slice>
              <time_slice>20:11</time_slice>
              <text_slice>We have seen examples, and we
understand that this is not</text_slice>
            </slice>
            <slice>
              <time_slice>20:14</time_slice>
              <text_slice>going to be the case always.</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>For example, as I just
discussed, if we have 2</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>recurrent classes, where
we start does matter.</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>The probability pi(j) of being
in that state j is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>20:28</time_slice>
              <text_slice>0 if we start here, but it would
be something positive if</text_slice>
            </slice>
            <slice>
              <time_slice>20:32</time_slice>
              <text_slice>we were to start in that lump.</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>So the initial state does matter
if we have multiple</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>recurrent classes.</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>But if we have only a single
class of recurrent states from</text_slice>
            </slice>
            <slice>
              <time_slice>20:45</time_slice>
              <text_slice>each one of which you can get
to any other one, then we</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>don't have that problem.</text_slice>
            </slice>
            <slice>
              <time_slice>20:49</time_slice>
              <text_slice>Then we expect initial
conditions to be forgotten.</text_slice>
            </slice>
            <slice>
              <time_slice>20:53</time_slice>
              <text_slice>So that's one condition
that we need.</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>And then the other condition
that we need is that the chain</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>is not periodic.</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>If the chain is periodic, then
these Rij's do not converge.</text_slice>
            </slice>
            <slice>
              <time_slice>21:07</time_slice>
              <text_slice>They keep oscillating.</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>If we do not have periodicity,
then there is hope that we</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>will get the convergence
that we need.</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>It turns out this is the big
theory of Markov chains-- the</text_slice>
            </slice>
            <slice>
              <time_slice>21:19</time_slice>
              <text_slice>steady-state convergence
theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>It turns out that yes, the
rijs do converge to a</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>steady-state limit, which
we call a steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>21:29</time_slice>
              <text_slice>probability as long as these two
conditions are satisfied.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>We're not going to prove
this theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>If you're really interested, the
end of chapter exercises</text_slice>
            </slice>
            <slice>
              <time_slice>21:41</time_slice>
              <text_slice>basically walk you through a
proof of this result, but it's</text_slice>
            </slice>
            <slice>
              <time_slice>21:45</time_slice>
              <text_slice>probably a little too much for
doing it in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>21:49</time_slice>
              <text_slice>What is the intuitive idea
behind this theorem?</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>Let's see.</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>Let's think intuitively
as to why the initial</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>state doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>21:59</time_slice>
              <text_slice>Think of two copies of the chain
that starts at different</text_slice>
            </slice>
            <slice>
              <time_slice>22:02</time_slice>
              <text_slice>initial states, and the
state moves randomly.</text_slice>
            </slice>
            <slice>
              <time_slice>22:06</time_slice>
              <text_slice>As the state moves randomly
starting from the two initial</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>states a random trajectory.</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>as long as you have a single
recurrent class at some point,</text_slice>
            </slice>
            <slice>
              <time_slice>22:15</time_slice>
              <text_slice>and you don't have periodicity
at some point, those states,</text_slice>
            </slice>
            <slice>
              <time_slice>22:19</time_slice>
              <text_slice>those two trajectories,
are going to collide.</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>Just because there's enough
randomness there.</text_slice>
            </slice>
            <slice>
              <time_slice>22:25</time_slice>
              <text_slice>Even though we started from
different places, the state is</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>going to be the same.</text_slice>
            </slice>
            <slice>
              <time_slice>22:30</time_slice>
              <text_slice>After the state becomes the
same, then the future of these</text_slice>
            </slice>
            <slice>
              <time_slice>22:33</time_slice>
              <text_slice>trajectories, probabilistically,
is the same</text_slice>
            </slice>
            <slice>
              <time_slice>22:37</time_slice>
              <text_slice>because they both started
at the same state.</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>So this means that the
initial conditions</text_slice>
            </slice>
            <slice>
              <time_slice>22:42</time_slice>
              <text_slice>stopped having any influence.</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>That's sort of the high-level
idea of why the initial state</text_slice>
            </slice>
            <slice>
              <time_slice>22:50</time_slice>
              <text_slice>gets forgotten.</text_slice>
            </slice>
            <slice>
              <time_slice>22:50</time_slice>
              <text_slice>Even if you started at different
initial states, at</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>some time, you may find yourself
to be in the same</text_slice>
            </slice>
            <slice>
              <time_slice>22:57</time_slice>
              <text_slice>state as the other trajectory.</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>And once that happens, your
initial conditions cannot have</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>any effect into the future.</text_slice>
            </slice>
            <slice>
              <time_slice>23:08</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>So let's see how we might
calculate those steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>23:16</time_slice>
              <text_slice>The way we calculate the
steady-state probabilities is</text_slice>
            </slice>
            <slice>
              <time_slice>23:19</time_slice>
              <text_slice>by taking this recursion, which
is always true for the</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>end-step transition
probabilities, and take the</text_slice>
            </slice>
            <slice>
              <time_slice>23:27</time_slice>
              <text_slice>limit of both sides.</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>The limit of this side is the
steady-state probability of</text_slice>
            </slice>
            <slice>
              <time_slice>23:32</time_slice>
              <text_slice>state j, which is pi sub j.</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>The limit of this
side, we put the</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>limit inside the summation.</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>Now, as n goes to infinity,
n - also goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>23:44</time_slice>
              <text_slice>So this Rik is going to be the
steady-state probability of</text_slice>
            </slice>
            <slice>
              <time_slice>23:48</time_slice>
              <text_slice>state k starting from state i.</text_slice>
            </slice>
            <slice>
              <time_slice>23:51</time_slice>
              <text_slice>Now where we started
doesn't matter.</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>So this is just the
steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>23:54</time_slice>
              <text_slice>probability of state k.</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>So this term converges to that
one, and this gives us an</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>equation that's satisfied
by the steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>24:03</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>Actually, it's not
one equation.</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>We get one equation for
each one of the j's.</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>So if we have 10 possible
states, we're going to get the</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>system of 10 linear equations.</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>In the unknowns, pi(1)
up to pi(10).</text_slice>
            </slice>
            <slice>
              <time_slice>24:18</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>10 unknowns, 10 equations.</text_slice>
            </slice>
            <slice>
              <time_slice>24:20</time_slice>
              <text_slice>You might think that
we are in business.</text_slice>
            </slice>
            <slice>
              <time_slice>24:23</time_slice>
              <text_slice>But actually, this system of
equations is singular.</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>0 is a possible solution
of this system.</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>If you plug pi equal to
zero everywhere, the</text_slice>
            </slice>
            <slice>
              <time_slice>24:32</time_slice>
              <text_slice>equations are satisfied.</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>It does not have a unique
solution, so maybe we need one</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>more condition to get the
uniquely solvable system of</text_slice>
            </slice>
            <slice>
              <time_slice>24:40</time_slice>
              <text_slice>linear equations.</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>It turns out that this
system of equations</text_slice>
            </slice>
            <slice>
              <time_slice>24:44</time_slice>
              <text_slice>has a unique solution.</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>If you impose an additional
condition, which is pretty</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>natural, the pi(j)'s are the
probabilities of the different</text_slice>
            </slice>
            <slice>
              <time_slice>24:52</time_slice>
              <text_slice>states, so they should
add to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>So you want this one equation
to the mix.</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>And once you do that, then this
system of equations is</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>going to have a unique
solution.</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>And so we can find the
steady-state probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>the Markov chain by just
solving these linear</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>equations, which is numerically
straightforward.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>Now, these equations are
quite important.</text_slice>
            </slice>
            <slice>
              <time_slice>25:18</time_slice>
              <text_slice>I mean, they're the central
point in the Markov chain.</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>They have a name.</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>They're called the balance
equations.</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>And it's worth interpreting
them in a</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>somewhat different way.</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>So intuitively, one can
sometimes think of</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>probabilities as frequencies.</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>For example, if I toss an
unbiased coin, probability 1/2</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>of heads, you could also say
that if I keep flipping that</text_slice>
            </slice>
            <slice>
              <time_slice>25:49</time_slice>
              <text_slice>coin, in the long run,
1/2 of the time, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>going to see heads.</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>Similarly, let's try an
interpretation of this pi(j),</text_slice>
            </slice>
            <slice>
              <time_slice>25:58</time_slice>
              <text_slice>the steady-state probability,
the long-term probability of</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>finding myself at state j.</text_slice>
            </slice>
            <slice>
              <time_slice>26:04</time_slice>
              <text_slice>Let's try to interpret it as
the frequency with which I</text_slice>
            </slice>
            <slice>
              <time_slice>26:08</time_slice>
              <text_slice>find myself at state j if
I run a very, very long</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>trajectory over that
Markov chain.</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>So the trajectory moves
around, visits states.</text_slice>
            </slice>
            <slice>
              <time_slice>26:18</time_slice>
              <text_slice>It visits the different states
with different frequencies.</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>And let's think of the
probability that you are at a</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>certain state as being sort of
the same as the frequency of</text_slice>
            </slice>
            <slice>
              <time_slice>26:32</time_slice>
              <text_slice>visiting that state.</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>This turns out to be a
correct statement.</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>If you were more rigorous, you
would have to prove it.</text_slice>
            </slice>
            <slice>
              <time_slice>26:41</time_slice>
              <text_slice>But it's an interpretation which
is valid and which gives</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>us a lot of intuition about what
these equation is saying.</text_slice>
            </slice>
            <slice>
              <time_slice>26:48</time_slice>
              <text_slice>So let's think as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>Let's focus on a particular
state j, and think of</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>transitions into the state j
versus transitions out of the</text_slice>
            </slice>
            <slice>
              <time_slice>27:00</time_slice>
              <text_slice>state j, or transitions into
j versus transitions</text_slice>
            </slice>
            <slice>
              <time_slice>27:05</time_slice>
              <text_slice>starting from j.</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>So transition starting
from that includes a</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>self-transition.</text_slice>
            </slice>
            <slice>
              <time_slice>27:14</time_slice>
              <text_slice>Ok.</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>So how often do we get a
transition, if we interpret</text_slice>
            </slice>
            <slice>
              <time_slice>27:18</time_slice>
              <text_slice>the pi(j)'s as frequencies,
how often do we get a</text_slice>
            </slice>
            <slice>
              <time_slice>27:21</time_slice>
              <text_slice>transition into j?</text_slice>
            </slice>
            <slice>
              <time_slice>27:23</time_slice>
              <text_slice>Here's how we think about it.</text_slice>
            </slice>
            <slice>
              <time_slice>27:25</time_slice>
              <text_slice>A fraction pi(1) of the time,
we're going to be at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>Whenever we are at state 1,
there's going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>probability, P1j, that we make
a transition of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>So out of the times that we're
at state 1, there's a</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>frequency, P1j with which the
next transition is into j.</text_slice>
            </slice>
            <slice>
              <time_slice>27:53</time_slice>
              <text_slice>So out of the overall number of
transitions that happen at</text_slice>
            </slice>
            <slice>
              <time_slice>27:57</time_slice>
              <text_slice>the trajectory, what fraction
of those transitions is</text_slice>
            </slice>
            <slice>
              <time_slice>28:01</time_slice>
              <text_slice>exactly of that kind?</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>That fraction of transitions is
the fraction of time that</text_slice>
            </slice>
            <slice>
              <time_slice>28:06</time_slice>
              <text_slice>you find yourself at 1 times the
fraction with which out of</text_slice>
            </slice>
            <slice>
              <time_slice>28:11</time_slice>
              <text_slice>one you happen to visit
next state j.</text_slice>
            </slice>
            <slice>
              <time_slice>28:15</time_slice>
              <text_slice>So we interpreted this number
as the frequency of</text_slice>
            </slice>
            <slice>
              <time_slice>28:19</time_slice>
              <text_slice>transitions of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>28:21</time_slice>
              <text_slice>At any given time, our chain
can do transitions of</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>different kinds, transitions of
the general form from some</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>k, I go to some l.</text_slice>
            </slice>
            <slice>
              <time_slice>28:30</time_slice>
              <text_slice>So we try to do some
accounting.</text_slice>
            </slice>
            <slice>
              <time_slice>28:33</time_slice>
              <text_slice>How often does a transition of
each particular kind happen?</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>And this is the frequency with
which transitions of that</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>particular kind happens.</text_slice>
            </slice>
            <slice>
              <time_slice>28:42</time_slice>
              <text_slice>Now, what's the total
frequency of</text_slice>
            </slice>
            <slice>
              <time_slice>28:44</time_slice>
              <text_slice>transitions into state j?</text_slice>
            </slice>
            <slice>
              <time_slice>28:46</time_slice>
              <text_slice>Transitions into state j can
happen by having a transition</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>from 1 to j, from 2 to j,
or from state m to j.</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>So to find the total frequency
with which we would observe</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>transitions into j is going
to be this particular sum.</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>Now, you are at state j if and
only if the last transition</text_slice>
            </slice>
            <slice>
              <time_slice>29:09</time_slice>
              <text_slice>was into state j.</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>So the frequency with which you
are at j is the frequency</text_slice>
            </slice>
            <slice>
              <time_slice>29:16</time_slice>
              <text_slice>with which transitions
into j happen.</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>So this equation expresses
exactly that statement.</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>The probability of being at
state j is the sum of the</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>probabilities that the last
transition was into state j.</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>Or in terms of frequencies, the
frequency with which you</text_slice>
            </slice>
            <slice>
              <time_slice>29:35</time_slice>
              <text_slice>find yourself at state j is the
sum of the frequencies of</text_slice>
            </slice>
            <slice>
              <time_slice>29:39</time_slice>
              <text_slice>all the possible transition
types that take you</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>inside state j.</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>So that's a useful intuition
to have, and we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>29:47</time_slice>
              <text_slice>see an example a little later
that it gives us short cuts</text_slice>
            </slice>
            <slice>
              <time_slice>29:52</time_slice>
              <text_slice>into analyzing Markov chains.</text_slice>
            </slice>
            <slice>
              <time_slice>29:55</time_slice>
              <text_slice>But before we move,
let's revisit the</text_slice>
            </slice>
            <slice>
              <time_slice>29:58</time_slice>
              <text_slice>example from last time.</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>And let us write down
the balance</text_slice>
            </slice>
            <slice>
              <time_slice>30:03</time_slice>
              <text_slice>equations for this example.</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>So the steady-state probability
that I find myself</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>at state 1 is the probability
that the previous time I was</text_slice>
            </slice>
            <slice>
              <time_slice>30:16</time_slice>
              <text_slice>at state 1 and I made
a self-transition--</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>So the probability that I was
here last time and I made a</text_slice>
            </slice>
            <slice>
              <time_slice>30:25</time_slice>
              <text_slice>transition of this kind, plus
the probability that the last</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>time I was here and I made a
transition of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>30:32</time_slice>
              <text_slice>So plus pi(2) times 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>And similarly, for the other
states, the steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>probably that I find myself at
state 2 is the probability</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>that last time I was at state 1
and I made a transition into</text_slice>
            </slice>
            <slice>
              <time_slice>30:50</time_slice>
              <text_slice>state 2, plus the probability
that the last time I was at</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>state 2 and I made the
transition into state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:57</time_slice>
              <text_slice>Now, these are two
equations and two</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>unknowns, pi(1) and pi(2).</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>But you notice that both of
these equations tell you the</text_slice>
            </slice>
            <slice>
              <time_slice>31:06</time_slice>
              <text_slice>same thing.</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>They tell you that 0.5pi(1)
equals 0.2pi(2).</text_slice>
            </slice>
            <slice>
              <time_slice>31:17</time_slice>
              <text_slice>Either of these equations tell
you exactly this if you move</text_slice>
            </slice>
            <slice>
              <time_slice>31:21</time_slice>
              <text_slice>terms around.</text_slice>
            </slice>
            <slice>
              <time_slice>31:22</time_slice>
              <text_slice>So these two equations are
not really two equations.</text_slice>
            </slice>
            <slice>
              <time_slice>31:25</time_slice>
              <text_slice>It's just one equation.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>They are linearly dependent
equations, and in order to</text_slice>
            </slice>
            <slice>
              <time_slice>31:30</time_slice>
              <text_slice>solve the problem, we need the
additional condition that</text_slice>
            </slice>
            <slice>
              <time_slice>31:33</time_slice>
              <text_slice>pi(1) + pi(2) is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>31:36</time_slice>
              <text_slice>Now, we have our system
of two equations,</text_slice>
            </slice>
            <slice>
              <time_slice>31:38</time_slice>
              <text_slice>which you can solve.</text_slice>
            </slice>
            <slice>
              <time_slice>31:39</time_slice>
              <text_slice>And once you solve it, you find
that pi(1) is 2/7 and</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>pi(2) is 5/7.</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>So these are the steady state
probabilities of the two</text_slice>
            </slice>
            <slice>
              <time_slice>31:52</time_slice>
              <text_slice>different states.</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>If we start this chain, at some
state, let's say state 1,</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>and we let it run for a long,
long time, the chain settles</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>into steady state.</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>What does that mean?</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>It does not mean that
the state itself</text_slice>
            </slice>
            <slice>
              <time_slice>32:12</time_slice>
              <text_slice>enters steady state.</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>The state will keep jumping
around forever and ever.</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>It will keep visiting both
states once in a while.</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>So the jumping never ceases.</text_slice>
            </slice>
            <slice>
              <time_slice>32:23</time_slice>
              <text_slice>The thing that gets into
steady state is the</text_slice>
            </slice>
            <slice>
              <time_slice>32:25</time_slice>
              <text_slice>probability of finding
yourself at state 1.</text_slice>
            </slice>
            <slice>
              <time_slice>32:30</time_slice>
              <text_slice>So the probability that you find
yourself at state 1 at</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>time one trillion is
approximately 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>32:37</time_slice>
              <text_slice>The probability you find
yourself at state 1 at time</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>two trillions is again,
approximately 2/7.</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>So the probability of being in
that state settles into a</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>steady value.</text_slice>
            </slice>
            <slice>
              <time_slice>32:52</time_slice>
              <text_slice>That's what the steady-state
convergence means.</text_slice>
            </slice>
            <slice>
              <time_slice>32:55</time_slice>
              <text_slice>It's convergence of
probabilities, not convergence</text_slice>
            </slice>
            <slice>
              <time_slice>32:58</time_slice>
              <text_slice>of the process itself.</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>And again, the two main things
that are happening in this</text_slice>
            </slice>
            <slice>
              <time_slice>33:04</time_slice>
              <text_slice>example, and more generally,
when we have a single class</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>and no periodicity, is
that the initial</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>state does not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>33:12</time_slice>
              <text_slice>There's enough randomness here
so that no matter where you</text_slice>
            </slice>
            <slice>
              <time_slice>33:15</time_slice>
              <text_slice>start, the randomness kind of
washes out any memory of where</text_slice>
            </slice>
            <slice>
              <time_slice>33:19</time_slice>
              <text_slice>you started.</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>And also in this example,
clearly, we do not have</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>periodicity because
we have self arcs.</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>And this, in particular, implies
that the exact time</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>does not matter.</text_slice>
            </slice>
            <slice>
              <time_slice>33:35</time_slice>
              <text_slice>So now, we're going to spend
the rest of our time by</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>looking into a special class
of chains that's a little</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>easier to deal with,
but still, it's</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>an important class.</text_slice>
            </slice>
            <slice>
              <time_slice>33:49</time_slice>
              <text_slice>So what's the moral from here?</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>This was a simple example with
two states, and we could find</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>the steady-state probabilities
by solving a simple system of</text_slice>
            </slice>
            <slice>
              <time_slice>33:59</time_slice>
              <text_slice>two-by-two equations.</text_slice>
            </slice>
            <slice>
              <time_slice>34:01</time_slice>
              <text_slice>If you have a chain with 100
states, it's no problem for a</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>computer to solve a system
of 100-by-100 equations.</text_slice>
            </slice>
            <slice>
              <time_slice>34:09</time_slice>
              <text_slice>But you can certainly not do it
by hand, and usually, you</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>cannot get any closed-form
formulas, so you do not</text_slice>
            </slice>
            <slice>
              <time_slice>34:15</time_slice>
              <text_slice>necessarily get a
lot of insight.</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>So one looks for special
structures or models that</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>maybe give you a little more
insight or maybe lead you to</text_slice>
            </slice>
            <slice>
              <time_slice>34:25</time_slice>
              <text_slice>closed-form formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>And an interesting subclass of
Markov chains in which all of</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>these nice things do happen,
is the class</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>of birth/death processes.</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>So what's a birth/death
process?</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>It's a Markov chain who's
diagram looks</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>basically like this.</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>So the states of the Markov
chain start from 0 and go up</text_slice>
            </slice>
            <slice>
              <time_slice>34:52</time_slice>
              <text_slice>to some finite integer m.</text_slice>
            </slice>
            <slice>
              <time_slice>34:54</time_slice>
              <text_slice>What's special about this chain
is that if you are at a</text_slice>
            </slice>
            <slice>
              <time_slice>34:57</time_slice>
              <text_slice>certain state, next time you can
either go up by 1, you can</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>go down by 1, or you
can stay in place.</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>So it's like keeping track
of some population</text_slice>
            </slice>
            <slice>
              <time_slice>35:09</time_slice>
              <text_slice>at any given time.</text_slice>
            </slice>
            <slice>
              <time_slice>35:11</time_slice>
              <text_slice>One person gets born,
or one person</text_slice>
            </slice>
            <slice>
              <time_slice>35:13</time_slice>
              <text_slice>dies, or nothing happens.</text_slice>
            </slice>
            <slice>
              <time_slice>35:15</time_slice>
              <text_slice>Again, we're not accounting
for twins here.</text_slice>
            </slice>
            <slice>
              <time_slice>35:19</time_slice>
              <text_slice>So we're given this structure,
and we are given the</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>transition probabilities, the
probabilities associated with</text_slice>
            </slice>
            <slice>
              <time_slice>35:27</time_slice>
              <text_slice>transitions of the
different types.</text_slice>
            </slice>
            <slice>
              <time_slice>35:29</time_slice>
              <text_slice>So we use P's for the upward
transitions, Q's for the</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>downward transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>35:34</time_slice>
              <text_slice>An example of a chain of this
kind was the supermarket</text_slice>
            </slice>
            <slice>
              <time_slice>35:37</time_slice>
              <text_slice>counter model that we
discussed last time.</text_slice>
            </slice>
            <slice>
              <time_slice>35:40</time_slice>
              <text_slice>That is, a customer arrives,
so this increments</text_slice>
            </slice>
            <slice>
              <time_slice>35:45</time_slice>
              <text_slice>the state by 1.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>Or a customer finishes service,
in which case, the</text_slice>
            </slice>
            <slice>
              <time_slice>35:49</time_slice>
              <text_slice>state gets decremented by 1,
or nothing happens in which</text_slice>
            </slice>
            <slice>
              <time_slice>35:53</time_slice>
              <text_slice>you stay in place, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>35:55</time_slice>
              <text_slice>In the supermarket model, these
P's inside here were all</text_slice>
            </slice>
            <slice>
              <time_slice>35:59</time_slice>
              <text_slice>taken to be equal because we
assume that the arrival rate</text_slice>
            </slice>
            <slice>
              <time_slice>36:03</time_slice>
              <text_slice>was sort of constant
at each time slot.</text_slice>
            </slice>
            <slice>
              <time_slice>36:07</time_slice>
              <text_slice>But you can generalize a little
bit by assuming that</text_slice>
            </slice>
            <slice>
              <time_slice>36:10</time_slice>
              <text_slice>these transition probabilities
P1 here, P2 there, and so on</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>may be different from
state to state.</text_slice>
            </slice>
            <slice>
              <time_slice>36:18</time_slice>
              <text_slice>So in general, from state
i, there's going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>transition probability
Pi that the next</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>transition is upwards.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>And there's going to be a
probability Qi that the next</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>transition is downwards.</text_slice>
            </slice>
            <slice>
              <time_slice>36:31</time_slice>
              <text_slice>And so from that state, the
probability that the next</text_slice>
            </slice>
            <slice>
              <time_slice>36:35</time_slice>
              <text_slice>transition is downwards is
going to be Q_(i+1).</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>So this is the structure
of our chain.</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>As I said, it's a crude model
of what happens at the</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>supermarket counter but it's
also a good model for lots of</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>types of service systems.</text_slice>
            </slice>
            <slice>
              <time_slice>36:55</time_slice>
              <text_slice>Again, you have a server
somewhere that has a buffer.</text_slice>
            </slice>
            <slice>
              <time_slice>36:59</time_slice>
              <text_slice>Jobs come into the buffer.</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>So the buffer builds up.</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>The server processes jobs, so
the buffer keeps going down.</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>And the state of the chain would
be the number of jobs</text_slice>
            </slice>
            <slice>
              <time_slice>37:10</time_slice>
              <text_slice>that you have inside
your buffer.</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>Or you could be thinking about
active phone calls out of a</text_slice>
            </slice>
            <slice>
              <time_slice>37:18</time_slice>
              <text_slice>certain city.</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>Each time that the phone call
is placed, the number of</text_slice>
            </slice>
            <slice>
              <time_slice>37:22</time_slice>
              <text_slice>active phone calls
goes up by 1.</text_slice>
            </slice>
            <slice>
              <time_slice>37:24</time_slice>
              <text_slice>Each time that the phone call
stops happening, is</text_slice>
            </slice>
            <slice>
              <time_slice>37:28</time_slice>
              <text_slice>terminated, then the count
goes down by 1.</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>So it's for processes of this
kind that a model with this</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>structure is going to show up.</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>And they do show up in
many, many models.</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>Or you can think about the
number of people in a certain</text_slice>
            </slice>
            <slice>
              <time_slice>37:43</time_slice>
              <text_slice>population that have
a disease.</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>So 1 more person gets the
flu, the count goes up.</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>1 more person gets healed,
the count goes down.</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>And these probabilities in such
an epidemic model would</text_slice>
            </slice>
            <slice>
              <time_slice>37:58</time_slice>
              <text_slice>certainly depend on
the current state.</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>If lots of people already have
the flu, the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>another person catches it
would be pretty high.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>Whereas, if no one has the flu,
then the probability that</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>you get a transition where
someone catches the flu, that</text_slice>
            </slice>
            <slice>
              <time_slice>38:16</time_slice>
              <text_slice>probability would
be pretty small.</text_slice>
            </slice>
            <slice>
              <time_slice>38:18</time_slice>
              <text_slice>So the transition rates, the
incidence of new people who</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>have the disease definitely
depends on how many people</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>already have the disease.</text_slice>
            </slice>
            <slice>
              <time_slice>38:31</time_slice>
              <text_slice>And that motivates cases where
those P's, the upward</text_slice>
            </slice>
            <slice>
              <time_slice>38:34</time_slice>
              <text_slice>transition probabilities,
depend on the</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>state of the chain.</text_slice>
            </slice>
            <slice>
              <time_slice>38:42</time_slice>
              <text_slice>So how do we study this chain?</text_slice>
            </slice>
            <slice>
              <time_slice>38:44</time_slice>
              <text_slice>You can sit down and write the
system of n linear equations</text_slice>
            </slice>
            <slice>
              <time_slice>38:49</time_slice>
              <text_slice>in the pi's.</text_slice>
            </slice>
            <slice>
              <time_slice>38:50</time_slice>
              <text_slice>And this way, find the
steady-state probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>38:52</time_slice>
              <text_slice>this chain.</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>But this is a little harder.</text_slice>
            </slice>
            <slice>
              <time_slice>38:55</time_slice>
              <text_slice>It's more work than one
actually needs to do.</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>There's a very clever shortcut
that applies</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>to birth/death processes.</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>And it's based on the frequency
interpretation that</text_slice>
            </slice>
            <slice>
              <time_slice>39:08</time_slice>
              <text_slice>we discussed a little
while ago.</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>Let's put a line somewhere in
the middle of this chain, and</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>focus on the relation between
this part and that part in</text_slice>
            </slice>
            <slice>
              <time_slice>39:21</time_slice>
              <text_slice>more detail.</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>So think of the chain continuing
in this direction,</text_slice>
            </slice>
            <slice>
              <time_slice>39:25</time_slice>
              <text_slice>that direction.</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>But let's just focus on 2
adjacent states, and look at</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>this particular cut.</text_slice>
            </slice>
            <slice>
              <time_slice>39:32</time_slice>
              <text_slice>What is the chain going to do?</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>Let's say it starts here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>It's going to move around.</text_slice>
            </slice>
            <slice>
              <time_slice>39:37</time_slice>
              <text_slice>At some point, it makes a
transition to the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>39:40</time_slice>
              <text_slice>And that's a transition
from i to i+1.</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>It stays on the other
side for some time.</text_slice>
            </slice>
            <slice>
              <time_slice>39:45</time_slice>
              <text_slice>It gets here, and eventually,
it's going to make a</text_slice>
            </slice>
            <slice>
              <time_slice>39:48</time_slice>
              <text_slice>transition to this side.</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>Then it keeps moving
and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>Now, there's a certain balance
that must be obeyed here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:57</time_slice>
              <text_slice>The number of upward transitions
through this line</text_slice>
            </slice>
            <slice>
              <time_slice>40:01</time_slice>
              <text_slice>cannot be very different from
the number of downward</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>40:06</time_slice>
              <text_slice>Because we cross this
way, then next time,</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>we'll cross that way.</text_slice>
            </slice>
            <slice>
              <time_slice>40:10</time_slice>
              <text_slice>Then next time, we'll
cross this way.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>We'll cross that way.</text_slice>
            </slice>
            <slice>
              <time_slice>40:13</time_slice>
              <text_slice>So the frequency with which
transitions of this kind occur</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>has to be the same as the
long-term frequency that</text_slice>
            </slice>
            <slice>
              <time_slice>40:21</time_slice>
              <text_slice>transitions of that
kind occur.</text_slice>
            </slice>
            <slice>
              <time_slice>40:24</time_slice>
              <text_slice>You cannot go up 100 times and
go down only 50 times.</text_slice>
            </slice>
            <slice>
              <time_slice>40:28</time_slice>
              <text_slice>If you have gone up 100 times,
it means that you have gone</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>down 99, or 100, or 101,
but nothing much more</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>different than that.</text_slice>
            </slice>
            <slice>
              <time_slice>40:38</time_slice>
              <text_slice>So the frequency with
which transitions of</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>this kind get observed.</text_slice>
            </slice>
            <slice>
              <time_slice>40:43</time_slice>
              <text_slice>That is, out of a large number
of transitions, what fraction</text_slice>
            </slice>
            <slice>
              <time_slice>40:47</time_slice>
              <text_slice>of transitions are
of these kind?</text_slice>
            </slice>
            <slice>
              <time_slice>40:49</time_slice>
              <text_slice>That fraction has to be the
same as the fraction of</text_slice>
            </slice>
            <slice>
              <time_slice>40:52</time_slice>
              <text_slice>transitions that happened
to be of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>What are these fractions?</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>We discussed that before.</text_slice>
            </slice>
            <slice>
              <time_slice>40:58</time_slice>
              <text_slice>The fraction of times at which
transitions of this kind are</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>observed is the fraction of time
that we happen to be at</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>that state.</text_slice>
            </slice>
            <slice>
              <time_slice>41:09</time_slice>
              <text_slice>And out of the times that we
are in that state, the</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>fraction of transitions that
happen to be upward</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>transitions.</text_slice>
            </slice>
            <slice>
              <time_slice>41:16</time_slice>
              <text_slice>So this is the frequency with
which transitions of this kind</text_slice>
            </slice>
            <slice>
              <time_slice>41:20</time_slice>
              <text_slice>are observed.</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>And with the same argument,
this is the frequency with</text_slice>
            </slice>
            <slice>
              <time_slice>41:25</time_slice>
              <text_slice>which transitions of that
kind are observed.</text_slice>
            </slice>
            <slice>
              <time_slice>41:28</time_slice>
              <text_slice>Since these two frequencies
are the same, these two</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>numbers must be the same, and
we get an equation that</text_slice>
            </slice>
            <slice>
              <time_slice>41:34</time_slice>
              <text_slice>relates the Pi to P_(i+1).</text_slice>
            </slice>
            <slice>
              <time_slice>41:38</time_slice>
              <text_slice>This has a nice form because
it gives us a recursion.</text_slice>
            </slice>
            <slice>
              <time_slice>41:43</time_slice>
              <text_slice>If we knew pi(i), we could then</text_slice>
            </slice>
            <slice>
              <time_slice>41:45</time_slice>
              <text_slice>immediately calculate pi(i+1).</text_slice>
            </slice>
            <slice>
              <time_slice>41:48</time_slice>
              <text_slice>So it's a system of equations
that's very</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>easy to solve almost.</text_slice>
            </slice>
            <slice>
              <time_slice>41:54</time_slice>
              <text_slice>But how do we get started?</text_slice>
            </slice>
            <slice>
              <time_slice>41:57</time_slice>
              <text_slice>If I knew pi(0), I could find
by pi(1) and then use this</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>recursion to find pi(2),
pi(3), and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>42:05</time_slice>
              <text_slice>But we don't know pi(0).</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>It's one more unknown.</text_slice>
            </slice>
            <slice>
              <time_slice>42:09</time_slice>
              <text_slice>It's an unknown, and we need
to actually use the extra</text_slice>
            </slice>
            <slice>
              <time_slice>42:14</time_slice>
              <text_slice>normalization condition that
the sum of the pi's is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>And after we use that
normalization condition, then</text_slice>
            </slice>
            <slice>
              <time_slice>42:23</time_slice>
              <text_slice>we can find all of the pi's.</text_slice>
            </slice>
            <slice>
              <time_slice>42:32</time_slice>
              <text_slice>So you basically fix pi(0) as a
symbol, solve this equation</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>symbolically, and
everything gets</text_slice>
            </slice>
            <slice>
              <time_slice>42:41</time_slice>
              <text_slice>expressed in terms of pi(0).</text_slice>
            </slice>
            <slice>
              <time_slice>42:43</time_slice>
              <text_slice>And then use that normalization
condition to</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>find pi(0), and you're done.</text_slice>
            </slice>
            <slice>
              <time_slice>42:48</time_slice>
              <text_slice>Let's illustrate the details
of this procedure on a</text_slice>
            </slice>
            <slice>
              <time_slice>42:51</time_slice>
              <text_slice>particular special case.</text_slice>
            </slice>
            <slice>
              <time_slice>42:53</time_slice>
              <text_slice>So in our special case, we're
going to simplify things now</text_slice>
            </slice>
            <slice>
              <time_slice>42:57</time_slice>
              <text_slice>by assuming that all those
upward P's are the same, and</text_slice>
            </slice>
            <slice>
              <time_slice>43:01</time_slice>
              <text_slice>all of those downward
Q's are the same.</text_slice>
            </slice>
            <slice>
              <time_slice>43:05</time_slice>
              <text_slice>So at each point in time, if
you're sitting somewhere in</text_slice>
            </slice>
            <slice>
              <time_slice>43:08</time_slice>
              <text_slice>the middle, you have probability
P of moving up and</text_slice>
            </slice>
            <slice>
              <time_slice>43:13</time_slice>
              <text_slice>probability Q of moving down.</text_slice>
            </slice>
            <slice>
              <time_slice>43:16</time_slice>
              <text_slice>This rho, the ratio of P/Q is
frequency of going up versus</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>frequency of going down.</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>If it's a service system, you
can think of it as a measure</text_slice>
            </slice>
            <slice>
              <time_slice>43:29</time_slice>
              <text_slice>of how loaded the system is.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>If P is equal to Q, it's means
that if you're at this state,</text_slice>
            </slice>
            <slice>
              <time_slice>43:39</time_slice>
              <text_slice>you're equally likely to move
left or right, so the system</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>is kind of balanced.</text_slice>
            </slice>
            <slice>
              <time_slice>43:44</time_slice>
              <text_slice>The state doesn't have a
tendency to move in this</text_slice>
            </slice>
            <slice>
              <time_slice>43:46</time_slice>
              <text_slice>direction or in that
direction.</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>If rho is bigger than 1 so that
P is bigger than Q, it</text_slice>
            </slice>
            <slice>
              <time_slice>43:53</time_slice>
              <text_slice>means that whenever I'm at some
state in the middle, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>more likely to move right rather
than move left, which</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>means that my state, of course
it's random, but it has a</text_slice>
            </slice>
            <slice>
              <time_slice>44:04</time_slice>
              <text_slice>tendency to move in
that direction.</text_slice>
            </slice>
            <slice>
              <time_slice>44:07</time_slice>
              <text_slice>And if you think of this as a
number of customers in queue,</text_slice>
            </slice>
            <slice>
              <time_slice>44:10</time_slice>
              <text_slice>it means your system has the
tendency to become loaded and</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>to build up a queue.</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>So rho being bigger than 1
corresponds to a heavy load,</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>where queues build up.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>Rho less than 1 corresponds to
the system where queues have</text_slice>
            </slice>
            <slice>
              <time_slice>44:25</time_slice>
              <text_slice>the tendency to drain down.</text_slice>
            </slice>
            <slice>
              <time_slice>44:30</time_slice>
              <text_slice>Now, let's write down
the equations.</text_slice>
            </slice>
            <slice>
              <time_slice>44:32</time_slice>
              <text_slice>We have this recursion P_(i+1)
is Pi times Pi over Qi.</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>In our case here, the P's and
the Q's do not depend on the</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>particular index, so we
get this relation.</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>And this P over Q is just
the load factor rho.</text_slice>
            </slice>
            <slice>
              <time_slice>44:51</time_slice>
              <text_slice>Once you look at this equation,
clearly you realize</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>that by pi(1) is rho
times pi(0).</text_slice>
            </slice>
            <slice>
              <time_slice>44:58</time_slice>
              <text_slice>pi(2) is going to be --</text_slice>
            </slice>
            <slice>
              <time_slice>45:02</time_slice>
              <text_slice>So we'll do it in detail.</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>So pi(1) is pi(0) times rho.</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>pi(2) is pi(1) times rho,
which is pi(0) times</text_slice>
            </slice>
            <slice>
              <time_slice>45:15</time_slice>
              <text_slice>rho-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>And then you continue doing
this calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>45:21</time_slice>
              <text_slice>And you find that you can
express every pi(i) in terms</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>of pi(0) and you get this
factor of rho^i.</text_slice>
            </slice>
            <slice>
              <time_slice>45:31</time_slice>
              <text_slice>And then you use the last
equation that we have -- that</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>the sum of the probabilities
has to be equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:38</time_slice>
              <text_slice>And that equation is going to
tell us that the sum over all</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>i's from 0 to m of pi(0) rho
to the i is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>And therefore, pi(0) is 1 over
(the sum over the rho to the i</text_slice>
            </slice>
            <slice>
              <time_slice>45:58</time_slice>
              <text_slice>for i going from 0 to m).</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>So now we found pi(0), and by
plugging in this expression,</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>we have the steady-state
probabilities of all of the</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>different states.</text_slice>
            </slice>
            <slice>
              <time_slice>46:14</time_slice>
              <text_slice>Let's look at some special
cases of this.</text_slice>
            </slice>
            <slice>
              <time_slice>46:18</time_slice>
              <text_slice>Suppose that rho
is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>If rho is equal to 1, then
pi(i) is equal to pi(0).</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>It means that all
the steady-state</text_slice>
            </slice>
            <slice>
              <time_slice>46:33</time_slice>
              <text_slice>probabilities are equal.</text_slice>
            </slice>
            <slice>
              <time_slice>46:35</time_slice>
              <text_slice>It's means that every
state is equally</text_slice>
            </slice>
            <slice>
              <time_slice>46:39</time_slice>
              <text_slice>likely in the long run.</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>So this is an example.</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>It's called a symmetric
random walk.</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>It's a very popular model for
modeling people who are drunk.</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>So you start at a state
at any point in time.</text_slice>
            </slice>
            <slice>
              <time_slice>46:56</time_slice>
              <text_slice>Either you stay in place, or you
have an equal probability</text_slice>
            </slice>
            <slice>
              <time_slice>47:00</time_slice>
              <text_slice>of going left or going right.</text_slice>
            </slice>
            <slice>
              <time_slice>47:02</time_slice>
              <text_slice>There's no bias in
either direction.</text_slice>
            </slice>
            <slice>
              <time_slice>47:06</time_slice>
              <text_slice>You might think that in such a
process, you will tend to kind</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>of get stuck near one end
or the other end.</text_slice>
            </slice>
            <slice>
              <time_slice>47:14</time_slice>
              <text_slice>Well, it's not really clear
what to expect.</text_slice>
            </slice>
            <slice>
              <time_slice>47:17</time_slice>
              <text_slice>It turns out that in such a
model, in the long run, the</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>drunk person is equally
likely to be at any</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>one of those states.</text_slice>
            </slice>
            <slice>
              <time_slice>47:26</time_slice>
              <text_slice>The steady-state probability is
the same for all i's if rho</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>47:33</time_slice>
              <text_slice>And so if you show up at a
random time, and you ask where</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>is my state, you will be told
it's equally likely to be at</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>any one of those places.</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>So let's make that note.</text_slice>
            </slice>
            <slice>
              <time_slice>47:48</time_slice>
              <text_slice>If rho equal to 1, implies
that all the</text_slice>
            </slice>
            <slice>
              <time_slice>47:51</time_slice>
              <text_slice>pi(i)'s are 1/(M+1) --</text_slice>
            </slice>
            <slice>
              <time_slice>47:57</time_slice>
              <text_slice>M+1 because that's how many
states we have in our model.</text_slice>
            </slice>
            <slice>
              <time_slice>48:01</time_slice>
              <text_slice>Now, let's look at
a different case.</text_slice>
            </slice>
            <slice>
              <time_slice>48:04</time_slice>
              <text_slice>Suppose that M is
a huge number.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>So essentially, our supermarket
has a very large</text_slice>
            </slice>
            <slice>
              <time_slice>48:13</time_slice>
              <text_slice>space, a lot of space to
store their customers.</text_slice>
            </slice>
            <slice>
              <time_slice>48:19</time_slice>
              <text_slice>But suppose that the system
is on the stable side.</text_slice>
            </slice>
            <slice>
              <time_slice>48:24</time_slice>
              <text_slice>P is less than Q, which means
that there's a tendency for</text_slice>
            </slice>
            <slice>
              <time_slice>48:27</time_slice>
              <text_slice>customers to be served faster
than they arrive.</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>The drift in this chain, it
tends to be in that direction.</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>So when rho is less than 1,
which is this case, and when M</text_slice>
            </slice>
            <slice>
              <time_slice>48:41</time_slice>
              <text_slice>is going to infinity, this
infinite sum is the sum of a</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>geometric series.</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>And you recognize it
(hopefully) --</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>this series is going
to 1/(1-rho).</text_slice>
            </slice>
            <slice>
              <time_slice>48:56</time_slice>
              <text_slice>And because it's in the
denominator, pi(0) ends up</text_slice>
            </slice>
            <slice>
              <time_slice>49:00</time_slice>
              <text_slice>being 1-rho.</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>So by taking the limit as M
goes to infinity, in this</text_slice>
            </slice>
            <slice>
              <time_slice>49:06</time_slice>
              <text_slice>case, and when rho is less than
1 so that this series is</text_slice>
            </slice>
            <slice>
              <time_slice>49:09</time_slice>
              <text_slice>convergent, we get
this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>49:12</time_slice>
              <text_slice>So we get the closed-form
formula for the pi(i)'s.</text_slice>
            </slice>
            <slice>
              <time_slice>49:15</time_slice>
              <text_slice>In particular, pi(i) is (1-
rho)(rho to the i).</text_slice>
            </slice>
            <slice>
              <time_slice>49:19</time_slice>
              <text_slice>to</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>So these pi(i)'s are essentially
a probability</text_slice>
            </slice>
            <slice>
              <time_slice>49:24</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>49:26</time_slice>
              <text_slice>They tell us if we show up at
time 1 billion and we ask,</text_slice>
            </slice>
            <slice>
              <time_slice>49:32</time_slice>
              <text_slice>where is my state?</text_slice>
            </slice>
            <slice>
              <time_slice>49:33</time_slice>
              <text_slice>You will be told that
the state is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>49:37</time_slice>
              <text_slice>Your system is empty with
probability 1-rho, minus or</text_slice>
            </slice>
            <slice>
              <time_slice>49:41</time_slice>
              <text_slice>there's one customer in the
system, and that happens with</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>probability (rho
- 1) times rho.</text_slice>
            </slice>
            <slice>
              <time_slice>49:46</time_slice>
              <text_slice>And it keeps going
down this way.</text_slice>
            </slice>
            <slice>
              <time_slice>49:49</time_slice>
              <text_slice>And it's pretty much a geometric
distribution except</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>that it has shifted so that it
starts at 0 whereas the usual</text_slice>
            </slice>
            <slice>
              <time_slice>49:58</time_slice>
              <text_slice>geometric distribution
starts at 1.</text_slice>
            </slice>
            <slice>
              <time_slice>50:00</time_slice>
              <text_slice>So this is a mini introduction
into queuing theory.</text_slice>
            </slice>
            <slice>
              <time_slice>50:04</time_slice>
              <text_slice>This is the first and simplest
model that one encounters when</text_slice>
            </slice>
            <slice>
              <time_slice>50:08</time_slice>
              <text_slice>you start studying
queuing theory.</text_slice>
            </slice>
            <slice>
              <time_slice>50:10</time_slice>
              <text_slice>This is clearly a model of a
queueing phenomenon such as</text_slice>
            </slice>
            <slice>
              <time_slice>50:13</time_slice>
              <text_slice>the supermarket counter with
the P's corresponding to</text_slice>
            </slice>
            <slice>
              <time_slice>50:16</time_slice>
              <text_slice>arrivals, the Q's corresponding
to departures.</text_slice>
            </slice>
            <slice>
              <time_slice>50:19</time_slice>
              <text_slice>And this particular queuing
system when M is very, very</text_slice>
            </slice>
            <slice>
              <time_slice>50:22</time_slice>
              <text_slice>large and rho is less than 1,
has a very simple and nice</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>solution in closed form.</text_slice>
            </slice>
            <slice>
              <time_slice>50:28</time_slice>
              <text_slice>And that's why it's
very much liked.</text_slice>
            </slice>
            <slice>
              <time_slice>50:31</time_slice>
              <text_slice>And let me just take
two seconds to</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>draw one last picture.</text_slice>
            </slice>
            <slice>
              <time_slice>50:38</time_slice>
              <text_slice>So this is the probability
of the different i's.</text_slice>
            </slice>
            <slice>
              <time_slice>50:40</time_slice>
              <text_slice>It gives you a PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>This PMF has an expected
value.</text_slice>
            </slice>
            <slice>
              <time_slice>50:43</time_slice>
              <text_slice>And the expectation, the
expected number of customers</text_slice>
            </slice>
            <slice>
              <time_slice>50:47</time_slice>
              <text_slice>in the system, is given
by this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>And this formula, which is
interesting to anyone who</text_slice>
            </slice>
            <slice>
              <time_slice>50:54</time_slice>
              <text_slice>tries to analyze a system
of this kind,</text_slice>
            </slice>
            <slice>
              <time_slice>50:57</time_slice>
              <text_slice>tells you the following.</text_slice>
            </slice>
            <slice>
              <time_slice>50:58</time_slice>
              <text_slice>That as long as a rho is less
than 1, then the expected</text_slice>
            </slice>
            <slice>
              <time_slice>51:03</time_slice>
              <text_slice>number of customers in
the system is finite.</text_slice>
            </slice>
            <slice>
              <time_slice>51:07</time_slice>
              <text_slice>But if rho becomes very
close to 1 --</text_slice>
            </slice>
            <slice>
              <time_slice>51:09</time_slice>
              <text_slice>So if your load factor is
something like .99, you expect</text_slice>
            </slice>
            <slice>
              <time_slice>51:13</time_slice>
              <text_slice>to have a large number of
customers in the system at any</text_slice>
            </slice>
            <slice>
              <time_slice>51:17</time_slice>
              <text_slice>given time.</text_slice>
            </slice>
            <slice>
              <time_slice>51:19</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>51:20</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>51:22</time_slice>
              <text_slice>Have a good weekend.</text_slice>
            </slice>
            <slice>
              <time_slice>51:23</time_slice>
              <text_slice>We'll continue next time.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Bayesian Statistical Inference - II (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l22/</lecture_pdf_url>
      <lectureno>22</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 22f()
Readings: pp. 225-226; Sections 8.3-8.41/6
4 10 
Topics
fX(x |)(Bayesian) Least means squares (LMS)|
estimation 1/2
(Bayesian) Linear LMS estimation 
1 +1
 X =g(EstimatorX)
fX(x )| |g( )f()

x 10
MAP estimate:  maximizes f|X(MAP|x)
LMS estimation:
=E[ |X] minimizes ( g())2E X
over all estimators g() 
4
for any x,=E[ |X=x]
minimizes ( )2E 
over a
|X=x
xll estimates 
3 5 9 11y 
Conditional mean squared error Some properties of LMS estimation
E[(E[ |])2X |X=x] Estimator: =E[ |X]
same as Var( |X=x): variance of the Estimation error: =
conditional distribution of 

x E[]=0 E[X=x]=010|
E[h(X)] = 0, for any function h
cov( ,)=0
4
Since=
x:
3 5 9 11y var( )=v a r ( )+v a r ( )
x |
(|
10 VarX=x):
onal distr|
ibution of
4
x
3 5 9 11y 
p()
Np
X|(x |)
X

p()
Np
X|(x |)
Estimatorf()
g()
=g(X)fX|(x |)
g()
=g(X)p()
N
pX|(x |)
X
Estimatorf()
fX|(x |)
=g(X)f()
fX|(x |)
g( )f()
fX|(x |)
g()
=g(X)
1/2
1
+1f()
fX|(x |)
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
x 
410
3 5 9 11y f()
fX|(x |) x
g()
=g(X)
1/2
1
+1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
x 
410
3 5 9 11y f()
fX|(x |) x
g()
=g(X)
1/2
1
+1E stim a tion wit h discrete da t a
f |X( |x) =f ( )pX| (x| )
pX(x)
pX(x) = 
f ( )pX| (x| )d 
E xa m ple:EstimatorY  
1EstimatorY XN 
pY|X(|)
X 
pY|X(|)
X
 
 
pY|X(y|x)
X
 
pY(y; )N 
pY|X(y|x)
X
 
pY(y; ) 
pY|X(y|x)
X
 
pY(y; )
pX(x) 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W pW(w)
Y=X+W 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W pW(w)
Y=X+W+ 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W fW(w)
Y=X+W
object at unknown location X
sensors
pX| ( 1 | ) =
=P(se nsor i se nses  t h e o b je c t | = )
=h( dist a n c e o f  fro m se nsor i)
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 4 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 
p()
Np
X|(x|)
X
EstimatorW
 fW(w) {0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|)
X

E st i m a t or
WfW(w) {0,1} X=+W
f() 1 / 6 4 1 0
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
1/6 4 10 

p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimatorf()
fX|(x |)
g()
=g(X)f()
fX|(x |)
g()
=g(X)
p()
Np
X|(x |)
X
Estimatorf()
fX|(x |)
g()
=g(X)f()
fX|(x |)
g()
=g(X)f()
g()
=g(X)
1/2
1
+1f()
fX|(x |)
g()
=g(X)
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
x 
410
3 5 9 11y f()
fX|(x |) x
g()
=g(X)
1/2
1
+1

p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimatorf()
fX!(x !)
g(")
=g(X)f()
fX|(x |)
g()
=g(X)
p()
Np
X|(x |)
X
Estimatorf()
fX|(x |)
g()
=g(X)f()
fX|(x |)
g()
=g(X)f
fX|x|
g 
 g X
/

f()
fX|(x |)
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10
f()
fX|(x |)
g()
=g(X)
1/2
1
+1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10
f()
fX|(x |)
g()
=g(X)
1/2
1
+1f()
fX|(x |)
g()
=g(X)
1/2
1
+1Conditional mean squared error
E[(E[X])2X=x]
same as variance of the
conditi 
Predicting Xbased on Y
Two r.v.s X,Y
we observe that Y=y
new universe: condition on Y=y
E
(Xc)2|Y=y
is minimized by
c=
View predictor as a function g(y)
E[X |Y] minimizes
E[(Xg(Y))2]
over all predictors g()
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Linear LMS Linear LMS properties
Consider estimators of ,Cov(X, ) L=E[]+ (of the form =aX+Xbvar(X)E[X])
Minimize E
(  b)2aX
[(2 2)2EL)]=( 1  
Best choice of a,b; best linear estimator:
Linear LMS with multiple data
Cov(X, )L=E[]+ (XE[X])var(X) Consider estimators of the form:
 =a1X1++anXn+b
x 10Find best choices of a1,...,a n,b
Minimize:
E[( ++ + )2a1X1 anXnb ]
Set derivatives to zero
4 linear system in band the ai
Only means, variances, covariances matter
x
3 5 9 11y 
The cleanest linear LMS example Big picture
Standard examples:Xi=+Wi,,W 1,...,W nindependent
202,0Wi ,iXiuniform on [0, ];
2/ +0
=i
L nn
2 uniform prior on Xi/i
=1
XiBernoulli(p );
12/i uniform (or Beta) prior on p
i=0
(weighted average of , X 1,...,X n) Xinormal with mean , known variance2;
normal prior on ;If all normal, L=E[ |X1,...,X n]
Xi=+Wi
Estimation methods:
Choosing Xiin linear LMS MAP
E[ |X] is the same as E[ |3X] MSE
Linear LMS is di erent: Linear MSE
= + ver us  3aX b s = aX +b
Also consider =3a1X+2a2X+a3X+b
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimatorf()
fX!(x !)
g(")
=g(X)f()
fX|(x |)
g()
=g(X)
p()
Np
X|(x |)
X
Estimatorf()
fX|(x |)
g()
=g(X)f()
fX|(x |)
g()
=g(X)f
fX|x|
g 
 g X
/

f()
fX|(x |)
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1f()
fX|(x |) x
g()
=g(X)
1/2
1
+1
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10
f()
fX|(x |)
g()
=g(X)
1/2
1
+1
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-18-markov-chains-iii/</video_url>
          <video_title>Lecture 18: Markov Chains III</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Bernoulli Process (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 13 The Bernoulli process
A sequence of independentThe Bernoulli process
Bernoulli trials
Readings: Section 6.1
At each trial, i:
P(success) = P(Xi= 1) = p
Lecture outline
P(failure) = P(Xi= 0) = 1 p
Denition of Bernoulli process
Examples:Random processes
Sequence of lottery wins/losses
Basic properties of Bernoulli process
Sequence of ups and downs of the Dow
Distribution of interarrival times Jones
Arrivals (each second) to a bankThe time of the kth success
Arrivals (at each time slot) to server
Merging and splitting
Random processes Number of successes Sinntime slots
First view: P(S=k)=
sequence of random variables X1,X2,...
E[Xt]= E[S]=
Var( Xt)=
Var( S)=
Second view:
what is the right sample space?
P(Xt=1f o ra l l t)=
Random processes we will study:
Bernoulli process(memoryless, discrete time)
Poisson process(memoryless, continuous time)
Markov chains(with memory/dependence across time)
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Interarrival times Time of the kth arrival
T1: number of trials until rst success Given that rst arrival was at time t
i.e., T1=t:
P(T1=t)=additional time, T2, until next arrival
Memoryless property has the same (geometric) distribution
independent of T1E[T1]=
Var( T1)=Yk: number of trials to kth success
E[YIf you buy a lottery ticket every day, whatk]=
is the distribution of the length of the
rst string of losing days? Var( Yk)=
P(Yk=t)=
Splitting of a Bernoulli Process Merging of Indep. Bernoulli Processes
(using independent coin ips)
Bernoulli (p)
time
timeMerged process:
Bernoulli (p q pq q + )time
Original
processtimeBernoulli (q)
1  q time
time
yields a Bernoulli process
yields Bernoulli processes (collisions are counted as one arrival)Sec. 6.1 The Bernoulli Process 305
Splitting and Merging of Bernoulli Processes
Starting with a Bernoulli process in which there is a probability pof an arrival
at each time, consider splitting it as follows. Whenever there is an arrival, we
choose to either keep it (with probability q), or to discard it (with probability
1q); see Fig. 6.3. Assume that the decisions to keep or discard are independent
for di erent arrivals. If we focus on the process of arrivals that are kept, we see
that it is a Bernoulli process: in each time slot, there is a probability pqof a
kept arrival, independent of what happens in other slots. For the same reason,
the process of discarded arrivals is also a Bernoulli process, with a probabilityof a discarded arrival at each time slot equal to p(1q).
Figure 6.3: Splitting of a Bernoulli process.
In a reverse situation, we start with two independent Bernoulli processes
(with parameters pand q, respectively) and merge them into a single process,
as follows. An arrival is recorded in the merged process if and only if thereis an arrival in at least one of the two original processes. This happens withprobability p+qpq[one minus the probability (1 p)(1q) of no arrival in
either process]. Since di erent time slots in either of the original processes are
independent, dierent slots in the merged process are also independent. Thus,the merged process is Bernoulli, with success probability p+qpqat each time
step; see Fig. 6.4.
Splitting and merging of Bernoulli (or other) arrival processes arises in
many contexts. For example, a two-machine work center may see a stream ofarriving parts to be processed and split them by sending each part to a randomlychosen machine. Conversely, a machine may be faced with arrivals of di erent
types that can be merged into a single arrival stream.
The Poisson Approximation to the BinomialThe number of successes in nindependent Bernoulli trials is a binomial random
variable with parameters nand p, and its mean is np. In this subsection, we306 The Bernoulli and Poisson Processes Chap. 6
Figure 6.4: Merging of independent Bernoulli processes.
concentrate on the special case where nis large but pis small, so that the mean
nphas a moderate value. A situation of this type arises when one passes from
discrete to continuous time, a theme to be picked up in the next section. For
some examples, think of the number of airplane accidents on any given day:there is a large number nof trials (airplane ights), but each one has a very
small probability pof being involved in an accident. Or think of counting the
number of typos in a book: there is a large number of words, but a very smallprobability of misspelling any single one.
Mathematically, we can address situations of this kind, by letting ngrow
while simultaneously decreasing p, in a manner that keeps the product npat a
constant value . In the limit, it turns out that the formula for the binomial PMF
simplies to the Poisson PMF. A precise statement is provided next, togetherwith a reminder of some of the properties of the Poisson PMF that were derivedin Chapter 2.
Poisson Approximation to the Binomial
A Poisson random variable Zwith parameter takes nonnegative
integer values and is described by the PMF
pZ(k)=ek
k!,k =0,1,2,....
Its mean and variance are given by
E[Z]=, var(Z)=.
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-19-weak-law-of-large-numbers/</video_url>
          <video_title>Lecture 19: Weak Law of Large Numbers</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:17</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:21</time_slice>
              <text_slice>JOHN TSITSIKLIS: We're going
to start today a new unit.</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>so we will be talking about
limit theorems.</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>So just to introduce the topic,
let's think of the</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>following situation.</text_slice>
            </slice>
            <slice>
              <time_slice>0:35</time_slice>
              <text_slice>There's a population
of penguins down</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>at the South Pole.</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>And if you were to pick a
penguin at random and measure</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>their height, the expected value
of their height would be</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>the average of the heights of
the different penguins in the</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>population.</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>So suppose when you
pick one, every</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>penguin is equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>Then the expected value is just
the average of all the</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>penguins out there.</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>So your boss asks you to
find out what that the</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>expected value is.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>One way would be to
go and measure</text_slice>
            </slice>
            <slice>
              <time_slice>1:04</time_slice>
              <text_slice>each and every penguin.</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>That might be a little
time consuming.</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>So alternatively, what you can
do is to go and pick penguins</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>at random, pick a few of them,
let's say a number n of them.</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>So you measure the height
of each one.</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>And then you calculate the
average of the heights of</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>those penguins that you
have collected.</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>So this is your estimate
of the expected value.</text_slice>
            </slice>
            <slice>
              <time_slice>1:33</time_slice>
              <text_slice>Now, we called this the sample
mean, which is the mean value,</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>but within the sample that
you have collected.</text_slice>
            </slice>
            <slice>
              <time_slice>1:44</time_slice>
              <text_slice>This is something that's sort
of feels the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>1:48</time_slice>
              <text_slice>expected value, which
is again, the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>1:52</time_slice>
              <text_slice>But the expected value's a
different kind of mean.</text_slice>
            </slice>
            <slice>
              <time_slice>1:54</time_slice>
              <text_slice>The expected value is the mean
over the entire population,</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>whereas the sample mean is the
average over the smaller</text_slice>
            </slice>
            <slice>
              <time_slice>2:01</time_slice>
              <text_slice>sample that you have measured.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>The expected value
is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>The sample mean is a
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:09</time_slice>
              <text_slice>It's a random variable because
the sample you have</text_slice>
            </slice>
            <slice>
              <time_slice>2:11</time_slice>
              <text_slice>collected is random.</text_slice>
            </slice>
            <slice>
              <time_slice>2:15</time_slice>
              <text_slice>Now, we think that this is a
reasonable way of estimating</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>So in the limit as n goes to
infinity, it's plausible that</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>the sample mean, the estimate
that we are constructing,</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>should somehow get close
to the expected value.</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>What does this mean?</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>What does it mean
to get close?</text_slice>
            </slice>
            <slice>
              <time_slice>2:36</time_slice>
              <text_slice>In what sense?</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>And is this statement true?</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>This is the kind of statement
that we deal with when dealing</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>with limit theorems.</text_slice>
            </slice>
            <slice>
              <time_slice>2:45</time_slice>
              <text_slice>That's the subject of limit
theorems, when what happens if</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>you're dealing with lots and
lots of random variables, and</text_slice>
            </slice>
            <slice>
              <time_slice>2:52</time_slice>
              <text_slice>perhaps take averages
and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>2:54</time_slice>
              <text_slice>So why do we bother
about this?</text_slice>
            </slice>
            <slice>
              <time_slice>2:57</time_slice>
              <text_slice>Well, if you're in the sampling
business, it would be</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>reassuring to know that this
particular way of estimating</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>the expected value
actually gets you</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>close to the true answer.</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>There's also a higher level
reason, which is a little more</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>abstract and mathematical.</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>So probability problems are easy
to deal with if you're</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>having in your hands one or
two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>You can write down their mass
functions, joints density</text_slice>
            </slice>
            <slice>
              <time_slice>3:23</time_slice>
              <text_slice>functions, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>You can calculate on paper
or on a computer,</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>you can get the answers.</text_slice>
            </slice>
            <slice>
              <time_slice>3:29</time_slice>
              <text_slice>Probability problems become
computationally intractable if</text_slice>
            </slice>
            <slice>
              <time_slice>3:33</time_slice>
              <text_slice>you're dealing, let's say, with
100 random variables and</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>you're trying to get the exact
answers for anything.</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>So in principle, the same
formulas that we have, they</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>still apply.</text_slice>
            </slice>
            <slice>
              <time_slice>3:44</time_slice>
              <text_slice>But they involve summations
over large ranges of</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>combinations of indices.</text_slice>
            </slice>
            <slice>
              <time_slice>3:48</time_slice>
              <text_slice>And that makes life extremely
difficult.</text_slice>
            </slice>
            <slice>
              <time_slice>3:51</time_slice>
              <text_slice>But when you push the envelope
and you go to a situation</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>where you're dealing with a
very, very large number of</text_slice>
            </slice>
            <slice>
              <time_slice>3:58</time_slice>
              <text_slice>variables, then you can
start taking limits.</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>And when you take limits,
wonderful things happen.</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>Many formulas start simplifying,
and you can</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>actually get useful answers by
considering those limits.</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>And that's sort of the big
reason why looking at limit</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>theorems is a useful
thing to do.</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>So what we're going to do today,
first we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>start with a useful, simple tool
that allows us to relates</text_slice>
            </slice>
            <slice>
              <time_slice>4:27</time_slice>
              <text_slice>probabilities with
expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>4:30</time_slice>
              <text_slice>The Markov inequality is the
first inequality we're going</text_slice>
            </slice>
            <slice>
              <time_slice>4:33</time_slice>
              <text_slice>to write down.</text_slice>
            </slice>
            <slice>
              <time_slice>4:33</time_slice>
              <text_slice>And then using that, we're going
to get the Chebyshev's</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>inequality, a related
inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>Then we need to define what do
we mean by convergence when we</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>talk about random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>4:45</time_slice>
              <text_slice>It's a notion that's a
generalization of the notion</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>of the usual convergence
of limits of</text_slice>
            </slice>
            <slice>
              <time_slice>4:51</time_slice>
              <text_slice>a sequence of numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>4:52</time_slice>
              <text_slice>And once we have our notion of
convergence, we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>see that, indeed, the sample
mean converges to the true</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>mean, converges to the expected
value of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>And this statement is called the
weak law of large numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>5:08</time_slice>
              <text_slice>The reason it's called the weak
law is because there's</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>also a strong law, which is
a statement with the same</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>flavor, but with a somewhat
different</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>mathematical content.</text_slice>
            </slice>
            <slice>
              <time_slice>5:18</time_slice>
              <text_slice>But it's a little more abstract,
and we will not be</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>getting into this.</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>So the weak law is all that
you're going to get.</text_slice>
            </slice>
            <slice>
              <time_slice>5:26</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>So now we start our
digression.</text_slice>
            </slice>
            <slice>
              <time_slice>5:31</time_slice>
              <text_slice>And our first tool will be the
so-called Markov inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>So let's take a random variable
that's always</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>No matter what, it gets
no negative values.</text_slice>
            </slice>
            <slice>
              <time_slice>5:51</time_slice>
              <text_slice>To keep things simple,
let's assume it's a</text_slice>
            </slice>
            <slice>
              <time_slice>5:53</time_slice>
              <text_slice>discrete random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>So the expected value is the sum
over all possible values</text_slice>
            </slice>
            <slice>
              <time_slice>5:59</time_slice>
              <text_slice>that a random variable
can take.</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>The values of the random
variables that can take</text_slice>
            </slice>
            <slice>
              <time_slice>6:06</time_slice>
              <text_slice>weighted according to their
corresponding probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>6:10</time_slice>
              <text_slice>Now, this is a sum
over all x's.</text_slice>
            </slice>
            <slice>
              <time_slice>6:13</time_slice>
              <text_slice>But x takes non-negative
values.</text_slice>
            </slice>
            <slice>
              <time_slice>6:16</time_slice>
              <text_slice>And the PMF is also
non-negative.</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>So if I take a sum over fewer
things, I'm going to get a</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>smaller value.</text_slice>
            </slice>
            <slice>
              <time_slice>6:25</time_slice>
              <text_slice>So the sum when I add over
everything is less than or</text_slice>
            </slice>
            <slice>
              <time_slice>6:29</time_slice>
              <text_slice>equal to the sum that I will get
if I only add those terms</text_slice>
            </slice>
            <slice>
              <time_slice>6:33</time_slice>
              <text_slice>that are bigger than
a certain constant.</text_slice>
            </slice>
            <slice>
              <time_slice>6:38</time_slice>
              <text_slice>Now, if I'm adding over x's that
are bigger than a, the x</text_slice>
            </slice>
            <slice>
              <time_slice>6:45</time_slice>
              <text_slice>that shows up up there
will always be larger</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>than or equal to a.</text_slice>
            </slice>
            <slice>
              <time_slice>6:50</time_slice>
              <text_slice>So we get this inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>6:58</time_slice>
              <text_slice>And now, a is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>I can pull it outside
the summation.</text_slice>
            </slice>
            <slice>
              <time_slice>7:02</time_slice>
              <text_slice>And then I'm left with the
probabilities of all the x's</text_slice>
            </slice>
            <slice>
              <time_slice>7:05</time_slice>
              <text_slice>that are bigger than a.</text_slice>
            </slice>
            <slice>
              <time_slice>7:06</time_slice>
              <text_slice>And that's just the
probability of</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>being bigger than a.</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>OK, so that's the Markov
inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>Basically tells us that the
expected value is larger than</text_slice>
            </slice>
            <slice>
              <time_slice>7:23</time_slice>
              <text_slice>or equal to this number.</text_slice>
            </slice>
            <slice>
              <time_slice>7:26</time_slice>
              <text_slice>It relates expected values
to probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>7:30</time_slice>
              <text_slice>It tells us that if the expected
value is small, then</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>the probability that x is big
is also going to be small.</text_slice>
            </slice>
            <slice>
              <time_slice>7:39</time_slice>
              <text_slice>So it's translates a statement
about smallness of expected</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>values to a statement about
smallness of probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>What we actually need is a
somewhat different version of</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>this same statement.</text_slice>
            </slice>
            <slice>
              <time_slice>7:57</time_slice>
              <text_slice>And what we're going to do is to
apply this inequality to a</text_slice>
            </slice>
            <slice>
              <time_slice>8:03</time_slice>
              <text_slice>non-negative random variable
of a special type.</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>And you can think of applying
this same calculation to a</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>random variable of this form, (X
minus mu)-squared, where mu</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>is the expected value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>Now, this is a non-negative
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>So, the expected value of this
random variable, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>8:37</time_slice>
              <text_slice>variance, by following the same
thinking as we had in</text_slice>
            </slice>
            <slice>
              <time_slice>8:42</time_slice>
              <text_slice>that derivation up to there, is
bigger than the probability</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>that this random variable
is bigger than some--</text_slice>
            </slice>
            <slice>
              <time_slice>8:58</time_slice>
              <text_slice>let me use a-squared
instead of an a</text_slice>
            </slice>
            <slice>
              <time_slice>9:04</time_slice>
              <text_slice>times the value a-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>So now of course, this
probability is the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>probability that the absolute
value of X minus mu is bigger</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>than a times a-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>And this side is equal to the
variance of X. So this relates</text_slice>
            </slice>
            <slice>
              <time_slice>9:34</time_slice>
              <text_slice>the variance of X to the
probability that our random</text_slice>
            </slice>
            <slice>
              <time_slice>9:40</time_slice>
              <text_slice>variable is far away
from its mean.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>If the variance is small, then
it means that the probability</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>of being far away from the
mean is also small.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>So I derived this by applying
the Markov inequality to this</text_slice>
            </slice>
            <slice>
              <time_slice>10:02</time_slice>
              <text_slice>particular non-negative
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>10:04</time_slice>
              <text_slice>Or just to reinforce, perhaps,
the message, and increase your</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>confidence in this inequality,
let's just look at the</text_slice>
            </slice>
            <slice>
              <time_slice>10:13</time_slice>
              <text_slice>derivation once more, where I'm
going, here, to start from</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>first principles, but use the
same idea as the one that was</text_slice>
            </slice>
            <slice>
              <time_slice>10:20</time_slice>
              <text_slice>used in the proof out here.</text_slice>
            </slice>
            <slice>
              <time_slice>10:23</time_slice>
              <text_slice>Ok.</text_slice>
            </slice>
            <slice>
              <time_slice>10:23</time_slice>
              <text_slice>So just for variety, now let's
think of X as being a</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>continuous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>The derivation is the same
whether it's discrete or</text_slice>
            </slice>
            <slice>
              <time_slice>10:31</time_slice>
              <text_slice>continuous.</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>So by definition, the variance
is the integral, is this</text_slice>
            </slice>
            <slice>
              <time_slice>10:35</time_slice>
              <text_slice>particular integral.</text_slice>
            </slice>
            <slice>
              <time_slice>10:38</time_slice>
              <text_slice>Now, the integral is going to
become smaller if I integrate,</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>instead of integrating over
the full range, I only</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>integrate over x's that are
far away from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>10:51</time_slice>
              <text_slice>So mu is the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>Think of c as some big number.</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>These are x's that are far
away from the mean to the</text_slice>
            </slice>
            <slice>
              <time_slice>11:02</time_slice>
              <text_slice>left, from minus infinity
to mu minus c.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>And these are the x's that are
far away from the mean on the</text_slice>
            </slice>
            <slice>
              <time_slice>11:09</time_slice>
              <text_slice>positive side.</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>So by integrating over
fewer stuff, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>getting a smaller integral.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>Now, for any x in this range,
this distance, x minus mu, is</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>at least c.</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>So that squared is at
least c squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:26</time_slice>
              <text_slice>So this term over this
range of integration</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>is at least c squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:30</time_slice>
              <text_slice>So I can take it outside
the integral.</text_slice>
            </slice>
            <slice>
              <time_slice>11:33</time_slice>
              <text_slice>And I'm left just with the
integral of the density.</text_slice>
            </slice>
            <slice>
              <time_slice>11:36</time_slice>
              <text_slice>Same thing on the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>11:38</time_slice>
              <text_slice>And so what factors out is
this term c squared.</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>And inside, we're left with the
probability of being to</text_slice>
            </slice>
            <slice>
              <time_slice>11:45</time_slice>
              <text_slice>the left of mu minus c, and then
the probability of being</text_slice>
            </slice>
            <slice>
              <time_slice>11:49</time_slice>
              <text_slice>to the right of mu plus c,
which is the same as the</text_slice>
            </slice>
            <slice>
              <time_slice>11:52</time_slice>
              <text_slice>probability that the absolute
value of the distance from the</text_slice>
            </slice>
            <slice>
              <time_slice>11:55</time_slice>
              <text_slice>mean is larger than
or equal to c.</text_slice>
            </slice>
            <slice>
              <time_slice>11:58</time_slice>
              <text_slice>So that's the same inequality
that we proved there, except</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>that here I'm using c.</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>There I used a, but it's
exactly the same one.</text_slice>
            </slice>
            <slice>
              <time_slice>12:10</time_slice>
              <text_slice>This inequality was maybe better
to understand if you</text_slice>
            </slice>
            <slice>
              <time_slice>12:12</time_slice>
              <text_slice>take that term and send it
to the other side and</text_slice>
            </slice>
            <slice>
              <time_slice>12:16</time_slice>
              <text_slice>write it this form.</text_slice>
            </slice>
            <slice>
              <time_slice>12:18</time_slice>
              <text_slice>What does it tell us?</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>It tells us that if c is a big
number, it tells us that the</text_slice>
            </slice>
            <slice>
              <time_slice>12:25</time_slice>
              <text_slice>probability of being more than
c away from the mean is going</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>to be a small number.</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>When c is big, this is small.</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>Now, this is intuitive.</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>The variance is a measure
of the spread of the</text_slice>
            </slice>
            <slice>
              <time_slice>12:38</time_slice>
              <text_slice>distribution, how wide it is.</text_slice>
            </slice>
            <slice>
              <time_slice>12:40</time_slice>
              <text_slice>It tells us that if the
variance is small, the</text_slice>
            </slice>
            <slice>
              <time_slice>12:43</time_slice>
              <text_slice>distribution is not very wide.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>And mathematically, this
translates to this statement</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>that when the variance is small,
the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>being far away is going
to be small.</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>And the further away you're
looking, that is, if c is a</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>bigger number, that probability</text_slice>
            </slice>
            <slice>
              <time_slice>13:00</time_slice>
              <text_slice>also becomes small.</text_slice>
            </slice>
            <slice>
              <time_slice>13:04</time_slice>
              <text_slice>Maybe an even more intuitive way
to think about the content</text_slice>
            </slice>
            <slice>
              <time_slice>13:07</time_slice>
              <text_slice>of this inequality is to,
instead of c, use the number</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>k, where k is positive
and sigma is</text_slice>
            </slice>
            <slice>
              <time_slice>13:16</time_slice>
              <text_slice>the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>So let's just plug k sigma
in the place of c.</text_slice>
            </slice>
            <slice>
              <time_slice>13:22</time_slice>
              <text_slice>So this becomes k
sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>These sigma squared's cancel.</text_slice>
            </slice>
            <slice>
              <time_slice>13:27</time_slice>
              <text_slice>We're left with 1
over k-square.</text_slice>
            </slice>
            <slice>
              <time_slice>13:29</time_slice>
              <text_slice>Now, what is this?</text_slice>
            </slice>
            <slice>
              <time_slice>13:31</time_slice>
              <text_slice>This is the event that you are
k standard deviations away</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>So for example, this statement
here tells you that if you</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>look at the test scores from a
quiz, what fraction of the</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>class are 3 standard deviations
away from the mean?</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>It's possible, but it's not
going to be a lot of people.</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>It's going to be at most, 1/9
of the class that can be 3</text_slice>
            </slice>
            <slice>
              <time_slice>13:57</time_slice>
              <text_slice>standard deviations or more
away from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>So the Chebyshev inequality
is a really useful one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>It comes in handy whenever you
want to relate probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>and expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>14:12</time_slice>
              <text_slice>So if you know that your
expected values or, in</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>particular, that your variance
is small, this tells you</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>something about tailed
probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>So this is the end of our
first digression.</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>We have this inequality
in our hands.</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>Our second digression is
talk about limits.</text_slice>
            </slice>
            <slice>
              <time_slice>14:34</time_slice>
              <text_slice>We want to eventually talk
about limits of random</text_slice>
            </slice>
            <slice>
              <time_slice>14:37</time_slice>
              <text_slice>variables, but as a warm up,
we're going to start with</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>limits of sequences.</text_slice>
            </slice>
            <slice>
              <time_slice>14:42</time_slice>
              <text_slice>So you're given a sequence
of numbers, a1,</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>a2, a3, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>And we want to define the
notion that a sequence</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>converges to a number.</text_slice>
            </slice>
            <slice>
              <time_slice>14:56</time_slice>
              <text_slice>You sort of know what this
means, but let's just go</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>through it some more.</text_slice>
            </slice>
            <slice>
              <time_slice>15:06</time_slice>
              <text_slice>So here's a.</text_slice>
            </slice>
            <slice>
              <time_slice>15:09</time_slice>
              <text_slice>We have our sequence of
values as n increases.</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>What do we mean by the sequence
converging to a is</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>that when you look at those
values, they get closer and</text_slice>
            </slice>
            <slice>
              <time_slice>15:23</time_slice>
              <text_slice>closer to a.</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>So this value here is your
typical a sub n.</text_slice>
            </slice>
            <slice>
              <time_slice>15:29</time_slice>
              <text_slice>They get closer and closer to
a, and they stay closer.</text_slice>
            </slice>
            <slice>
              <time_slice>15:33</time_slice>
              <text_slice>So let's try to make
that more precise.</text_slice>
            </slice>
            <slice>
              <time_slice>15:36</time_slice>
              <text_slice>What it means is let's
fix a sense of what</text_slice>
            </slice>
            <slice>
              <time_slice>15:40</time_slice>
              <text_slice>it means to be close.</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>Let me look at an interval that
goes from a - epsilon to</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>a + epsilon.</text_slice>
            </slice>
            <slice>
              <time_slice>15:50</time_slice>
              <text_slice>Then if my sequence converges
to a, this means that as n</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>increases, eventually the values
of the sequence that I</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>get stay inside this band.</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>Since they converge to a, this
means that eventually they</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>will be smaller than
a + epsilon and</text_slice>
            </slice>
            <slice>
              <time_slice>16:14</time_slice>
              <text_slice>bigger than a - epsilon.</text_slice>
            </slice>
            <slice>
              <time_slice>16:16</time_slice>
              <text_slice>So convergence means that
given a band of positive</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>length around the number a,
the values of the sequence</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>that you get eventually
get inside and</text_slice>
            </slice>
            <slice>
              <time_slice>16:28</time_slice>
              <text_slice>stay inside that band.</text_slice>
            </slice>
            <slice>
              <time_slice>16:31</time_slice>
              <text_slice>So that's sort of the picture
definition of</text_slice>
            </slice>
            <slice>
              <time_slice>16:34</time_slice>
              <text_slice>what convergence means.</text_slice>
            </slice>
            <slice>
              <time_slice>16:35</time_slice>
              <text_slice>So now let's translate this into
a mathematical statement.</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>Given a band of positive length,
no matter how wide</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>that band is or how narrow it
is, so for every epsilon</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>positive, eventually the
sequence gets inside the band.</text_slice>
            </slice>
            <slice>
              <time_slice>16:56</time_slice>
              <text_slice>What does eventually mean?</text_slice>
            </slice>
            <slice>
              <time_slice>16:58</time_slice>
              <text_slice>There exists a time,
so that after that</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>time something happens.</text_slice>
            </slice>
            <slice>
              <time_slice>17:03</time_slice>
              <text_slice>And the something that happens
is that after that time, we</text_slice>
            </slice>
            <slice>
              <time_slice>17:07</time_slice>
              <text_slice>are inside that band.</text_slice>
            </slice>
            <slice>
              <time_slice>17:09</time_slice>
              <text_slice>So this is a formal mathematical
definition, which</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>actually translates what I was
telling in the wordy way</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>before, and showing in
terms of the picture.</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>Given a certain band, even if
it's narrow, eventually, after</text_slice>
            </slice>
            <slice>
              <time_slice>17:25</time_slice>
              <text_slice>a certain time n0, the values
of the sequence are going to</text_slice>
            </slice>
            <slice>
              <time_slice>17:28</time_slice>
              <text_slice>stay inside this band.</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>Now, if I were to take epsilon
to be very small, this thing</text_slice>
            </slice>
            <slice>
              <time_slice>17:35</time_slice>
              <text_slice>would still be true that
eventually I'm going to get</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>inside of the band, except that
I may have to wait longer</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>for the values to
get inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>17:45</time_slice>
              <text_slice>All right, that's what it means
for a deterministic</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>sequence to converge
to something.</text_slice>
            </slice>
            <slice>
              <time_slice>17:51</time_slice>
              <text_slice>Now, how about random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:54</time_slice>
              <text_slice>What does it mean for a sequence
of random variables</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>to converge to a number?</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>We're just going to twist
a little bit of the word</text_slice>
            </slice>
            <slice>
              <time_slice>18:02</time_slice>
              <text_slice>definition.</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>For numbers, we said that
eventually the numbers get</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>inside that band.</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>But if instead of numbers we
have random variables with a</text_slice>
            </slice>
            <slice>
              <time_slice>18:13</time_slice>
              <text_slice>certain distribution, so here
instead of a_n we're dealing</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>with a random variable that has
a distribution, let's say,</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>of this kind, what we want is
that this distribution gets</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>inside this band, so it gets
concentrated inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>What does it means that
the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>18:33</time_slice>
              <text_slice>gets inside this band?</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>I mean a random variable
has a distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>18:36</time_slice>
              <text_slice>It may have some tails, so
maybe not the entire</text_slice>
            </slice>
            <slice>
              <time_slice>18:40</time_slice>
              <text_slice>distribution gets concentrated
inside of the band.</text_slice>
            </slice>
            <slice>
              <time_slice>18:43</time_slice>
              <text_slice>But we want that more and more
of this distribution is</text_slice>
            </slice>
            <slice>
              <time_slice>18:48</time_slice>
              <text_slice>concentrated in this band.</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>So that --</text_slice>
            </slice>
            <slice>
              <time_slice>18:51</time_slice>
              <text_slice>in a sense that --</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>the probability of falling
outside the band converges to</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>0 -- becomes smaller
and smaller.</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>So in words, we're going to say
that the sequence random</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>variables or a sequence of
probability distributions,</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>that would be the same,
converges to a particular</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>number a if the following
is true.</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>If I consider a small band
around a, then the probability</text_slice>
            </slice>
            <slice>
              <time_slice>19:22</time_slice>
              <text_slice>that my random variable falls
outside this band, which is</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>the area under this curve,
this probability becomes</text_slice>
            </slice>
            <slice>
              <time_slice>19:29</time_slice>
              <text_slice>smaller and smaller as
n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>The probability of being
outside this band</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>converges to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>So that's the intuitive idea.</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>So in the beginning, maybe our
distribution is sitting</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>everywhere.</text_slice>
            </slice>
            <slice>
              <time_slice>19:46</time_slice>
              <text_slice>As n increases, the distribution
starts to get</text_slice>
            </slice>
            <slice>
              <time_slice>19:49</time_slice>
              <text_slice>concentrating inside the band.</text_slice>
            </slice>
            <slice>
              <time_slice>19:51</time_slice>
              <text_slice>When a is even bigger, our
distribution is even more</text_slice>
            </slice>
            <slice>
              <time_slice>19:57</time_slice>
              <text_slice>inside that band, so that these
outside probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>20:00</time_slice>
              <text_slice>become smaller and smaller.</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>So the corresponding
mathematical</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>statement is the following.</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>I fix a band around
a, a +/- epsilon.</text_slice>
            </slice>
            <slice>
              <time_slice>20:13</time_slice>
              <text_slice>Given that band, the probability
of falling outside</text_slice>
            </slice>
            <slice>
              <time_slice>20:18</time_slice>
              <text_slice>this band, this probability
converges to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>Or another way to say it is
that the limit of this</text_slice>
            </slice>
            <slice>
              <time_slice>20:23</time_slice>
              <text_slice>probability is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>20:26</time_slice>
              <text_slice>If you were to translate this
into a complete mathematical</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>statement, you would have
to write down the</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>following messy thing.</text_slice>
            </slice>
            <slice>
              <time_slice>20:34</time_slice>
              <text_slice>For every epsilon positive --</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>that's this statement --</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>the limit is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>What does it mean that the
limit of something is 0?</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>We flip back to the
previous slide.</text_slice>
            </slice>
            <slice>
              <time_slice>20:47</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>Because a probability
is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>So here we're talking about
a sequence of numbers</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>convergent to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>20:56</time_slice>
              <text_slice>What does it mean for a
sequence of numbers to</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>converge to 0?</text_slice>
            </slice>
            <slice>
              <time_slice>20:59</time_slice>
              <text_slice>It means that for any epsilon
prime positive, there exists</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>some n0 such that for every
n bigger than n0 the</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>following is true --</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>that this probability
is less than or</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>equal to epsilon prime.</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>So the mathematical statement
is a little hard to parse.</text_slice>
            </slice>
            <slice>
              <time_slice>21:27</time_slice>
              <text_slice>For every size of that band,
and then you take the</text_slice>
            </slice>
            <slice>
              <time_slice>21:32</time_slice>
              <text_slice>definition of what it means for
the limit of a sequence of</text_slice>
            </slice>
            <slice>
              <time_slice>21:34</time_slice>
              <text_slice>numbers to converge to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>21:37</time_slice>
              <text_slice>But it's a lot easier to
describe this in words and,</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>basically, think in terms
of this picture.</text_slice>
            </slice>
            <slice>
              <time_slice>21:45</time_slice>
              <text_slice>That as n increases, the
probability of falling outside</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>those bands just become
smaller and smaller.</text_slice>
            </slice>
            <slice>
              <time_slice>21:51</time_slice>
              <text_slice>So the statement is that our
distribution gets concentrated</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>in arbitrarily narrow little
bands around that</text_slice>
            </slice>
            <slice>
              <time_slice>22:01</time_slice>
              <text_slice>particular number a.</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>So let's look at an example.</text_slice>
            </slice>
            <slice>
              <time_slice>22:07</time_slice>
              <text_slice>Suppose a random variable Yn has
a discrete distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>this particular type.</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>Does it converge to something?</text_slice>
            </slice>
            <slice>
              <time_slice>22:17</time_slice>
              <text_slice>Well, the probability
distribution of this random</text_slice>
            </slice>
            <slice>
              <time_slice>22:19</time_slice>
              <text_slice>variable gets concentrated
at 0 --</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>there's more and more
probability of being at 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>If I fix a band around 0 --</text_slice>
            </slice>
            <slice>
              <time_slice>22:29</time_slice>
              <text_slice>so if I take the band from minus
epsilon to epsilon and</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>look at that band--</text_slice>
            </slice>
            <slice>
              <time_slice>22:36</time_slice>
              <text_slice>the probability of falling
outside this band is 1/n.</text_slice>
            </slice>
            <slice>
              <time_slice>22:42</time_slice>
              <text_slice>As n goes to infinity, that
probability goes to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>So in this case, we do
have convergence.</text_slice>
            </slice>
            <slice>
              <time_slice>22:50</time_slice>
              <text_slice>And Yn converges in probability
to the number 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>So this just captures the
facts obvious from this</text_slice>
            </slice>
            <slice>
              <time_slice>23:00</time_slice>
              <text_slice>picture, that more and more of
our probability distribution</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>gets concentrated around 0,
as n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>23:07</time_slice>
              <text_slice>Now, an interesting thing to
notice is the following, that</text_slice>
            </slice>
            <slice>
              <time_slice>23:10</time_slice>
              <text_slice>even though Yn converges to 0,
if you were to write down the</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>expected value for Yn,
what would it be?</text_slice>
            </slice>
            <slice>
              <time_slice>23:20</time_slice>
              <text_slice>It's going to be n times the
probability of this value,</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>which is 1/n.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>So the expected value
turns out to be 1.</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>And if you were to look at the
expected value of Yn-squared,</text_slice>
            </slice>
            <slice>
              <time_slice>23:34</time_slice>
              <text_slice>this would be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>times this probability, and
then n-squared times this</text_slice>
            </slice>
            <slice>
              <time_slice>23:41</time_slice>
              <text_slice>probability, which
is equal to n.</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>And this actually goes
to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>So we have this, perhaps,
strange situation where a</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>random variable goes to 0, but
the expected value of this</text_slice>
            </slice>
            <slice>
              <time_slice>23:58</time_slice>
              <text_slice>random variable does
not go to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>24:01</time_slice>
              <text_slice>And the second moment of that
random variable actually goes</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>24:05</time_slice>
              <text_slice>So this tells us that
convergence in probability</text_slice>
            </slice>
            <slice>
              <time_slice>24:08</time_slice>
              <text_slice>tells you something,
but it doesn't tell</text_slice>
            </slice>
            <slice>
              <time_slice>24:11</time_slice>
              <text_slice>you the whole story.</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>Convergence to 0 of a random
variable doesn't imply</text_slice>
            </slice>
            <slice>
              <time_slice>24:17</time_slice>
              <text_slice>anything about convergence
of expected values or of</text_slice>
            </slice>
            <slice>
              <time_slice>24:20</time_slice>
              <text_slice>variances and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>24:23</time_slice>
              <text_slice>So the reason is that
convergence in probability</text_slice>
            </slice>
            <slice>
              <time_slice>24:26</time_slice>
              <text_slice>tells you that this
tail probability</text_slice>
            </slice>
            <slice>
              <time_slice>24:28</time_slice>
              <text_slice>here is very small.</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>But it doesn't tell you how
far does this tail go.</text_slice>
            </slice>
            <slice>
              <time_slice>24:34</time_slice>
              <text_slice>As in this example, the tail
probability is small, but that</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>tail acts far away, so it
gives a disproportionate</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>contribution to the expected
value or the</text_slice>
            </slice>
            <slice>
              <time_slice>24:45</time_slice>
              <text_slice>expected value squared.</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>So now we've got everything that
we need to go back to the</text_slice>
            </slice>
            <slice>
              <time_slice>24:59</time_slice>
              <text_slice>sample mean and study
its properties.</text_slice>
            </slice>
            <slice>
              <time_slice>25:02</time_slice>
              <text_slice>So the sad thing is
that we have a</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>sequence of random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>25:07</time_slice>
              <text_slice>They're independent.</text_slice>
            </slice>
            <slice>
              <time_slice>25:08</time_slice>
              <text_slice>They have the same
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>25:10</time_slice>
              <text_slice>And we assume that they
have a finite mean</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>and a finite variance.</text_slice>
            </slice>
            <slice>
              <time_slice>25:14</time_slice>
              <text_slice>We're looking at the
sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>25:18</time_slice>
              <text_slice>Now in principle, you can
calculate the probability</text_slice>
            </slice>
            <slice>
              <time_slice>25:21</time_slice>
              <text_slice>distribution of the sample mean,
because we know how to</text_slice>
            </slice>
            <slice>
              <time_slice>25:25</time_slice>
              <text_slice>find the distributions
of sums of</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>independent random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>You use the convolution
formula over and over.</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>But this is pretty
complicated, so</text_slice>
            </slice>
            <slice>
              <time_slice>25:32</time_slice>
              <text_slice>let's not look at that.</text_slice>
            </slice>
            <slice>
              <time_slice>25:34</time_slice>
              <text_slice>Let's just look at expected
values, variances, and the</text_slice>
            </slice>
            <slice>
              <time_slice>25:38</time_slice>
              <text_slice>probabilities that the sample
mean is far away</text_slice>
            </slice>
            <slice>
              <time_slice>25:42</time_slice>
              <text_slice>from the true mean.</text_slice>
            </slice>
            <slice>
              <time_slice>25:44</time_slice>
              <text_slice>So what is the expected value
of this random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>25:47</time_slice>
              <text_slice>The expected value of a sum of
random variables is the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>25:51</time_slice>
              <text_slice>the expected values.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>And then we have this factor
of n in the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>26:00</time_slice>
              <text_slice>Each one of these expected
values is mu, so we get mu.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>So the sample mean, the average
value of this Mn in</text_slice>
            </slice>
            <slice>
              <time_slice>26:13</time_slice>
              <text_slice>expectation is the same as
the true mean inside our</text_slice>
            </slice>
            <slice>
              <time_slice>26:18</time_slice>
              <text_slice>population.</text_slice>
            </slice>
            <slice>
              <time_slice>26:20</time_slice>
              <text_slice>Now here, this is a fine
conceptual point, there's two</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>kinds of averages involved
when you write down this</text_slice>
            </slice>
            <slice>
              <time_slice>26:29</time_slice>
              <text_slice>expression.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>We understand that
expectations are</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>some kind of average.</text_slice>
            </slice>
            <slice>
              <time_slice>26:36</time_slice>
              <text_slice>The sample mean is also an
average over the values that</text_slice>
            </slice>
            <slice>
              <time_slice>26:40</time_slice>
              <text_slice>we have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>But it's two different
kinds of averages.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>The sample mean is the average
of the heights of the penguins</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>that we collected over
a single expedition.</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>The expected value is to be
thought of as follows, my</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>probabilistic experiment
is one expedition</text_slice>
            </slice>
            <slice>
              <time_slice>27:02</time_slice>
              <text_slice>to the South Pole.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>Expected value here means
thinking on the average over a</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>huge number of expeditions.</text_slice>
            </slice>
            <slice>
              <time_slice>27:12</time_slice>
              <text_slice>So my expedition is a random
experiment, I collect random</text_slice>
            </slice>
            <slice>
              <time_slice>27:16</time_slice>
              <text_slice>samples, and they record Mn.</text_slice>
            </slice>
            <slice>
              <time_slice>27:21</time_slice>
              <text_slice>The average result of an
expedition is what we would</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>get if we were to carry out
a zillion expeditions and</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>average the averages that we
get at each particular</text_slice>
            </slice>
            <slice>
              <time_slice>27:35</time_slice>
              <text_slice>expedition.</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>So this Mn is the average during
a single expedition.</text_slice>
            </slice>
            <slice>
              <time_slice>27:39</time_slice>
              <text_slice>This expectation is the average
over an imagined</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>infinite sequence
of expeditions.</text_slice>
            </slice>
            <slice>
              <time_slice>27:49</time_slice>
              <text_slice>And of course, the other thing
to always keep in mind is that</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>expectations give you numbers,
whereas the sample mean is</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>actually a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>So this random variable,
how random is it?</text_slice>
            </slice>
            <slice>
              <time_slice>28:03</time_slice>
              <text_slice>How big is its variance?</text_slice>
            </slice>
            <slice>
              <time_slice>28:05</time_slice>
              <text_slice>So the variance of a sum of
random variables is the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>the variances.</text_slice>
            </slice>
            <slice>
              <time_slice>28:12</time_slice>
              <text_slice>But since we're dividing by n,
when you calculate variances</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>this brings in a factor
of n-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>28:19</time_slice>
              <text_slice>So the variance is sigma-squared
over n.</text_slice>
            </slice>
            <slice>
              <time_slice>28:24</time_slice>
              <text_slice>And in particular, the variance
of the sample mean</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>becomes smaller and smaller.</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>It means that when you estimate
that average height</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>of penguins, if you take a
large sample, then your</text_slice>
            </slice>
            <slice>
              <time_slice>28:34</time_slice>
              <text_slice>estimate is not going
to be too random.</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>The randomness in your estimates
become small if you</text_slice>
            </slice>
            <slice>
              <time_slice>28:41</time_slice>
              <text_slice>have a large sample size.</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>Having a large sample size kind
of removes the randomness</text_slice>
            </slice>
            <slice>
              <time_slice>28:46</time_slice>
              <text_slice>from your experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>28:47</time_slice>
              <text_slice>Now let's apply the Chebyshev
inequality to say something</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>about tail probabilities
for the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>The probability that you are
more than epsilon away from</text_slice>
            </slice>
            <slice>
              <time_slice>28:59</time_slice>
              <text_slice>the true mean is less than or
equal to the variance of this</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>quantity divided by this
number squared.</text_slice>
            </slice>
            <slice>
              <time_slice>29:07</time_slice>
              <text_slice>So that's just the translation
of the Chebyshev inequality to</text_slice>
            </slice>
            <slice>
              <time_slice>29:09</time_slice>
              <text_slice>the particular context
we've got here.</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>We found the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>29:13</time_slice>
              <text_slice>It's sigma-squared over n.</text_slice>
            </slice>
            <slice>
              <time_slice>29:15</time_slice>
              <text_slice>So we end up with
this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>29:18</time_slice>
              <text_slice>So what does this
expression do?</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>For any given epsilon, if
I fix epsilon, then this</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>probability, which is less
than sigma-squared over n</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>epsilon-squared, converges to
0 as n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>29:44</time_slice>
              <text_slice>And this is just the definition
of convergence in</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>probability.</text_slice>
            </slice>
            <slice>
              <time_slice>29:49</time_slice>
              <text_slice>If this happens, that the
probability of being more than</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>epsilon away from the mean, that
probability goes to 0,</text_slice>
            </slice>
            <slice>
              <time_slice>29:57</time_slice>
              <text_slice>and this is true no matter how
I choose my epsilon, then by</text_slice>
            </slice>
            <slice>
              <time_slice>30:01</time_slice>
              <text_slice>definition we have convergence
in probability.</text_slice>
            </slice>
            <slice>
              <time_slice>30:04</time_slice>
              <text_slice>So we have proved that the
sample mean converges in</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>probability to the true mean.</text_slice>
            </slice>
            <slice>
              <time_slice>30:11</time_slice>
              <text_slice>And this is what the weak law
of large numbers tells us.</text_slice>
            </slice>
            <slice>
              <time_slice>30:16</time_slice>
              <text_slice>So in some vague sense, it
tells us that the sample</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>means, when you take the
average of many, many</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>measurements in your sample,
then the sample mean is a good</text_slice>
            </slice>
            <slice>
              <time_slice>30:28</time_slice>
              <text_slice>estimate of the true mean in the
sense that it approaches</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>the true mean as your sample
size increases.</text_slice>
            </slice>
            <slice>
              <time_slice>30:36</time_slice>
              <text_slice>It approaches the true mean,
but of course in a very</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>specific sense, in probability,
according to this</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>notion of convergence
that we have used.</text_slice>
            </slice>
            <slice>
              <time_slice>30:46</time_slice>
              <text_slice>So since we're talking about
sampling, let's go over an</text_slice>
            </slice>
            <slice>
              <time_slice>30:51</time_slice>
              <text_slice>example, which is the typical
situation faced by someone</text_slice>
            </slice>
            <slice>
              <time_slice>30:56</time_slice>
              <text_slice>who's constructing a poll.</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>So you're interested in some
property of the population.</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>So what fraction of
the population</text_slice>
            </slice>
            <slice>
              <time_slice>31:05</time_slice>
              <text_slice>prefers Coke to Pepsi?</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>So there's a number f, which
is that fraction of the</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>population.</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>And so this is an
exact number.</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>So out of a population of 100
million, 20 million prefer</text_slice>
            </slice>
            <slice>
              <time_slice>31:20</time_slice>
              <text_slice>Coke, then f would be 0.2.</text_slice>
            </slice>
            <slice>
              <time_slice>31:25</time_slice>
              <text_slice>We want to find out what
that fraction is.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>We cannot ask everyone.</text_slice>
            </slice>
            <slice>
              <time_slice>31:30</time_slice>
              <text_slice>What we're going to do is to
take a random sample of people</text_slice>
            </slice>
            <slice>
              <time_slice>31:34</time_slice>
              <text_slice>and ask them for their
preferences.</text_slice>
            </slice>
            <slice>
              <time_slice>31:37</time_slice>
              <text_slice>So the ith person either says
yes for Coke or no.</text_slice>
            </slice>
            <slice>
              <time_slice>31:42</time_slice>
              <text_slice>And we record that by putting
a 1 each time that we get a</text_slice>
            </slice>
            <slice>
              <time_slice>31:46</time_slice>
              <text_slice>yes answer.</text_slice>
            </slice>
            <slice>
              <time_slice>31:49</time_slice>
              <text_slice>And then we form the average
of these x's.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>What is this average?</text_slice>
            </slice>
            <slice>
              <time_slice>31:53</time_slice>
              <text_slice>It's the number of 1's that
we got divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>31:57</time_slice>
              <text_slice>So this is a fraction, but
calculated only on the basis</text_slice>
            </slice>
            <slice>
              <time_slice>32:02</time_slice>
              <text_slice>of the sample that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>32:04</time_slice>
              <text_slice>So you can think of this as
being an estimate, f_hat,</text_slice>
            </slice>
            <slice>
              <time_slice>32:10</time_slice>
              <text_slice>based on the sample
that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>32:13</time_slice>
              <text_slice>Now, even though we used the
lower case letter here, this</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>f_hat is, of course,
a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>32:20</time_slice>
              <text_slice>f is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>32:23</time_slice>
              <text_slice>This is the true fraction in
the overall population.</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>f_hat is the estimate
that we get by using</text_slice>
            </slice>
            <slice>
              <time_slice>32:30</time_slice>
              <text_slice>our particular sample.</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>Ok.</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>So your boss told you, I need to
know what f is, but go and</text_slice>
            </slice>
            <slice>
              <time_slice>32:38</time_slice>
              <text_slice>do some sampling.</text_slice>
            </slice>
            <slice>
              <time_slice>32:40</time_slice>
              <text_slice>What are you going to respond?</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>Unless I ask everyone in the
whole population, there's no</text_slice>
            </slice>
            <slice>
              <time_slice>32:46</time_slice>
              <text_slice>way for me to know f exactly.</text_slice>
            </slice>
            <slice>
              <time_slice>32:51</time_slice>
              <text_slice>Right?</text_slice>
            </slice>
            <slice>
              <time_slice>32:51</time_slice>
              <text_slice>There's no way.</text_slice>
            </slice>
            <slice>
              <time_slice>32:54</time_slice>
              <text_slice>OK, so the boss tells you, well
OK, then that'll me f</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>within an accuracy.</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>I want an answer from you,
that's your answer, which is</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>close to the correct answer
within 1 % point.</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>So if the true f is 0.4, your
answer should be somewhere</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>between 0.39 and 0.41.</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>I want a really accurate
answer.</text_slice>
            </slice>
            <slice>
              <time_slice>33:25</time_slice>
              <text_slice>What are you going to say?</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>Well, there's no guarantee
that my answer</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>will be within 1 %.</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>Maybe I'm unlucky and I just
happen to sample the wrong set</text_slice>
            </slice>
            <slice>
              <time_slice>33:37</time_slice>
              <text_slice>of people and my answer
comes out to be wrong.</text_slice>
            </slice>
            <slice>
              <time_slice>33:40</time_slice>
              <text_slice>So I cannot give you a hard
guarantee that this inequality</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>will be satisfied.</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>But perhaps, I can give you a
guarantee that this inequality</text_slice>
            </slice>
            <slice>
              <time_slice>33:51</time_slice>
              <text_slice>will be satisfied, this accuracy
requirement will be</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>satisfied, with high
confidence.</text_slice>
            </slice>
            <slice>
              <time_slice>33:59</time_slice>
              <text_slice>That is, there's going to be
a smaller probability that</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>things go wrong, that
I'm unlikely</text_slice>
            </slice>
            <slice>
              <time_slice>34:04</time_slice>
              <text_slice>and I use a bad sample.</text_slice>
            </slice>
            <slice>
              <time_slice>34:07</time_slice>
              <text_slice>But leaving aside that smaller
probability of being unlucky,</text_slice>
            </slice>
            <slice>
              <time_slice>34:10</time_slice>
              <text_slice>my answer will be accurate
within the accuracy</text_slice>
            </slice>
            <slice>
              <time_slice>34:13</time_slice>
              <text_slice>requirement that you have.</text_slice>
            </slice>
            <slice>
              <time_slice>34:16</time_slice>
              <text_slice>So these two numbers are the
usual specs that one has when</text_slice>
            </slice>
            <slice>
              <time_slice>34:20</time_slice>
              <text_slice>designing polls.</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>So this number is the accuracy
that we want.</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>It's the desired accuracy.</text_slice>
            </slice>
            <slice>
              <time_slice>34:29</time_slice>
              <text_slice>And this number has to do with
the confidence that we want.</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>So 1 minus that number, we could
call it the confidence</text_slice>
            </slice>
            <slice>
              <time_slice>34:40</time_slice>
              <text_slice>that we want out
of our sample.</text_slice>
            </slice>
            <slice>
              <time_slice>34:43</time_slice>
              <text_slice>So this is really 1
minus confidence.</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>So now your job is to figure out
how large an n, how large</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>a sample should you be using, in
order to satisfy the specs</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>that your boss gave you.</text_slice>
            </slice>
            <slice>
              <time_slice>34:59</time_slice>
              <text_slice>All you know at this stage is
the Chebyshev inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>So you just try to use it.</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>The probability of getting an
answer that's more than 0.01</text_slice>
            </slice>
            <slice>
              <time_slice>35:09</time_slice>
              <text_slice>away from the true answer is, by
Chebyshev's inequality, the</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>variance of this random variable
divided by this</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>number squared.</text_slice>
            </slice>
            <slice>
              <time_slice>35:21</time_slice>
              <text_slice>The variance, as we argued
a little earlier, is the</text_slice>
            </slice>
            <slice>
              <time_slice>35:25</time_slice>
              <text_slice>variance of the x's
divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>35:29</time_slice>
              <text_slice>So we get this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>35:31</time_slice>
              <text_slice>So we would like this
number to be less</text_slice>
            </slice>
            <slice>
              <time_slice>35:35</time_slice>
              <text_slice>than or equal to 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>35:38</time_slice>
              <text_slice>OK, here we hit a little
bit off a difficulty.</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>The variance, (sigma_x)-squared,
what is it?</text_slice>
            </slice>
            <slice>
              <time_slice>35:49</time_slice>
              <text_slice>(Sigma_x)-squared is, if you
remember the variance of a</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>Bernoulli random variable,
is this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>But we don't know it.</text_slice>
            </slice>
            <slice>
              <time_slice>35:59</time_slice>
              <text_slice>f is what we're trying to
estimate in the first place.</text_slice>
            </slice>
            <slice>
              <time_slice>36:02</time_slice>
              <text_slice>So the variance is not known,
so I cannot plug in a number</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>inside here.</text_slice>
            </slice>
            <slice>
              <time_slice>36:08</time_slice>
              <text_slice>What I can do is to be
conservative and use an upper</text_slice>
            </slice>
            <slice>
              <time_slice>36:12</time_slice>
              <text_slice>bound of the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>36:14</time_slice>
              <text_slice>How large can this number get?</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>Well, you can plot
f times (1-f).</text_slice>
            </slice>
            <slice>
              <time_slice>36:25</time_slice>
              <text_slice>It's a parabola.</text_slice>
            </slice>
            <slice>
              <time_slice>36:26</time_slice>
              <text_slice>It has a root at 0 and at 1.</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>So the maximum value is going to
be, by symmetry, at 1/2 and</text_slice>
            </slice>
            <slice>
              <time_slice>36:34</time_slice>
              <text_slice>when f is 1/2, then this
variance becomes 1/4.</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>So I don't know
(sigma_x)-squared, but I'm</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>going to use the worst case
value for (sigma_x)-squared,</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>which is 4.</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>And this is now an inequality
that I know to be always true.</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>I've got my specs, and my specs
tell me that I want this</text_slice>
            </slice>
            <slice>
              <time_slice>36:56</time_slice>
              <text_slice>number to be less than 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>36:59</time_slice>
              <text_slice>And given what I know, the best
thing I can do is to say,</text_slice>
            </slice>
            <slice>
              <time_slice>37:04</time_slice>
              <text_slice>OK, I'm going to take
this number and make</text_slice>
            </slice>
            <slice>
              <time_slice>37:07</time_slice>
              <text_slice>it less than 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>37:14</time_slice>
              <text_slice>If I choose my n so that this
is less than 0.05, then I'm</text_slice>
            </slice>
            <slice>
              <time_slice>37:20</time_slice>
              <text_slice>certain that this probability
is also less than 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>37:24</time_slice>
              <text_slice>What does it take for this
inequality to be true?</text_slice>
            </slice>
            <slice>
              <time_slice>37:28</time_slice>
              <text_slice>You can solve for n here, and
you find that to satisfy this</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>inequality, n should be larger
than or equal to 50,000.</text_slice>
            </slice>
            <slice>
              <time_slice>37:40</time_slice>
              <text_slice>So you can just let n
be equal to 50,000.</text_slice>
            </slice>
            <slice>
              <time_slice>37:44</time_slice>
              <text_slice>So the Chebyshev inequality
tells us that if you take n</text_slice>
            </slice>
            <slice>
              <time_slice>37:47</time_slice>
              <text_slice>equal to 50,000, then by the
Chebyshev inequality, we're</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>guaranteed to satisfy the specs
that we were given.</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>Ok.</text_slice>
            </slice>
            <slice>
              <time_slice>37:57</time_slice>
              <text_slice>Now, 50,000 is a bit of
a large sample size.</text_slice>
            </slice>
            <slice>
              <time_slice>38:03</time_slice>
              <text_slice>Right?</text_slice>
            </slice>
            <slice>
              <time_slice>38:05</time_slice>
              <text_slice>If you read anything in the
newspapers where they say so</text_slice>
            </slice>
            <slice>
              <time_slice>38:09</time_slice>
              <text_slice>much of the voters think this
and that, this was determined</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>on the basis of a sample of
1,200 likely voters or so.</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>So the numbers that you will
typically see in these news</text_slice>
            </slice>
            <slice>
              <time_slice>38:23</time_slice>
              <text_slice>items about polling, they
usually involve sample sizes</text_slice>
            </slice>
            <slice>
              <time_slice>38:27</time_slice>
              <text_slice>about the 1,000 or so.</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>You will never see a sample
size of 50,000.</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>That's too much.</text_slice>
            </slice>
            <slice>
              <time_slice>38:37</time_slice>
              <text_slice>So where can we cut
some corners?</text_slice>
            </slice>
            <slice>
              <time_slice>38:41</time_slice>
              <text_slice>Well, we can cut corners
basically in three places.</text_slice>
            </slice>
            <slice>
              <time_slice>38:46</time_slice>
              <text_slice>This requirement is a
little too tight.</text_slice>
            </slice>
            <slice>
              <time_slice>38:49</time_slice>
              <text_slice>Newspaper stories will usually
tell you, we have an accuracy</text_slice>
            </slice>
            <slice>
              <time_slice>38:53</time_slice>
              <text_slice>of +/- 3 % points, instead
of 1 % point.</text_slice>
            </slice>
            <slice>
              <time_slice>38:58</time_slice>
              <text_slice>And because this number comes up
as a square, by making it 3</text_slice>
            </slice>
            <slice>
              <time_slice>39:03</time_slice>
              <text_slice>% points instead of 1, saves
you a factor of 10.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>Then, the five percent
confidence, I guess that's</text_slice>
            </slice>
            <slice>
              <time_slice>39:12</time_slice>
              <text_slice>usually OK.</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>If we use that factor of 10,
then we make our sample that</text_slice>
            </slice>
            <slice>
              <time_slice>39:19</time_slice>
              <text_slice>we gain from here, then we get
a sample size of 10,000.</text_slice>
            </slice>
            <slice>
              <time_slice>39:23</time_slice>
              <text_slice>And that's, again,
a little too big.</text_slice>
            </slice>
            <slice>
              <time_slice>39:25</time_slice>
              <text_slice>So where can we fix things?</text_slice>
            </slice>
            <slice>
              <time_slice>39:28</time_slice>
              <text_slice>Well, it turns out that this
inequality that we're using</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>here, Chebyshev's inequality,
is just an inequality.</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>It's not that tight.</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>It's not very accurate.</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>Maybe there's a better way of
calculating or estimating this</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>quantity, which is smaller
than this.</text_slice>
            </slice>
            <slice>
              <time_slice>39:46</time_slice>
              <text_slice>And using a more accurate
inequality or a more accurate</text_slice>
            </slice>
            <slice>
              <time_slice>39:49</time_slice>
              <text_slice>bound, then we can convince
ourselves that we can settle</text_slice>
            </slice>
            <slice>
              <time_slice>39:55</time_slice>
              <text_slice>with a smaller sample size.</text_slice>
            </slice>
            <slice>
              <time_slice>39:57</time_slice>
              <text_slice>This more accurate kind of
inequality comes out of a</text_slice>
            </slice>
            <slice>
              <time_slice>40:01</time_slice>
              <text_slice>difference limit theorem,
which is the next limit</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>theorem we're going
to consider.</text_slice>
            </slice>
            <slice>
              <time_slice>40:06</time_slice>
              <text_slice>We're going to start the
discussion today, but we're</text_slice>
            </slice>
            <slice>
              <time_slice>40:08</time_slice>
              <text_slice>going to continue with
it next week.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>Before I tell you exactly what
that other limit theorem says,</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>let me give you the
big picture of</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>what's involved here.</text_slice>
            </slice>
            <slice>
              <time_slice>40:24</time_slice>
              <text_slice>We're dealing with sums of
i.i.d random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>Each X has a distribution
of its own.</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>So suppose that X has a
distribution which is</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>something like this.</text_slice>
            </slice>
            <slice>
              <time_slice>40:43</time_slice>
              <text_slice>This is the density of X. If I
add lots of X's together, what</text_slice>
            </slice>
            <slice>
              <time_slice>40:48</time_slice>
              <text_slice>kind of distribution
do I expect?</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>The mean is going to be
n times the mean of an</text_slice>
            </slice>
            <slice>
              <time_slice>40:55</time_slice>
              <text_slice>individual X. So if this is mu,
I'm going to get a mean of</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>n times mu.</text_slice>
            </slice>
            <slice>
              <time_slice>41:02</time_slice>
              <text_slice>But my variance will
also increase.</text_slice>
            </slice>
            <slice>
              <time_slice>41:06</time_slice>
              <text_slice>When I add the random
variables,</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>I'm adding the variances.</text_slice>
            </slice>
            <slice>
              <time_slice>41:10</time_slice>
              <text_slice>So since the variance increases,
we're going to get</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>a distribution that's
pretty wide.</text_slice>
            </slice>
            <slice>
              <time_slice>41:17</time_slice>
              <text_slice>So this is the density of X1
plus all the way to Xn.</text_slice>
            </slice>
            <slice>
              <time_slice>41:23</time_slice>
              <text_slice>So as n increases, my
distribution shifts, because</text_slice>
            </slice>
            <slice>
              <time_slice>41:27</time_slice>
              <text_slice>the mean is positive.</text_slice>
            </slice>
            <slice>
              <time_slice>41:28</time_slice>
              <text_slice>So I keep adding things.</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>And also, my distribution
becomes wider and wider.</text_slice>
            </slice>
            <slice>
              <time_slice>41:33</time_slice>
              <text_slice>The variance increases.</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>Well, we started a different
scaling.</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>We started a scaled version of
this quantity when we looked</text_slice>
            </slice>
            <slice>
              <time_slice>41:42</time_slice>
              <text_slice>at the weak law of
large numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>41:46</time_slice>
              <text_slice>In the weak law of large
numbers, we take this random</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>variable and divide it by n.</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>And what the weak law tells us
is that we're going to get a</text_slice>
            </slice>
            <slice>
              <time_slice>41:56</time_slice>
              <text_slice>distribution that's very highly
concentrated around the</text_slice>
            </slice>
            <slice>
              <time_slice>42:01</time_slice>
              <text_slice>true mean, which is mu.</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>So this here would be the
density of X1 plus</text_slice>
            </slice>
            <slice>
              <time_slice>42:07</time_slice>
              <text_slice>Xn divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>42:12</time_slice>
              <text_slice>Because I've divided by n, the
mean has become the original</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>mean, which is mu.</text_slice>
            </slice>
            <slice>
              <time_slice>42:19</time_slice>
              <text_slice>But the weak law of large
numbers tells us that the</text_slice>
            </slice>
            <slice>
              <time_slice>42:22</time_slice>
              <text_slice>distribution of this random
variable is very concentrated</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>around the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>42:27</time_slice>
              <text_slice>So we get a distribution
that's very</text_slice>
            </slice>
            <slice>
              <time_slice>42:29</time_slice>
              <text_slice>narrow in this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>42:31</time_slice>
              <text_slice>In the limit, this distribution
becomes one</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>that's just concentrated
on top of mu.</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>So it's sort of a degenerate
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>So these are two extremes, no
scaling for the sum, a scaling</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>where we divide by n.</text_slice>
            </slice>
            <slice>
              <time_slice>42:47</time_slice>
              <text_slice>In this extreme, we get the
trivial case of a distribution</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>that flattens out completely.</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>In this scaling, we get a
distribution that gets</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>concentrated around
a single point.</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>Again, we look at some
intermediate scaling that</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>makes things more interesting.</text_slice>
            </slice>
            <slice>
              <time_slice>43:04</time_slice>
              <text_slice>Things do become interesting
if we scale by dividing the</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>sum by square root of n instead
of dividing by n.</text_slice>
            </slice>
            <slice>
              <time_slice>43:14</time_slice>
              <text_slice>What effect does this have?</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>When we scale by dividing by
square root of n, the variance</text_slice>
            </slice>
            <slice>
              <time_slice>43:22</time_slice>
              <text_slice>of Sn over square root of n is
going to be the variance of Sn</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>over sum divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>That's how variances behave.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>The variance of Sn is n
sigma-squared, divide by n,</text_slice>
            </slice>
            <slice>
              <time_slice>43:37</time_slice>
              <text_slice>which is sigma squared, which
means that when we scale in</text_slice>
            </slice>
            <slice>
              <time_slice>43:41</time_slice>
              <text_slice>this particular way,
as n changes, the</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>variance doesn't change.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>So the width of our
distribution</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>will be sort of constant.</text_slice>
            </slice>
            <slice>
              <time_slice>43:52</time_slice>
              <text_slice>The distribution changes shape,
but it doesn't become</text_slice>
            </slice>
            <slice>
              <time_slice>43:56</time_slice>
              <text_slice>narrower as was the case here.</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>It doesn't become wider, kind
of keeps the same width.</text_slice>
            </slice>
            <slice>
              <time_slice>44:04</time_slice>
              <text_slice>So perhaps in the limit, this
distribution is going to take</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>an interesting shape.</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>And that's indeed the case.</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>So let's do what
we did before.</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>So we're looking at the sum, and
we want to divide the sum</text_slice>
            </slice>
            <slice>
              <time_slice>44:25</time_slice>
              <text_slice>by something that goes like
square root of n.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>So the variance of Sn
is n sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>The variance of the sigma Sn
is the square root of that.</text_slice>
            </slice>
            <slice>
              <time_slice>44:38</time_slice>
              <text_slice>It's this number.</text_slice>
            </slice>
            <slice>
              <time_slice>44:39</time_slice>
              <text_slice>So effectively, we're scaling
by order of square root n.</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>Now, I'm doing another
thing here.</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>If my random variable has a
positive mean, then this</text_slice>
            </slice>
            <slice>
              <time_slice>44:52</time_slice>
              <text_slice>quantity is going to
have a mean that's</text_slice>
            </slice>
            <slice>
              <time_slice>44:55</time_slice>
              <text_slice>positive and growing.</text_slice>
            </slice>
            <slice>
              <time_slice>44:56</time_slice>
              <text_slice>It's going to be shifting
to the right.</text_slice>
            </slice>
            <slice>
              <time_slice>44:59</time_slice>
              <text_slice>Why is that?</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>Sn has a mean that's
proportional to n.</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>When I divide by square root n,
then it means that the mean</text_slice>
            </slice>
            <slice>
              <time_slice>45:09</time_slice>
              <text_slice>scales like square root of n.</text_slice>
            </slice>
            <slice>
              <time_slice>45:11</time_slice>
              <text_slice>So my distribution would
still keep shifting</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>after I do this division.</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>I want to keep my distribution
in place, so I subtract out</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>the mean of Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>45:23</time_slice>
              <text_slice>So what we're doing here is
a standard technique or</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>transformation where you take
a random variable and you</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>so-called standardize it.</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>I remove the mean of that random
variable and I divide</text_slice>
            </slice>
            <slice>
              <time_slice>45:38</time_slice>
              <text_slice>by the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>This results in a random
variable that has 0 mean and</text_slice>
            </slice>
            <slice>
              <time_slice>45:43</time_slice>
              <text_slice>unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>45:44</time_slice>
              <text_slice>What Zn measures is the
following, Zn tells me how</text_slice>
            </slice>
            <slice>
              <time_slice>45:49</time_slice>
              <text_slice>many standard deviations am
I away from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>Sn minus (n times expected value
of X) tells me how much</text_slice>
            </slice>
            <slice>
              <time_slice>45:59</time_slice>
              <text_slice>is Sn away from the
mean value of Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>And by dividing by the standard
deviation of Sn --</text_slice>
            </slice>
            <slice>
              <time_slice>46:06</time_slice>
              <text_slice>this tells me how many standard
deviations away from</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>the mean am I.</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>So we're going to look at this
random variable, which is just</text_slice>
            </slice>
            <slice>
              <time_slice>46:15</time_slice>
              <text_slice>a transformation Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>46:17</time_slice>
              <text_slice>It's a linear transformation
of Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>S And we're going to compare
this random variable to a</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>standard normal random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>46:27</time_slice>
              <text_slice>So a standard normal is the
random variable that you are</text_slice>
            </slice>
            <slice>
              <time_slice>46:30</time_slice>
              <text_slice>familiar with, given by the
usual formula, and for which</text_slice>
            </slice>
            <slice>
              <time_slice>46:35</time_slice>
              <text_slice>we have tables for it.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>This Zn has 0 mean and
unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>So in that respect, it has the
same statistics as the</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>The distribution of Zn
could be anything --</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>can be pretty messy.</text_slice>
            </slice>
            <slice>
              <time_slice>46:50</time_slice>
              <text_slice>But there is this amazing
theorem called the central</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>limit theorem that tells us that
the distribution of Zn</text_slice>
            </slice>
            <slice>
              <time_slice>46:58</time_slice>
              <text_slice>approaches the distribution of
the standard normal in the</text_slice>
            </slice>
            <slice>
              <time_slice>47:01</time_slice>
              <text_slice>following sense, that
probability is that you can</text_slice>
            </slice>
            <slice>
              <time_slice>47:06</time_slice>
              <text_slice>calculate --</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>of this type --</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>that you can calculate
for Zn --</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>is the limit becomes the same as
the probabilities that you</text_slice>
            </slice>
            <slice>
              <time_slice>47:13</time_slice>
              <text_slice>would get from the standard
normal tables for Z.</text_slice>
            </slice>
            <slice>
              <time_slice>47:17</time_slice>
              <text_slice>It's a statement about
the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>47:19</time_slice>
              <text_slice>distribution functions.</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>This quantity, as a function
of c, is the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>47:25</time_slice>
              <text_slice>distribution function of
the random variable Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>47:27</time_slice>
              <text_slice>This is the cumulative
distribution function of the</text_slice>
            </slice>
            <slice>
              <time_slice>47:30</time_slice>
              <text_slice>standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>47:32</time_slice>
              <text_slice>The central limit theorem tells
us that the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>47:34</time_slice>
              <text_slice>distribution function of the
sum of a number of random</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>variables, after they're
appropriately standardized,</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>approaches the cumulative
distribution function over the</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>standard normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>In particular, this tells
us that we can calculate</text_slice>
            </slice>
            <slice>
              <time_slice>47:53</time_slice>
              <text_slice>probabilities for Zn when n is
large by calculating instead</text_slice>
            </slice>
            <slice>
              <time_slice>47:59</time_slice>
              <text_slice>probabilities for Z. And that's
going to be a good</text_slice>
            </slice>
            <slice>
              <time_slice>48:02</time_slice>
              <text_slice>approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>48:04</time_slice>
              <text_slice>Probabilities for Z are easy to
calculate because they're</text_slice>
            </slice>
            <slice>
              <time_slice>48:07</time_slice>
              <text_slice>well tabulated.</text_slice>
            </slice>
            <slice>
              <time_slice>48:09</time_slice>
              <text_slice>So we get a very nice shortcut
for calculating</text_slice>
            </slice>
            <slice>
              <time_slice>48:12</time_slice>
              <text_slice>probabilities for Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>48:14</time_slice>
              <text_slice>Now, it's not Zn that you're
interested in.</text_slice>
            </slice>
            <slice>
              <time_slice>48:17</time_slice>
              <text_slice>What you're interested
in is Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>48:20</time_slice>
              <text_slice>And Sn --</text_slice>
            </slice>
            <slice>
              <time_slice>48:23</time_slice>
              <text_slice>inverting this relation
here --</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>Sn is square root n sigma
Zn plus n expected</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>value of X. All right.</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>Now, if you can calculate
probabilities for Zn, even</text_slice>
            </slice>
            <slice>
              <time_slice>48:46</time_slice>
              <text_slice>approximately, then you can
certainly calculate</text_slice>
            </slice>
            <slice>
              <time_slice>48:49</time_slice>
              <text_slice>probabilities for Sn, because
one is a linear</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>function of the other.</text_slice>
            </slice>
            <slice>
              <time_slice>48:55</time_slice>
              <text_slice>And we're going to do a little
bit of that next time.</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>You're going to get, also, some
practice in recitation.</text_slice>
            </slice>
            <slice>
              <time_slice>49:02</time_slice>
              <text_slice>At a more vague level, you could
describe the central</text_slice>
            </slice>
            <slice>
              <time_slice>49:04</time_slice>
              <text_slice>limit theorem as saying the
following, when n is large,</text_slice>
            </slice>
            <slice>
              <time_slice>49:08</time_slice>
              <text_slice>you can pretend that Zn is
a standard normal random</text_slice>
            </slice>
            <slice>
              <time_slice>49:12</time_slice>
              <text_slice>variable and do the calculations
as if Zn was</text_slice>
            </slice>
            <slice>
              <time_slice>49:15</time_slice>
              <text_slice>standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>49:16</time_slice>
              <text_slice>Now, pretending that Zn is
normal is the same as</text_slice>
            </slice>
            <slice>
              <time_slice>49:21</time_slice>
              <text_slice>pretending that Sn is normal,
because Sn is a linear</text_slice>
            </slice>
            <slice>
              <time_slice>49:25</time_slice>
              <text_slice>function of Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>49:27</time_slice>
              <text_slice>And we know that linear
functions of normal random</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>variables are normal.</text_slice>
            </slice>
            <slice>
              <time_slice>49:32</time_slice>
              <text_slice>So the central limit theorem
essentially tells us that we</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>can pretend that Sn is a normal
random variable and do</text_slice>
            </slice>
            <slice>
              <time_slice>49:40</time_slice>
              <text_slice>the calculations just as if it
were a normal random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>49:44</time_slice>
              <text_slice>Mathematically speaking though,
the central limit</text_slice>
            </slice>
            <slice>
              <time_slice>49:47</time_slice>
              <text_slice>theorem does not talk about
the distribution of Sn,</text_slice>
            </slice>
            <slice>
              <time_slice>49:50</time_slice>
              <text_slice>because the distribution of Sn
becomes degenerate in the</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>limit, just a very flat
and long thing.</text_slice>
            </slice>
            <slice>
              <time_slice>49:57</time_slice>
              <text_slice>So strictly speaking
mathematically, it's a</text_slice>
            </slice>
            <slice>
              <time_slice>49:59</time_slice>
              <text_slice>statement about cumulative
distributions of Zn's.</text_slice>
            </slice>
            <slice>
              <time_slice>50:03</time_slice>
              <text_slice>Practically, the way you use it
is by just pretending that</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>Sn is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>50:08</time_slice>
              <text_slice>Very good.</text_slice>
            </slice>
            <slice>
              <time_slice>50:09</time_slice>
              <text_slice>Enjoy the Thanksgiving Holiday.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Continuous Random Variables (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l08/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Mixed distributions Gaussian (normal) PDF
Schematic drawing of a combination of 1 2Standard normal (0 1): ()=x /2N , fXx e
a PDF and a PMF 2
1/2
Normal CDF F (x)Normal PDF f (x) Xx1
0.5
-1 0 12-1 0 12 x x  
E[X] = var( X)=1
0 1/2 1
x 0
General normal N(2,):
The corresponding CDF: 1()222fX( x  / x)= e2FX(x)= P(Xx)
CDF
out that:
1It turns
E[2X]=and Var( X)=.
3/4
Let Y=aX+b
1/4Then: E[Y] = Var( Y)=
Fact: Y (2N a +2b, a)
1/2 1 x 
Calculating normal probabilities The constellation of concepts
No closed form available for CDF
but there are tables
(for standard normal) pX(x) fX(x)
F( )X Xx
IfXN(2,), thenN()E[X],var(X) 
IfXpN(2,16): X,Y(x, y) fX,Y(x, y)
pX Y(x|y) f3|Y(xX2X y)
P(X2| |
3) =P/parenleftbigg
4/parenrightbigg
= CDF(0.25)4
.00 .01 .02 .03 .04 .05 .06 .07 .08 .09
0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359
0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .57530.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .61410.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .65170.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879
0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224
0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .75490.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .78520.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .81330.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389
1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621
1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .88301.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .90151.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .91771.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319
1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441
1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .95451.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .96331.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .97061.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767
2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817
Sec. 3.3 Normal Random Variables 155
2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857
2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890
2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916
2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936
2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952
2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964
2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974
2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981
2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986
3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990
3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993
3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995
3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997
3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998
The standard normal table. The entries in this table provide the numerical values
of(y)=P(Yy), where Yis a standard normal random variable, for ybetween 0
and 3.49. For example, to nd (1.71), we look at the row corresponding to 1.7 and
the column corresponding to 0.01, so that (1.71) = .9564. When yis negative, the
value of (y) can be found using the formula (y)=1(y ).
2</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 8 Continuous r.v.s and pdfs
A continuous r.v. is described by aReadings: Sections 3.1-3.3probability density function fX
Lecture outline
f (x)XSample Space
Probability density functions
Cumulative distribution functions
a b x Event { a &lt; X &lt; b }
Normal random variables
P(aXb)=/integraldisplayb
fX(x)dx
a
/integraldisplay
fX(x)dx=1

P(xXx+)=/integraldisplayx+
fX(s)ds
xfX(x) 
P(XB)=/integraldisplay
fX(x)dx, for nice sets B
B
Means and variances Cumulative distribution function
E[X]=/integraldisplay (CDF)
xfX(x)dx

xE[g(X)]/integraldisplay
 = g(x)f X(x)dxFX(x)= P(Xx)=/integraldisplay
fX(t)dt

var( )=2=/integraldisplay
( [ ])2X  xEX f X(x)X  dx
 f(x CDFX )
Continuous Uniform r.v.
f  (x  ) X
a b x a b x 
Also for discrete r.v.s:
a b x FX(x)= P(Xx)=/summationdisplay
pX(k)
kx
fX(x)= axb3/6
2/6
E[X]= 1/6
/integraldisplayb/parenleftbigg+/parenrightbigg21 ( )2ax 
2 a b b 4
= x
a1 2 4 x 1 2dx=X2 ba 12
1</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-20-the-central-limit-theorem/</video_url>
          <video_title>Lecture 20: Central Limit Theorem</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high-quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: We're going to finish
today our discussion of</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>limit theorems.</text_slice>
            </slice>
            <slice>
              <time_slice>0:27</time_slice>
              <text_slice>I'm going to remind you what the
central limit theorem is,</text_slice>
            </slice>
            <slice>
              <time_slice>0:30</time_slice>
              <text_slice>which we introduced
briefly last time.</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>We're going to discuss what
exactly it says and its</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>implications.</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>And then we're going to apply
to a couple of examples,</text_slice>
            </slice>
            <slice>
              <time_slice>0:42</time_slice>
              <text_slice>mostly on the binomial
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>0:45</time_slice>
              <text_slice>OK, so the situation is that
we are dealing with a large</text_slice>
            </slice>
            <slice>
              <time_slice>0:49</time_slice>
              <text_slice>number of independent,
identically</text_slice>
            </slice>
            <slice>
              <time_slice>0:52</time_slice>
              <text_slice>distributed random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>0:55</time_slice>
              <text_slice>And we want to look at the sum
of them and say something</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>about the distribution
of the sum.</text_slice>
            </slice>
            <slice>
              <time_slice>1:03</time_slice>
              <text_slice>We might want to say that
the sum is distributed</text_slice>
            </slice>
            <slice>
              <time_slice>1:06</time_slice>
              <text_slice>approximately as a normal random
variable, although,</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>formally, this is
not quite right.</text_slice>
            </slice>
            <slice>
              <time_slice>1:12</time_slice>
              <text_slice>As n goes to infinity, the
distribution of the sum</text_slice>
            </slice>
            <slice>
              <time_slice>1:16</time_slice>
              <text_slice>becomes very spread out, and
it doesn't converge to a</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>limiting distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>In order to get an interesting
limit, we need first to take</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>the sum and standardize it.</text_slice>
            </slice>
            <slice>
              <time_slice>1:28</time_slice>
              <text_slice>By standardizing it, what we
mean is to subtract the mean</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>and then divide by the
standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>Now, the mean is, of course, n
times the expected value of</text_slice>
            </slice>
            <slice>
              <time_slice>1:41</time_slice>
              <text_slice>each one of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>And the standard deviation
is the</text_slice>
            </slice>
            <slice>
              <time_slice>1:45</time_slice>
              <text_slice>square root of the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>The variance is n times sigma
squared, where sigma is the</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>variance of the X's --</text_slice>
            </slice>
            <slice>
              <time_slice>1:52</time_slice>
              <text_slice>so that's the standard
deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>And after we do this, we obtain
a random variable that</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>has 0 mean -- its centered
-- and the</text_slice>
            </slice>
            <slice>
              <time_slice>2:01</time_slice>
              <text_slice>variance is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>And so the variance stays the
same, no matter how large n is</text_slice>
            </slice>
            <slice>
              <time_slice>2:07</time_slice>
              <text_slice>going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>2:08</time_slice>
              <text_slice>So the distribution of Zn keeps
changing with n, but it</text_slice>
            </slice>
            <slice>
              <time_slice>2:12</time_slice>
              <text_slice>cannot change too much.</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>It stays in place.</text_slice>
            </slice>
            <slice>
              <time_slice>2:15</time_slice>
              <text_slice>The mean is 0, and the width
remains also roughly the same</text_slice>
            </slice>
            <slice>
              <time_slice>2:19</time_slice>
              <text_slice>because the variance is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>The surprising thing is that, as
n grows, that distribution</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>of Zn kind of settles in a
certain asymptotic shape.</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>And that's the shape
of a standard</text_slice>
            </slice>
            <slice>
              <time_slice>2:33</time_slice>
              <text_slice>normal random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:35</time_slice>
              <text_slice>So standard normal means
that it has 0</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>mean and unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>More precisely, what the central
limit theorem tells us</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>is a relation between the
cumulative distribution</text_slice>
            </slice>
            <slice>
              <time_slice>2:46</time_slice>
              <text_slice>function of Zn and its relation
to the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>distribution function of
the standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>2:51</time_slice>
              <text_slice>So for any given number, c,
the probability that Zn is</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>less than or equal to c, in the
limit, becomes the same as</text_slice>
            </slice>
            <slice>
              <time_slice>3:01</time_slice>
              <text_slice>the probability that the
standard normal becomes less</text_slice>
            </slice>
            <slice>
              <time_slice>3:04</time_slice>
              <text_slice>than or equal to c.</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>And of course, this is useful
because these probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>3:08</time_slice>
              <text_slice>are available from the normal
tables, whereas the</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>distribution of Zn might be a
very complicated expression if</text_slice>
            </slice>
            <slice>
              <time_slice>3:15</time_slice>
              <text_slice>you were to calculate
it exactly.</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>So some comments about the
central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>First thing is that it's quite
amazing that it's universal.</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>It doesn't matter what the
distribution of the X's is.</text_slice>
            </slice>
            <slice>
              <time_slice>3:31</time_slice>
              <text_slice>It can be any distribution
whatsoever, as long as it has</text_slice>
            </slice>
            <slice>
              <time_slice>3:35</time_slice>
              <text_slice>finite mean and finite
variance.</text_slice>
            </slice>
            <slice>
              <time_slice>3:39</time_slice>
              <text_slice>And when you go and do your
approximations using the</text_slice>
            </slice>
            <slice>
              <time_slice>3:42</time_slice>
              <text_slice>central limit theorem, the only
thing that you need to</text_slice>
            </slice>
            <slice>
              <time_slice>3:44</time_slice>
              <text_slice>know about the distribution
of the X's are the</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>mean and the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>You need those in order
to standardize Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>3:52</time_slice>
              <text_slice>I mean -- to subtract the mean
and divide by the standard</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>deviation --</text_slice>
            </slice>
            <slice>
              <time_slice>3:56</time_slice>
              <text_slice>you need to know the mean
and the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>But these are the only things
that you need to know in order</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>to apply it.</text_slice>
            </slice>
            <slice>
              <time_slice>4:06</time_slice>
              <text_slice>In addition, it's
a very accurate</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>computational shortcut.</text_slice>
            </slice>
            <slice>
              <time_slice>4:10</time_slice>
              <text_slice>So the distribution of this
Zn's, in principle, you can</text_slice>
            </slice>
            <slice>
              <time_slice>4:14</time_slice>
              <text_slice>calculate it by convolution of
the distribution of the X's</text_slice>
            </slice>
            <slice>
              <time_slice>4:18</time_slice>
              <text_slice>with itself many, many times.</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>But this is tedious, and if you
try to do it analytically,</text_slice>
            </slice>
            <slice>
              <time_slice>4:23</time_slice>
              <text_slice>it might be a very complicated
expression.</text_slice>
            </slice>
            <slice>
              <time_slice>4:26</time_slice>
              <text_slice>Whereas by just appealing to the
standard normal table for</text_slice>
            </slice>
            <slice>
              <time_slice>4:29</time_slice>
              <text_slice>the standard normal random
variable, things are done in a</text_slice>
            </slice>
            <slice>
              <time_slice>4:33</time_slice>
              <text_slice>very quick way.</text_slice>
            </slice>
            <slice>
              <time_slice>4:35</time_slice>
              <text_slice>So it's a nice computational
shortcut if you don't want to</text_slice>
            </slice>
            <slice>
              <time_slice>4:39</time_slice>
              <text_slice>get an exact answer to a
probability problem.</text_slice>
            </slice>
            <slice>
              <time_slice>4:42</time_slice>
              <text_slice>Now, at a more philosophical
level, it justifies why we are</text_slice>
            </slice>
            <slice>
              <time_slice>4:47</time_slice>
              <text_slice>really interested in normal
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>Whenever you have a phenomenon
which is noisy, and the noise</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>that you observe is created by
adding the lots of little</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>pieces of randomness that are
independent of each other, the</text_slice>
            </slice>
            <slice>
              <time_slice>5:03</time_slice>
              <text_slice>overall effect that you're
going to observe can be</text_slice>
            </slice>
            <slice>
              <time_slice>5:06</time_slice>
              <text_slice>described by a normal
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:10</time_slice>
              <text_slice>So in a classic example that
goes 100 years back or so,</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>suppose that you have a fluid,
and inside that fluid, there's</text_slice>
            </slice>
            <slice>
              <time_slice>5:19</time_slice>
              <text_slice>a little particle of dust
or whatever that's</text_slice>
            </slice>
            <slice>
              <time_slice>5:23</time_slice>
              <text_slice>suspended in there.</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>That little particle gets
hit by molecules</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>completely at random --</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>and so what you're going to see
is that particle kind of</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>moving randomly inside
that liquid.</text_slice>
            </slice>
            <slice>
              <time_slice>5:36</time_slice>
              <text_slice>Now that random motion, if you
ask, after one second, how</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>much is my particle displaced,
let's say, in the x-axis along</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>the x direction.</text_slice>
            </slice>
            <slice>
              <time_slice>5:47</time_slice>
              <text_slice>That displacement is very, very
well modeled by a normal</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:51</time_slice>
              <text_slice>And the reason is that the
position of that particle is</text_slice>
            </slice>
            <slice>
              <time_slice>5:55</time_slice>
              <text_slice>decided by the cumulative effect
of lots of random hits</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>by molecules that hit
that particle.</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>So that's a sort of celebrated
physical model that goes under</text_slice>
            </slice>
            <slice>
              <time_slice>6:11</time_slice>
              <text_slice>the name of Brownian motion.</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>And it's the same model that
some people use to describe</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>the movement in the
financial markets.</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>The argument might go that the
movement of prices has to do</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>with lots of little decisions
and lots of little events by</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>many, many different
actors that are</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>involved in the market.</text_slice>
            </slice>
            <slice>
              <time_slice>6:32</time_slice>
              <text_slice>So the distribution of stock
prices might be well described</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>by normal random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>At least that's what people
wanted to believe until</text_slice>
            </slice>
            <slice>
              <time_slice>6:43</time_slice>
              <text_slice>somewhat recently.</text_slice>
            </slice>
            <slice>
              <time_slice>6:45</time_slice>
              <text_slice>Now, the evidence is that,
actually, these distributions</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>are a little more heavy-tailed
in the sense that extreme</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>events are a little more likely
to occur that what</text_slice>
            </slice>
            <slice>
              <time_slice>6:55</time_slice>
              <text_slice>normal random variables would
seem to indicate.</text_slice>
            </slice>
            <slice>
              <time_slice>6:58</time_slice>
              <text_slice>But as a first model, again,
it could be a plausible</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>argument to have, at least as
a starting model, one that</text_slice>
            </slice>
            <slice>
              <time_slice>7:07</time_slice>
              <text_slice>involves normal random
variables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:10</time_slice>
              <text_slice>So this is the philosophical
side of things.</text_slice>
            </slice>
            <slice>
              <time_slice>7:13</time_slice>
              <text_slice>On the more accurate,
mathematical side, it's</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>important to appreciate
exactly quite kind of</text_slice>
            </slice>
            <slice>
              <time_slice>7:18</time_slice>
              <text_slice>statement the central
limit theorem is.</text_slice>
            </slice>
            <slice>
              <time_slice>7:21</time_slice>
              <text_slice>It's a statement about the
convergence of the CDF of</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>these standardized random
variables to</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>the CDF of a normal.</text_slice>
            </slice>
            <slice>
              <time_slice>7:29</time_slice>
              <text_slice>So it's a statement about
convergence of CDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>7:32</time_slice>
              <text_slice>It's not a statement about
convergence of PMFs, or</text_slice>
            </slice>
            <slice>
              <time_slice>7:36</time_slice>
              <text_slice>convergence of PDFs.</text_slice>
            </slice>
            <slice>
              <time_slice>7:39</time_slice>
              <text_slice>Now, if one makes additional
mathematical assumptions,</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>there are variations of the
central limit theorem that</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>talk about PDFs and PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>7:47</time_slice>
              <text_slice>But in general, that's not
necessarily the case.</text_slice>
            </slice>
            <slice>
              <time_slice>7:51</time_slice>
              <text_slice>And I'm going to illustrate
this with--</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>I have a plot here which
is not in your slides.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>But just to make the point,
consider two different</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>discrete distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>This discrete distribution
takes values 1, 4, 7.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>This discrete distribution can
take values 1, 2, 4, 6, and 7.</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>So this one has sort of a
periodicity of 3, this one,</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>the range of values is a little
more interesting.</text_slice>
            </slice>
            <slice>
              <time_slice>8:27</time_slice>
              <text_slice>The numbers in these two
distributions are cooked up so</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>that they have the same mean
and the same variance.</text_slice>
            </slice>
            <slice>
              <time_slice>8:34</time_slice>
              <text_slice>Now, what I'm going to do is
to take eight independent</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>copies of the random variable
and plot the PMF of the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>eight random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>Now, if I plot the PMF of the
sum of 8 of these, I get the</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>plot, which corresponds to these
bullets in this diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>8:59</time_slice>
              <text_slice>If I take 8 random variables,
according to this</text_slice>
            </slice>
            <slice>
              <time_slice>9:03</time_slice>
              <text_slice>distribution, and add them up
and compute their PMF, the PMF</text_slice>
            </slice>
            <slice>
              <time_slice>9:07</time_slice>
              <text_slice>I get is the one denoted
here by the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>The two PMFs look really
different, at least, when you</text_slice>
            </slice>
            <slice>
              <time_slice>9:15</time_slice>
              <text_slice>eyeball them.</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>On the other hand, if you were
to plot the CDFs of them, then</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>the CDFs, if you compare them
with the normal CDF, which is</text_slice>
            </slice>
            <slice>
              <time_slice>9:34</time_slice>
              <text_slice>this continuous curve, the CDF,
of course, it goes up in</text_slice>
            </slice>
            <slice>
              <time_slice>9:38</time_slice>
              <text_slice>steps because we're looking at
discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>But it's very close
to the normal CDF.</text_slice>
            </slice>
            <slice>
              <time_slice>9:47</time_slice>
              <text_slice>And if we, instead of n equal to
8, we were to take 16, then</text_slice>
            </slice>
            <slice>
              <time_slice>9:52</time_slice>
              <text_slice>the coincidence would
be even better.</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>So in terms of CDFs, when we add
8 or 16 of these, we get</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>very close to the normal CDF.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>We would get essentially the
same picture if I were to take</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>8 or 16 of these.</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>So the CDFs sit, essentially, on
top of each other, although</text_slice>
            </slice>
            <slice>
              <time_slice>10:11</time_slice>
              <text_slice>the two PMFs look
quite different.</text_slice>
            </slice>
            <slice>
              <time_slice>10:14</time_slice>
              <text_slice>So this is to appreciate that,
formally speaking, we only</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>have a statement about
CDFs, not about PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>Now in practice, how do you use
the central limit theorem?</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>Well, it tells us that we can
calculate probabilities by</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>treating Zn as if it
were a standard</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>normal random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>10:34</time_slice>
              <text_slice>Now Zn is a linear
function of Sn.</text_slice>
            </slice>
            <slice>
              <time_slice>10:38</time_slice>
              <text_slice>Conversely, Sn is a linear
function of Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>Linear functions of normals
are normal.</text_slice>
            </slice>
            <slice>
              <time_slice>10:45</time_slice>
              <text_slice>So if I pretend that Zn is
normal, it's essentially the</text_slice>
            </slice>
            <slice>
              <time_slice>10:49</time_slice>
              <text_slice>same as if we pretend
that Sn is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>10:53</time_slice>
              <text_slice>And so we can calculate
probabilities that have to do</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>with Sn as if Sn were normal.</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>Now, the central limit theorem
does not tell us that Sn is</text_slice>
            </slice>
            <slice>
              <time_slice>11:03</time_slice>
              <text_slice>approximately normal.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>The formal statement is about
Zn, but, practically speaking,</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>when you use the result,
you can just</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>pretend that Sn is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>11:14</time_slice>
              <text_slice>Finally, it's a limit theorem,
so it tells us about what</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>happens when n goes
to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>If we are to use it in practice,
of course, n is not</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>going to be infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>11:25</time_slice>
              <text_slice>Maybe n is equal to 15.</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>Can we use a limit theorem when
n is a small number, as</text_slice>
            </slice>
            <slice>
              <time_slice>11:32</time_slice>
              <text_slice>small as 15?</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>Well, it turns out that it's
a very good approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>11:36</time_slice>
              <text_slice>Even for quite small values
of n, it gives us</text_slice>
            </slice>
            <slice>
              <time_slice>11:41</time_slice>
              <text_slice>very accurate answers.</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>So n over the order of 15, or
20, or so give us very good</text_slice>
            </slice>
            <slice>
              <time_slice>11:49</time_slice>
              <text_slice>results in practice.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>There are no good theorems
that will give us hard</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>guarantees because the quality
of the approximation does</text_slice>
            </slice>
            <slice>
              <time_slice>11:58</time_slice>
              <text_slice>depend on the details of the
distribution of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>12:03</time_slice>
              <text_slice>If the X's have a distribution
that, from the outset, looks a</text_slice>
            </slice>
            <slice>
              <time_slice>12:07</time_slice>
              <text_slice>little bit like the normal, then
for small values of n,</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>you are going to see,
essentially, a normal</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>distribution for the sum.</text_slice>
            </slice>
            <slice>
              <time_slice>12:16</time_slice>
              <text_slice>If the distribution of the X's
is very different from the</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>normal, it's going to take a
larger value of n for the</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>central limit theorem
to take effect.</text_slice>
            </slice>
            <slice>
              <time_slice>12:25</time_slice>
              <text_slice>So let's illustrates this with
a few representative plots.</text_slice>
            </slice>
            <slice>
              <time_slice>12:32</time_slice>
              <text_slice>So here, we're starting with a
discrete uniform distribution</text_slice>
            </slice>
            <slice>
              <time_slice>12:36</time_slice>
              <text_slice>that goes from 1 to 8.</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>Let's add 2 of these random
variables, 2 random variables</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>with this PMF, and find
the PMF of the sum.</text_slice>
            </slice>
            <slice>
              <time_slice>12:47</time_slice>
              <text_slice>This is a convolution of 2
discrete uniforms, and I</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>believe you have seen this
exercise before.</text_slice>
            </slice>
            <slice>
              <time_slice>12:54</time_slice>
              <text_slice>When you convolve this with
itself, you get a triangle.</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>So this is the PMF for the sum
of two discrete uniforms.</text_slice>
            </slice>
            <slice>
              <time_slice>13:04</time_slice>
              <text_slice>Now let's continue.</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>Let's convolve this
with itself.</text_slice>
            </slice>
            <slice>
              <time_slice>13:07</time_slice>
              <text_slice>These was going to give
us the PMF of a sum</text_slice>
            </slice>
            <slice>
              <time_slice>13:10</time_slice>
              <text_slice>of 4 discrete uniforms.</text_slice>
            </slice>
            <slice>
              <time_slice>13:13</time_slice>
              <text_slice>And we get this, which starts
looking like a normal.</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>If we go to n equal to 32, then
it looks, essentially,</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>exactly like a normal.</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>And it's an excellent
approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>13:27</time_slice>
              <text_slice>So this is the PMF of the sum
of 32 discrete random</text_slice>
            </slice>
            <slice>
              <time_slice>13:32</time_slice>
              <text_slice>variables with this uniform
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>If we start with a PMF which
is not symmetric--</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>this one is symmetric
around the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>But if we start with a PMF which
is non-symmetric, so</text_slice>
            </slice>
            <slice>
              <time_slice>13:47</time_slice>
              <text_slice>this is, here, is a truncated
geometric PMF, then things do</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>not work out as nicely when
I add 8 of these.</text_slice>
            </slice>
            <slice>
              <time_slice>13:58</time_slice>
              <text_slice>That is, if I convolve this
with itself 8 times, I get</text_slice>
            </slice>
            <slice>
              <time_slice>14:03</time_slice>
              <text_slice>this PMF, which maybe resembles
a little bit to the</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>normal one.</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>But you can really tell that
it's different from the normal</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>if you focus at the details
here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>14:16</time_slice>
              <text_slice>Here it sort of rises sharply.</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>Here it tails off
a bit slower.</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>So there's an asymmetry here
that's present, and which is a</text_slice>
            </slice>
            <slice>
              <time_slice>14:27</time_slice>
              <text_slice>consequence of the
asymmetry of the</text_slice>
            </slice>
            <slice>
              <time_slice>14:29</time_slice>
              <text_slice>distribution we started with.</text_slice>
            </slice>
            <slice>
              <time_slice>14:31</time_slice>
              <text_slice>If we go to 16, it looks a
little better, but still you</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>can see the asymmetry between
this tail and that tail.</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>If you get to 32 there's still a
little bit of asymmetry, but</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>at least now it starts looking
like a normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>14:48</time_slice>
              <text_slice>So the moral from these plots
is that it might vary, a</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>little bit, what kind of values
of n you need before</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>you get the really good
approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>But for values of n in the range
20 to 30 or so, usually</text_slice>
            </slice>
            <slice>
              <time_slice>15:04</time_slice>
              <text_slice>you expect to get a pretty
good approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>At least that's what the visual
inspection of these</text_slice>
            </slice>
            <slice>
              <time_slice>15:10</time_slice>
              <text_slice>graphs tells us.</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>So now that we know that we have
a good approximation in</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>our hands, let's use it.</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>Let's use it by revisiting an
example from last time.</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>This is the polling problem.</text_slice>
            </slice>
            <slice>
              <time_slice>15:24</time_slice>
              <text_slice>We're interested in the fraction
of population that</text_slice>
            </slice>
            <slice>
              <time_slice>15:28</time_slice>
              <text_slice>has a certain habit been.</text_slice>
            </slice>
            <slice>
              <time_slice>15:30</time_slice>
              <text_slice>And we try to find what f is.</text_slice>
            </slice>
            <slice>
              <time_slice>15:33</time_slice>
              <text_slice>And the way we do it is by
polling people at random and</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>recording the answers that they
give, whether they have</text_slice>
            </slice>
            <slice>
              <time_slice>15:40</time_slice>
              <text_slice>the habit or not.</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>So for each person, we get the
Bernoulli random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>15:45</time_slice>
              <text_slice>With probability f, a person is
going to respond 1, or yes,</text_slice>
            </slice>
            <slice>
              <time_slice>15:52</time_slice>
              <text_slice>so this is with probability f.</text_slice>
            </slice>
            <slice>
              <time_slice>15:55</time_slice>
              <text_slice>And with the remaining
probability 1-f, the person</text_slice>
            </slice>
            <slice>
              <time_slice>15:58</time_slice>
              <text_slice>responds no.</text_slice>
            </slice>
            <slice>
              <time_slice>16:00</time_slice>
              <text_slice>We record this number, which
is how many people answered</text_slice>
            </slice>
            <slice>
              <time_slice>16:04</time_slice>
              <text_slice>yes, divided by the total
number of people.</text_slice>
            </slice>
            <slice>
              <time_slice>16:06</time_slice>
              <text_slice>That's the fraction of the
population that we asked.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>This is the fraction inside our
sample that answered yes.</text_slice>
            </slice>
            <slice>
              <time_slice>16:16</time_slice>
              <text_slice>And as we discussed last time,
you might start with some</text_slice>
            </slice>
            <slice>
              <time_slice>16:21</time_slice>
              <text_slice>specs for the poll.</text_slice>
            </slice>
            <slice>
              <time_slice>16:23</time_slice>
              <text_slice>And the specs have
two parameters--</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>the accuracy that you want and
the confidence that you want</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>to have that you did really
obtain the desired accuracy.</text_slice>
            </slice>
            <slice>
              <time_slice>16:33</time_slice>
              <text_slice>So the specs here is that we
want, probability 95% that our</text_slice>
            </slice>
            <slice>
              <time_slice>16:40</time_slice>
              <text_slice>estimate is within 1 % point
from the true answer.</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>So the event of interest
is this.</text_slice>
            </slice>
            <slice>
              <time_slice>16:48</time_slice>
              <text_slice>That's the result of the poll
minus distance from the true</text_slice>
            </slice>
            <slice>
              <time_slice>16:53</time_slice>
              <text_slice>answer is less or bigger
than 1 % point.</text_slice>
            </slice>
            <slice>
              <time_slice>16:59</time_slice>
              <text_slice>And we're interested in
calculating or approximating</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>this particular probability.</text_slice>
            </slice>
            <slice>
              <time_slice>17:04</time_slice>
              <text_slice>So we want to do it using the
central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>And one way of arranging the
mechanics of this calculation</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>is to take the event of interest
and massage it by</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>subtracting and dividing things
from both sides of this</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>inequality so that you bring
him to the picture the</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>standardized random variable,
the Zn, and then apply the</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>17:33</time_slice>
              <text_slice>So the event of interest, let
me write it in full, Mn is</text_slice>
            </slice>
            <slice>
              <time_slice>17:38</time_slice>
              <text_slice>this quantity, so I'm putting it
here, minus f, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>same as nf divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>So this is the same
as that event.</text_slice>
            </slice>
            <slice>
              <time_slice>17:46</time_slice>
              <text_slice>We're going to calculate the
probability of this.</text_slice>
            </slice>
            <slice>
              <time_slice>17:49</time_slice>
              <text_slice>This is not exactly in the form
in which we apply the</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>17:53</time_slice>
              <text_slice>To apply the central limit
theorem, we need, down here,</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>to have sigma square root n.</text_slice>
            </slice>
            <slice>
              <time_slice>17:59</time_slice>
              <text_slice>So how can I put sigma
square root n here?</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>I can divide both sides of
this inequality by sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>18:07</time_slice>
              <text_slice>And then I can take a factor of
square root n from here and</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>send it to the other side.</text_slice>
            </slice>
            <slice>
              <time_slice>18:13</time_slice>
              <text_slice>So this event is the
same as that event.</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>This will happen if and only
if that will happen.</text_slice>
            </slice>
            <slice>
              <time_slice>18:19</time_slice>
              <text_slice>So calculating the probability
of this event here is the same</text_slice>
            </slice>
            <slice>
              <time_slice>18:23</time_slice>
              <text_slice>as calculating the probability
that this events happens.</text_slice>
            </slice>
            <slice>
              <time_slice>18:27</time_slice>
              <text_slice>And now we are in business
because the random variable</text_slice>
            </slice>
            <slice>
              <time_slice>18:30</time_slice>
              <text_slice>that we got in here is Zn, or
the absolute value of Zn, and</text_slice>
            </slice>
            <slice>
              <time_slice>18:36</time_slice>
              <text_slice>we're talking about the
probability that Zn, absolute</text_slice>
            </slice>
            <slice>
              <time_slice>18:41</time_slice>
              <text_slice>value of Zn, is bigger than
a certain number.</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>Since Zn is to be approximated
by a standard normal random</text_slice>
            </slice>
            <slice>
              <time_slice>18:50</time_slice>
              <text_slice>variable, our approximation is
going to be, instead of asking</text_slice>
            </slice>
            <slice>
              <time_slice>18:54</time_slice>
              <text_slice>for Zn being bigger than this
number, we will ask for Z,</text_slice>
            </slice>
            <slice>
              <time_slice>18:59</time_slice>
              <text_slice>absolute value of Z, being
bigger than this number.</text_slice>
            </slice>
            <slice>
              <time_slice>19:02</time_slice>
              <text_slice>So this is the probability that
we want to calculate.</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>And now Z is a standard normal
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>19:09</time_slice>
              <text_slice>There's a small difficulty,
the one that we also</text_slice>
            </slice>
            <slice>
              <time_slice>19:12</time_slice>
              <text_slice>encountered last time.</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>And the difficulty is that the
standard deviation, sigma, of</text_slice>
            </slice>
            <slice>
              <time_slice>19:18</time_slice>
              <text_slice>the Xi's is not known.</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>Sigma is equal to f times--</text_slice>
            </slice>
            <slice>
              <time_slice>19:24</time_slice>
              <text_slice>sigma, in this example, is f
times (1-f), and the only</text_slice>
            </slice>
            <slice>
              <time_slice>19:30</time_slice>
              <text_slice>thing that we know about sigma
is that it's going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>number less than 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>OK, so we're going to have to
use an inequality here.</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>We're going to use a
conservative value of sigma,</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>the value of sigma equal to 1/2
and use that instead of</text_slice>
            </slice>
            <slice>
              <time_slice>19:54</time_slice>
              <text_slice>the exact value of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>19:55</time_slice>
              <text_slice>And this gives us an inequality
going this way.</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>Let's just make sure why the
inequality goes this way.</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>We got, on our axis,
two numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>One number is 0.01 square
root n divided by sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>And the other number is
0.02 square root of n.</text_slice>
            </slice>
            <slice>
              <time_slice>20:27</time_slice>
              <text_slice>And my claim is that the numbers
are related to each</text_slice>
            </slice>
            <slice>
              <time_slice>20:30</time_slice>
              <text_slice>other in this particular way.</text_slice>
            </slice>
            <slice>
              <time_slice>20:32</time_slice>
              <text_slice>Why is this?</text_slice>
            </slice>
            <slice>
              <time_slice>20:33</time_slice>
              <text_slice>Sigma is less than 2.</text_slice>
            </slice>
            <slice>
              <time_slice>20:35</time_slice>
              <text_slice>So 1/sigma is bigger than 2.</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>So since 1/sigma is bigger than
2 this means that this</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>numbers sits to the right
of that number.</text_slice>
            </slice>
            <slice>
              <time_slice>20:47</time_slice>
              <text_slice>So here we have the probability
that Z is bigger</text_slice>
            </slice>
            <slice>
              <time_slice>20:51</time_slice>
              <text_slice>than this number.</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>The probability of falling out
there is less than the</text_slice>
            </slice>
            <slice>
              <time_slice>20:59</time_slice>
              <text_slice>probability of falling
in this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>21:03</time_slice>
              <text_slice>So that's what that last
inequality is saying--</text_slice>
            </slice>
            <slice>
              <time_slice>21:06</time_slice>
              <text_slice>this probability is smaller
than that probability.</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>This is the probability that
we're interested in, but since</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>we don't know sigma, we take the
conservative value, and we</text_slice>
            </slice>
            <slice>
              <time_slice>21:16</time_slice>
              <text_slice>use an upper bound in terms
of the probability of this</text_slice>
            </slice>
            <slice>
              <time_slice>21:21</time_slice>
              <text_slice>interval here.</text_slice>
            </slice>
            <slice>
              <time_slice>21:23</time_slice>
              <text_slice>And now we are in business.</text_slice>
            </slice>
            <slice>
              <time_slice>21:26</time_slice>
              <text_slice>We can start using our normal
tables to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>probabilities of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>21:33</time_slice>
              <text_slice>So for example, let's say that's
we take n to be 10,000.</text_slice>
            </slice>
            <slice>
              <time_slice>21:40</time_slice>
              <text_slice>How is the calculation
going to go?</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>We want to calculate the
probability that the absolute</text_slice>
            </slice>
            <slice>
              <time_slice>21:45</time_slice>
              <text_slice>value of Z is bigger than 0.2
times 1000, which is the</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>probability that the absolute
value of Z is larger than or</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>And here let's do
some mechanics,</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>just to stay in shape.</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>The probability that you're
larger than or equal to 2 in</text_slice>
            </slice>
            <slice>
              <time_slice>22:05</time_slice>
              <text_slice>absolute value, since the normal
is symmetric around the</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>mean, this is going to be twice
the probability that Z</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>is larger than or equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:16</time_slice>
              <text_slice>Can we use the cumulative
distribution function of Z to</text_slice>
            </slice>
            <slice>
              <time_slice>22:22</time_slice>
              <text_slice>calculate this?</text_slice>
            </slice>
            <slice>
              <time_slice>22:23</time_slice>
              <text_slice>Well, almost the cumulative
gives us probabilities of</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>being less than something, not
bigger than something.</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>So we need one more step and
write this as 1 minus the</text_slice>
            </slice>
            <slice>
              <time_slice>22:33</time_slice>
              <text_slice>probability that Z is less
than or equal to 2.</text_slice>
            </slice>
            <slice>
              <time_slice>22:38</time_slice>
              <text_slice>And this probability, now,
you can read off</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>from the normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>And the normal tables will
tell you that this</text_slice>
            </slice>
            <slice>
              <time_slice>22:46</time_slice>
              <text_slice>probability is 0.9772.</text_slice>
            </slice>
            <slice>
              <time_slice>22:52</time_slice>
              <text_slice>And you do get an answer.</text_slice>
            </slice>
            <slice>
              <time_slice>22:54</time_slice>
              <text_slice>And the answer is 0.0456.</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>OK, so we tried 10,000.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>And we find that our probably
of error is 4.5%, so we're</text_slice>
            </slice>
            <slice>
              <time_slice>23:10</time_slice>
              <text_slice>doing better than the
spec that we had.</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>So this tells us that maybe
we have some leeway.</text_slice>
            </slice>
            <slice>
              <time_slice>23:19</time_slice>
              <text_slice>Maybe we can use a smaller
sample size and still stay</text_slice>
            </slice>
            <slice>
              <time_slice>23:24</time_slice>
              <text_slice>without our specs.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>Let's try to find how much
we can push the envelope.</text_slice>
            </slice>
            <slice>
              <time_slice>23:29</time_slice>
              <text_slice>How much smaller
can we take n?</text_slice>
            </slice>
            <slice>
              <time_slice>23:34</time_slice>
              <text_slice>To answer that question, we
need to do this kind of</text_slice>
            </slice>
            <slice>
              <time_slice>23:37</time_slice>
              <text_slice>calculation, essentially,
going backwards.</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>We're going to fix this number
to be 0.05 and work backwards</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>here to find--</text_slice>
            </slice>
            <slice>
              <time_slice>23:49</time_slice>
              <text_slice>did I do a mistake here?</text_slice>
            </slice>
            <slice>
              <time_slice>23:50</time_slice>
              <text_slice>10,000.</text_slice>
            </slice>
            <slice>
              <time_slice>23:51</time_slice>
              <text_slice>So I'm missing a 0 here.</text_slice>
            </slice>
            <slice>
              <time_slice>23:57</time_slice>
              <text_slice>Ah, but I'm taking the square
root, so it's 100.</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>Where did the 0.02
come in from?</text_slice>
            </slice>
            <slice>
              <time_slice>24:11</time_slice>
              <text_slice>Ah, from here.</text_slice>
            </slice>
            <slice>
              <time_slice>24:12</time_slice>
              <text_slice>OK, all right.</text_slice>
            </slice>
            <slice>
              <time_slice>24:15</time_slice>
              <text_slice>0.02 times 100, that
gives us 2.</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>OK, all right.</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>Very good, OK.</text_slice>
            </slice>
            <slice>
              <time_slice>24:24</time_slice>
              <text_slice>So we'll have to do this
calculation now backwards,</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>figure out if this is 0.05,
what kind of number we're</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>going to need here and then
here, and from this we will be</text_slice>
            </slice>
            <slice>
              <time_slice>24:41</time_slice>
              <text_slice>able to tell what value
of n do we need.</text_slice>
            </slice>
            <slice>
              <time_slice>24:45</time_slice>
              <text_slice>OK, so we want to find n such
that the probability that Z is</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>bigger than 0.02 square
root n is 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>25:04</time_slice>
              <text_slice>OK, so Z is a standard normal
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>And we want the probability
that we are</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>outside this range.</text_slice>
            </slice>
            <slice>
              <time_slice>25:18</time_slice>
              <text_slice>We want the probability of
those two tails together.</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>Those two tails together
should have</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>probability of 0.05.</text_slice>
            </slice>
            <slice>
              <time_slice>25:29</time_slice>
              <text_slice>This means that this tail,
by itself, should have</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>probability 0.025.</text_slice>
            </slice>
            <slice>
              <time_slice>25:36</time_slice>
              <text_slice>And this means that this
probability should be 0.975.</text_slice>
            </slice>
            <slice>
              <time_slice>25:45</time_slice>
              <text_slice>Now, if this probability
is to be 0.975, what</text_slice>
            </slice>
            <slice>
              <time_slice>25:52</time_slice>
              <text_slice>should that number be?</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>You go to the normal tables,
and you find which is the</text_slice>
            </slice>
            <slice>
              <time_slice>25:59</time_slice>
              <text_slice>entry that corresponds
to that number.</text_slice>
            </slice>
            <slice>
              <time_slice>26:03</time_slice>
              <text_slice>I actually brought a normal
table with me.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>And 0.975 is down here.</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>And it tells you that
to the number that</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>corresponds to it is 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>26:19</time_slice>
              <text_slice>So this tells us that
this number</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>should be equal to 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>And now, from here, you
do the calculations.</text_slice>
            </slice>
            <slice>
              <time_slice>26:36</time_slice>
              <text_slice>And you find that n is 9604.</text_slice>
            </slice>
            <slice>
              <time_slice>26:47</time_slice>
              <text_slice>So with a sample of 10,000, we
got probability of error 4.5%.</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>With a slightly smaller sample
size of 9,600, we can get the</text_slice>
            </slice>
            <slice>
              <time_slice>26:57</time_slice>
              <text_slice>probability of a mistake
to be 0.05, which</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>was exactly our spec.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>So these are essentially the two
ways that you're going to</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>be using the central
limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>Either you're given n and
you try to calculate</text_slice>
            </slice>
            <slice>
              <time_slice>27:12</time_slice>
              <text_slice>probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>Or you're given the
probabilities, and you want to</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>work backwards to
find n itself.</text_slice>
            </slice>
            <slice>
              <time_slice>27:20</time_slice>
              <text_slice>So in this example, the random
variable that we dealt with</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>was, of course, a binomial
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>27:30</time_slice>
              <text_slice>The Xi's were Bernoulli,
so the sum of</text_slice>
            </slice>
            <slice>
              <time_slice>27:38</time_slice>
              <text_slice>the Xi's were binomial.</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>So the central limit theorem
certainly applies to the</text_slice>
            </slice>
            <slice>
              <time_slice>27:44</time_slice>
              <text_slice>binomial distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>27:45</time_slice>
              <text_slice>To be more precise, of course,
it applies to the standardized</text_slice>
            </slice>
            <slice>
              <time_slice>27:49</time_slice>
              <text_slice>version of the binomial
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>So here's what we did,
essentially, in</text_slice>
            </slice>
            <slice>
              <time_slice>27:55</time_slice>
              <text_slice>the previous example.</text_slice>
            </slice>
            <slice>
              <time_slice>27:57</time_slice>
              <text_slice>We fixed the number p, which is
the probability of success</text_slice>
            </slice>
            <slice>
              <time_slice>28:00</time_slice>
              <text_slice>in our experiments.</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>p corresponds to f in the
previous example.</text_slice>
            </slice>
            <slice>
              <time_slice>28:06</time_slice>
              <text_slice>Let every Xi a Bernoulli
random variable and are</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>standing assumption is that
these random variables are</text_slice>
            </slice>
            <slice>
              <time_slice>28:13</time_slice>
              <text_slice>independent.</text_slice>
            </slice>
            <slice>
              <time_slice>28:17</time_slice>
              <text_slice>When we add them, we get a
random variable that has a</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>binomial distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>28:22</time_slice>
              <text_slice>We know the mean and the
variance of the binomial, so</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>we take Sn, we subtract the
mean, which is this, divide by</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>28:30</time_slice>
              <text_slice>The central limit theorem tells
us that the cumulative</text_slice>
            </slice>
            <slice>
              <time_slice>28:32</time_slice>
              <text_slice>distribution function of this
random variable is a standard</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>normal random variable
in the limit.</text_slice>
            </slice>
            <slice>
              <time_slice>28:39</time_slice>
              <text_slice>So let's do one more example
of a calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>Let's take n to be--</text_slice>
            </slice>
            <slice>
              <time_slice>28:47</time_slice>
              <text_slice>let's choose some specific
numbers to work with.</text_slice>
            </slice>
            <slice>
              <time_slice>28:52</time_slice>
              <text_slice>So in this example, first thing
to do is to find the</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>expected value of Sn,
which is n times p.</text_slice>
            </slice>
            <slice>
              <time_slice>29:02</time_slice>
              <text_slice>It's 18.</text_slice>
            </slice>
            <slice>
              <time_slice>29:04</time_slice>
              <text_slice>Then we need to write down
the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>The variance of Sn is the
sum of the variances.</text_slice>
            </slice>
            <slice>
              <time_slice>29:16</time_slice>
              <text_slice>It's np times (1-p).</text_slice>
            </slice>
            <slice>
              <time_slice>29:19</time_slice>
              <text_slice>And in this particular example,
p times (1-p) is 1/4,</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>n is 36, so this is 9.</text_slice>
            </slice>
            <slice>
              <time_slice>29:28</time_slice>
              <text_slice>And that tells us that the
standard deviation of this n</text_slice>
            </slice>
            <slice>
              <time_slice>29:33</time_slice>
              <text_slice>is equal to 3.</text_slice>
            </slice>
            <slice>
              <time_slice>29:37</time_slice>
              <text_slice>So what we're going to do is to
take the event of interest,</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>which is Sn less than 21, and
rewrite it in a way that</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>involves the standardized
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>So to do that, we need
to subtract the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>So we write this as Sn-3
should be less</text_slice>
            </slice>
            <slice>
              <time_slice>29:55</time_slice>
              <text_slice>than or equal to 21-3.</text_slice>
            </slice>
            <slice>
              <time_slice>29:58</time_slice>
              <text_slice>This is the same event.</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>And then divide by the standard
deviation, which is</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>3, and we end up with this.</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>So the event itself of--</text_slice>
            </slice>
            <slice>
              <time_slice>30:08</time_slice>
              <text_slice>AUDIENCE: [INAUDIBLE].</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>Should subtract, 18, yes, which
gives me a much nicer</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>number out here, which is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>So the event of interest, that
Sn is less than 21, is the</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>same as the event that a
standard normal random</text_slice>
            </slice>
            <slice>
              <time_slice>30:37</time_slice>
              <text_slice>variable is less than
or equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>30:41</time_slice>
              <text_slice>And once more, you can look this
up at the normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>And you find that the answer
that you get is 0.43.</text_slice>
            </slice>
            <slice>
              <time_slice>30:50</time_slice>
              <text_slice>Now it's interesting to compare
this answer that we</text_slice>
            </slice>
            <slice>
              <time_slice>30:53</time_slice>
              <text_slice>got through the central limit
theorem with the exact answer.</text_slice>
            </slice>
            <slice>
              <time_slice>30:57</time_slice>
              <text_slice>The exact answer involves the
exact binomial distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>31:01</time_slice>
              <text_slice>What we have here is the
binomial probability that, Sn</text_slice>
            </slice>
            <slice>
              <time_slice>31:08</time_slice>
              <text_slice>is equal to k.</text_slice>
            </slice>
            <slice>
              <time_slice>31:10</time_slice>
              <text_slice>Sn being equal to k is given
by this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>31:15</time_slice>
              <text_slice>And we add, over all values for
k going from 0 up to 21,</text_slice>
            </slice>
            <slice>
              <time_slice>31:22</time_slice>
              <text_slice>we write a two lines code to
calculate this sum, and we get</text_slice>
            </slice>
            <slice>
              <time_slice>31:28</time_slice>
              <text_slice>the exact answer,
which is 0.8785.</text_slice>
            </slice>
            <slice>
              <time_slice>31:32</time_slice>
              <text_slice>So there's a pretty good
agreements between the two,</text_slice>
            </slice>
            <slice>
              <time_slice>31:35</time_slice>
              <text_slice>although you wouldn't
call that's</text_slice>
            </slice>
            <slice>
              <time_slice>31:38</time_slice>
              <text_slice>necessarily excellent agreement.</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>Can we do a little
better than that?</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>OK.</text_slice>
            </slice>
            <slice>
              <time_slice>31:53</time_slice>
              <text_slice>It turns out that we can.</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>And here's the idea.</text_slice>
            </slice>
            <slice>
              <time_slice>32:02</time_slice>
              <text_slice>So our random variable
Sn has a mean of 18.</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>It has a binomial
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>It's described by a PMF that has
a shape roughly like this</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>and which keeps going on.</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>Using the central limit
theorem is basically</text_slice>
            </slice>
            <slice>
              <time_slice>32:20</time_slice>
              <text_slice>pretending that Sn is
normal with the</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>right mean and variance.</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>So pretending that Zn has
0 mean unit variance, we</text_slice>
            </slice>
            <slice>
              <time_slice>32:35</time_slice>
              <text_slice>approximate it with Z, that
has 0 mean unit variance.</text_slice>
            </slice>
            <slice>
              <time_slice>32:38</time_slice>
              <text_slice>If you were to pretend that
Sn is normal, you would</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>approximate it with a normal
that has the correct mean and</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>correct variance.</text_slice>
            </slice>
            <slice>
              <time_slice>32:46</time_slice>
              <text_slice>So it would still be
centered at 18.</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>And it would have the same
variance as the binomial PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>So using the central limit
theorem essentially means that</text_slice>
            </slice>
            <slice>
              <time_slice>32:57</time_slice>
              <text_slice>we keep the mean and the
variance what they are but we</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>pretend that our distribution
is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>33:03</time_slice>
              <text_slice>We want to calculate the
probability that Sn is less</text_slice>
            </slice>
            <slice>
              <time_slice>33:06</time_slice>
              <text_slice>than or equal to 21.</text_slice>
            </slice>
            <slice>
              <time_slice>33:09</time_slice>
              <text_slice>I pretend that my random
variable is normal, so I draw</text_slice>
            </slice>
            <slice>
              <time_slice>33:14</time_slice>
              <text_slice>a line here and I calculate
the area under the normal</text_slice>
            </slice>
            <slice>
              <time_slice>33:18</time_slice>
              <text_slice>curve going up to 21.</text_slice>
            </slice>
            <slice>
              <time_slice>33:22</time_slice>
              <text_slice>That's essentially
what we did.</text_slice>
            </slice>
            <slice>
              <time_slice>33:26</time_slice>
              <text_slice>Now, a smart person comes
around and says, Sn is a</text_slice>
            </slice>
            <slice>
              <time_slice>33:29</time_slice>
              <text_slice>discrete random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>33:31</time_slice>
              <text_slice>So the event that Sn is less
than or equal to 21 is the</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>same as Sn being strictly less
than 22 because nothing in</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>between can happen.</text_slice>
            </slice>
            <slice>
              <time_slice>33:41</time_slice>
              <text_slice>So I'm going to use the
central limit theorem</text_slice>
            </slice>
            <slice>
              <time_slice>33:43</time_slice>
              <text_slice>approximation by pretending
again that Sn is normal and</text_slice>
            </slice>
            <slice>
              <time_slice>33:48</time_slice>
              <text_slice>finding the probability of this
event while pretending</text_slice>
            </slice>
            <slice>
              <time_slice>33:51</time_slice>
              <text_slice>that Sn is normal.</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>So what this person would do
would be to draw a line here,</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>at 22, and calculate the area
under the normal curve</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>all the way to 22.</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>Who is right?</text_slice>
            </slice>
            <slice>
              <time_slice>34:06</time_slice>
              <text_slice>Which one is better?</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>Well neither, but we can do
better than both if we sort of</text_slice>
            </slice>
            <slice>
              <time_slice>34:15</time_slice>
              <text_slice>split the difference.</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>So another way of writing the
same event for Sn is to write</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>it as Sn being less than 21.5.</text_slice>
            </slice>
            <slice>
              <time_slice>34:25</time_slice>
              <text_slice>In terms of the discrete random
variable Sn, all three</text_slice>
            </slice>
            <slice>
              <time_slice>34:29</time_slice>
              <text_slice>of these are exactly
the same event.</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>But when you do the continuous
approximation, they give you</text_slice>
            </slice>
            <slice>
              <time_slice>34:35</time_slice>
              <text_slice>different probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>34:36</time_slice>
              <text_slice>It's a matter of whether you
integrate the area under the</text_slice>
            </slice>
            <slice>
              <time_slice>34:39</time_slice>
              <text_slice>normal curve up to here, up to
the midway point, or up to 22.</text_slice>
            </slice>
            <slice>
              <time_slice>34:46</time_slice>
              <text_slice>It turns out that integrating
up to the midpoint is what</text_slice>
            </slice>
            <slice>
              <time_slice>34:50</time_slice>
              <text_slice>gives us the better
numerical results.</text_slice>
            </slice>
            <slice>
              <time_slice>34:54</time_slice>
              <text_slice>So we take here 21 and 1/2,
and we integrate the area</text_slice>
            </slice>
            <slice>
              <time_slice>34:59</time_slice>
              <text_slice>under the normal curve
up to here.</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>So let's do this calculation
and see what we get.</text_slice>
            </slice>
            <slice>
              <time_slice>35:18</time_slice>
              <text_slice>What would we change here?</text_slice>
            </slice>
            <slice>
              <time_slice>35:21</time_slice>
              <text_slice>Instead of 21, we would
now write 21 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>35:27</time_slice>
              <text_slice>This 18 becomes, no, that
18 stays what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>But this 21 becomes
21 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>35:36</time_slice>
              <text_slice>And so this one becomes
1 + 0.5 by 3.</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>This is 117.</text_slice>
            </slice>
            <slice>
              <time_slice>35:48</time_slice>
              <text_slice>So we now look up into the
normal tables and ask for the</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>probability that Z is
less than 1.17.</text_slice>
            </slice>
            <slice>
              <time_slice>36:00</time_slice>
              <text_slice>So this here gets approximated
by the probability that the</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>standard normal is
less than 1.17.</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>And the normal tables will
tell us this is 0.879.</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>Going back to the previous
slide, what we got this time</text_slice>
            </slice>
            <slice>
              <time_slice>36:23</time_slice>
              <text_slice>with this improved approximation
is 0.879.</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>This is a really good
approximation</text_slice>
            </slice>
            <slice>
              <time_slice>36:33</time_slice>
              <text_slice>of the correct number.</text_slice>
            </slice>
            <slice>
              <time_slice>36:35</time_slice>
              <text_slice>This is what we got
using the 21.</text_slice>
            </slice>
            <slice>
              <time_slice>36:39</time_slice>
              <text_slice>This is what we get using
the 21 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>And it's an approximation that's
sort of right on-- a</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>very good one.</text_slice>
            </slice>
            <slice>
              <time_slice>36:48</time_slice>
              <text_slice>The moral from this numerical
example is that doing this 1</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>and 1/2 correction does give
us better approximations.</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>In fact, we can use this 1/2
idea to even calculate</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>individual probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>37:14</time_slice>
              <text_slice>So suppose you want to
approximate the probability</text_slice>
            </slice>
            <slice>
              <time_slice>37:17</time_slice>
              <text_slice>that Sn equal to 19.</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>If you were to pretend that Sn
is normal and calculate this</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>probability, the probability
that the normal random</text_slice>
            </slice>
            <slice>
              <time_slice>37:28</time_slice>
              <text_slice>variable is equal to 19 is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>37:31</time_slice>
              <text_slice>So you don't get an interesting
answer.</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>You get a more interesting
answer by writing this event,</text_slice>
            </slice>
            <slice>
              <time_slice>37:37</time_slice>
              <text_slice>19 as being the same as the
event of falling between 18</text_slice>
            </slice>
            <slice>
              <time_slice>37:41</time_slice>
              <text_slice>and 1/2 and 19 and 1/2 and using
the normal approximation</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>to calculate this probability.</text_slice>
            </slice>
            <slice>
              <time_slice>37:48</time_slice>
              <text_slice>In terms of our previous
picture, this corresponds to</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>the following.</text_slice>
            </slice>
            <slice>
              <time_slice>37:59</time_slice>
              <text_slice>We are interested in the
probability that</text_slice>
            </slice>
            <slice>
              <time_slice>38:04</time_slice>
              <text_slice>Sn is equal to 19.</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>So we're interested in the
height of this bar.</text_slice>
            </slice>
            <slice>
              <time_slice>38:11</time_slice>
              <text_slice>We're going to consider the area
under the normal curve</text_slice>
            </slice>
            <slice>
              <time_slice>38:15</time_slice>
              <text_slice>going from here to here,
and use this area as an</text_slice>
            </slice>
            <slice>
              <time_slice>38:21</time_slice>
              <text_slice>approximation for the height
of that particular bar.</text_slice>
            </slice>
            <slice>
              <time_slice>38:25</time_slice>
              <text_slice>So what we're basically doing
is, we take the probability</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>under the normal curve that's
assigned over a continuum of</text_slice>
            </slice>
            <slice>
              <time_slice>38:33</time_slice>
              <text_slice>values and attributed it to
different discrete values.</text_slice>
            </slice>
            <slice>
              <time_slice>38:38</time_slice>
              <text_slice>Whatever is above the midpoint
gets attributed to 19.</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>Whatever is below that
midpoint gets</text_slice>
            </slice>
            <slice>
              <time_slice>38:45</time_slice>
              <text_slice>attributed to 18.</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>So this is green area is our
approximation of the value of</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>the PMF at 19.</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>So similarly, if you wanted to
approximate the value of the</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>PMF at this point, you would
take this interval and</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>integrate the area
under the normal</text_slice>
            </slice>
            <slice>
              <time_slice>39:06</time_slice>
              <text_slice>curve over that interval.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>It turns out that this gives a
very good approximation of the</text_slice>
            </slice>
            <slice>
              <time_slice>39:13</time_slice>
              <text_slice>PMF of the binomial.</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>And actually, this was the
context in which the central</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>limit theorem was proved in
the first place, when this</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>business started.</text_slice>
            </slice>
            <slice>
              <time_slice>39:27</time_slice>
              <text_slice>So this business goes back
a few hundred years.</text_slice>
            </slice>
            <slice>
              <time_slice>39:33</time_slice>
              <text_slice>And the central limit theorem
was first approved by</text_slice>
            </slice>
            <slice>
              <time_slice>39:35</time_slice>
              <text_slice>considering the PMF of a
binomial random variable when</text_slice>
            </slice>
            <slice>
              <time_slice>39:39</time_slice>
              <text_slice>p is equal to 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>39:41</time_slice>
              <text_slice>People did the algebra, and they
found out that the exact</text_slice>
            </slice>
            <slice>
              <time_slice>39:45</time_slice>
              <text_slice>expression for the PMF is quite
well approximated by</text_slice>
            </slice>
            <slice>
              <time_slice>39:49</time_slice>
              <text_slice>that expression hat you would
get from a normal</text_slice>
            </slice>
            <slice>
              <time_slice>39:51</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>Then the proof was extended to
binomials for more general</text_slice>
            </slice>
            <slice>
              <time_slice>39:57</time_slice>
              <text_slice>values of p.</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>So here we talk about this as
a refinement of the general</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>central limit theorem, but,
historically, that refinement</text_slice>
            </slice>
            <slice>
              <time_slice>40:07</time_slice>
              <text_slice>was where the whole business
got started</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>in the first place.</text_slice>
            </slice>
            <slice>
              <time_slice>40:11</time_slice>
              <text_slice>All right, so let's go through
the mechanics of approximating</text_slice>
            </slice>
            <slice>
              <time_slice>40:18</time_slice>
              <text_slice>the probability that
Sn is equal to 19--</text_slice>
            </slice>
            <slice>
              <time_slice>40:21</time_slice>
              <text_slice>exactly 19.</text_slice>
            </slice>
            <slice>
              <time_slice>40:23</time_slice>
              <text_slice>As we said, we're going to write
this event as an event</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>that covers an interval of unit
length from 18 and 1/2 to</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>19 and 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>This is the event of interest.</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>First step is to massage the
event of interest so that it</text_slice>
            </slice>
            <slice>
              <time_slice>40:37</time_slice>
              <text_slice>involves our Zn random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>So subtract 18 from all sides.</text_slice>
            </slice>
            <slice>
              <time_slice>40:43</time_slice>
              <text_slice>Divide by the standard deviation
of 3 from all sides.</text_slice>
            </slice>
            <slice>
              <time_slice>40:46</time_slice>
              <text_slice>That's the equivalent
representation of the event.</text_slice>
            </slice>
            <slice>
              <time_slice>40:50</time_slice>
              <text_slice>This is our standardized
random variable Zn.</text_slice>
            </slice>
            <slice>
              <time_slice>40:54</time_slice>
              <text_slice>These are just these numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>And to do an approximation, we
want to find the probability</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>of this event, but Zn is
approximately normal, so we</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>plug in here the Z, which
is the standard normal.</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>So we want to find the
probability that the standard</text_slice>
            </slice>
            <slice>
              <time_slice>41:10</time_slice>
              <text_slice>normal falls inside
this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>41:12</time_slice>
              <text_slice>You find these using CDFs
because this is the</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>probability that you're
less than this but</text_slice>
            </slice>
            <slice>
              <time_slice>41:18</time_slice>
              <text_slice>not less than that.</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>So it's a difference between two
cumulative probabilities.</text_slice>
            </slice>
            <slice>
              <time_slice>41:25</time_slice>
              <text_slice>Then, you look up your
normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>41:27</time_slice>
              <text_slice>You find two numbers for these
quantities, and, finally, you</text_slice>
            </slice>
            <slice>
              <time_slice>41:30</time_slice>
              <text_slice>get a numerical answer for an
individual entry of the PMF of</text_slice>
            </slice>
            <slice>
              <time_slice>41:35</time_slice>
              <text_slice>the binomial.</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>This is a pretty good
approximation, it turns out.</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>If you were to do the
calculations using the exact</text_slice>
            </slice>
            <slice>
              <time_slice>41:42</time_slice>
              <text_slice>formula, you would
get something</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>which is pretty close--</text_slice>
            </slice>
            <slice>
              <time_slice>41:49</time_slice>
              <text_slice>an error in the third digit--</text_slice>
            </slice>
            <slice>
              <time_slice>41:52</time_slice>
              <text_slice>this is pretty good.</text_slice>
            </slice>
            <slice>
              <time_slice>41:56</time_slice>
              <text_slice>So I guess what we did here
with our discussion of the</text_slice>
            </slice>
            <slice>
              <time_slice>41:59</time_slice>
              <text_slice>binomial slightly contradicts
what I said before--</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>that the central limit theorem
is a statement about</text_slice>
            </slice>
            <slice>
              <time_slice>42:07</time_slice>
              <text_slice>cumulative distribution
functions.</text_slice>
            </slice>
            <slice>
              <time_slice>42:09</time_slice>
              <text_slice>In general, it doesn't tell you
what to do to approximate</text_slice>
            </slice>
            <slice>
              <time_slice>42:13</time_slice>
              <text_slice>PMFs themselves.</text_slice>
            </slice>
            <slice>
              <time_slice>42:15</time_slice>
              <text_slice>And that's indeed the
case in general.</text_slice>
            </slice>
            <slice>
              <time_slice>42:17</time_slice>
              <text_slice>One the other hand, for the
special case of a binomial</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>distribution, the central limit
theorem approximation,</text_slice>
            </slice>
            <slice>
              <time_slice>42:23</time_slice>
              <text_slice>with this 1/2 correction, is a
very good approximation even</text_slice>
            </slice>
            <slice>
              <time_slice>42:28</time_slice>
              <text_slice>for the individual PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>42:33</time_slice>
              <text_slice>All right, so we spent quite
a bit of time on mechanics.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>So let's spend the last few
minutes today thinking a bit</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>and look at a small puzzle.</text_slice>
            </slice>
            <slice>
              <time_slice>42:51</time_slice>
              <text_slice>So the puzzle is
the following.</text_slice>
            </slice>
            <slice>
              <time_slice>42:54</time_slice>
              <text_slice>Consider Poisson process that
runs over a unit interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>And where the arrival
rate is equal to 1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:07</time_slice>
              <text_slice>So this is the unit interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>And let X be the number
of arrivals.</text_slice>
            </slice>
            <slice>
              <time_slice>43:15</time_slice>
              <text_slice>And this is Poisson,
with mean 1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>Now, let me take this interval
and divide it</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>into n little pieces.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>So each piece has length 1/n.</text_slice>
            </slice>
            <slice>
              <time_slice>43:34</time_slice>
              <text_slice>And let Xi be the number
of arrivals during</text_slice>
            </slice>
            <slice>
              <time_slice>43:41</time_slice>
              <text_slice>the Ith little interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:48</time_slice>
              <text_slice>OK, what do we know about
the random variables Xi?</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>Is they are themselves
Poisson.</text_slice>
            </slice>
            <slice>
              <time_slice>43:55</time_slice>
              <text_slice>It's a number of arrivals
during a small interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:58</time_slice>
              <text_slice>We also know that when n is
big, so the length of the</text_slice>
            </slice>
            <slice>
              <time_slice>44:02</time_slice>
              <text_slice>interval is small, these Xi's
are approximately Bernoulli,</text_slice>
            </slice>
            <slice>
              <time_slice>44:08</time_slice>
              <text_slice>with mean 1/n.</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>Guess it doesn't matter whether
we model them as</text_slice>
            </slice>
            <slice>
              <time_slice>44:13</time_slice>
              <text_slice>Bernoulli or not.</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>What matters is that the
Xi's are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>44:19</time_slice>
              <text_slice>Why are they independent?</text_slice>
            </slice>
            <slice>
              <time_slice>44:20</time_slice>
              <text_slice>Because, in a Poisson process,
these joint intervals are</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>independent of each other.</text_slice>
            </slice>
            <slice>
              <time_slice>44:26</time_slice>
              <text_slice>So the Xi's are independent.</text_slice>
            </slice>
            <slice>
              <time_slice>44:31</time_slice>
              <text_slice>And they also have the
same distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>44:35</time_slice>
              <text_slice>And we have that X, the total
number of arrivals, is the sum</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>over the Xn's.</text_slice>
            </slice>
            <slice>
              <time_slice>44:44</time_slice>
              <text_slice>So the central limit theorem
tells us that, approximately,</text_slice>
            </slice>
            <slice>
              <time_slice>44:49</time_slice>
              <text_slice>the sum of independent,
identically distributed random</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>variables, when we have lots
of these random variables,</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>behaves like a normal
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:01</time_slice>
              <text_slice>So by using this decomposition
of X into a sum of i.i.d</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>random variables, and by using
values of n that are bigger</text_slice>
            </slice>
            <slice>
              <time_slice>45:11</time_slice>
              <text_slice>and bigger, by taking the limit,
it should follow that X</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>has a normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>On the other hand, we know
that X has a Poisson</text_slice>
            </slice>
            <slice>
              <time_slice>45:22</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>So something must be wrong
in this argument here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>Can we really use the
central limit</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>theorem in this situation?</text_slice>
            </slice>
            <slice>
              <time_slice>45:38</time_slice>
              <text_slice>So what do we need for the
central limit theorem?</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>We need to have independent,
identically</text_slice>
            </slice>
            <slice>
              <time_slice>45:44</time_slice>
              <text_slice>distributed random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>We have it here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:49</time_slice>
              <text_slice>We want them to have a finite
mean and finite variance.</text_slice>
            </slice>
            <slice>
              <time_slice>45:53</time_slice>
              <text_slice>We also have it here, means
variances are finite.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>What is another assumption that
was never made explicit,</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>but essentially was there?</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>Or in other words, what is the
flaw in this argument that</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>uses the central limit
theorem here?</text_slice>
            </slice>
            <slice>
              <time_slice>46:15</time_slice>
              <text_slice>Any thoughts?</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>So in the central limit theorem,
we said, consider--</text_slice>
            </slice>
            <slice>
              <time_slice>46:29</time_slice>
              <text_slice>fix a probability distribution,
and let the Xi's</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>be distributed according to that
probability distribution,</text_slice>
            </slice>
            <slice>
              <time_slice>46:38</time_slice>
              <text_slice>and add a larger and larger
number or Xi's.</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>But the underlying, unstated
assumption is that we fix the</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>distribution of the Xi's.</text_slice>
            </slice>
            <slice>
              <time_slice>46:49</time_slice>
              <text_slice>As we let n increase,
the statistics of</text_slice>
            </slice>
            <slice>
              <time_slice>46:52</time_slice>
              <text_slice>each Xi do not change.</text_slice>
            </slice>
            <slice>
              <time_slice>46:55</time_slice>
              <text_slice>Whereas here, I'm playing
a trick on you.</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>As I'm taking more and more
random variables, I'm actually</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>changing what those random
variables are.</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>When I take a larger n, the Xi's
are random variables with</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>a different mean and
different variance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:15</time_slice>
              <text_slice>So I'm adding more of these, but
at the same time, in this</text_slice>
            </slice>
            <slice>
              <time_slice>47:19</time_slice>
              <text_slice>example, I'm changing
their distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>47:23</time_slice>
              <text_slice>That's something that doesn't
fit the setting of the central</text_slice>
            </slice>
            <slice>
              <time_slice>47:26</time_slice>
              <text_slice>limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>47:27</time_slice>
              <text_slice>In the central limit theorem,
you first fix the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>You keep it fixed, and then you
consider adding more and</text_slice>
            </slice>
            <slice>
              <time_slice>47:35</time_slice>
              <text_slice>more according to that
particular fixed distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>47:38</time_slice>
              <text_slice>So that's the catch.</text_slice>
            </slice>
            <slice>
              <time_slice>47:40</time_slice>
              <text_slice>That's why the central limit
theorem does not</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>apply to this situation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:43</time_slice>
              <text_slice>And we're lucky that it
doesn't apply because,</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>otherwise, we would have a huge
contradiction destroying</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>47:52</time_slice>
              <text_slice>OK, but now that's still
leaves us with a</text_slice>
            </slice>
            <slice>
              <time_slice>48:02</time_slice>
              <text_slice>little bit of a dilemma.</text_slice>
            </slice>
            <slice>
              <time_slice>48:05</time_slice>
              <text_slice>Suppose that, here, essentially
we're adding</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>independent Bernoulli
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>So the issue is that the central
limit theorem has to</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>do with asymptotics as
n goes to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>And if we consider a binomial,
and somebody gives us specific</text_slice>
            </slice>
            <slice>
              <time_slice>48:34</time_slice>
              <text_slice>numbers about the parameters of
that binomial, it might not</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>necessarily be obvious
what kind of</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>approximation do we use.</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>In particular, we do have two
different approximations for</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>the binomial.</text_slice>
            </slice>
            <slice>
              <time_slice>48:47</time_slice>
              <text_slice>If we fix p, then the binomial
is the sum of Bernoulli's that</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>come from a fixed distribution,
we consider more</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>and more of these.</text_slice>
            </slice>
            <slice>
              <time_slice>48:56</time_slice>
              <text_slice>When we add them, the central
limit theorem tells us that we</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>get the normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>49:01</time_slice>
              <text_slice>There's another sort of limit,
which has the flavor of this</text_slice>
            </slice>
            <slice>
              <time_slice>49:04</time_slice>
              <text_slice>example, in which we still deal
with a binomial, sum of n</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>Bernoulli's.</text_slice>
            </slice>
            <slice>
              <time_slice>49:11</time_slice>
              <text_slice>We let that sum, the
number of the</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>Bernoulli's go to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>49:16</time_slice>
              <text_slice>But each Bernoulli has a
probability of success that</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>goes to 0, and we do this in a
way so that np, the expected</text_slice>
            </slice>
            <slice>
              <time_slice>49:23</time_slice>
              <text_slice>number of successes,
stays finite.</text_slice>
            </slice>
            <slice>
              <time_slice>49:27</time_slice>
              <text_slice>This is the situation that we
dealt with when we first</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>defined our Poisson process.</text_slice>
            </slice>
            <slice>
              <time_slice>49:32</time_slice>
              <text_slice>We have a very, very large
number so lots, of time slots,</text_slice>
            </slice>
            <slice>
              <time_slice>49:37</time_slice>
              <text_slice>but during each time slot,
there's a tiny probability of</text_slice>
            </slice>
            <slice>
              <time_slice>49:40</time_slice>
              <text_slice>obtaining an arrival.</text_slice>
            </slice>
            <slice>
              <time_slice>49:42</time_slice>
              <text_slice>Under that setting, in discrete
time, we have a</text_slice>
            </slice>
            <slice>
              <time_slice>49:48</time_slice>
              <text_slice>binomial distribution, or
Bernoulli process, but when we</text_slice>
            </slice>
            <slice>
              <time_slice>49:51</time_slice>
              <text_slice>take the limit, we obtain the
Poisson process and the</text_slice>
            </slice>
            <slice>
              <time_slice>49:54</time_slice>
              <text_slice>Poisson approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>49:56</time_slice>
              <text_slice>So these are two equally valid</text_slice>
            </slice>
            <slice>
              <time_slice>49:58</time_slice>
              <text_slice>approximations of the binomial.</text_slice>
            </slice>
            <slice>
              <time_slice>50:00</time_slice>
              <text_slice>But they're valid in different
asymptotic regimes.</text_slice>
            </slice>
            <slice>
              <time_slice>50:03</time_slice>
              <text_slice>In one regime, we fixed p,
let n go to infinity.</text_slice>
            </slice>
            <slice>
              <time_slice>50:06</time_slice>
              <text_slice>In the other regime, we let
both n and p change</text_slice>
            </slice>
            <slice>
              <time_slice>50:09</time_slice>
              <text_slice>simultaneously.</text_slice>
            </slice>
            <slice>
              <time_slice>50:11</time_slice>
              <text_slice>Now, in real life, you're
never dealing with the</text_slice>
            </slice>
            <slice>
              <time_slice>50:14</time_slice>
              <text_slice>limiting situations.</text_slice>
            </slice>
            <slice>
              <time_slice>50:15</time_slice>
              <text_slice>You're dealing with
actual numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>50:17</time_slice>
              <text_slice>So if somebody tells you that
the numbers are like this,</text_slice>
            </slice>
            <slice>
              <time_slice>50:21</time_slice>
              <text_slice>then you should probably say
that this is the situation</text_slice>
            </slice>
            <slice>
              <time_slice>50:25</time_slice>
              <text_slice>that fits the Poisson
description--</text_slice>
            </slice>
            <slice>
              <time_slice>50:27</time_slice>
              <text_slice>large number of slots with
each slot having a tiny</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>probability of success.</text_slice>
            </slice>
            <slice>
              <time_slice>50:32</time_slice>
              <text_slice>On the other hand, if p is
something like this, and n is</text_slice>
            </slice>
            <slice>
              <time_slice>50:36</time_slice>
              <text_slice>500, then you expect to get
the distribution for the</text_slice>
            </slice>
            <slice>
              <time_slice>50:40</time_slice>
              <text_slice>number of successes.</text_slice>
            </slice>
            <slice>
              <time_slice>50:41</time_slice>
              <text_slice>It's going to have a mean of 50
and to have a fair amount</text_slice>
            </slice>
            <slice>
              <time_slice>50:45</time_slice>
              <text_slice>of spread around there.</text_slice>
            </slice>
            <slice>
              <time_slice>50:47</time_slice>
              <text_slice>It turns out that the normal
approximation would be better</text_slice>
            </slice>
            <slice>
              <time_slice>50:50</time_slice>
              <text_slice>in this context.</text_slice>
            </slice>
            <slice>
              <time_slice>50:51</time_slice>
              <text_slice>As a rule of thumb, if n times p
is bigger than 10 or 20, you</text_slice>
            </slice>
            <slice>
              <time_slice>50:57</time_slice>
              <text_slice>can start using the normal
approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>50:59</time_slice>
              <text_slice>If n times p is a small number,
then you prefer to use</text_slice>
            </slice>
            <slice>
              <time_slice>51:04</time_slice>
              <text_slice>the Poisson approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>51:06</time_slice>
              <text_slice>But there's no hard theorems
or rules about</text_slice>
            </slice>
            <slice>
              <time_slice>51:08</time_slice>
              <text_slice>how to go about this.</text_slice>
            </slice>
            <slice>
              <time_slice>51:11</time_slice>
              <text_slice>OK, so from next time we're
going to switch base again.</text_slice>
            </slice>
            <slice>
              <time_slice>51:15</time_slice>
              <text_slice>And we're going to put together
everything we learned</text_slice>
            </slice>
            <slice>
              <time_slice>51:17</time_slice>
              <text_slice>in this class to start solving
inference problems.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Discrete Random Variables; Probability Mass Functions; Expectations (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 5 Random variables
Readings: Sections 2.1-2.3, start 2.4 An assignment of a value (number) to
every possible outcome
Lecture outline
Mathematically: A function
Random variables from the sample space to the real
numbers
Probability mass function (PMF)
discrete or continuous values
Expectation
Can have several random variablesVariancedened on the same sample space
Notation:
random variable X
numerical value x
Probability mass function (PMF) How to compute a PMF pX(x)
collect all possible outcomes for which
(probability law, Xis equal to x
probability distribution of X) add their probabilities
repeat for all x
Notation:
Example: Two independent rools of a
pX(x) =P(X=x) fair tetrahedral die
=P({s.t.X ()=x})
F: outcome of rst throw
/summationtext S: outcome of second throwpX(x)0 xpX(x)=1X= min(F, S )
Example: X=number of coin tosses
until rst head 4
assume independent tosses,3
P(H)=p&gt;0S = Second roll
2
pX(k)=P(X=k)
=P(TT TH)1
=(1p)k1p, k =1,2,... 12 3 4
F = First roll
 geometric PMF
pX(2) =
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Binomial PMF Expectation
X: number of heads in nindependentDenition:
coin tosses E[X]=/summationdisplay
xpX(x)
x
P(H)=p
Interpretations:
Letn=4 Center of gravity of PMF
Average in large number of repetitionspX(2) = P(HHTT )+P(HTHT )+P(HTTH )of the experiment
+P(THHT )+P(THTH )+P(TTHH ) (to be substantiated later in this course)
=62(1)2p p Example: Uniform on 0 ,1, . . . , n
4=/parenleftBig
2/parenrightBig2)2p(1 p p(x )X
In general:1/(n+1)
k)nkpX(k)=/parenleftBign/parenrightBig
p(1p ,k =0,1, . . . , n ...
k
0 1 x n- 1 n
1 1 1E[X]=0 +1 ++n =n+1 n+1n+1
Properties of expectations Variance
LetXbe a r.v. and let Y=g(X)Recall: E[g(X)] =/summationdisplay
g(x)p X(x)
x
Hard: E[Y]=/summationdisplay
ypY(y)
y Second moment: E[2 2X]=/summationtext
xx p (
Easy: E[Y]=/summationdisplayXx)
g(x)p X(x)
x Variance
Caution: In general, E[g(X)] = g(E[X]) var(X)= E/bracketleftBig
(XE[X])2/bracketrightBig
=/summationdisplay
( [ ])2xEX p X(x)
x
2erties: If,are constants, then: =E[2Prop X](E[X])
E[]=
Properties :
E[X]=var(X)0
E[X+]= var(X+)=2var(X)/negationslash
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-21-bayesian-statistical-inference-i/</video_url>
          <video_title>Lecture 21: Bayesian Statistical Inference I</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:21</time_slice>
              <text_slice>PROFESSOR: It involves real
phenomena out there.</text_slice>
            </slice>
            <slice>
              <time_slice>0:25</time_slice>
              <text_slice>So we have real stuff
that happens.</text_slice>
            </slice>
            <slice>
              <time_slice>0:28</time_slice>
              <text_slice>So it might be an arrival
process to a bank that we're</text_slice>
            </slice>
            <slice>
              <time_slice>0:33</time_slice>
              <text_slice>trying to model.</text_slice>
            </slice>
            <slice>
              <time_slice>0:35</time_slice>
              <text_slice>This is a reality, but
this is what we have</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>been doing so far.</text_slice>
            </slice>
            <slice>
              <time_slice>0:39</time_slice>
              <text_slice>We have been playing
with models of</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>probabilistic phenomena.</text_slice>
            </slice>
            <slice>
              <time_slice>0:43</time_slice>
              <text_slice>And somehow we need to
tie the two together.</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>The way these are tied is that
we observe the real world and</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>this gives us data.</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>And then based on these data, we
try to come up with a model</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>of what exactly is going on.</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>For example, for an arrival
process, you might ask the</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>model in question, is my arrival
process Poisson or is</text_slice>
            </slice>
            <slice>
              <time_slice>1:08</time_slice>
              <text_slice>it something different?</text_slice>
            </slice>
            <slice>
              <time_slice>1:10</time_slice>
              <text_slice>If it is Poisson, what is the
rate of the arrival process?</text_slice>
            </slice>
            <slice>
              <time_slice>1:14</time_slice>
              <text_slice>Once you come up with your model
and you come up with the</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>parameters of the model, then
you can use it to make</text_slice>
            </slice>
            <slice>
              <time_slice>1:21</time_slice>
              <text_slice>predictions about reality or to
figure out certain hidden</text_slice>
            </slice>
            <slice>
              <time_slice>1:27</time_slice>
              <text_slice>things, certain hidden aspects
of reality, that you do not</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>observe directly, but you try
to infer what they are.</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>So that's where the usefulness
of the model comes in.</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>Now this field is of course
tremendously useful.</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>And it shows up pretty
much everywhere.</text_slice>
            </slice>
            <slice>
              <time_slice>1:46</time_slice>
              <text_slice>So we talked about the polling
examples in the</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>last couple of lectures.</text_slice>
            </slice>
            <slice>
              <time_slice>1:51</time_slice>
              <text_slice>This is, of course, a
real application.</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>You sample and on the basis of
the sample that you have, you</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>try to make some inferences
about, let's say, the</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>preferences in a given
population.</text_slice>
            </slice>
            <slice>
              <time_slice>2:03</time_slice>
              <text_slice>Let's say in the medical field,
you want to try whether</text_slice>
            </slice>
            <slice>
              <time_slice>2:06</time_slice>
              <text_slice>a certain drug makes a
difference or not.</text_slice>
            </slice>
            <slice>
              <time_slice>2:08</time_slice>
              <text_slice>So people would do medical
trials, get some results, and</text_slice>
            </slice>
            <slice>
              <time_slice>2:14</time_slice>
              <text_slice>then from the data somehow you
need to make sense of them and</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>make a decision.</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>Is the new drug useful
or is it not?</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>How do we go systematically
about the</text_slice>
            </slice>
            <slice>
              <time_slice>2:23</time_slice>
              <text_slice>question of this type?</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>A sexier, more recent topic,
there's this famous Netflix</text_slice>
            </slice>
            <slice>
              <time_slice>2:32</time_slice>
              <text_slice>competition where Netflix gives
you a huge table of</text_slice>
            </slice>
            <slice>
              <time_slice>2:37</time_slice>
              <text_slice>movies and people.</text_slice>
            </slice>
            <slice>
              <time_slice>2:41</time_slice>
              <text_slice>And people have rated the
movies, but not everyone has</text_slice>
            </slice>
            <slice>
              <time_slice>2:45</time_slice>
              <text_slice>watched all of the
movies in there.</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>You have some of the ratings.</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>For example, this person gave a
4 to that particular movie.</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>So you get the table that's
partially filled.</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>And the Netflix asks
you to make</text_slice>
            </slice>
            <slice>
              <time_slice>2:58</time_slice>
              <text_slice>recommendations to people.</text_slice>
            </slice>
            <slice>
              <time_slice>2:59</time_slice>
              <text_slice>So this means trying to guess.</text_slice>
            </slice>
            <slice>
              <time_slice>3:02</time_slice>
              <text_slice>This person here, how much
would they like this</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>particular movie?</text_slice>
            </slice>
            <slice>
              <time_slice>3:07</time_slice>
              <text_slice>And you can start thinking,
well, maybe this person has</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>given somewhat similar ratings
with another person.</text_slice>
            </slice>
            <slice>
              <time_slice>3:14</time_slice>
              <text_slice>And if that other person has
also seen that movie, maybe</text_slice>
            </slice>
            <slice>
              <time_slice>3:18</time_slice>
              <text_slice>the rating of that other
person is relevant.</text_slice>
            </slice>
            <slice>
              <time_slice>3:21</time_slice>
              <text_slice>But of course it's a lot more
complicated than that.</text_slice>
            </slice>
            <slice>
              <time_slice>3:24</time_slice>
              <text_slice>And this has been a serious
competition where people have</text_slice>
            </slice>
            <slice>
              <time_slice>3:26</time_slice>
              <text_slice>been using every heavy, wet
machinery that there is in</text_slice>
            </slice>
            <slice>
              <time_slice>3:30</time_slice>
              <text_slice>statistics, trying to
come up with good</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>recommendation systems.</text_slice>
            </slice>
            <slice>
              <time_slice>3:35</time_slice>
              <text_slice>Then the other people, of
course, are trying to analyze</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>financial data.</text_slice>
            </slice>
            <slice>
              <time_slice>3:39</time_slice>
              <text_slice>Somebody gives you the sequence
of the values, let's</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>say of the SMP index.</text_slice>
            </slice>
            <slice>
              <time_slice>3:45</time_slice>
              <text_slice>You look at something like this</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>and you can ask questions.</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>How do I model these data using
any of the models that</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>we have in our bag of tools?</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>How can I make predictions about
what's going to happen</text_slice>
            </slice>
            <slice>
              <time_slice>4:00</time_slice>
              <text_slice>afterwards, and so on?</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>On the engineering side,
anywhere where you have noise</text_slice>
            </slice>
            <slice>
              <time_slice>4:09</time_slice>
              <text_slice>inference comes in.</text_slice>
            </slice>
            <slice>
              <time_slice>4:11</time_slice>
              <text_slice>Signal processing, in
some sense, is just</text_slice>
            </slice>
            <slice>
              <time_slice>4:13</time_slice>
              <text_slice>an inference problem.</text_slice>
            </slice>
            <slice>
              <time_slice>4:14</time_slice>
              <text_slice>You observe signals that are
noisy and you try to figure</text_slice>
            </slice>
            <slice>
              <time_slice>4:18</time_slice>
              <text_slice>out exactly what's happening
out there or what kind of</text_slice>
            </slice>
            <slice>
              <time_slice>4:21</time_slice>
              <text_slice>signal has been sent.</text_slice>
            </slice>
            <slice>
              <time_slice>4:24</time_slice>
              <text_slice>Maybe the beginning of the field
could be traced a few</text_slice>
            </slice>
            <slice>
              <time_slice>4:28</time_slice>
              <text_slice>hundred years ago where people
would observe, make</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>astronomical observations
of the position of the</text_slice>
            </slice>
            <slice>
              <time_slice>4:35</time_slice>
              <text_slice>planets in the sky.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>They would have some beliefs
that perhaps the orbits of</text_slice>
            </slice>
            <slice>
              <time_slice>4:41</time_slice>
              <text_slice>planets is an ellipse.</text_slice>
            </slice>
            <slice>
              <time_slice>4:44</time_slice>
              <text_slice>Or if it's a comet, maybe it's
a parabola, hyperbola, don't</text_slice>
            </slice>
            <slice>
              <time_slice>4:47</time_slice>
              <text_slice>know what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>But they would have
a model of that.</text_slice>
            </slice>
            <slice>
              <time_slice>4:51</time_slice>
              <text_slice>But, of course, astronomical
measurements would not be</text_slice>
            </slice>
            <slice>
              <time_slice>4:53</time_slice>
              <text_slice>perfectly exact.</text_slice>
            </slice>
            <slice>
              <time_slice>4:55</time_slice>
              <text_slice>And they would try to find the
curve that fits these data.</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>How do you go about choosing
this particular curve on the</text_slice>
            </slice>
            <slice>
              <time_slice>5:05</time_slice>
              <text_slice>base of noisy data and
try to do it in a</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>somewhat principled way?</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>OK, so questions of this
type-- clearly the</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>applications are all
over the place.</text_slice>
            </slice>
            <slice>
              <time_slice>5:17</time_slice>
              <text_slice>But how is this related
conceptually with what we have</text_slice>
            </slice>
            <slice>
              <time_slice>5:20</time_slice>
              <text_slice>been doing so far?</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>What's the relation between the
field of inference and the</text_slice>
            </slice>
            <slice>
              <time_slice>5:25</time_slice>
              <text_slice>field of probability
as we have been</text_slice>
            </slice>
            <slice>
              <time_slice>5:28</time_slice>
              <text_slice>practicing until now?</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>Well, mathematically speaking,
what's going to happen in the</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>next few lectures could be just
exercises or homework</text_slice>
            </slice>
            <slice>
              <time_slice>5:38</time_slice>
              <text_slice>problems in the class in based
on what we have done so far.</text_slice>
            </slice>
            <slice>
              <time_slice>5:44</time_slice>
              <text_slice>That means you're not going
to get any new facts about</text_slice>
            </slice>
            <slice>
              <time_slice>5:48</time_slice>
              <text_slice>probability theory.</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>Everything we're going to do
will be simple applications of</text_slice>
            </slice>
            <slice>
              <time_slice>5:53</time_slice>
              <text_slice>things that you already
do know.</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>So in some sense, statistics
and inference is just an</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>applied exercise
in probability.</text_slice>
            </slice>
            <slice>
              <time_slice>6:02</time_slice>
              <text_slice>But actually, things are
not that simple in</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>the following sense.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>If you get a probability
problem,</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>there's a correct answer.</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>There's a correct solution.</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>And that correct solution
is unique.</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>There's no ambiguity.</text_slice>
            </slice>
            <slice>
              <time_slice>6:20</time_slice>
              <text_slice>The theory of probability has
clearly defined rules.</text_slice>
            </slice>
            <slice>
              <time_slice>6:23</time_slice>
              <text_slice>These are the axioms.</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>You're given some information
about probability</text_slice>
            </slice>
            <slice>
              <time_slice>6:27</time_slice>
              <text_slice>distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>You're asked to calculate
certain other things.</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>There's no ambiguity.</text_slice>
            </slice>
            <slice>
              <time_slice>6:32</time_slice>
              <text_slice>Answers are always unique.</text_slice>
            </slice>
            <slice>
              <time_slice>6:34</time_slice>
              <text_slice>In statistical questions, it's
no longer the case that the</text_slice>
            </slice>
            <slice>
              <time_slice>6:39</time_slice>
              <text_slice>question has a unique answer.</text_slice>
            </slice>
            <slice>
              <time_slice>6:41</time_slice>
              <text_slice>If I give you data and I ask
you what's the best way of</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>estimating the motion of that
planet, reasonable people can</text_slice>
            </slice>
            <slice>
              <time_slice>6:49</time_slice>
              <text_slice>come up with different
methods.</text_slice>
            </slice>
            <slice>
              <time_slice>6:53</time_slice>
              <text_slice>And reasonable people will try
to argue that's my method has</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>these desirable properties but
somebody else may say, here's</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>another method that has certain
desirable properties.</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>And it's not clear what
the best method is.</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>So it's good to have some
understanding of what the</text_slice>
            </slice>
            <slice>
              <time_slice>7:11</time_slice>
              <text_slice>issues are and to know at least
what is the general</text_slice>
            </slice>
            <slice>
              <time_slice>7:16</time_slice>
              <text_slice>class of methods that one tries
to consider, how does</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>one go about such problems.</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>So we're going to see
lots and lots of</text_slice>
            </slice>
            <slice>
              <time_slice>7:24</time_slice>
              <text_slice>different inference methods.</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>We're not going to tell
you that one is</text_slice>
            </slice>
            <slice>
              <time_slice>7:27</time_slice>
              <text_slice>better than the other.</text_slice>
            </slice>
            <slice>
              <time_slice>7:28</time_slice>
              <text_slice>But it's important to understand
what are the</text_slice>
            </slice>
            <slice>
              <time_slice>7:30</time_slice>
              <text_slice>concepts between those
different methods.</text_slice>
            </slice>
            <slice>
              <time_slice>7:33</time_slice>
              <text_slice>And finally, statistics can
be misused really badly.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>That is, one can come up with
methods that you think are</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>sound, but in fact they're
not quite that.</text_slice>
            </slice>
            <slice>
              <time_slice>7:48</time_slice>
              <text_slice>I will bring some examples next
time and talk a little</text_slice>
            </slice>
            <slice>
              <time_slice>7:52</time_slice>
              <text_slice>more about this.</text_slice>
            </slice>
            <slice>
              <time_slice>7:54</time_slice>
              <text_slice>So, they want to say, you have
some data, you want to make</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>some inference from them, what
many people will do is to go</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>to Wikipedia, find a statistical
test that they</text_slice>
            </slice>
            <slice>
              <time_slice>8:06</time_slice>
              <text_slice>think it applies to that
situation, plug in numbers,</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>and present results.</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>Are the conclusions that they
get really justified or are</text_slice>
            </slice>
            <slice>
              <time_slice>8:14</time_slice>
              <text_slice>they misusing statistical
methods?</text_slice>
            </slice>
            <slice>
              <time_slice>8:16</time_slice>
              <text_slice>Well, too many people actually
do misuse statistics and</text_slice>
            </slice>
            <slice>
              <time_slice>8:20</time_slice>
              <text_slice>conclusions that people
get are often false.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>So it's important to, besides
just being able to copy</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>statistical tests and use them,
to understand what are</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>the assumptions between the
different methods and what</text_slice>
            </slice>
            <slice>
              <time_slice>8:35</time_slice>
              <text_slice>kind of guarantees they
have, if any.</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>All right, so we'll try to do a
quick tour through the field</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>of inference in this lecture and
the next few lectures that</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>we have left this semester and
try to highlight at the very</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>high level the main concept
skills, and</text_slice>
            </slice>
            <slice>
              <time_slice>8:53</time_slice>
              <text_slice>techniques that come in.</text_slice>
            </slice>
            <slice>
              <time_slice>8:56</time_slice>
              <text_slice>Let's start with some
generalities and some general</text_slice>
            </slice>
            <slice>
              <time_slice>8:59</time_slice>
              <text_slice>statements.</text_slice>
            </slice>
            <slice>
              <time_slice>9:03</time_slice>
              <text_slice>One first statement is that
statistics or inference</text_slice>
            </slice>
            <slice>
              <time_slice>9:07</time_slice>
              <text_slice>problems come up in very
different guises.</text_slice>
            </slice>
            <slice>
              <time_slice>9:11</time_slice>
              <text_slice>And they may look as if they are
of very different forms.</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>Although, at some fundamental
level, the basic issues turn</text_slice>
            </slice>
            <slice>
              <time_slice>9:20</time_slice>
              <text_slice>out to be always pretty
much the same.</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>So let's look at this example.</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>There's an unknown signal
that's being sent.</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>It's sent through some medium,
and that medium just takes the</text_slice>
            </slice>
            <slice>
              <time_slice>9:35</time_slice>
              <text_slice>signal and amplifies it
by a certain number.</text_slice>
            </slice>
            <slice>
              <time_slice>9:39</time_slice>
              <text_slice>So you can think of
somebody shouting.</text_slice>
            </slice>
            <slice>
              <time_slice>9:41</time_slice>
              <text_slice>There's the air out there.</text_slice>
            </slice>
            <slice>
              <time_slice>9:42</time_slice>
              <text_slice>What you shouted will be
attenuated through the air</text_slice>
            </slice>
            <slice>
              <time_slice>9:46</time_slice>
              <text_slice>until it gets to a receiver.</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>And that receiver then observes
this, but together</text_slice>
            </slice>
            <slice>
              <time_slice>9:51</time_slice>
              <text_slice>with some random noise.</text_slice>
            </slice>
            <slice>
              <time_slice>9:56</time_slice>
              <text_slice>Here I meant S. S is the signal
that's being sent.</text_slice>
            </slice>
            <slice>
              <time_slice>10:00</time_slice>
              <text_slice>And what you observe is an X.</text_slice>
            </slice>
            <slice>
              <time_slice>10:06</time_slice>
              <text_slice>You observe X, so what kind
of inference problems</text_slice>
            </slice>
            <slice>
              <time_slice>10:09</time_slice>
              <text_slice>could we have here?</text_slice>
            </slice>
            <slice>
              <time_slice>10:11</time_slice>
              <text_slice>In some cases, you want to build
a model of the physical</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>phenomenon that you're
dealing with.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>So for example, you don't know
the attenuation of your signal</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>and you try to find out what
this number is based on the</text_slice>
            </slice>
            <slice>
              <time_slice>10:25</time_slice>
              <text_slice>observations that you have.</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>So the way this is done in
engineering systems is that</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>you design a certain signal, you
know what it is, you shout</text_slice>
            </slice>
            <slice>
              <time_slice>10:35</time_slice>
              <text_slice>a particular word, and then
the receiver listens.</text_slice>
            </slice>
            <slice>
              <time_slice>10:39</time_slice>
              <text_slice>And based on the intensity of
the signal that they get, they</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>try to make a guess about A. So
you don't know A, but you</text_slice>
            </slice>
            <slice>
              <time_slice>10:48</time_slice>
              <text_slice>know S. And by observing X,
you get some information</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>about what A is.</text_slice>
            </slice>
            <slice>
              <time_slice>10:54</time_slice>
              <text_slice>So in this case, you're trying
to build a model of the medium</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>through which your signal
is propagating.</text_slice>
            </slice>
            <slice>
              <time_slice>11:01</time_slice>
              <text_slice>So sometimes one would call
problems of this kind, let's</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>say, system identification.</text_slice>
            </slice>
            <slice>
              <time_slice>11:07</time_slice>
              <text_slice>In a different version of an
inference problem that comes</text_slice>
            </slice>
            <slice>
              <time_slice>11:11</time_slice>
              <text_slice>with this picture, you've
done your modeling.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>You know your A. You know the
medium through which the</text_slice>
            </slice>
            <slice>
              <time_slice>11:18</time_slice>
              <text_slice>signal is going, but it's
a communication system.</text_slice>
            </slice>
            <slice>
              <time_slice>11:22</time_slice>
              <text_slice>This person is trying
to communicate</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>something to that person.</text_slice>
            </slice>
            <slice>
              <time_slice>11:26</time_slice>
              <text_slice>So you send the signal S, but
that person receives a noisy</text_slice>
            </slice>
            <slice>
              <time_slice>11:30</time_slice>
              <text_slice>version of S. So that person
tries to reconstruct S based</text_slice>
            </slice>
            <slice>
              <time_slice>11:35</time_slice>
              <text_slice>on X.</text_slice>
            </slice>
            <slice>
              <time_slice>11:36</time_slice>
              <text_slice>So in both cases, we have a
linear relation between X and</text_slice>
            </slice>
            <slice>
              <time_slice>11:42</time_slice>
              <text_slice>the unknown quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>In one version, A is the unknown
and we know S. In the</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>other version, A is known,
and so we try to infer S.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>Mathematically, you can see that
this is essentially the</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>same kind of problem
in both cases.</text_slice>
            </slice>
            <slice>
              <time_slice>11:57</time_slice>
              <text_slice>Although, the kind of practical
problem that you're</text_slice>
            </slice>
            <slice>
              <time_slice>12:03</time_slice>
              <text_slice>trying to solve is a
little different.</text_slice>
            </slice>
            <slice>
              <time_slice>12:07</time_slice>
              <text_slice>So we will not be making any
distinctions between problems</text_slice>
            </slice>
            <slice>
              <time_slice>12:11</time_slice>
              <text_slice>of the model building type as
opposed to models where you</text_slice>
            </slice>
            <slice>
              <time_slice>12:15</time_slice>
              <text_slice>try to estimate some unknown
signal and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>Because conceptually, the tools
that one uses for both</text_slice>
            </slice>
            <slice>
              <time_slice>12:22</time_slice>
              <text_slice>types of problems are
essentially the same.</text_slice>
            </slice>
            <slice>
              <time_slice>12:26</time_slice>
              <text_slice>OK, next a very useful
classification</text_slice>
            </slice>
            <slice>
              <time_slice>12:30</time_slice>
              <text_slice>of inference problems--</text_slice>
            </slice>
            <slice>
              <time_slice>12:34</time_slice>
              <text_slice>the unknown quantity that you're
trying to estimate</text_slice>
            </slice>
            <slice>
              <time_slice>12:37</time_slice>
              <text_slice>could be either a discrete
one that takes a</text_slice>
            </slice>
            <slice>
              <time_slice>12:40</time_slice>
              <text_slice>small number of values.</text_slice>
            </slice>
            <slice>
              <time_slice>12:43</time_slice>
              <text_slice>So this could be discrete
problems, such as the airplane</text_slice>
            </slice>
            <slice>
              <time_slice>12:45</time_slice>
              <text_slice>radar problem we encountered
back a long</text_slice>
            </slice>
            <slice>
              <time_slice>12:48</time_slice>
              <text_slice>time ago in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>12:50</time_slice>
              <text_slice>So there's two possibilities--</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>an airplane is out there or an
airplane is not out there.</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>And you're trying to
make a decision</text_slice>
            </slice>
            <slice>
              <time_slice>12:57</time_slice>
              <text_slice>between these two options.</text_slice>
            </slice>
            <slice>
              <time_slice>12:58</time_slice>
              <text_slice>Or you can have other problems
would you have, let's say,</text_slice>
            </slice>
            <slice>
              <time_slice>13:01</time_slice>
              <text_slice>four possible options.</text_slice>
            </slice>
            <slice>
              <time_slice>13:03</time_slice>
              <text_slice>You don't know which one is
true, but you get data and you</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>try to figure out which
one is true.</text_slice>
            </slice>
            <slice>
              <time_slice>13:09</time_slice>
              <text_slice>In problems of these kind,
usually you want to make a</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>decision based on your data.</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>And you're interested in the
probability of making a</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>correct decision.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>You would like that
probability to</text_slice>
            </slice>
            <slice>
              <time_slice>13:19</time_slice>
              <text_slice>be as high as possible.</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>Estimation problems are
a little different.</text_slice>
            </slice>
            <slice>
              <time_slice>13:24</time_slice>
              <text_slice>Here you have some continuous
quantity that's not known.</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>And you try to make a good
guess of that quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>13:31</time_slice>
              <text_slice>And you would like your guess to
be as close as possible to</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>the true quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>So the polling problem
was of this type.</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>There was an unknown fraction
f of the population that had</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>some property.</text_slice>
            </slice>
            <slice>
              <time_slice>13:45</time_slice>
              <text_slice>And you try to estimate f as
accurately as you can.</text_slice>
            </slice>
            <slice>
              <time_slice>13:50</time_slice>
              <text_slice>So the distinction here is that
usually here the unknown</text_slice>
            </slice>
            <slice>
              <time_slice>13:53</time_slice>
              <text_slice>quantity takes on discrete
set of values.</text_slice>
            </slice>
            <slice>
              <time_slice>13:56</time_slice>
              <text_slice>Here the unknown quantity
takes a</text_slice>
            </slice>
            <slice>
              <time_slice>13:57</time_slice>
              <text_slice>continuous set of values.</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>Here we're interested in the
probability of error.</text_slice>
            </slice>
            <slice>
              <time_slice>14:02</time_slice>
              <text_slice>Here we're interested in
the size of the error.</text_slice>
            </slice>
            <slice>
              <time_slice>14:07</time_slice>
              <text_slice>Broadly speaking, most inference
problems fall either</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>in this category or
in that category.</text_slice>
            </slice>
            <slice>
              <time_slice>14:13</time_slice>
              <text_slice>Although, if you want to
complicate life, you can also</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>think or construct problems
where both of these aspects</text_slice>
            </slice>
            <slice>
              <time_slice>14:20</time_slice>
              <text_slice>are simultaneously present.</text_slice>
            </slice>
            <slice>
              <time_slice>14:24</time_slice>
              <text_slice>OK, finally since we're in
classification mode, there is</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>a very big, important dichotomy
into how one goes</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>about inference problems.</text_slice>
            </slice>
            <slice>
              <time_slice>14:35</time_slice>
              <text_slice>And here there's two
fundamentally different</text_slice>
            </slice>
            <slice>
              <time_slice>14:39</time_slice>
              <text_slice>philosophical points of view,
which is how do we model the</text_slice>
            </slice>
            <slice>
              <time_slice>14:46</time_slice>
              <text_slice>quantity that is unknown?</text_slice>
            </slice>
            <slice>
              <time_slice>14:50</time_slice>
              <text_slice>In one approach, you say there's
a certain quantity</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>that has a definite value.</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>It just happens that
they don't know it.</text_slice>
            </slice>
            <slice>
              <time_slice>15:00</time_slice>
              <text_slice>But it's a number.</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>There's nothing random
about it.</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>So think of trying to estimate
some physical quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>15:10</time_slice>
              <text_slice>You're making measurements, you
try to estimate the mass</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>of an electron, which
is a sort of</text_slice>
            </slice>
            <slice>
              <time_slice>15:15</time_slice>
              <text_slice>universal physical constant.</text_slice>
            </slice>
            <slice>
              <time_slice>15:18</time_slice>
              <text_slice>There's nothing random
about it.</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>It's a fixed number.</text_slice>
            </slice>
            <slice>
              <time_slice>15:22</time_slice>
              <text_slice>You get data, because you have
some measuring apparatus.</text_slice>
            </slice>
            <slice>
              <time_slice>15:29</time_slice>
              <text_slice>And that measuring apparatus,
depending on what that results</text_slice>
            </slice>
            <slice>
              <time_slice>15:33</time_slice>
              <text_slice>that you get are affected by the
true mass of the electron,</text_slice>
            </slice>
            <slice>
              <time_slice>15:37</time_slice>
              <text_slice>but there's also some noise.</text_slice>
            </slice>
            <slice>
              <time_slice>15:39</time_slice>
              <text_slice>You take the data out of your
measuring apparatus and you</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>try to come up with
some estimate of</text_slice>
            </slice>
            <slice>
              <time_slice>15:44</time_slice>
              <text_slice>that quantity theta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:47</time_slice>
              <text_slice>So this is definitely a
legitimate picture, but the</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>important thing in this picture
is that this theta is</text_slice>
            </slice>
            <slice>
              <time_slice>15:52</time_slice>
              <text_slice>written as lowercase.</text_slice>
            </slice>
            <slice>
              <time_slice>15:54</time_slice>
              <text_slice>And that's to make the point
that it's a real number, not a</text_slice>
            </slice>
            <slice>
              <time_slice>15:58</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>16:00</time_slice>
              <text_slice>There's a different
philosophical approach which</text_slice>
            </slice>
            <slice>
              <time_slice>16:03</time_slice>
              <text_slice>says, well, anything that I
don't know I should model it</text_slice>
            </slice>
            <slice>
              <time_slice>16:08</time_slice>
              <text_slice>as a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>Yes, I know.</text_slice>
            </slice>
            <slice>
              <time_slice>16:11</time_slice>
              <text_slice>The mass of the electron
is not really random.</text_slice>
            </slice>
            <slice>
              <time_slice>16:14</time_slice>
              <text_slice>It's a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>16:15</time_slice>
              <text_slice>But I don't know what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>I have some vague sense,
perhaps, what it is perhaps</text_slice>
            </slice>
            <slice>
              <time_slice>16:22</time_slice>
              <text_slice>because of the experiments
that some other</text_slice>
            </slice>
            <slice>
              <time_slice>16:24</time_slice>
              <text_slice>people carried out.</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>So perhaps I have a prior
distribution on the possible</text_slice>
            </slice>
            <slice>
              <time_slice>16:30</time_slice>
              <text_slice>values of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>16:32</time_slice>
              <text_slice>And that prior distribution
doesn't mean that the nature</text_slice>
            </slice>
            <slice>
              <time_slice>16:34</time_slice>
              <text_slice>is random, but it's more of a
subjective description of my</text_slice>
            </slice>
            <slice>
              <time_slice>16:39</time_slice>
              <text_slice>subjective beliefs of where do
I think this constant number</text_slice>
            </slice>
            <slice>
              <time_slice>16:44</time_slice>
              <text_slice>happens to be.</text_slice>
            </slice>
            <slice>
              <time_slice>16:46</time_slice>
              <text_slice>So even though it's not truly
random, I model my initial</text_slice>
            </slice>
            <slice>
              <time_slice>16:50</time_slice>
              <text_slice>beliefs before the experiment
starts.</text_slice>
            </slice>
            <slice>
              <time_slice>16:52</time_slice>
              <text_slice>In terms of a prior
distribution, I view it as a</text_slice>
            </slice>
            <slice>
              <time_slice>16:55</time_slice>
              <text_slice>random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>Then I observe another related
random variable through some</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>measuring apparatus.</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>And then I use this again
to create an estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>17:08</time_slice>
              <text_slice>So these two pictures
philosophically are very</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>different from each other.</text_slice>
            </slice>
            <slice>
              <time_slice>17:13</time_slice>
              <text_slice>Here we treat the unknown
quantities as unknown numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>Here we treat them as
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:20</time_slice>
              <text_slice>When we treat them as a random
variables, then we know pretty</text_slice>
            </slice>
            <slice>
              <time_slice>17:24</time_slice>
              <text_slice>much already what we
should be doing.</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>We should just use
the Bayes rule.</text_slice>
            </slice>
            <slice>
              <time_slice>17:29</time_slice>
              <text_slice>Based on X, find
the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>17:31</time_slice>
              <text_slice>distribution of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>17:33</time_slice>
              <text_slice>And that's what we will be doing
mostly over this lecture</text_slice>
            </slice>
            <slice>
              <time_slice>17:37</time_slice>
              <text_slice>and the next lecture.</text_slice>
            </slice>
            <slice>
              <time_slice>17:40</time_slice>
              <text_slice>Now in both cases, what you end
up getting at the end is</text_slice>
            </slice>
            <slice>
              <time_slice>17:44</time_slice>
              <text_slice>an estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>17:47</time_slice>
              <text_slice>But actually, that estimate is
what kind of object is it?</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>It's a random variable
in both cases.</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>Even in this case where
theta was a</text_slice>
            </slice>
            <slice>
              <time_slice>17:58</time_slice>
              <text_slice>constant, my data are random.</text_slice>
            </slice>
            <slice>
              <time_slice>18:01</time_slice>
              <text_slice>I do my data processing.</text_slice>
            </slice>
            <slice>
              <time_slice>18:02</time_slice>
              <text_slice>So I calculate a function
of the data, the</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>data are random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>18:07</time_slice>
              <text_slice>So out here we output something
which is a function</text_slice>
            </slice>
            <slice>
              <time_slice>18:11</time_slice>
              <text_slice>of a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>So this quantity here
will be also random.</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>It's affected by the noise and
the experiment that I have</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>been doing.</text_slice>
            </slice>
            <slice>
              <time_slice>18:19</time_slice>
              <text_slice>That's why these estimators
will be denoted</text_slice>
            </slice>
            <slice>
              <time_slice>18:22</time_slice>
              <text_slice>by uppercase Thetas.</text_slice>
            </slice>
            <slice>
              <time_slice>18:24</time_slice>
              <text_slice>And we will be using hats.</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>Hat, usually in estimation,
means</text_slice>
            </slice>
            <slice>
              <time_slice>18:29</time_slice>
              <text_slice>an estimate of something.</text_slice>
            </slice>
            <slice>
              <time_slice>18:32</time_slice>
              <text_slice>All right, so this is
the big picture.</text_slice>
            </slice>
            <slice>
              <time_slice>18:35</time_slice>
              <text_slice>We're going to start with
the Bayesian version.</text_slice>
            </slice>
            <slice>
              <time_slice>18:38</time_slice>
              <text_slice>And then the last few lectures
we're going to talk about the</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>non-Bayesian version or
the classical one.</text_slice>
            </slice>
            <slice>
              <time_slice>18:45</time_slice>
              <text_slice>By the way, I should say that
statisticians have been</text_slice>
            </slice>
            <slice>
              <time_slice>18:48</time_slice>
              <text_slice>debating fiercely for 100 years
whether the right way to</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>approach statistics is to go
the classical way or the</text_slice>
            </slice>
            <slice>
              <time_slice>18:56</time_slice>
              <text_slice>Bayesian way.</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>And there have been tides going
back and forth between</text_slice>
            </slice>
            <slice>
              <time_slice>19:00</time_slice>
              <text_slice>the two sides.</text_slice>
            </slice>
            <slice>
              <time_slice>19:02</time_slice>
              <text_slice>These days, Bayesian methods
tend to become a little more</text_slice>
            </slice>
            <slice>
              <time_slice>19:05</time_slice>
              <text_slice>popular for various reasons.</text_slice>
            </slice>
            <slice>
              <time_slice>19:07</time_slice>
              <text_slice>We're going to come back
to this later.</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>All right, so in Bayesian
estimation, what we got in our</text_slice>
            </slice>
            <slice>
              <time_slice>19:14</time_slice>
              <text_slice>hands is Bayes rule.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>And if you have Bayes rule,
there's not a lot</text_slice>
            </slice>
            <slice>
              <time_slice>19:19</time_slice>
              <text_slice>that's left to do.</text_slice>
            </slice>
            <slice>
              <time_slice>19:21</time_slice>
              <text_slice>We have different forms of the
Bayes rule, depending on</text_slice>
            </slice>
            <slice>
              <time_slice>19:24</time_slice>
              <text_slice>whether we're dealing with
discrete data, And discrete</text_slice>
            </slice>
            <slice>
              <time_slice>19:27</time_slice>
              <text_slice>quantities to estimate, or
continuous data, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>In the hypothesis testing
problem, the unknown quantity</text_slice>
            </slice>
            <slice>
              <time_slice>19:36</time_slice>
              <text_slice>Theta is discrete.</text_slice>
            </slice>
            <slice>
              <time_slice>19:38</time_slice>
              <text_slice>So in both cases here,
we have a P of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>19:42</time_slice>
              <text_slice>We obtain data, the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>And on the basis of the X that
we observe, we can calculate</text_slice>
            </slice>
            <slice>
              <time_slice>19:49</time_slice>
              <text_slice>the posterior distribution
of Theta, given the data.</text_slice>
            </slice>
            <slice>
              <time_slice>19:53</time_slice>
              <text_slice>So to use Bayesian inference,
what do we start with?</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>We start with some priors.</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>These are our initial
beliefs about what</text_slice>
            </slice>
            <slice>
              <time_slice>20:05</time_slice>
              <text_slice>Theta that might be.</text_slice>
            </slice>
            <slice>
              <time_slice>20:07</time_slice>
              <text_slice>That's before we do
the experiment.</text_slice>
            </slice>
            <slice>
              <time_slice>20:10</time_slice>
              <text_slice>We have a model of the
experimental aparatus.</text_slice>
            </slice>
            <slice>
              <time_slice>20:17</time_slice>
              <text_slice>And the model of the
experimental apparatus tells</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>us if this Theta is true, I'm
going to see X's of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>20:28</time_slice>
              <text_slice>If that other Theta is true, I'm
going to see X's that they</text_slice>
            </slice>
            <slice>
              <time_slice>20:31</time_slice>
              <text_slice>are somewhere else.</text_slice>
            </slice>
            <slice>
              <time_slice>20:33</time_slice>
              <text_slice>That models my apparatus.</text_slice>
            </slice>
            <slice>
              <time_slice>20:35</time_slice>
              <text_slice>And based on that knowledge,
once I observe I have these</text_slice>
            </slice>
            <slice>
              <time_slice>20:39</time_slice>
              <text_slice>two functions in my hands, we
have already seen that if you</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>know those two functions, you
can also calculate the</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>denominator here.</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>So all of these functions are
available, so you can compute,</text_slice>
            </slice>
            <slice>
              <time_slice>20:50</time_slice>
              <text_slice>you can find a formula for
this function as well.</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>And as soon as you observe the
data, that X's, you plug in</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>here the numerical value
of those X's.</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>And you get a function
of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>21:04</time_slice>
              <text_slice>And this is the posterior
distribution of Theta, given</text_slice>
            </slice>
            <slice>
              <time_slice>21:07</time_slice>
              <text_slice>the data that you have seen.</text_slice>
            </slice>
            <slice>
              <time_slice>21:09</time_slice>
              <text_slice>So you've already done
a fair number of</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>exercises of these kind.</text_slice>
            </slice>
            <slice>
              <time_slice>21:13</time_slice>
              <text_slice>So we not say more about this.</text_slice>
            </slice>
            <slice>
              <time_slice>21:17</time_slice>
              <text_slice>And there's a similar formula as
you know for the case where</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>we have continuous data.</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>If the X's are continuous random
variable, then the</text_slice>
            </slice>
            <slice>
              <time_slice>21:25</time_slice>
              <text_slice>formula is the same, except
that X's are described by</text_slice>
            </slice>
            <slice>
              <time_slice>21:28</time_slice>
              <text_slice>densities instead of being
described by a probability</text_slice>
            </slice>
            <slice>
              <time_slice>21:31</time_slice>
              <text_slice>mass functions.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>OK, now if Theta is continuous,
then we're dealing</text_slice>
            </slice>
            <slice>
              <time_slice>21:40</time_slice>
              <text_slice>with estimation problems.</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>But the story is once
more the same.</text_slice>
            </slice>
            <slice>
              <time_slice>21:44</time_slice>
              <text_slice>You're going to use the Bayes
rule to come up with the</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>posterior density of Theta,
given the data</text_slice>
            </slice>
            <slice>
              <time_slice>21:51</time_slice>
              <text_slice>that you have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>21:53</time_slice>
              <text_slice>Now just for the sake of the
example, let's come back to</text_slice>
            </slice>
            <slice>
              <time_slice>21:57</time_slice>
              <text_slice>this picture here.</text_slice>
            </slice>
            <slice>
              <time_slice>21:58</time_slice>
              <text_slice>Suppose that something is flying
in the air, and maybe</text_slice>
            </slice>
            <slice>
              <time_slice>22:03</time_slice>
              <text_slice>this is just an object in the
air close to the Earth.</text_slice>
            </slice>
            <slice>
              <time_slice>22:07</time_slice>
              <text_slice>So because of gravity, the
trajectory that it's going to</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>follow it's going to
be a parabola.</text_slice>
            </slice>
            <slice>
              <time_slice>22:15</time_slice>
              <text_slice>So this is the general equation
of a parabola.</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>Zt is the position of my
objects at time t.</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>But I don't know exactly
which parabola it is.</text_slice>
            </slice>
            <slice>
              <time_slice>22:29</time_slice>
              <text_slice>So the parameters of the
parabola are unknown</text_slice>
            </slice>
            <slice>
              <time_slice>22:32</time_slice>
              <text_slice>quantities.</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>What I can do is to go and
measure the position of my</text_slice>
            </slice>
            <slice>
              <time_slice>22:37</time_slice>
              <text_slice>objects at different times.</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>But unfortunately, my
measurements are noisy.</text_slice>
            </slice>
            <slice>
              <time_slice>22:47</time_slice>
              <text_slice>What I want to do is to model
the motion of my object.</text_slice>
            </slice>
            <slice>
              <time_slice>22:51</time_slice>
              <text_slice>So I guess in the picture, the
axis would be t going this way</text_slice>
            </slice>
            <slice>
              <time_slice>22:56</time_slice>
              <text_slice>and Z going this way.</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>And on the basis of the
data that they get,</text_slice>
            </slice>
            <slice>
              <time_slice>23:02</time_slice>
              <text_slice>these are my X's.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>I want to figure
out the Thetas.</text_slice>
            </slice>
            <slice>
              <time_slice>23:07</time_slice>
              <text_slice>That is, I want to figure
out the exact</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>equation of this parabola.</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>Now if somebody gives you
probability distributions for</text_slice>
            </slice>
            <slice>
              <time_slice>23:14</time_slice>
              <text_slice>Theta, these would
be your priors.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>So this is given.</text_slice>
            </slice>
            <slice>
              <time_slice>23:23</time_slice>
              <text_slice>We need the conditional
distribution of the X's given</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>the Thetas.</text_slice>
            </slice>
            <slice>
              <time_slice>23:27</time_slice>
              <text_slice>Well, we have the conditional
distribution of Z, given the</text_slice>
            </slice>
            <slice>
              <time_slice>23:30</time_slice>
              <text_slice>Thetas from this equation.</text_slice>
            </slice>
            <slice>
              <time_slice>23:32</time_slice>
              <text_slice>And then by playing with this
equation, you can also find</text_slice>
            </slice>
            <slice>
              <time_slice>23:36</time_slice>
              <text_slice>how is X distributed if Theta
takes a particular value.</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>So you do have all of the
densities that you might need.</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>And you can apply
the Bayes rule.</text_slice>
            </slice>
            <slice>
              <time_slice>23:48</time_slice>
              <text_slice>And at the end, your end result
would be a formula for</text_slice>
            </slice>
            <slice>
              <time_slice>23:53</time_slice>
              <text_slice>the distribution of Theta,
given to the X</text_slice>
            </slice>
            <slice>
              <time_slice>23:57</time_slice>
              <text_slice>that you have observed--</text_slice>
            </slice>
            <slice>
              <time_slice>23:59</time_slice>
              <text_slice>except for one sort of
computation, or to make things</text_slice>
            </slice>
            <slice>
              <time_slice>24:03</time_slice>
              <text_slice>more interesting.</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>Instead of these X's and Theta's
being single random</text_slice>
            </slice>
            <slice>
              <time_slice>24:07</time_slice>
              <text_slice>variables that we have here,
typically those X's and</text_slice>
            </slice>
            <slice>
              <time_slice>24:11</time_slice>
              <text_slice>Theta's will be
multi-dimensional random</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>variables or will correspond
to multiple ones.</text_slice>
            </slice>
            <slice>
              <time_slice>24:16</time_slice>
              <text_slice>So this little Theta here
actually stands for a triplet</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>of Theta0, Theta1, and Theta2.</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>And that X here stands here for
the entire sequence of X's</text_slice>
            </slice>
            <slice>
              <time_slice>24:26</time_slice>
              <text_slice>that we have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>24:28</time_slice>
              <text_slice>So in reality, the object that
you're going to get at to the</text_slice>
            </slice>
            <slice>
              <time_slice>24:31</time_slice>
              <text_slice>end after inference is done is
a function that you plug in</text_slice>
            </slice>
            <slice>
              <time_slice>24:35</time_slice>
              <text_slice>the values of the data and you
get the function of the</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>Theta's that tells you the
relative likelihoods of</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>different Theta triplets.</text_slice>
            </slice>
            <slice>
              <time_slice>24:46</time_slice>
              <text_slice>So what I'm saying is that this
is no harder than the</text_slice>
            </slice>
            <slice>
              <time_slice>24:49</time_slice>
              <text_slice>problems that you have dealt
with so far, except perhaps</text_slice>
            </slice>
            <slice>
              <time_slice>24:53</time_slice>
              <text_slice>for the complication that's
usually in interesting</text_slice>
            </slice>
            <slice>
              <time_slice>24:56</time_slice>
              <text_slice>inference problems.</text_slice>
            </slice>
            <slice>
              <time_slice>24:57</time_slice>
              <text_slice>Your Theta's and X's are often
the vectors of random</text_slice>
            </slice>
            <slice>
              <time_slice>25:01</time_slice>
              <text_slice>variables instead of individual
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>Now if you are to do estimation
in a case where you</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>have discrete data, again the
situation is no different.</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>We still have a Bayes rule of
the same kind, except that</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>densities gets replaced
by PMF's.</text_slice>
            </slice>
            <slice>
              <time_slice>25:19</time_slice>
              <text_slice>If X is discrete, you put a P
here instead of putting an f.</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>So an example of an estimation
problem with discrete data is</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>similar to the polling
problem.</text_slice>
            </slice>
            <slice>
              <time_slice>25:29</time_slice>
              <text_slice>You have a coin.</text_slice>
            </slice>
            <slice>
              <time_slice>25:31</time_slice>
              <text_slice>It has an unknown
parameter Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>This is the probability
of obtaining heads.</text_slice>
            </slice>
            <slice>
              <time_slice>25:35</time_slice>
              <text_slice>You flip the coin many times.</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>What can you tell me about
the true value of Theta?</text_slice>
            </slice>
            <slice>
              <time_slice>25:41</time_slice>
              <text_slice>A classical statistician, at
this point, would say, OK, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>25:46</time_slice>
              <text_slice>going to use an estimator,
the most reasonable</text_slice>
            </slice>
            <slice>
              <time_slice>25:48</time_slice>
              <text_slice>one, which is this.</text_slice>
            </slice>
            <slice>
              <time_slice>25:50</time_slice>
              <text_slice>How many heads did they
obtain in n trials?</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>Divide by the total
number of trials.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>This is my estimate of
the bias of my coin.</text_slice>
            </slice>
            <slice>
              <time_slice>26:00</time_slice>
              <text_slice>And then the classical
statistician would continue</text_slice>
            </slice>
            <slice>
              <time_slice>26:02</time_slice>
              <text_slice>from here and try to prove some
properties and argue that</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>this estimate is a good one.</text_slice>
            </slice>
            <slice>
              <time_slice>26:10</time_slice>
              <text_slice>For example, we have the weak
law of large numbers that</text_slice>
            </slice>
            <slice>
              <time_slice>26:12</time_slice>
              <text_slice>tells us that this particular
estimate converges in</text_slice>
            </slice>
            <slice>
              <time_slice>26:15</time_slice>
              <text_slice>probability to the
true parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>26:17</time_slice>
              <text_slice>This is a kind of guarantee
that's useful to have.</text_slice>
            </slice>
            <slice>
              <time_slice>26:21</time_slice>
              <text_slice>And the classical statistician
would pretty much close the</text_slice>
            </slice>
            <slice>
              <time_slice>26:23</time_slice>
              <text_slice>subject in this way.</text_slice>
            </slice>
            <slice>
              <time_slice>26:27</time_slice>
              <text_slice>What would the Bayesian
person do differently?</text_slice>
            </slice>
            <slice>
              <time_slice>26:30</time_slice>
              <text_slice>The Bayesian person would start
by assuming a prior</text_slice>
            </slice>
            <slice>
              <time_slice>26:35</time_slice>
              <text_slice>distribution of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>Instead of treating Theta as
an unknown constant, they</text_slice>
            </slice>
            <slice>
              <time_slice>26:39</time_slice>
              <text_slice>would say that Theta would speak
randomly or pretend that</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>it would speak randomly
and assume a</text_slice>
            </slice>
            <slice>
              <time_slice>26:47</time_slice>
              <text_slice>distribution on Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>26:49</time_slice>
              <text_slice>So for example, if you don't
know they need anything more,</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>you might assume that any value
for the bias of the coin</text_slice>
            </slice>
            <slice>
              <time_slice>26:57</time_slice>
              <text_slice>is as likely as any other value
of the bias of the coin.</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>And this way so the probability
distribution</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>that's uniform.</text_slice>
            </slice>
            <slice>
              <time_slice>27:05</time_slice>
              <text_slice>Or if you have a little more
faith in the manufacturing</text_slice>
            </slice>
            <slice>
              <time_slice>27:09</time_slice>
              <text_slice>processes that's created that
coin, you might choose your</text_slice>
            </slice>
            <slice>
              <time_slice>27:13</time_slice>
              <text_slice>prior to be a distribution
that's centered around 1/2 and</text_slice>
            </slice>
            <slice>
              <time_slice>27:17</time_slice>
              <text_slice>sits fairly narrowly centered
around 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>27:21</time_slice>
              <text_slice>That would be a prior
distribution in which you say,</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>well, I believe that the
manufacturer tried to make my</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>coin to be fair.</text_slice>
            </slice>
            <slice>
              <time_slice>27:29</time_slice>
              <text_slice>But they often makes some
mistakes, so it's going to be,</text_slice>
            </slice>
            <slice>
              <time_slice>27:33</time_slice>
              <text_slice>I believe, it's approximately
1/2 but not quite.</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>So depending on your beliefs,
you would choose an</text_slice>
            </slice>
            <slice>
              <time_slice>27:40</time_slice>
              <text_slice>appropriate prior for the
distribution of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>And then you would use the
Bayes rule to find the</text_slice>
            </slice>
            <slice>
              <time_slice>27:48</time_slice>
              <text_slice>probabilities of different
values of Theta, based on the</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>data that you have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>27:59</time_slice>
              <text_slice>So no matter which version of
the Bayes rule that you use,</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>the end product of the Bayes
rule is going to be either a</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>plot of this kind or a
plot of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>28:14</time_slice>
              <text_slice>So what am I plotting here?</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>This axis is the Theta axis.</text_slice>
            </slice>
            <slice>
              <time_slice>28:19</time_slice>
              <text_slice>These are the possible values
of the unknown quantity that</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>we're trying to estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>In the continuous
case, theta is a</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>continuous random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>28:30</time_slice>
              <text_slice>I obtain my data.</text_slice>
            </slice>
            <slice>
              <time_slice>28:32</time_slice>
              <text_slice>And I plot for the posterior
probability distribution after</text_slice>
            </slice>
            <slice>
              <time_slice>28:36</time_slice>
              <text_slice>observing my data.</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>And I'm plotting here the
probability density for Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>28:42</time_slice>
              <text_slice>So this is a plot
of that density.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>In the discrete case, theta can
take finitely many values</text_slice>
            </slice>
            <slice>
              <time_slice>28:49</time_slice>
              <text_slice>or a discrete set of values.</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>And for each one of those
values, I'm telling you how</text_slice>
            </slice>
            <slice>
              <time_slice>28:54</time_slice>
              <text_slice>likely is that the value to be
the correct one, given the</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>data that I have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>29:01</time_slice>
              <text_slice>And in general, what you would
go back to your boss and</text_slice>
            </slice>
            <slice>
              <time_slice>29:04</time_slice>
              <text_slice>report after you've done all
your inference work would be</text_slice>
            </slice>
            <slice>
              <time_slice>29:08</time_slice>
              <text_slice>either a plot of this kinds
or of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>29:10</time_slice>
              <text_slice>So you go to your boss
who asks you, what is</text_slice>
            </slice>
            <slice>
              <time_slice>29:14</time_slice>
              <text_slice>the value of Theta?</text_slice>
            </slice>
            <slice>
              <time_slice>29:15</time_slice>
              <text_slice>And you say, well, I only
have limited data.</text_slice>
            </slice>
            <slice>
              <time_slice>29:17</time_slice>
              <text_slice>That I don't know what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>29:19</time_slice>
              <text_slice>It could be this, with
so much probability.</text_slice>
            </slice>
            <slice>
              <time_slice>29:22</time_slice>
              <text_slice>There's probability.</text_slice>
            </slice>
            <slice>
              <time_slice>29:24</time_slice>
              <text_slice>OK, let's throw in some
numbers here.</text_slice>
            </slice>
            <slice>
              <time_slice>29:27</time_slice>
              <text_slice>There's probability 0.3 that
Theta is this value.</text_slice>
            </slice>
            <slice>
              <time_slice>29:32</time_slice>
              <text_slice>There's probability 0.2 that
Theta is this value, 0.1 that</text_slice>
            </slice>
            <slice>
              <time_slice>29:36</time_slice>
              <text_slice>it's this one, 0.1 that it's
this one, 0.2 that it's that</text_slice>
            </slice>
            <slice>
              <time_slice>29:39</time_slice>
              <text_slice>one, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>OK, now bosses often want
simple answers.</text_slice>
            </slice>
            <slice>
              <time_slice>29:44</time_slice>
              <text_slice>They say, OK, you're
talking too much.</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>What do you think Theta is?</text_slice>
            </slice>
            <slice>
              <time_slice>29:51</time_slice>
              <text_slice>And now you're forced
to make a decision.</text_slice>
            </slice>
            <slice>
              <time_slice>29:55</time_slice>
              <text_slice>If that was the situation and
you have to make a decision,</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>how would you make it?</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>Well, I'm going to make a
decision that's most likely to</text_slice>
            </slice>
            <slice>
              <time_slice>30:06</time_slice>
              <text_slice>be correct.</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>If I make this decision,
what's going to happen?</text_slice>
            </slice>
            <slice>
              <time_slice>30:13</time_slice>
              <text_slice>Theta is this value with
probability 0.2, which means</text_slice>
            </slice>
            <slice>
              <time_slice>30:17</time_slice>
              <text_slice>there's probably 0.8 that
they make an error</text_slice>
            </slice>
            <slice>
              <time_slice>30:21</time_slice>
              <text_slice>if I make that guess.</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>If I make that decision, this
decision has probably 0.3 of</text_slice>
            </slice>
            <slice>
              <time_slice>30:29</time_slice>
              <text_slice>being the correct one.</text_slice>
            </slice>
            <slice>
              <time_slice>30:30</time_slice>
              <text_slice>So I have probably
of error 0.7.</text_slice>
            </slice>
            <slice>
              <time_slice>30:34</time_slice>
              <text_slice>So if you want to just maximize
the probability of</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>giving the correct decision, or
if you want to minimize the</text_slice>
            </slice>
            <slice>
              <time_slice>30:41</time_slice>
              <text_slice>probability of making an
incorrect decision, what</text_slice>
            </slice>
            <slice>
              <time_slice>30:44</time_slice>
              <text_slice>you're going to choose to report
is that value of Theta</text_slice>
            </slice>
            <slice>
              <time_slice>30:48</time_slice>
              <text_slice>for which the probability
is highest.</text_slice>
            </slice>
            <slice>
              <time_slice>30:51</time_slice>
              <text_slice>So in this case, I would
choose to report this</text_slice>
            </slice>
            <slice>
              <time_slice>30:54</time_slice>
              <text_slice>particular value, the most
likely value of Theta, given</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>what I have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>And that value is called them
maximum a posteriori</text_slice>
            </slice>
            <slice>
              <time_slice>31:04</time_slice>
              <text_slice>probability estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>It's going to be this
one in our case.</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>So picking the point in the
posterior PMF that has the</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>highest probability.</text_slice>
            </slice>
            <slice>
              <time_slice>31:19</time_slice>
              <text_slice>That's the reasonable
thing to do.</text_slice>
            </slice>
            <slice>
              <time_slice>31:20</time_slice>
              <text_slice>This is the optimal thing to do
if you want to minimize the</text_slice>
            </slice>
            <slice>
              <time_slice>31:23</time_slice>
              <text_slice>probability of an incorrect
inference.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>And that's what people do
usually if they need to report</text_slice>
            </slice>
            <slice>
              <time_slice>31:31</time_slice>
              <text_slice>a single answer, if they need
to report a single decision.</text_slice>
            </slice>
            <slice>
              <time_slice>31:35</time_slice>
              <text_slice>How about in the estimation
context?</text_slice>
            </slice>
            <slice>
              <time_slice>31:39</time_slice>
              <text_slice>If that's what you know about
Theta, Theta could be around</text_slice>
            </slice>
            <slice>
              <time_slice>31:43</time_slice>
              <text_slice>here, but there's also some
sharp probability that it is</text_slice>
            </slice>
            <slice>
              <time_slice>31:46</time_slice>
              <text_slice>around here.</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>What's the single answer that
you would give to your boss?</text_slice>
            </slice>
            <slice>
              <time_slice>31:52</time_slice>
              <text_slice>One option is to use the same
philosophy and say, OK, I'm</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>going to find the Theta at which
this posterior density</text_slice>
            </slice>
            <slice>
              <time_slice>32:00</time_slice>
              <text_slice>is highest.</text_slice>
            </slice>
            <slice>
              <time_slice>32:01</time_slice>
              <text_slice>So I would pick this point
here and report this</text_slice>
            </slice>
            <slice>
              <time_slice>32:06</time_slice>
              <text_slice>particular Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>32:06</time_slice>
              <text_slice>So this would be my Theta,
again, Theta MAP, the Theta</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>that has the highest a
posteriori probability, just</text_slice>
            </slice>
            <slice>
              <time_slice>32:15</time_slice>
              <text_slice>because it corresponds to
the peak of the density.</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>But in this context, the
maximum a posteriori</text_slice>
            </slice>
            <slice>
              <time_slice>32:23</time_slice>
              <text_slice>probability theta was the
one that was most</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>likely to be true.</text_slice>
            </slice>
            <slice>
              <time_slice>32:28</time_slice>
              <text_slice>In the continuous case, you
cannot really say that this is</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>the most likely value
of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>32:34</time_slice>
              <text_slice>In a continuous setting, any
value of Theta has zero</text_slice>
            </slice>
            <slice>
              <time_slice>32:38</time_slice>
              <text_slice>probability, so when we
talk about densities.</text_slice>
            </slice>
            <slice>
              <time_slice>32:41</time_slice>
              <text_slice>So it's not the most likely.</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>It's the one for which the
density, so the probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>of that neighborhoods,
are highest.</text_slice>
            </slice>
            <slice>
              <time_slice>32:51</time_slice>
              <text_slice>So the rationale for picking
this particular estimate in</text_slice>
            </slice>
            <slice>
              <time_slice>32:56</time_slice>
              <text_slice>the continuous case is much
less compelling than the</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>rationale that we had in here.</text_slice>
            </slice>
            <slice>
              <time_slice>33:02</time_slice>
              <text_slice>So in this case, reasonable
people might choose different</text_slice>
            </slice>
            <slice>
              <time_slice>33:05</time_slice>
              <text_slice>quantities to report.</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>And the very popular one would
be to report instead the</text_slice>
            </slice>
            <slice>
              <time_slice>33:11</time_slice>
              <text_slice>conditional expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>33:13</time_slice>
              <text_slice>So I don't know quite
what Theta is.</text_slice>
            </slice>
            <slice>
              <time_slice>33:15</time_slice>
              <text_slice>Given the data that I have,
Theta has this distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>33:19</time_slice>
              <text_slice>Let me just report the average
over that distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>Let me report to the center
of gravity of this figure.</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>And in this figure, the center
of gravity would probably be</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>somewhere around here.</text_slice>
            </slice>
            <slice>
              <time_slice>33:32</time_slice>
              <text_slice>And that would be a different
estimate that you</text_slice>
            </slice>
            <slice>
              <time_slice>33:35</time_slice>
              <text_slice>might choose to report.</text_slice>
            </slice>
            <slice>
              <time_slice>33:37</time_slice>
              <text_slice>So center of gravity is
something around here.</text_slice>
            </slice>
            <slice>
              <time_slice>33:40</time_slice>
              <text_slice>And this is a conditional
expectation of Theta, given</text_slice>
            </slice>
            <slice>
              <time_slice>33:43</time_slice>
              <text_slice>the data that you have.</text_slice>
            </slice>
            <slice>
              <time_slice>33:46</time_slice>
              <text_slice>So these are two, in some sense,
fairly reasonable ways</text_slice>
            </slice>
            <slice>
              <time_slice>33:51</time_slice>
              <text_slice>of choosing what to report
to your boss.</text_slice>
            </slice>
            <slice>
              <time_slice>33:53</time_slice>
              <text_slice>Some people might choose
to report this.</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>Some people might choose
to report that.</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>And a priori, if there's no
compelling reason why one</text_slice>
            </slice>
            <slice>
              <time_slice>34:03</time_slice>
              <text_slice>would be preferable than other
one, unless you set some rules</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>for the game and you describe
a little more precisely what</text_slice>
            </slice>
            <slice>
              <time_slice>34:12</time_slice>
              <text_slice>your objectives are.</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>But no matter which one you
report, a single answer, a</text_slice>
            </slice>
            <slice>
              <time_slice>34:19</time_slice>
              <text_slice>point estimate, doesn't really
tell you the whole story.</text_slice>
            </slice>
            <slice>
              <time_slice>34:24</time_slice>
              <text_slice>There's a lot more information
conveyed by this posterior</text_slice>
            </slice>
            <slice>
              <time_slice>34:28</time_slice>
              <text_slice>distribution plot than
any single number</text_slice>
            </slice>
            <slice>
              <time_slice>34:31</time_slice>
              <text_slice>that you might report.</text_slice>
            </slice>
            <slice>
              <time_slice>34:32</time_slice>
              <text_slice>So in general, you may wish to
convince your boss that's it's</text_slice>
            </slice>
            <slice>
              <time_slice>34:36</time_slice>
              <text_slice>worth their time to look at the
entire plot, because that</text_slice>
            </slice>
            <slice>
              <time_slice>34:40</time_slice>
              <text_slice>plot sort of covers all
the possibilities.</text_slice>
            </slice>
            <slice>
              <time_slice>34:43</time_slice>
              <text_slice>It tells your boss most likely
we're in that range, but</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>there's also a distinct change
that our Theta happens to lie</text_slice>
            </slice>
            <slice>
              <time_slice>34:51</time_slice>
              <text_slice>in that range.</text_slice>
            </slice>
            <slice>
              <time_slice>34:54</time_slice>
              <text_slice>All right, now let us try to
perhaps differentiate between</text_slice>
            </slice>
            <slice>
              <time_slice>34:58</time_slice>
              <text_slice>these two and see under what
circumstances this one might</text_slice>
            </slice>
            <slice>
              <time_slice>35:02</time_slice>
              <text_slice>be the better estimate
to perform.</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>Better with respect to what?</text_slice>
            </slice>
            <slice>
              <time_slice>35:07</time_slice>
              <text_slice>We need some rules.</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>So we're going to throw
in some rules.</text_slice>
            </slice>
            <slice>
              <time_slice>35:14</time_slice>
              <text_slice>As a warm up, we're going to
deal with the problem of</text_slice>
            </slice>
            <slice>
              <time_slice>35:17</time_slice>
              <text_slice>making an estimation if you
had no information at all,</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>except for a prior
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>35:24</time_slice>
              <text_slice>So this is a warm up for what's
coming next, which</text_slice>
            </slice>
            <slice>
              <time_slice>35:27</time_slice>
              <text_slice>would be estimation that takes
into account some information.</text_slice>
            </slice>
            <slice>
              <time_slice>35:32</time_slice>
              <text_slice>So we have a Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>35:34</time_slice>
              <text_slice>And because of your subjective
beliefs or models by others,</text_slice>
            </slice>
            <slice>
              <time_slice>35:38</time_slice>
              <text_slice>you believe that Theta is
uniformly distributed between,</text_slice>
            </slice>
            <slice>
              <time_slice>35:41</time_slice>
              <text_slice>let's say, 4 and 10.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>You want to come up with
a point estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>35:51</time_slice>
              <text_slice>Let's try to look
for an estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>Call it c, in this case.</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>I want to pick a number
with which to estimate</text_slice>
            </slice>
            <slice>
              <time_slice>36:00</time_slice>
              <text_slice>the value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>I will be interested in the size
of the error that I make.</text_slice>
            </slice>
            <slice>
              <time_slice>36:08</time_slice>
              <text_slice>And I really dislike large
errors, so I'm going to focus</text_slice>
            </slice>
            <slice>
              <time_slice>36:12</time_slice>
              <text_slice>on the square of the error
that they make.</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>So I pick c.</text_slice>
            </slice>
            <slice>
              <time_slice>36:19</time_slice>
              <text_slice>Theta that has a random value
that I don't know.</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>But whatever it is, once it
becomes known, it results into</text_slice>
            </slice>
            <slice>
              <time_slice>36:25</time_slice>
              <text_slice>a squared error between
what it is and what I</text_slice>
            </slice>
            <slice>
              <time_slice>36:28</time_slice>
              <text_slice>guessed that it was.</text_slice>
            </slice>
            <slice>
              <time_slice>36:30</time_slice>
              <text_slice>And I'm interested in making
a small air on the average,</text_slice>
            </slice>
            <slice>
              <time_slice>36:35</time_slice>
              <text_slice>where the average is taken
with respect to all the</text_slice>
            </slice>
            <slice>
              <time_slice>36:38</time_slice>
              <text_slice>possible and unknown
values of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>So the problem, this is a least
squares formulation of</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>the problem, where we
try to minimize the</text_slice>
            </slice>
            <slice>
              <time_slice>36:49</time_slice>
              <text_slice>least squares errors.</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>How do you find the optimal c?</text_slice>
            </slice>
            <slice>
              <time_slice>36:53</time_slice>
              <text_slice>Well, we take that expression
and expand it.</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>And it is, using linearity
of expectations--</text_slice>
            </slice>
            <slice>
              <time_slice>37:05</time_slice>
              <text_slice>square minus 2c expected
Theta plus c squared--</text_slice>
            </slice>
            <slice>
              <time_slice>37:11</time_slice>
              <text_slice>that's the quantity that
we want to minimize,</text_slice>
            </slice>
            <slice>
              <time_slice>37:13</time_slice>
              <text_slice>with respect to c.</text_slice>
            </slice>
            <slice>
              <time_slice>37:16</time_slice>
              <text_slice>To do the minimization, take the
derivative with respect to</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>c and set it to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>37:21</time_slice>
              <text_slice>So that differentiation gives us
from here minus 2 expected</text_slice>
            </slice>
            <slice>
              <time_slice>37:27</time_slice>
              <text_slice>value of Theta plus
2c is equal to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>37:32</time_slice>
              <text_slice>And the answer that you get by
solving this equation is that</text_slice>
            </slice>
            <slice>
              <time_slice>37:36</time_slice>
              <text_slice>c is the expected
value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>So when you do this
optimization, you find that</text_slice>
            </slice>
            <slice>
              <time_slice>37:42</time_slice>
              <text_slice>the optimal estimate, the
things you should be</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>reporting, is the expected
value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>37:47</time_slice>
              <text_slice>So in this particular example,
you would choose your estimate</text_slice>
            </slice>
            <slice>
              <time_slice>37:51</time_slice>
              <text_slice>c to be just the middle
of these values,</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>which would be 7.</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>OK, and in case your
boss asks you, how</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>good is your estimate?</text_slice>
            </slice>
            <slice>
              <time_slice>38:08</time_slice>
              <text_slice>How big is your error
going to be?</text_slice>
            </slice>
            <slice>
              <time_slice>38:14</time_slice>
              <text_slice>What you could report is the
average size of the estimation</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>error that you are making.</text_slice>
            </slice>
            <slice>
              <time_slice>38:22</time_slice>
              <text_slice>We picked our estimates to be
the expected value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>So for this particular way that
I'm choosing to do my</text_slice>
            </slice>
            <slice>
              <time_slice>38:29</time_slice>
              <text_slice>estimation, this is the mean
squared error that I get.</text_slice>
            </slice>
            <slice>
              <time_slice>38:33</time_slice>
              <text_slice>And this is a familiar
quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>It's just the variance
of the distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>38:38</time_slice>
              <text_slice>So the expectation is that
best way to estimate a</text_slice>
            </slice>
            <slice>
              <time_slice>38:41</time_slice>
              <text_slice>quantity, if you're interested
in the mean squared error.</text_slice>
            </slice>
            <slice>
              <time_slice>38:45</time_slice>
              <text_slice>And the resulting mean squared
error is the variance itself.</text_slice>
            </slice>
            <slice>
              <time_slice>38:50</time_slice>
              <text_slice>How will this story change if
we now have data as well?</text_slice>
            </slice>
            <slice>
              <time_slice>38:56</time_slice>
              <text_slice>Now having data means that
we can compute posterior</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>distributions or conditional
distributions.</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>So we get transported into a new
universe where instead the</text_slice>
            </slice>
            <slice>
              <time_slice>39:10</time_slice>
              <text_slice>working with the original
distribution of Theta, the</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>prior distribution, now we work
with the condition of</text_slice>
            </slice>
            <slice>
              <time_slice>39:18</time_slice>
              <text_slice>distribution of Theta,
given the data</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>that we have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>39:24</time_slice>
              <text_slice>Now remember our old slogan that
conditional models and</text_slice>
            </slice>
            <slice>
              <time_slice>39:30</time_slice>
              <text_slice>conditional probabilities are
no different than ordinary</text_slice>
            </slice>
            <slice>
              <time_slice>39:33</time_slice>
              <text_slice>probabilities, except that we
live now in a new universe</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>where the new information has
been taken into account.</text_slice>
            </slice>
            <slice>
              <time_slice>39:42</time_slice>
              <text_slice>So if you use that philosophy
and you're asked to minimize</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>the squared error but now that
you live in a new universe</text_slice>
            </slice>
            <slice>
              <time_slice>39:53</time_slice>
              <text_slice>where X has been fixed to
something, what would the</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>optimal solution be?</text_slice>
            </slice>
            <slice>
              <time_slice>39:59</time_slice>
              <text_slice>It would again be the
expectation of theta, but</text_slice>
            </slice>
            <slice>
              <time_slice>40:03</time_slice>
              <text_slice>which expectation?</text_slice>
            </slice>
            <slice>
              <time_slice>40:04</time_slice>
              <text_slice>It's the expectation which
applies in the new conditional</text_slice>
            </slice>
            <slice>
              <time_slice>40:08</time_slice>
              <text_slice>universe in which we
live right now.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>So because of what we did
before, by the same</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>calculation, we would find that
the optimal estimates is</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>the expected value of X of
Theta, but the optimal</text_slice>
            </slice>
            <slice>
              <time_slice>40:24</time_slice>
              <text_slice>estimate that takes
into account the</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>information that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>40:29</time_slice>
              <text_slice>So the conclusion, once you get
your data, if you want to</text_slice>
            </slice>
            <slice>
              <time_slice>40:33</time_slice>
              <text_slice>minimize the mean squared error,
you should just report</text_slice>
            </slice>
            <slice>
              <time_slice>40:40</time_slice>
              <text_slice>the conditional estimation of
this unknown quantity based on</text_slice>
            </slice>
            <slice>
              <time_slice>40:43</time_slice>
              <text_slice>the data that you have.</text_slice>
            </slice>
            <slice>
              <time_slice>40:46</time_slice>
              <text_slice>So the picture here is that
Theta is unknown.</text_slice>
            </slice>
            <slice>
              <time_slice>40:53</time_slice>
              <text_slice>You have your apparatus that
creates measurements.</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>So this creates an X. You take
an X, and here you have a box</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>that does calculations.</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>It does calculations and it
spits out the conditional</text_slice>
            </slice>
            <slice>
              <time_slice>41:18</time_slice>
              <text_slice>expectation of Theta, given the
particular data that you</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>41:24</time_slice>
              <text_slice>And what we have done in this
class so far is, to some</text_slice>
            </slice>
            <slice>
              <time_slice>41:28</time_slice>
              <text_slice>extent, developing the
computational tools and skills</text_slice>
            </slice>
            <slice>
              <time_slice>41:33</time_slice>
              <text_slice>to do with this particular
calculation--</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>how to calculate the posterior
density for Theta and how to</text_slice>
            </slice>
            <slice>
              <time_slice>41:39</time_slice>
              <text_slice>calculate expectations,
conditional expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>41:42</time_slice>
              <text_slice>So in principle, we know
how to do this.</text_slice>
            </slice>
            <slice>
              <time_slice>41:45</time_slice>
              <text_slice>In principle, we can program a
computer to take the data and</text_slice>
            </slice>
            <slice>
              <time_slice>41:50</time_slice>
              <text_slice>to spit out condition
expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>41:56</time_slice>
              <text_slice>Somebody who doesn't think like
us might instead design a</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>calculating machine that does
something differently and</text_slice>
            </slice>
            <slice>
              <time_slice>42:09</time_slice>
              <text_slice>produces some other estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>42:16</time_slice>
              <text_slice>So we went through this argument
and we decided to</text_slice>
            </slice>
            <slice>
              <time_slice>42:20</time_slice>
              <text_slice>program our computer to
calculate conditional</text_slice>
            </slice>
            <slice>
              <time_slice>42:23</time_slice>
              <text_slice>expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>Somebody else came up with some
other crazy idea for how</text_slice>
            </slice>
            <slice>
              <time_slice>42:28</time_slice>
              <text_slice>to estimate the random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>They came up with some function
g and the programmed</text_slice>
            </slice>
            <slice>
              <time_slice>42:34</time_slice>
              <text_slice>it, and they designed a machine
that estimates Theta's</text_slice>
            </slice>
            <slice>
              <time_slice>42:38</time_slice>
              <text_slice>by outputting a certain
g of X.</text_slice>
            </slice>
            <slice>
              <time_slice>42:43</time_slice>
              <text_slice>That could be an alternative
estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>42:47</time_slice>
              <text_slice>Which one is better?</text_slice>
            </slice>
            <slice>
              <time_slice>42:50</time_slice>
              <text_slice>Well, we convinced ourselves
that this is the optimal one</text_slice>
            </slice>
            <slice>
              <time_slice>42:56</time_slice>
              <text_slice>in a universe where we have
fixed the particular</text_slice>
            </slice>
            <slice>
              <time_slice>42:59</time_slice>
              <text_slice>value of the data.</text_slice>
            </slice>
            <slice>
              <time_slice>43:01</time_slice>
              <text_slice>So what we have proved so far
is a relation of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>In this conditional universe,
the mean squared</text_slice>
            </slice>
            <slice>
              <time_slice>43:09</time_slice>
              <text_slice>error that I get--</text_slice>
            </slice>
            <slice>
              <time_slice>43:11</time_slice>
              <text_slice>I'm the one who's using
this estimator--</text_slice>
            </slice>
            <slice>
              <time_slice>43:15</time_slice>
              <text_slice>is less than or equal than the
mean squared error that this</text_slice>
            </slice>
            <slice>
              <time_slice>43:18</time_slice>
              <text_slice>person will get, the person
who uses that estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>43:23</time_slice>
              <text_slice>For any particular value of
the data, I'm going to do</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>better than the other person.</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>Now the data themselves
are random.</text_slice>
            </slice>
            <slice>
              <time_slice>43:32</time_slice>
              <text_slice>If I average over all possible
values of the data, I should</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>still be better off.</text_slice>
            </slice>
            <slice>
              <time_slice>43:40</time_slice>
              <text_slice>If I'm better off for any
possible value X, then I</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>should be better off on the
average over all possible</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>values of X.</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>So let us average both sides of
this quantity with respect</text_slice>
            </slice>
            <slice>
              <time_slice>43:55</time_slice>
              <text_slice>to the probability distribution
of X. If you want</text_slice>
            </slice>
            <slice>
              <time_slice>43:58</time_slice>
              <text_slice>to do it formally, you can write
this inequality between</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>numbers as an inequality between
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>44:06</time_slice>
              <text_slice>And it tells that no matter
what that random variable</text_slice>
            </slice>
            <slice>
              <time_slice>44:10</time_slice>
              <text_slice>turns out to be, this quantity
is better than that quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>Take expectations of both
sides, and you get this</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>inequality between expectations
overall.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>And this last inequality tells
me that the person who's using</text_slice>
            </slice>
            <slice>
              <time_slice>44:29</time_slice>
              <text_slice>this estimator who produces
estimates according to this</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>machine will have a mean squared
estimation error</text_slice>
            </slice>
            <slice>
              <time_slice>44:45</time_slice>
              <text_slice>that's less than or equal to
the estimation error that's</text_slice>
            </slice>
            <slice>
              <time_slice>44:48</time_slice>
              <text_slice>produced by the other person.</text_slice>
            </slice>
            <slice>
              <time_slice>44:51</time_slice>
              <text_slice>In a few words, the conditional
expectation</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>estimator is the optimal
estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>44:58</time_slice>
              <text_slice>It's the ultimate estimating
machine.</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>That's how you should solve
estimation problems and report</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>a single value.</text_slice>
            </slice>
            <slice>
              <time_slice>45:10</time_slice>
              <text_slice>If you're forced to report a
single value and if you're</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>interested in estimation
errors.</text_slice>
            </slice>
            <slice>
              <time_slice>45:18</time_slice>
              <text_slice>OK, while we could have told you
that story, of course, a</text_slice>
            </slice>
            <slice>
              <time_slice>45:24</time_slice>
              <text_slice>month or two ago, this is really
about interpretation --</text_slice>
            </slice>
            <slice>
              <time_slice>45:29</time_slice>
              <text_slice>about realizing that conditional
expectations have</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>a very nice property.</text_slice>
            </slice>
            <slice>
              <time_slice>45:35</time_slice>
              <text_slice>But other than that, any
probabilistic skills that come</text_slice>
            </slice>
            <slice>
              <time_slice>45:38</time_slice>
              <text_slice>into this business are just the
probabilistic skills of</text_slice>
            </slice>
            <slice>
              <time_slice>45:41</time_slice>
              <text_slice>being able to calculate
conditional expectations,</text_slice>
            </slice>
            <slice>
              <time_slice>45:44</time_slice>
              <text_slice>which you already
know how to do.</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>So conclusion, all of optimal
Bayesian estimation just means</text_slice>
            </slice>
            <slice>
              <time_slice>45:51</time_slice>
              <text_slice>calculating and reporting
conditional expectations.</text_slice>
            </slice>
            <slice>
              <time_slice>45:54</time_slice>
              <text_slice>Well, if the world were that
simple, then statisticians</text_slice>
            </slice>
            <slice>
              <time_slice>45:58</time_slice>
              <text_slice>wouldn't be able to find jobs
if life is that simple.</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>So real life is not
that simple.</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>There are complications.</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>And that perhaps makes their
life a little more</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>interesting.</text_slice>
            </slice>
            <slice>
              <time_slice>46:22</time_slice>
              <text_slice>OK, one complication is that we
would deal with the vectors</text_slice>
            </slice>
            <slice>
              <time_slice>46:25</time_slice>
              <text_slice>instead of just single
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>46:28</time_slice>
              <text_slice>I use the notation here
as if X was a</text_slice>
            </slice>
            <slice>
              <time_slice>46:31</time_slice>
              <text_slice>single random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>46:33</time_slice>
              <text_slice>In real life, you get
several data.</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>Does our story change?</text_slice>
            </slice>
            <slice>
              <time_slice>46:39</time_slice>
              <text_slice>Not really, same argument--</text_slice>
            </slice>
            <slice>
              <time_slice>46:41</time_slice>
              <text_slice>given all the data that you
have observed, you should</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>still report the conditional
expectation of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>46:47</time_slice>
              <text_slice>But what kind of work does it
take in order to report this</text_slice>
            </slice>
            <slice>
              <time_slice>46:51</time_slice>
              <text_slice>conditional expectation?</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>One issue is that you need to
cook up a plausible prior</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>distribution for Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>46:58</time_slice>
              <text_slice>How do you do that?</text_slice>
            </slice>
            <slice>
              <time_slice>46:59</time_slice>
              <text_slice>In a given application , this
is a bit of a judgment call,</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>what prior would you
be working with.</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>And there's a certain
skill there of not</text_slice>
            </slice>
            <slice>
              <time_slice>47:08</time_slice>
              <text_slice>making silly choices.</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>A more pragmatic, practical
issue is that this is a</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>formula that's extremely nice
and compact and simple that</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>you can write with
minimal ink.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>But the behind it there could
be hidden a huge amount of</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>So doing any sort of
calculations that involve</text_slice>
            </slice>
            <slice>
              <time_slice>47:34</time_slice>
              <text_slice>multiple random variables really
involves calculating</text_slice>
            </slice>
            <slice>
              <time_slice>47:39</time_slice>
              <text_slice>multi-dimensional integrals.</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>And the multi-dimensional
integrals are hard to compute.</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>So implementing actually this
calculating machine here may</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>not be easy, might be
complicated computationally.</text_slice>
            </slice>
            <slice>
              <time_slice>47:54</time_slice>
              <text_slice>It's also complicated in terms
of not being able to derive</text_slice>
            </slice>
            <slice>
              <time_slice>47:58</time_slice>
              <text_slice>intuition about it.</text_slice>
            </slice>
            <slice>
              <time_slice>47:59</time_slice>
              <text_slice>So perhaps you might want to
have a simpler version, a</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>simpler alternative to this
formula that's easier to work</text_slice>
            </slice>
            <slice>
              <time_slice>48:07</time_slice>
              <text_slice>with and easier to calculate.</text_slice>
            </slice>
            <slice>
              <time_slice>48:10</time_slice>
              <text_slice>We will be talking about
one such simpler</text_slice>
            </slice>
            <slice>
              <time_slice>48:13</time_slice>
              <text_slice>alternative next time.</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>So again, to conclude, at
the high level, Bayesian</text_slice>
            </slice>
            <slice>
              <time_slice>48:18</time_slice>
              <text_slice>estimation is very, very simple,
given that you have</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>mastered everything that
has happened in</text_slice>
            </slice>
            <slice>
              <time_slice>48:24</time_slice>
              <text_slice>this course so far.</text_slice>
            </slice>
            <slice>
              <time_slice>48:26</time_slice>
              <text_slice>There are certain practical
issues and it's also good to</text_slice>
            </slice>
            <slice>
              <time_slice>48:29</time_slice>
              <text_slice>be familiar with the concepts
and the issues that in</text_slice>
            </slice>
            <slice>
              <time_slice>48:33</time_slice>
              <text_slice>general, you would prefer to
report that complete posterior</text_slice>
            </slice>
            <slice>
              <time_slice>48:36</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>48:37</time_slice>
              <text_slice>But if you're forced to report a
point estimate, then there's</text_slice>
            </slice>
            <slice>
              <time_slice>48:40</time_slice>
              <text_slice>a number of reasonable
ways to do it.</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>And perhaps the most reasonable
one is to just the</text_slice>
            </slice>
            <slice>
              <time_slice>48:45</time_slice>
              <text_slice>report the conditional
expectation itself.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Bayesian Statistical Inference - I (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 21 Types of Inference models/approaches
Model building versus inferring unknoReadings: Sections 8.1-8.2 wn
variables. E.g., assume X=aS+W
Model building:It is the mark of truly educated peopleknow signal S, observe X, infer ato be deeply moved by statistics .Estimation in the presence of noise:
(Oscar Wilde)know a, observe X, estimate S.
ModelHypothesis testing: unknown takes one of
Reality
(e.g., customer arrivals) (e.g., Poisson) few possible values; aim at small
probability of incorrect decision
Data Estimation: aim at a small estimation error
Design &amp; interpretation of experiments
polling, medical/pharmaceutical trials ...Classical statistics:
NNetix competition Finance
 X pX(x;)Estimatorobjects
2 1 4 5
5 4 ? 1 3
3 5 2
4 ? 5 3 ?: unknown parameter (not a r.v.)
4 1 3 5
2 1 ? 4s r1 5 5 4E.g., = mass of electrono 2 ? 5 ? 4s n3 3 1 5 2 1e3 1 2 3s4 5 1 3 Bayesian: Use priors &amp; Bayes rule
3 3 ? 5
2 ? 1 1
5 2 ? 4 4 N
1 3 1 5 4 5
1 2 4 5 ?
 X 
p x EstimatorSignal processingX|( |)p()
Tracking, detection, speaker identication, ...
Bayesian inference: Use Bayes rule Estimation with discrete data
Hypothesis testing
discrete data f()pX(x )
fX(|
p()pX|
pX(x)=|(x|x)=|
|) pX(x)
| |pX(x)pX(x)=/integraldisplay
f()pX(x|)d|
continuous data
p()fX(x )
p Example:X(|
|x)=|
|fX(x)
Coin with unknown parameter 
Observe Xheads in ntosses
Estimation; continuous data
What is the Bayesian approach?
f()fX(x|) Want to nd f|X(|x)f|X(|x)=|
fX(x)Assume a prior on (e.g., uniform)
Zt=0+t1+2t2
Xt=Zt+Wt,t =1,2, . . . , n
Bayes rule gives:
f0,1,2|X1,...,X (n0,1,2|x1, . . . , x n)EstimatorY 
1EstimatorY XN
pY|X( | )
X
pY|X( | )
X


pY|X(y |x)
X

pY(y;)N
pY|X(y |x)
X

pY(y;)
pY|X(y |x)
X
p
Y(y;)
pX(x)
pY|X(y|x)
X

pY(y;)
pX(x)
X{0,1}
WfW(w)
Y=X+W
Matrix Completion
!Partially observed matrix: goal to predict the
unobserved entries
measurementSample Applications
Polling
Design of experiments/sampling methodologies
Lancet study on Iraq death toll
Medical/pharmaceutical trials
Data mining
Netix competition
Finance
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
p()
Np
X|(x |)
X

p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
Estimator
p()
pX|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|) pX(x;)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10
p()
Np
X|(x |)
X

p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x |)
Estimator
p()
N
X

Estimator
p()
pX|(x |)
X
Estimator
N
pX|(x |)
X
Estimatorp()
N
pX|(x |)
X
Estimator
1Graph of S&amp;P 500 index removed
due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Output of Bayesian Inference Least Mean Squares Estimation
Posterior distribution: Estimation in the absence of information
pmf p|X(|x) or pdf f(x)|X |
f()
1/6
If interested in a single answer: 4 10 
Maximum a posteriori probability (MAP):
nd estimate c, to:
pX(|x) = max p|X(|x)
|2minimizes probability of error; minimize E/bracketleftBig
(c)
often used in hypothesis testing/bracketrightBig
Optimal estimate: c=E[]f x|X(|x) = max f|X(|)
Optimal mean squared error:Conditional expectation:
( [])2E E = Var()E[ |X=y]=/integraldisplay
f|X( d/bracketleftBig
|x)/bracketrightBig
Single answers can be misleading!
LMS Estimation of based on X LMS Estimation w. several measurements
Two r.v.s ,X Unknown r.v. 
we observe that X=xObserve values of r.v.s X1, . . . , X n
new universe: condition on X=x
Best estimator: E[
2|X1, . . . , X n]
E
Can be hard to
c/bracketleftBig
(c) |X=x/bracketrightBig
is minimized by
compute/implement
=
/bracketleftBiginvolves multi-dimensional integrals, etc.
E(E[ |X=x])2|X=x/bracketrightBig
E[(g(x))2|X=x]
E/bracketleftBig
(E[ | g(2X])2|X/bracketrightBig
E/bracketleftBig
( X)) |X/bracketrightBig
E/bracketleftBig
(E[ |])2/bracketrightBig
2X E/bracketleftBig
(g(X))/bracketrightBig
E[ |X] minimizes E/bracketleftBig
((X))2g
over all estimators g()/bracketrightBigE stim a tion wit h discrete da t a
f |X( |x) =f ( )pX| (x| )
pX(x)
pX(x) = 
f ( )pX| (x| )d 
E xa m ple:EstimatorY  
1EstimatorY XN 
pY|X(|)
X 
pY|X(|)
X
 
 
pY|X(y|x)
X
 
pY(y; )N 
pY|X(y|x)
X
 
pY(y; ) 
pY|X(y|x)
X
 
pY(y; )
pX(x) 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W pW(w)
Y=X+W 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W pW(w)
Y=X+W+ 
pY|X(y|x)
X
 
pY(y; )
pX(x)
X {0,1}
W fW(w)
Y=X+W
object at unknown location X
sensors
pX| ( 1 | ) =
=P(se nsor i se nses  t h e o b je c t | = )
=h( dist a n c e o f  fro m se nsor i)
p()
Np
X|(x |)
X
Estimator
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 4 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 
p()
Np
X|(x|)
X
EstimatorW
 fW(w) {0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
f() 1/6 4 10 
p()
Np
X|(x|)
X

E st i m a t or
WfW(w) {0,1} X=+W
f() 1 / 6 4 1 0
p()
Np
X|(x|)
X
EstimatorWf
W(w){0,1} X=+W
1/6 4 10 
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-22-bayesian-statistical-inference-ii/</video_url>
          <video_title>Lecture 22: Bayesian Statistical Inference II</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Classical Inference - II (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l24/</lecture_pdf_url>
      <lectureno>24</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 24 Review
Maximum likeliho d estimationReference: Section 9.3 o
Have model with unknown parameters:
Course Evaluations (until 12/16) XpX(x;)
http://web.mit.edu/subjectevaluation Pick that makes data most likely
maxpX(x;)

Compare to Bayesian MAP estimation:Outline
pX(x)p()Review maxpX( m a x|
| |x)o r|
 pY(y)
Maximum likelihood estimation
Sample mean estimate of =E[X]Condence intervals
n=(X1+
Linear regression+Xn)/n

1condence interval
Binary hypothesis testing+P(n n)1,
Types of error
condence interval for sample meanLikelihood ratio test (LRT)
letzbe s.t. (z)=1 /2
z zPn n+n n
1
Regression Linear regression
yResidual Model y0+x 1
 (xi,yi) xy i01xi nmin (i01xi)2y
0,1i=1 
xy=0+1x
  Solution (set derivatives to zero):
 x1++xn y1+ + ynx= , y =
=0 yx n n
Data: (x 1,y1),(x2,y2),...,(x n,yn)n( )(1=i=1xi
nx y i
(x x)2y)
Model: y0+1x

i=1 i
n
2 0= min (1i) ( )y xyi0x 1
0,1=1
i 
Interpretation of the form of the solution
One interpretation:Assume a model Y=0+1X+W
Yi=0+21xi+Wi,WiN(0,), i.i.d.Windependent of X,w i t hz e r om e a n
Likelihood function fX,Y (x, y;)i s :| Check that
1ncov( YE[Y])cexp X, E(y0Y) (XE[X])(
i 1xi)2
2 1= = 
2 2i=1
var(X)
E(XE[X])
Take logs, same as (*)Solution formula for  
1uses natural
Least sq. pretend Wii.i.d. normal estimates of the variance and covariance476 Classical Statistical Inference Chap. 9
in the context of various probabilistic frameworks, which provide perspective and
a mechanism for quantitative analysis.
We rst consider the case of only two variables, and then gene ralize. We
wish to model the relation between two variables of interest, xand y(e.g., years
of education and income), based on a collection of data pairs ( xi,yi),i=1,...,n .
For example, xicould be the years of education and yithe annual income of the
ith person in the sample. Often a two-dimensional plot of these samples indicates
a systematic, approximately linear relation between xiand yi. Then, it is natural
to attempt to build a linear model of the form
y0+1x,
where 0and 1are unknown parameters to be estimated.
In particular, given some estimates 0and 1of the resulting parameters,
the value yicorresponding to xi, as predicted by the model, is
yi=0+1xi.
Generally,  yiwill be di erent from the given value yi, and the corresponding
dierence
yi=yiyi,
is called the ithresidual . A choice of estimates that results in small residuals
is considered to provide a good t to the data. With this motivation, the linearregression approach chooses the parameter estimates 
0and 1that minimize
the sum of the squared residuals,
n
i=1(yiyi)2=n
i=1(yi01xi)2,
over all 1and 2; see Fig. 9.5 for an illustration.
Figure 9.5: Illustration of a set of data pairs ( xi,yi), and a linear model y=
0+1x,obtained by minimizing over 0,1the sum of the squares of the residuals
yi01xi.
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>The world of linear regression The world of regression (ctd.)
Multiple linear regression: In practice, one also reports
d a t a : (xi,x,x,yi),i=1,...,n Condence intervals for the i i i
m o d e l : y0+x+x+xStandard error (estimate of )
f o r m u l a t i o n : 2R, a measure of explanatory power
n
min
(2yi0xixix
i)
,,i=1   
Some common concerns
HeteroskedasticityChoosing the right variables
Multicollinearitymodel y0+1h(x)
e.g.,2y0+1x Sometimes misused to conclude causal
relationswork with data points (y i,h(x))
etc.formulation:
n
min
(yi201h1(xi))
i=1
Binary hypothesis testing Likelihood ratio test (LRT)
Binary ; new terminology: Bayesian case (MAP rule): choose H1if:
P(H1|X=x)&gt;P(H0|X=x)n u l l h y p o t h e s i s H0:
orXpX(x;H0)[ o r fX(x;H0)]
P(X=x|H1)P(H1)P(X=x H 0)P(H0)a l t e r n a t i v e h y p o t h e s i s H1: &gt;|
P(X=x) P(X=x)XpX(x;H1)[ o r fX(x;H1)]or
P(X=x|H1)P(H0)&gt;Partition the space of possible data vectors P(X=x|H0)P(H1)
Rejection region R: (likelihood ratio test)
reject H0idataR
Nonbayesian version: choose H1if
Types of errors:
P(X=x;H1)&gt;(discrete case)T y p e I (false rejection ,f a l s ea l a r m ) : P(X=x;H0)
H0true, but rejected
fX(x;H1)&gt; (continuous case)(R)=P(XR;H0) fX(x;H0)
T y p e I I (false acceptance ,threshold trades o the two types of error
missed detection):
choose so that P(reject H;H)=H0false, but accepted0 0
(e.g., =0.05)
(R)=P(XR;H1)
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-23-classical-statistical-inference-i/</video_url>
          <video_title>Lecture 23: Classical Statistical Inference I</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality, educational</text_slice>
            </slice>
            <slice>
              <time_slice>0:08</time_slice>
              <text_slice>resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:22</time_slice>
              <text_slice>PROFESSOR: So for the last three
lectures we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>0:24</time_slice>
              <text_slice>talk about classical statistics,
the way statistics</text_slice>
            </slice>
            <slice>
              <time_slice>0:28</time_slice>
              <text_slice>can be done if you don't want to
assume a prior distribution</text_slice>
            </slice>
            <slice>
              <time_slice>0:32</time_slice>
              <text_slice>on the unknown parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>Today we're going to focus,
mostly, on the estimation side</text_slice>
            </slice>
            <slice>
              <time_slice>0:38</time_slice>
              <text_slice>and leave hypothesis testing
for the next two lectures.</text_slice>
            </slice>
            <slice>
              <time_slice>0:41</time_slice>
              <text_slice>So where there is one generic
method that one can use to</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>carry out parameter estimation,
that's the maximum</text_slice>
            </slice>
            <slice>
              <time_slice>0:50</time_slice>
              <text_slice>likelihood method.</text_slice>
            </slice>
            <slice>
              <time_slice>0:51</time_slice>
              <text_slice>We're going to define
what it is.</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>Then we will look at the most
common estimation problem</text_slice>
            </slice>
            <slice>
              <time_slice>0:58</time_slice>
              <text_slice>there is, which is to estimate
the mean of a given</text_slice>
            </slice>
            <slice>
              <time_slice>1:00</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>1:02</time_slice>
              <text_slice>And we're going to talk about
confidence intervals, which</text_slice>
            </slice>
            <slice>
              <time_slice>1:05</time_slice>
              <text_slice>refers to providing an
interval around your</text_slice>
            </slice>
            <slice>
              <time_slice>1:09</time_slice>
              <text_slice>estimates, which has some
properties of the kind that</text_slice>
            </slice>
            <slice>
              <time_slice>1:13</time_slice>
              <text_slice>the parameter is highly likely
to be inside that interval,</text_slice>
            </slice>
            <slice>
              <time_slice>1:17</time_slice>
              <text_slice>but we will be careful about
how to interpret that</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>particular statement.</text_slice>
            </slice>
            <slice>
              <time_slice>1:22</time_slice>
              <text_slice>Ok.</text_slice>
            </slice>
            <slice>
              <time_slice>1:22</time_slice>
              <text_slice>So the big framework first.</text_slice>
            </slice>
            <slice>
              <time_slice>1:25</time_slice>
              <text_slice>The picture is almost the same
as the one that we had in the</text_slice>
            </slice>
            <slice>
              <time_slice>1:29</time_slice>
              <text_slice>case of Bayesian statistics.</text_slice>
            </slice>
            <slice>
              <time_slice>1:31</time_slice>
              <text_slice>We have some unknown
parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>1:33</time_slice>
              <text_slice>And we have a measuring
device.</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>There is some noise,
some randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>And we get an observation, X,
whose distribution depends on</text_slice>
            </slice>
            <slice>
              <time_slice>1:42</time_slice>
              <text_slice>the value of the parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>1:44</time_slice>
              <text_slice>However, the big change from the
Bayesian setting is that</text_slice>
            </slice>
            <slice>
              <time_slice>1:47</time_slice>
              <text_slice>here, this parameter
is just a number.</text_slice>
            </slice>
            <slice>
              <time_slice>1:50</time_slice>
              <text_slice>It's not modeled as
a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>It does not have a probability
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>1:55</time_slice>
              <text_slice>There's nothing random
about it.</text_slice>
            </slice>
            <slice>
              <time_slice>1:57</time_slice>
              <text_slice>It's a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>It just happens that we don't
know what that constant is.</text_slice>
            </slice>
            <slice>
              <time_slice>2:02</time_slice>
              <text_slice>And in particular, this
probability distribution here,</text_slice>
            </slice>
            <slice>
              <time_slice>2:05</time_slice>
              <text_slice>the distribution of X,
depends on Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>2:10</time_slice>
              <text_slice>But this is not a conditional
distribution in the usual</text_slice>
            </slice>
            <slice>
              <time_slice>2:13</time_slice>
              <text_slice>sense of the word.</text_slice>
            </slice>
            <slice>
              <time_slice>2:15</time_slice>
              <text_slice>Conditional distributions were
defined when we had two random</text_slice>
            </slice>
            <slice>
              <time_slice>2:18</time_slice>
              <text_slice>variables and we condition one
random variable on the other.</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>And we used the bar to separate
the X from the Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>2:25</time_slice>
              <text_slice>To make the point that this
is not a conditioned</text_slice>
            </slice>
            <slice>
              <time_slice>2:27</time_slice>
              <text_slice>distribution, we use a
different notation.</text_slice>
            </slice>
            <slice>
              <time_slice>2:29</time_slice>
              <text_slice>We put a semicolon here.</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>And what this is meant to say is
that X has a distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>2:35</time_slice>
              <text_slice>That distribution has
a certain parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>2:39</time_slice>
              <text_slice>And we don't know what
that parameter is.</text_slice>
            </slice>
            <slice>
              <time_slice>2:42</time_slice>
              <text_slice>So for example, this might be
a normal distribution, with</text_slice>
            </slice>
            <slice>
              <time_slice>2:46</time_slice>
              <text_slice>variance 1 but a mean Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>2:49</time_slice>
              <text_slice>We don't know what Theta is.</text_slice>
            </slice>
            <slice>
              <time_slice>2:50</time_slice>
              <text_slice>And we want to estimate it.</text_slice>
            </slice>
            <slice>
              <time_slice>2:52</time_slice>
              <text_slice>Now once we have this setting,
then your job is to design</text_slice>
            </slice>
            <slice>
              <time_slice>2:55</time_slice>
              <text_slice>this box, the estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>2:57</time_slice>
              <text_slice>The estimator is some data
processing box that takes the</text_slice>
            </slice>
            <slice>
              <time_slice>3:00</time_slice>
              <text_slice>measurements and produces
an estimate</text_slice>
            </slice>
            <slice>
              <time_slice>3:03</time_slice>
              <text_slice>of the unknown parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>3:06</time_slice>
              <text_slice>Now the notation that's used
here is as if X and Theta were</text_slice>
            </slice>
            <slice>
              <time_slice>3:11</time_slice>
              <text_slice>one-dimensional quantities.</text_slice>
            </slice>
            <slice>
              <time_slice>3:13</time_slice>
              <text_slice>But actually, everything we
say remains valid if you</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>interpret X and Theta as
vectors of parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>3:20</time_slice>
              <text_slice>So for example, you
may obtain several</text_slice>
            </slice>
            <slice>
              <time_slice>3:22</time_slice>
              <text_slice>measurements, X1 up to 2Xn.</text_slice>
            </slice>
            <slice>
              <time_slice>3:25</time_slice>
              <text_slice>And there may be several unknown
parameters in the</text_slice>
            </slice>
            <slice>
              <time_slice>3:27</time_slice>
              <text_slice>background.</text_slice>
            </slice>
            <slice>
              <time_slice>3:30</time_slice>
              <text_slice>Once more, we do not have, and
we do not want to assume, a</text_slice>
            </slice>
            <slice>
              <time_slice>3:34</time_slice>
              <text_slice>prior distribution on Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>3:35</time_slice>
              <text_slice>It's a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>And if you want to think
mathematically about this</text_slice>
            </slice>
            <slice>
              <time_slice>3:39</time_slice>
              <text_slice>situation, it's as if you
have many different</text_slice>
            </slice>
            <slice>
              <time_slice>3:41</time_slice>
              <text_slice>probabilistic models.</text_slice>
            </slice>
            <slice>
              <time_slice>3:43</time_slice>
              <text_slice>So a normal with this mean or
a normal with that mean or a</text_slice>
            </slice>
            <slice>
              <time_slice>3:46</time_slice>
              <text_slice>normal with that mean, these
are alternative candidate</text_slice>
            </slice>
            <slice>
              <time_slice>3:49</time_slice>
              <text_slice>probabilistic models.</text_slice>
            </slice>
            <slice>
              <time_slice>3:50</time_slice>
              <text_slice>And we want to try to make a
decision about which one is</text_slice>
            </slice>
            <slice>
              <time_slice>3:55</time_slice>
              <text_slice>the correct model.</text_slice>
            </slice>
            <slice>
              <time_slice>3:56</time_slice>
              <text_slice>In some cases, we have to choose
just between a small</text_slice>
            </slice>
            <slice>
              <time_slice>3:59</time_slice>
              <text_slice>number of models.</text_slice>
            </slice>
            <slice>
              <time_slice>4:00</time_slice>
              <text_slice>For example, you have a coin
with an unknown bias.</text_slice>
            </slice>
            <slice>
              <time_slice>4:03</time_slice>
              <text_slice>The bias is either 1/2 or 3/4.</text_slice>
            </slice>
            <slice>
              <time_slice>4:06</time_slice>
              <text_slice>You're going to flip the
coin a few times.</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>And you try to decide whether
the true bias is this one or</text_slice>
            </slice>
            <slice>
              <time_slice>4:13</time_slice>
              <text_slice>is that one.</text_slice>
            </slice>
            <slice>
              <time_slice>4:14</time_slice>
              <text_slice>So in this case, we have two
specific, alternative</text_slice>
            </slice>
            <slice>
              <time_slice>4:17</time_slice>
              <text_slice>probabilistic models from which
we want to distinguish.</text_slice>
            </slice>
            <slice>
              <time_slice>4:20</time_slice>
              <text_slice>But sometimes things are a
little more complicated.</text_slice>
            </slice>
            <slice>
              <time_slice>4:25</time_slice>
              <text_slice>For example, you have a coin.</text_slice>
            </slice>
            <slice>
              <time_slice>4:26</time_slice>
              <text_slice>And you have one hypothesis
that my coin is unbiased.</text_slice>
            </slice>
            <slice>
              <time_slice>4:30</time_slice>
              <text_slice>And the other hypothesis is
that my coin is biased.</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>And you do your experiments.</text_slice>
            </slice>
            <slice>
              <time_slice>4:36</time_slice>
              <text_slice>And you want to come up with a
decision that decides whether</text_slice>
            </slice>
            <slice>
              <time_slice>4:40</time_slice>
              <text_slice>this is true or this
one is true.</text_slice>
            </slice>
            <slice>
              <time_slice>4:43</time_slice>
              <text_slice>In this case, we're not
dealing with just two</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>alternative probabilistic
models.</text_slice>
            </slice>
            <slice>
              <time_slice>4:48</time_slice>
              <text_slice>This one is a specific
model for the coin.</text_slice>
            </slice>
            <slice>
              <time_slice>4:51</time_slice>
              <text_slice>But this one actually
corresponds to lots of</text_slice>
            </slice>
            <slice>
              <time_slice>4:54</time_slice>
              <text_slice>possible, alternative
coin models.</text_slice>
            </slice>
            <slice>
              <time_slice>4:56</time_slice>
              <text_slice>So this includes the model where
Theta is 0.6, the model</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>where Theta is 0.7, Theta
is 0.8, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>5:03</time_slice>
              <text_slice>So we're trying to discriminate
between one model</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>and lots of alternative
models.</text_slice>
            </slice>
            <slice>
              <time_slice>5:09</time_slice>
              <text_slice>How does one go about this?</text_slice>
            </slice>
            <slice>
              <time_slice>5:11</time_slice>
              <text_slice>Well, there's some systematic
ways that one can approach</text_slice>
            </slice>
            <slice>
              <time_slice>5:14</time_slice>
              <text_slice>problems of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>And we will start talking
about these next time.</text_slice>
            </slice>
            <slice>
              <time_slice>5:19</time_slice>
              <text_slice>So today, we're going to focus
on estimation problems.</text_slice>
            </slice>
            <slice>
              <time_slice>5:22</time_slice>
              <text_slice>In estimation problems, theta is
a quantity, which is a real</text_slice>
            </slice>
            <slice>
              <time_slice>5:27</time_slice>
              <text_slice>number, a continuous
parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>5:29</time_slice>
              <text_slice>We're to design this box, so
what we get out of this box is</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>an estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>5:34</time_slice>
              <text_slice>Now notice that this estimate
here is a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:37</time_slice>
              <text_slice>Even though theta is
deterministic, this is random,</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>because it's a function of
the data that we observe.</text_slice>
            </slice>
            <slice>
              <time_slice>5:45</time_slice>
              <text_slice>The data are random.</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>We're applying a function
to the data to</text_slice>
            </slice>
            <slice>
              <time_slice>5:49</time_slice>
              <text_slice>construct our estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>5:50</time_slice>
              <text_slice>So, since it's a function of
random variables, it's a</text_slice>
            </slice>
            <slice>
              <time_slice>5:52</time_slice>
              <text_slice>random variable itself.</text_slice>
            </slice>
            <slice>
              <time_slice>5:54</time_slice>
              <text_slice>The distribution of Theta hat
depends on the distribution of</text_slice>
            </slice>
            <slice>
              <time_slice>5:57</time_slice>
              <text_slice>X. The distribution of X
is affected by Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>6:01</time_slice>
              <text_slice>So in the end, the distribution
of your estimate</text_slice>
            </slice>
            <slice>
              <time_slice>6:03</time_slice>
              <text_slice>Theta hat will also be affected
by whatever Theta</text_slice>
            </slice>
            <slice>
              <time_slice>6:08</time_slice>
              <text_slice>happens to be.</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>Our general objective, when
designing estimators, is that</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>we want to get, in the end, an
error, an estimation error,</text_slice>
            </slice>
            <slice>
              <time_slice>6:17</time_slice>
              <text_slice>which is not too large.</text_slice>
            </slice>
            <slice>
              <time_slice>6:19</time_slice>
              <text_slice>But we'll have to make
that specific.</text_slice>
            </slice>
            <slice>
              <time_slice>6:21</time_slice>
              <text_slice>Again, what exactly do
we mean by that?</text_slice>
            </slice>
            <slice>
              <time_slice>6:24</time_slice>
              <text_slice>So how do we go about
this problem?</text_slice>
            </slice>
            <slice>
              <time_slice>6:29</time_slice>
              <text_slice>One general approach is to pick
a Theta, under which the</text_slice>
            </slice>
            <slice>
              <time_slice>6:40</time_slice>
              <text_slice>data that we observe, that
this is the X's, our most</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>likely to have occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>6:47</time_slice>
              <text_slice>So I observe X. For any given
Theta, I can calculate this</text_slice>
            </slice>
            <slice>
              <time_slice>6:52</time_slice>
              <text_slice>quantity, which tells me, under
this particular Theta,</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>the X that you observed had this
probability of occurring.</text_slice>
            </slice>
            <slice>
              <time_slice>7:00</time_slice>
              <text_slice>Under that Theta, the X that
you observe had that</text_slice>
            </slice>
            <slice>
              <time_slice>7:03</time_slice>
              <text_slice>probability of occurring.</text_slice>
            </slice>
            <slice>
              <time_slice>7:04</time_slice>
              <text_slice>You just choose that Theta,
which makes the data that you</text_slice>
            </slice>
            <slice>
              <time_slice>7:08</time_slice>
              <text_slice>observed most likely.</text_slice>
            </slice>
            <slice>
              <time_slice>7:12</time_slice>
              <text_slice>It's interesting to compare
this maximum likelihood</text_slice>
            </slice>
            <slice>
              <time_slice>7:15</time_slice>
              <text_slice>estimate with the estimates that
you would have, if you</text_slice>
            </slice>
            <slice>
              <time_slice>7:19</time_slice>
              <text_slice>were in a Bayesian setting,
and you were using maximum</text_slice>
            </slice>
            <slice>
              <time_slice>7:22</time_slice>
              <text_slice>approach theory probability
estimation.</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>In the Bayesian setting, what
we do is, given the data, we</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>use the prior distribution
on Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>7:34</time_slice>
              <text_slice>And we calculate the posterior
distribution of Theta given X.</text_slice>
            </slice>
            <slice>
              <time_slice>7:41</time_slice>
              <text_slice>Notice that this is sort
of the opposite from</text_slice>
            </slice>
            <slice>
              <time_slice>7:44</time_slice>
              <text_slice>what we have here.</text_slice>
            </slice>
            <slice>
              <time_slice>7:46</time_slice>
              <text_slice>This is the probability of X
for a particular value of</text_slice>
            </slice>
            <slice>
              <time_slice>7:49</time_slice>
              <text_slice>Theta, whereas this is the
probability of Theta for a</text_slice>
            </slice>
            <slice>
              <time_slice>7:51</time_slice>
              <text_slice>particular X. So it's the
opposite type of conditioning.</text_slice>
            </slice>
            <slice>
              <time_slice>7:55</time_slice>
              <text_slice>In the Bayesian setting, Theta
is a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>7:58</time_slice>
              <text_slice>So we can talk about
the probability</text_slice>
            </slice>
            <slice>
              <time_slice>7:59</time_slice>
              <text_slice>distribution of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>8:01</time_slice>
              <text_slice>So how do these two compare,
except for this syntactic</text_slice>
            </slice>
            <slice>
              <time_slice>8:04</time_slice>
              <text_slice>difference that the order X's
and Theta's are reversed?</text_slice>
            </slice>
            <slice>
              <time_slice>8:08</time_slice>
              <text_slice>Let's write down, in full
detail, what this posterior</text_slice>
            </slice>
            <slice>
              <time_slice>8:11</time_slice>
              <text_slice>distribution of Theta is.</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>By the Bayes rule, this
conditional distribution is</text_slice>
            </slice>
            <slice>
              <time_slice>8:17</time_slice>
              <text_slice>obtained from the prior, and the
model of the measurement</text_slice>
            </slice>
            <slice>
              <time_slice>8:20</time_slice>
              <text_slice>process that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>8:21</time_slice>
              <text_slice>And we get to this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>So in Bayesian estimation, we
want to find the most likely</text_slice>
            </slice>
            <slice>
              <time_slice>8:29</time_slice>
              <text_slice>value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>And we need to maximize
this quantity over</text_slice>
            </slice>
            <slice>
              <time_slice>8:33</time_slice>
              <text_slice>all possible Theta's.</text_slice>
            </slice>
            <slice>
              <time_slice>8:34</time_slice>
              <text_slice>First thing to notice is that
the denominator is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>8:38</time_slice>
              <text_slice>It does not involve Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>8:40</time_slice>
              <text_slice>So when you maximize this
quantity, you don't care about</text_slice>
            </slice>
            <slice>
              <time_slice>8:43</time_slice>
              <text_slice>the denominator.</text_slice>
            </slice>
            <slice>
              <time_slice>8:44</time_slice>
              <text_slice>You just want to maximize
the numerator.</text_slice>
            </slice>
            <slice>
              <time_slice>8:47</time_slice>
              <text_slice>Now, here, things start to look
a little more similar.</text_slice>
            </slice>
            <slice>
              <time_slice>8:52</time_slice>
              <text_slice>And they would be exactly of
the same kind, if that term</text_slice>
            </slice>
            <slice>
              <time_slice>8:56</time_slice>
              <text_slice>here was absent, it the
prior was absent.</text_slice>
            </slice>
            <slice>
              <time_slice>8:59</time_slice>
              <text_slice>The two are going to become
the same if that prior was</text_slice>
            </slice>
            <slice>
              <time_slice>9:03</time_slice>
              <text_slice>just a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>9:05</time_slice>
              <text_slice>So if that prior is a constant,
then maximum</text_slice>
            </slice>
            <slice>
              <time_slice>9:10</time_slice>
              <text_slice>likelihood estimation takes
exactly the same form as</text_slice>
            </slice>
            <slice>
              <time_slice>9:13</time_slice>
              <text_slice>Bayesian maximum posterior
probability estimation.</text_slice>
            </slice>
            <slice>
              <time_slice>9:17</time_slice>
              <text_slice>So you can give this particular
interpretation of</text_slice>
            </slice>
            <slice>
              <time_slice>9:21</time_slice>
              <text_slice>maximum likelihood estimation.</text_slice>
            </slice>
            <slice>
              <time_slice>9:22</time_slice>
              <text_slice>Maximum likelihood estimation
is essentially what you have</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>done, if you were in a Bayesian
world, and you had</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>assumed a prior on the Theta's
that's uniform, all the</text_slice>
            </slice>
            <slice>
              <time_slice>9:35</time_slice>
              <text_slice>Theta's being equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>9:42</time_slice>
              <text_slice>Okay.</text_slice>
            </slice>
            <slice>
              <time_slice>9:42</time_slice>
              <text_slice>So let's look at a
simple example.</text_slice>
            </slice>
            <slice>
              <time_slice>9:45</time_slice>
              <text_slice>Suppose that the Xi's are
independent, identically</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>distributed random
variables, with a</text_slice>
            </slice>
            <slice>
              <time_slice>9:50</time_slice>
              <text_slice>certain parameter Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>9:52</time_slice>
              <text_slice>So the distribution of each
one of the Xi's is this</text_slice>
            </slice>
            <slice>
              <time_slice>9:55</time_slice>
              <text_slice>particular term.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>So Theta is one-dimensional.</text_slice>
            </slice>
            <slice>
              <time_slice>9:59</time_slice>
              <text_slice>It's a one-dimensional
parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>10:01</time_slice>
              <text_slice>But we have several data.</text_slice>
            </slice>
            <slice>
              <time_slice>10:03</time_slice>
              <text_slice>We write down the formula
for the probability of a</text_slice>
            </slice>
            <slice>
              <time_slice>10:07</time_slice>
              <text_slice>particular X vector, given a
particular value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>10:12</time_slice>
              <text_slice>But again, when I use the word,
given, here it's not in</text_slice>
            </slice>
            <slice>
              <time_slice>10:14</time_slice>
              <text_slice>the conditioning sense.</text_slice>
            </slice>
            <slice>
              <time_slice>10:16</time_slice>
              <text_slice>It's the value of the
density for a</text_slice>
            </slice>
            <slice>
              <time_slice>10:18</time_slice>
              <text_slice>particular choice of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>10:21</time_slice>
              <text_slice>Here, I wrote down, I defined
maximum likelihood estimation</text_slice>
            </slice>
            <slice>
              <time_slice>10:24</time_slice>
              <text_slice>in terms of PMFs.</text_slice>
            </slice>
            <slice>
              <time_slice>10:26</time_slice>
              <text_slice>That's what you would
do if the X's were</text_slice>
            </slice>
            <slice>
              <time_slice>10:28</time_slice>
              <text_slice>discrete random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>10:29</time_slice>
              <text_slice>Here, the X's are continuous
random variables, so instead</text_slice>
            </slice>
            <slice>
              <time_slice>10:32</time_slice>
              <text_slice>of I'm using the PDF
instead of the PMF.</text_slice>
            </slice>
            <slice>
              <time_slice>10:36</time_slice>
              <text_slice>So this a definition, here,
generalizes to the case of</text_slice>
            </slice>
            <slice>
              <time_slice>10:39</time_slice>
              <text_slice>continuous random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>10:40</time_slice>
              <text_slice>And you use F's instead of
X's, our usual recipe.</text_slice>
            </slice>
            <slice>
              <time_slice>10:44</time_slice>
              <text_slice>So the maximum likelihood
estimate is defined.</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>Now, since the Xi's are
independent, the joint density</text_slice>
            </slice>
            <slice>
              <time_slice>10:51</time_slice>
              <text_slice>of all the X's together
is the product of</text_slice>
            </slice>
            <slice>
              <time_slice>10:54</time_slice>
              <text_slice>the individual densities.</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>So you look at this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>10:59</time_slice>
              <text_slice>This is the density or sort of
probability of observing a</text_slice>
            </slice>
            <slice>
              <time_slice>11:03</time_slice>
              <text_slice>particular sequence of X's.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>And we ask the question, what's
the value of Theta that</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>makes the X's that we
observe most likely?</text_slice>
            </slice>
            <slice>
              <time_slice>11:10</time_slice>
              <text_slice>So we want to carry out
this maximization.</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>Now this maximization is just
a calculational problem.</text_slice>
            </slice>
            <slice>
              <time_slice>11:17</time_slice>
              <text_slice>We're going to do this
maximization by taking the</text_slice>
            </slice>
            <slice>
              <time_slice>11:19</time_slice>
              <text_slice>logarithm of this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>11:21</time_slice>
              <text_slice>Maximizing an expression
is the same as</text_slice>
            </slice>
            <slice>
              <time_slice>11:23</time_slice>
              <text_slice>maximizing the logarithm.</text_slice>
            </slice>
            <slice>
              <time_slice>11:25</time_slice>
              <text_slice>So the logarithm of this
expression, the logarithm of a</text_slice>
            </slice>
            <slice>
              <time_slice>11:28</time_slice>
              <text_slice>product is the sum of
the logarithms.</text_slice>
            </slice>
            <slice>
              <time_slice>11:31</time_slice>
              <text_slice>You get contributions from
this Theta term.</text_slice>
            </slice>
            <slice>
              <time_slice>11:34</time_slice>
              <text_slice>There's n of these, so we
get an n log Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>And then we have the sum of the
logarithms of these terms.</text_slice>
            </slice>
            <slice>
              <time_slice>11:40</time_slice>
              <text_slice>It gives us minus Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>And then the sum of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>11:45</time_slice>
              <text_slice>So we need to maximize
this expression</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>with respect to Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>11:48</time_slice>
              <text_slice>The way to do this maximization
is you take the</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>derivative, with respect
to Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>11:53</time_slice>
              <text_slice>And you get n over Theta equals
to the sum of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>11:58</time_slice>
              <text_slice>And then you solve for Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>12:00</time_slice>
              <text_slice>And you find that the
maximum likelihood</text_slice>
            </slice>
            <slice>
              <time_slice>12:02</time_slice>
              <text_slice>estimate is this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>12:04</time_slice>
              <text_slice>Which sort of makes sense,
because this is the reciprocal</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>of the sample mean of X's.</text_slice>
            </slice>
            <slice>
              <time_slice>12:16</time_slice>
              <text_slice>Theta, in an exponential
distribution, we know that</text_slice>
            </slice>
            <slice>
              <time_slice>12:19</time_slice>
              <text_slice>it's 1 over (the mean of the
exponential distribution).</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>So it looks like a reasonable
estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>12:26</time_slice>
              <text_slice>So in any case, this is the
estimates that the maximum</text_slice>
            </slice>
            <slice>
              <time_slice>12:29</time_slice>
              <text_slice>likelihood estimation procedure
tells us that we</text_slice>
            </slice>
            <slice>
              <time_slice>12:33</time_slice>
              <text_slice>should report.</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>This formula here, of course,
tells you what to do if you</text_slice>
            </slice>
            <slice>
              <time_slice>12:39</time_slice>
              <text_slice>have already observed
specific numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>12:42</time_slice>
              <text_slice>If you have observed specific
numbers, then you observe this</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>particular number as your
estimate of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>If you want to describe your
estimation procedure more</text_slice>
            </slice>
            <slice>
              <time_slice>12:52</time_slice>
              <text_slice>abstractly, what you have
constructed is an estimator,</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>which is a box that's takes in
the random variables, capital</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>X1 up to Capital Xn, and
produces out your estimate,</text_slice>
            </slice>
            <slice>
              <time_slice>13:05</time_slice>
              <text_slice>which is also a random
variable.</text_slice>
            </slice>
            <slice>
              <time_slice>13:07</time_slice>
              <text_slice>Because it's a function of these
random variables and is</text_slice>
            </slice>
            <slice>
              <time_slice>13:10</time_slice>
              <text_slice>denoted by an upper case Theta,
to indicate that this</text_slice>
            </slice>
            <slice>
              <time_slice>13:14</time_slice>
              <text_slice>is now a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>13:17</time_slice>
              <text_slice>So this is an equality
about numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>This is a description of the
general procedure, which is an</text_slice>
            </slice>
            <slice>
              <time_slice>13:23</time_slice>
              <text_slice>equality between two
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>13:28</time_slice>
              <text_slice>And this gives you the more
abstract view of what we're</text_slice>
            </slice>
            <slice>
              <time_slice>13:31</time_slice>
              <text_slice>doing here.</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>13:35</time_slice>
              <text_slice>So what can we tell about
our estimate?</text_slice>
            </slice>
            <slice>
              <time_slice>13:37</time_slice>
              <text_slice>Is it good or is it bad?</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>So we should look at this
particular random variable and</text_slice>
            </slice>
            <slice>
              <time_slice>13:42</time_slice>
              <text_slice>talk about the statistical
properties that it has.</text_slice>
            </slice>
            <slice>
              <time_slice>13:46</time_slice>
              <text_slice>What we would like is this
random variable to be close to</text_slice>
            </slice>
            <slice>
              <time_slice>13:49</time_slice>
              <text_slice>the true value of Theta, with
high probability, no matter</text_slice>
            </slice>
            <slice>
              <time_slice>13:55</time_slice>
              <text_slice>what Theta is, since we don't
know what Theta is.</text_slice>
            </slice>
            <slice>
              <time_slice>13:59</time_slice>
              <text_slice>Let's make a little
more specific the</text_slice>
            </slice>
            <slice>
              <time_slice>14:01</time_slice>
              <text_slice>properties that we want.</text_slice>
            </slice>
            <slice>
              <time_slice>14:05</time_slice>
              <text_slice>So we cook up the estimator
somehow.</text_slice>
            </slice>
            <slice>
              <time_slice>14:08</time_slice>
              <text_slice>So this estimator corresponds,
again, to a box that takes</text_slice>
            </slice>
            <slice>
              <time_slice>14:11</time_slice>
              <text_slice>data in, the capital X's,
and produces an</text_slice>
            </slice>
            <slice>
              <time_slice>14:15</time_slice>
              <text_slice>estimate Theta hat.</text_slice>
            </slice>
            <slice>
              <time_slice>14:17</time_slice>
              <text_slice>This estimate is random.</text_slice>
            </slice>
            <slice>
              <time_slice>14:18</time_slice>
              <text_slice>Sometimes it will be above
the true value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>Sometimes it will be below.</text_slice>
            </slice>
            <slice>
              <time_slice>14:25</time_slice>
              <text_slice>Ideally, we would like it to not
have a systematic error,</text_slice>
            </slice>
            <slice>
              <time_slice>14:30</time_slice>
              <text_slice>on the positive side or
the negative side.</text_slice>
            </slice>
            <slice>
              <time_slice>14:32</time_slice>
              <text_slice>So a reasonable wish to have,
for a good estimator, is that,</text_slice>
            </slice>
            <slice>
              <time_slice>14:37</time_slice>
              <text_slice>on the average, it gives
you the correct value.</text_slice>
            </slice>
            <slice>
              <time_slice>14:41</time_slice>
              <text_slice>Now here, let's be a little more
specific about what that</text_slice>
            </slice>
            <slice>
              <time_slice>14:45</time_slice>
              <text_slice>expectation is.</text_slice>
            </slice>
            <slice>
              <time_slice>14:47</time_slice>
              <text_slice>This is an expectation, with
respect to the probability</text_slice>
            </slice>
            <slice>
              <time_slice>14:51</time_slice>
              <text_slice>distribution of Theta hat.</text_slice>
            </slice>
            <slice>
              <time_slice>14:54</time_slice>
              <text_slice>The probability distribution
of Theta hat is affected by</text_slice>
            </slice>
            <slice>
              <time_slice>14:58</time_slice>
              <text_slice>the probability distribution
of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>Because Theta hat is a
function of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:03</time_slice>
              <text_slice>And the probability distribution
of the X's is</text_slice>
            </slice>
            <slice>
              <time_slice>15:05</time_slice>
              <text_slice>affected by the true
value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:09</time_slice>
              <text_slice>So depending on which one is the
true value of Theta, this</text_slice>
            </slice>
            <slice>
              <time_slice>15:13</time_slice>
              <text_slice>is going to be a different
expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>So if you were to write this
expectation out in more</text_slice>
            </slice>
            <slice>
              <time_slice>15:20</time_slice>
              <text_slice>detail, it would look
something like this.</text_slice>
            </slice>
            <slice>
              <time_slice>15:25</time_slice>
              <text_slice>You need to write down
the probability</text_slice>
            </slice>
            <slice>
              <time_slice>15:28</time_slice>
              <text_slice>distribution of Theta hat.</text_slice>
            </slice>
            <slice>
              <time_slice>15:32</time_slice>
              <text_slice>And this is going to
be some function.</text_slice>
            </slice>
            <slice>
              <time_slice>15:36</time_slice>
              <text_slice>But this function depends on the
true Theta, is affected by</text_slice>
            </slice>
            <slice>
              <time_slice>15:41</time_slice>
              <text_slice>the true Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>And then you integrate this
with respect to Theta hat.</text_slice>
            </slice>
            <slice>
              <time_slice>15:48</time_slice>
              <text_slice>What's the point here?</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>Again, Theta hat is a
function of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:53</time_slice>
              <text_slice>So the density of Theta
hat is affected by the</text_slice>
            </slice>
            <slice>
              <time_slice>15:57</time_slice>
              <text_slice>density of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:58</time_slice>
              <text_slice>The density of the X's
is affected by the</text_slice>
            </slice>
            <slice>
              <time_slice>16:00</time_slice>
              <text_slice>true value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>So the distribution of Theta
hat is affected by</text_slice>
            </slice>
            <slice>
              <time_slice>16:05</time_slice>
              <text_slice>the value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>16:07</time_slice>
              <text_slice>Another way to put it is, as
I've mentioned a few minutes</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>ago, in this business, it's
as if we are considering</text_slice>
            </slice>
            <slice>
              <time_slice>16:14</time_slice>
              <text_slice>different possible probabilistic
models, one</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>probabilistic model for
each choice of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>And we're trying to guess
which one of these</text_slice>
            </slice>
            <slice>
              <time_slice>16:22</time_slice>
              <text_slice>probabilistic models
is the true one.</text_slice>
            </slice>
            <slice>
              <time_slice>16:25</time_slice>
              <text_slice>One way of emphasizing the
fact that this expression</text_slice>
            </slice>
            <slice>
              <time_slice>16:28</time_slice>
              <text_slice>depends on the true Theta is
to put a little subscript</text_slice>
            </slice>
            <slice>
              <time_slice>16:31</time_slice>
              <text_slice>here, expectation, under the
particular value of the</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>parameter Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>16:38</time_slice>
              <text_slice>So depending on what value the
true parameter Theta takes,</text_slice>
            </slice>
            <slice>
              <time_slice>16:42</time_slice>
              <text_slice>this expectation will have
a different value.</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>And what we would like is that
no matter what the true value</text_slice>
            </slice>
            <slice>
              <time_slice>16:49</time_slice>
              <text_slice>is, that our estimate will not
have a bias on the positive or</text_slice>
            </slice>
            <slice>
              <time_slice>16:55</time_slice>
              <text_slice>the negative sides.</text_slice>
            </slice>
            <slice>
              <time_slice>16:57</time_slice>
              <text_slice>So this is a property
that's desirable.</text_slice>
            </slice>
            <slice>
              <time_slice>17:00</time_slice>
              <text_slice>Is it always going to be true?</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>Not necessarily, it depends on
what estimator we construct.</text_slice>
            </slice>
            <slice>
              <time_slice>17:09</time_slice>
              <text_slice>Is it true for our exponential
example?</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>Unfortunately not, the estimate
that we have in the</text_slice>
            </slice>
            <slice>
              <time_slice>17:14</time_slice>
              <text_slice>exponential example turns
out to be biased.</text_slice>
            </slice>
            <slice>
              <time_slice>17:18</time_slice>
              <text_slice>And one extreme way of seeing
this is to consider the case</text_slice>
            </slice>
            <slice>
              <time_slice>17:22</time_slice>
              <text_slice>where our sample size is 1.</text_slice>
            </slice>
            <slice>
              <time_slice>17:25</time_slice>
              <text_slice>We're trying to estimate
Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>17:27</time_slice>
              <text_slice>And the estimator from the
previous slide, in that case,</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>is just 1/X1.</text_slice>
            </slice>
            <slice>
              <time_slice>17:33</time_slice>
              <text_slice>Now X1 has a fair amount of
density in the vicinity of 0,</text_slice>
            </slice>
            <slice>
              <time_slice>17:37</time_slice>
              <text_slice>which means that 1/X1 has
significant probability of</text_slice>
            </slice>
            <slice>
              <time_slice>17:41</time_slice>
              <text_slice>being very large.</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>And if you do the calculation,
this ultimately makes the</text_slice>
            </slice>
            <slice>
              <time_slice>17:46</time_slice>
              <text_slice>expected value of 1/X1
to be infinite.</text_slice>
            </slice>
            <slice>
              <time_slice>17:49</time_slice>
              <text_slice>Now infinity is definitely
not the correct value.</text_slice>
            </slice>
            <slice>
              <time_slice>17:52</time_slice>
              <text_slice>So our estimate is
biased upwards.</text_slice>
            </slice>
            <slice>
              <time_slice>17:56</time_slice>
              <text_slice>And it's actually biased
a lot upwards.</text_slice>
            </slice>
            <slice>
              <time_slice>18:00</time_slice>
              <text_slice>So that's how things are.</text_slice>
            </slice>
            <slice>
              <time_slice>18:01</time_slice>
              <text_slice>Maximum likelihood estimates,
in general, will be biased.</text_slice>
            </slice>
            <slice>
              <time_slice>18:06</time_slice>
              <text_slice>But under some conditions,
they will turn out to be</text_slice>
            </slice>
            <slice>
              <time_slice>18:10</time_slice>
              <text_slice>asymptotically unbiased.</text_slice>
            </slice>
            <slice>
              <time_slice>18:12</time_slice>
              <text_slice>That is, as you get more and
more data, as your X vector is</text_slice>
            </slice>
            <slice>
              <time_slice>18:16</time_slice>
              <text_slice>longer and longer, with
independent data, the estimate</text_slice>
            </slice>
            <slice>
              <time_slice>18:21</time_slice>
              <text_slice>that you're going to have, the
expected value of your</text_slice>
            </slice>
            <slice>
              <time_slice>18:25</time_slice>
              <text_slice>estimator is going
to get closer and</text_slice>
            </slice>
            <slice>
              <time_slice>18:26</time_slice>
              <text_slice>closer to the true value.</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>So you do have some nice
asymptotic properties, but</text_slice>
            </slice>
            <slice>
              <time_slice>18:31</time_slice>
              <text_slice>we're not going to prove
anything like this.</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>Speaking of asymptotic
properties, in general, what</text_slice>
            </slice>
            <slice>
              <time_slice>18:37</time_slice>
              <text_slice>we would like to have is that,
as you collect more and more</text_slice>
            </slice>
            <slice>
              <time_slice>18:40</time_slice>
              <text_slice>data, you get the correct
answer, in some sense.</text_slice>
            </slice>
            <slice>
              <time_slice>18:46</time_slice>
              <text_slice>And the sense that we're going
to use here is the limiting</text_slice>
            </slice>
            <slice>
              <time_slice>18:49</time_slice>
              <text_slice>sense of convergence in
probability, since this is the</text_slice>
            </slice>
            <slice>
              <time_slice>18:52</time_slice>
              <text_slice>only notion of convergence of
random variables that we have</text_slice>
            </slice>
            <slice>
              <time_slice>18:55</time_slice>
              <text_slice>in our hands.</text_slice>
            </slice>
            <slice>
              <time_slice>18:56</time_slice>
              <text_slice>This is similar to what
we had in the pollster</text_slice>
            </slice>
            <slice>
              <time_slice>18:59</time_slice>
              <text_slice>problem, for example.</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>If we had a bigger and bigger
sample size, we could be more</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>and more confident that the
estimate that we obtained is</text_slice>
            </slice>
            <slice>
              <time_slice>19:08</time_slice>
              <text_slice>close to the unknown true
parameter of the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>19:11</time_slice>
              <text_slice>that we have.</text_slice>
            </slice>
            <slice>
              <time_slice>19:13</time_slice>
              <text_slice>So this is a desirable
property.</text_slice>
            </slice>
            <slice>
              <time_slice>19:16</time_slice>
              <text_slice>If you have an infinitely large
amount of data, you</text_slice>
            </slice>
            <slice>
              <time_slice>19:20</time_slice>
              <text_slice>should be able to estimate
an unknown parameter</text_slice>
            </slice>
            <slice>
              <time_slice>19:25</time_slice>
              <text_slice>more or less exactly.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>So this is it desirable property
of estimators.</text_slice>
            </slice>
            <slice>
              <time_slice>19:32</time_slice>
              <text_slice>It turns out that maximum
likelihood estimation, given</text_slice>
            </slice>
            <slice>
              <time_slice>19:35</time_slice>
              <text_slice>independent data, does have
this property, under mild</text_slice>
            </slice>
            <slice>
              <time_slice>19:39</time_slice>
              <text_slice>conditions.</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>So maximum likelihood
estimation, in this respect,</text_slice>
            </slice>
            <slice>
              <time_slice>19:43</time_slice>
              <text_slice>is a good approach.</text_slice>
            </slice>
            <slice>
              <time_slice>19:45</time_slice>
              <text_slice>So let's see, do we have this
consistency property in our</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>exponential example?</text_slice>
            </slice>
            <slice>
              <time_slice>19:50</time_slice>
              <text_slice>In our exponential example, we
used this quantity to estimate</text_slice>
            </slice>
            <slice>
              <time_slice>19:56</time_slice>
              <text_slice>the unknown parameter Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>What properties does
this quantity have</text_slice>
            </slice>
            <slice>
              <time_slice>20:01</time_slice>
              <text_slice>as n goes to infinity?</text_slice>
            </slice>
            <slice>
              <time_slice>20:03</time_slice>
              <text_slice>Well this quantity is the
reciprocal of that quantity up</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>here, which is the
sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>20:09</time_slice>
              <text_slice>We know from the weak law of
large numbers, that the sample</text_slice>
            </slice>
            <slice>
              <time_slice>20:12</time_slice>
              <text_slice>mean converges to
the expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>So this property here
comes from the weak</text_slice>
            </slice>
            <slice>
              <time_slice>20:19</time_slice>
              <text_slice>law of large numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>In probability, this quantity
converges to the expected</text_slice>
            </slice>
            <slice>
              <time_slice>20:24</time_slice>
              <text_slice>value, which, for exponential
distributions, is 1/Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>Now, if something converges to
something, then the reciprocal</text_slice>
            </slice>
            <slice>
              <time_slice>20:33</time_slice>
              <text_slice>of that should converge to
the reciprocal of that.</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>That's a property that's
certainly correct for numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>But you're not talking about
convergence of numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>We're talking about convergence
in probability,</text_slice>
            </slice>
            <slice>
              <time_slice>20:46</time_slice>
              <text_slice>which is a more complicated
notion.</text_slice>
            </slice>
            <slice>
              <time_slice>20:48</time_slice>
              <text_slice>Fortunately, it turns out that
the same thing is true, when</text_slice>
            </slice>
            <slice>
              <time_slice>20:52</time_slice>
              <text_slice>we deal with convergence
in probability.</text_slice>
            </slice>
            <slice>
              <time_slice>20:54</time_slice>
              <text_slice>One can show, although we will
not bother doing this, that</text_slice>
            </slice>
            <slice>
              <time_slice>20:58</time_slice>
              <text_slice>indeed, the reciprocal of this,
which is our estimate,</text_slice>
            </slice>
            <slice>
              <time_slice>21:01</time_slice>
              <text_slice>converges in probability to
the reciprocal of that.</text_slice>
            </slice>
            <slice>
              <time_slice>21:05</time_slice>
              <text_slice>And that reciprocal is the
true parameter Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>21:08</time_slice>
              <text_slice>So for this particular
exponential example, we do</text_slice>
            </slice>
            <slice>
              <time_slice>21:11</time_slice>
              <text_slice>have the desirable property,
that as the number of data</text_slice>
            </slice>
            <slice>
              <time_slice>21:15</time_slice>
              <text_slice>becomes larger and larger,
the estimate that we have</text_slice>
            </slice>
            <slice>
              <time_slice>21:18</time_slice>
              <text_slice>constructed will get closer
and closer to the true</text_slice>
            </slice>
            <slice>
              <time_slice>21:20</time_slice>
              <text_slice>parameter value.</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>And this is true no matter
what Theta is.</text_slice>
            </slice>
            <slice>
              <time_slice>21:27</time_slice>
              <text_slice>No matter what the true
parameter Theta is, we're</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>going to get close to it as
we collect more data.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>Okay.</text_slice>
            </slice>
            <slice>
              <time_slice>21:35</time_slice>
              <text_slice>So these are two rough
qualitative properties that</text_slice>
            </slice>
            <slice>
              <time_slice>21:39</time_slice>
              <text_slice>would be nice to have.</text_slice>
            </slice>
            <slice>
              <time_slice>21:42</time_slice>
              <text_slice>If you want to get a little
more quantitative, you can</text_slice>
            </slice>
            <slice>
              <time_slice>21:47</time_slice>
              <text_slice>start looking at the mean
squared error that your</text_slice>
            </slice>
            <slice>
              <time_slice>21:50</time_slice>
              <text_slice>estimator gives.</text_slice>
            </slice>
            <slice>
              <time_slice>21:52</time_slice>
              <text_slice>Now, once more, the comment I
was making up there applies.</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>Namely, that this expectation
here is an expectation with</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>respect to the probability
distribution of Theta hat that</text_slice>
            </slice>
            <slice>
              <time_slice>22:04</time_slice>
              <text_slice>corresponds to a particular
value of little theta.</text_slice>
            </slice>
            <slice>
              <time_slice>22:07</time_slice>
              <text_slice>So fix a little theta.</text_slice>
            </slice>
            <slice>
              <time_slice>22:09</time_slice>
              <text_slice>Write down this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>22:11</time_slice>
              <text_slice>Look at the probability
distribution of Theta hat,</text_slice>
            </slice>
            <slice>
              <time_slice>22:14</time_slice>
              <text_slice>under that little theta.</text_slice>
            </slice>
            <slice>
              <time_slice>22:16</time_slice>
              <text_slice>And do this calculation.</text_slice>
            </slice>
            <slice>
              <time_slice>22:18</time_slice>
              <text_slice>You're going to get some
quantity that depends on the</text_slice>
            </slice>
            <slice>
              <time_slice>22:20</time_slice>
              <text_slice>little theta.</text_slice>
            </slice>
            <slice>
              <time_slice>22:24</time_slice>
              <text_slice>And so all quantities in this
equality here should be</text_slice>
            </slice>
            <slice>
              <time_slice>22:28</time_slice>
              <text_slice>interpreted as quantities under
that particular value of</text_slice>
            </slice>
            <slice>
              <time_slice>22:33</time_slice>
              <text_slice>little theta.</text_slice>
            </slice>
            <slice>
              <time_slice>22:34</time_slice>
              <text_slice>So if you wanted to make this
more explicit, you could start</text_slice>
            </slice>
            <slice>
              <time_slice>22:38</time_slice>
              <text_slice>throwing little subscripts
everywhere in those</text_slice>
            </slice>
            <slice>
              <time_slice>22:41</time_slice>
              <text_slice>expressions.</text_slice>
            </slice>
            <slice>
              <time_slice>22:44</time_slice>
              <text_slice>And let's see what those
expressions tell us.</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>The expected value squared of
a random variable, we know</text_slice>
            </slice>
            <slice>
              <time_slice>22:55</time_slice>
              <text_slice>that it's always equal to the
variance of this random</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>variable, plus the expectation
of the</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>random variable squared.</text_slice>
            </slice>
            <slice>
              <time_slice>23:05</time_slice>
              <text_slice>So the expectation value of that
random variable, squared.</text_slice>
            </slice>
            <slice>
              <time_slice>23:12</time_slice>
              <text_slice>This equality here is just our
familiar formula, that the</text_slice>
            </slice>
            <slice>
              <time_slice>23:17</time_slice>
              <text_slice>expected value of X squared is
the variance of X plus the</text_slice>
            </slice>
            <slice>
              <time_slice>23:23</time_slice>
              <text_slice>expected value of X squared.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>So we apply this formula
to X equal to</text_slice>
            </slice>
            <slice>
              <time_slice>23:30</time_slice>
              <text_slice>Theta hat minus Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>23:37</time_slice>
              <text_slice>Now, remember that, in this
classical setting, theta is</text_slice>
            </slice>
            <slice>
              <time_slice>23:41</time_slice>
              <text_slice>just a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>23:42</time_slice>
              <text_slice>We have fixed Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>23:43</time_slice>
              <text_slice>We want to calculate the
variance of this quantity,</text_slice>
            </slice>
            <slice>
              <time_slice>23:45</time_slice>
              <text_slice>under that particular Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>23:47</time_slice>
              <text_slice>When you add or subtract a
constant to a random variable,</text_slice>
            </slice>
            <slice>
              <time_slice>23:51</time_slice>
              <text_slice>the variance doesn't change.</text_slice>
            </slice>
            <slice>
              <time_slice>23:54</time_slice>
              <text_slice>This is the same as the variance
of our estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>23:56</time_slice>
              <text_slice>And what we've got here is
the bias of our estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>It tells us, on the average,
whether we</text_slice>
            </slice>
            <slice>
              <time_slice>24:02</time_slice>
              <text_slice>fall above or below.</text_slice>
            </slice>
            <slice>
              <time_slice>24:04</time_slice>
              <text_slice>And we're taking the bias
to be b squared.</text_slice>
            </slice>
            <slice>
              <time_slice>24:06</time_slice>
              <text_slice>If we have an unbiased
estimator, the bias</text_slice>
            </slice>
            <slice>
              <time_slice>24:10</time_slice>
              <text_slice>term will be 0.</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>So ideally we want Theta hat
to be very close to Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>24:18</time_slice>
              <text_slice>And since Theta is a constant,
if that happens, the variance</text_slice>
            </slice>
            <slice>
              <time_slice>24:21</time_slice>
              <text_slice>of Theta hat would
be very small.</text_slice>
            </slice>
            <slice>
              <time_slice>24:25</time_slice>
              <text_slice>So Theta is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>24:26</time_slice>
              <text_slice>If Theta hat has a distribution
that's</text_slice>
            </slice>
            <slice>
              <time_slice>24:30</time_slice>
              <text_slice>concentrated just around own
little theta, then Theta hat</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>would have a small variance.</text_slice>
            </slice>
            <slice>
              <time_slice>24:35</time_slice>
              <text_slice>So this is one desire
that have.</text_slice>
            </slice>
            <slice>
              <time_slice>24:37</time_slice>
              <text_slice>We're going to have
a small variance.</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>But we also want to have a small
bias at the same time.</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>So the general form of the mean
squared error has two</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>contributions.</text_slice>
            </slice>
            <slice>
              <time_slice>24:48</time_slice>
              <text_slice>One is the variance
of our estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>24:50</time_slice>
              <text_slice>The other is the bias.</text_slice>
            </slice>
            <slice>
              <time_slice>24:52</time_slice>
              <text_slice>And one usually wants to design
an estimator that</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>simultaneously keeps both
of these terms small.</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>So here's an estimation method
that would do very well with</text_slice>
            </slice>
            <slice>
              <time_slice>25:03</time_slice>
              <text_slice>respect to this term,
but badly with</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>respect to that term.</text_slice>
            </slice>
            <slice>
              <time_slice>25:06</time_slice>
              <text_slice>So suppose that my distribution
is, let's say,</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>normal with an unknown mean
Theta and variance 1.</text_slice>
            </slice>
            <slice>
              <time_slice>25:13</time_slice>
              <text_slice>And I use as my estimator
something very dumb.</text_slice>
            </slice>
            <slice>
              <time_slice>25:17</time_slice>
              <text_slice>I always produce an estimate
that says my estimate is 100.</text_slice>
            </slice>
            <slice>
              <time_slice>25:23</time_slice>
              <text_slice>So I'm just ignoring the
data and report 100.</text_slice>
            </slice>
            <slice>
              <time_slice>25:26</time_slice>
              <text_slice>What does this do?</text_slice>
            </slice>
            <slice>
              <time_slice>25:27</time_slice>
              <text_slice>The variance of my
estimator is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>25:30</time_slice>
              <text_slice>There's no randomness in the
estimate that I report.</text_slice>
            </slice>
            <slice>
              <time_slice>25:33</time_slice>
              <text_slice>But the bias is going
to be pretty bad.</text_slice>
            </slice>
            <slice>
              <time_slice>25:37</time_slice>
              <text_slice>The bias is going to be Theta
hat, which is 100 minus the</text_slice>
            </slice>
            <slice>
              <time_slice>25:44</time_slice>
              <text_slice>true value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>25:46</time_slice>
              <text_slice>And for some Theta's, my bias
is going to be horrible.</text_slice>
            </slice>
            <slice>
              <time_slice>25:50</time_slice>
              <text_slice>If my true Theta happens
to be 0, my bias</text_slice>
            </slice>
            <slice>
              <time_slice>25:54</time_slice>
              <text_slice>squared is a huge term.</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>And I get a large error.</text_slice>
            </slice>
            <slice>
              <time_slice>25:57</time_slice>
              <text_slice>So what's the moral
of this example?</text_slice>
            </slice>
            <slice>
              <time_slice>26:00</time_slice>
              <text_slice>There are ways of making that
variance very small, but, in</text_slice>
            </slice>
            <slice>
              <time_slice>26:03</time_slice>
              <text_slice>those cases, you pay a
price in the bias.</text_slice>
            </slice>
            <slice>
              <time_slice>26:07</time_slice>
              <text_slice>So you want to do something a
little more delicate, where</text_slice>
            </slice>
            <slice>
              <time_slice>26:10</time_slice>
              <text_slice>you try to keep both terms
small at the same time.</text_slice>
            </slice>
            <slice>
              <time_slice>26:14</time_slice>
              <text_slice>So these types of considerations
become</text_slice>
            </slice>
            <slice>
              <time_slice>26:16</time_slice>
              <text_slice>important when you start to try
to design sophisticated</text_slice>
            </slice>
            <slice>
              <time_slice>26:20</time_slice>
              <text_slice>estimators for more complicated
problems.</text_slice>
            </slice>
            <slice>
              <time_slice>26:22</time_slice>
              <text_slice>But we will not do this
in this class.</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>This belongs to further
classes on</text_slice>
            </slice>
            <slice>
              <time_slice>26:26</time_slice>
              <text_slice>statistics and inference.</text_slice>
            </slice>
            <slice>
              <time_slice>26:28</time_slice>
              <text_slice>For this class, for parameter
estimation, we will basically</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>stick to two very
simple methods.</text_slice>
            </slice>
            <slice>
              <time_slice>26:34</time_slice>
              <text_slice>One is the maximum likelihood
method we've just discussed.</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>And the other method is what you
would do if you were still</text_slice>
            </slice>
            <slice>
              <time_slice>26:41</time_slice>
              <text_slice>in high school and didn't
know any probability.</text_slice>
            </slice>
            <slice>
              <time_slice>26:44</time_slice>
              <text_slice>You get data.</text_slice>
            </slice>
            <slice>
              <time_slice>26:46</time_slice>
              <text_slice>And these data come from
some distribution</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>with an unknown mean.</text_slice>
            </slice>
            <slice>
              <time_slice>26:51</time_slice>
              <text_slice>And you want to estimate
that the unknown mean.</text_slice>
            </slice>
            <slice>
              <time_slice>26:53</time_slice>
              <text_slice>What would you do?</text_slice>
            </slice>
            <slice>
              <time_slice>26:54</time_slice>
              <text_slice>You would just take those data
and average them out.</text_slice>
            </slice>
            <slice>
              <time_slice>26:57</time_slice>
              <text_slice>So let's make this a little
more specific.</text_slice>
            </slice>
            <slice>
              <time_slice>27:00</time_slice>
              <text_slice>We have X's that come from
a given distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>27:04</time_slice>
              <text_slice>We know the general form of
the distribution, perhaps.</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>We do know, perhaps, the
variance of that distribution,</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>or, perhaps, we don't know it.</text_slice>
            </slice>
            <slice>
              <time_slice>27:17</time_slice>
              <text_slice>But we do not know the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>And we want to estimate the
mean of that distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>27:22</time_slice>
              <text_slice>Now, we can write
this situation.</text_slice>
            </slice>
            <slice>
              <time_slice>27:25</time_slice>
              <text_slice>We can represent it in
a different form.</text_slice>
            </slice>
            <slice>
              <time_slice>27:27</time_slice>
              <text_slice>The Xi's are equal to Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>27:30</time_slice>
              <text_slice>This is the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>27:31</time_slice>
              <text_slice>Plus a 0 mean random
variable, that you</text_slice>
            </slice>
            <slice>
              <time_slice>27:34</time_slice>
              <text_slice>can think of as noise.</text_slice>
            </slice>
            <slice>
              <time_slice>27:36</time_slice>
              <text_slice>So this corresponds to the usual
situation you would have</text_slice>
            </slice>
            <slice>
              <time_slice>27:39</time_slice>
              <text_slice>in a lab, where you
go and try to</text_slice>
            </slice>
            <slice>
              <time_slice>27:41</time_slice>
              <text_slice>measure an unknown quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>You get lots of measurements.</text_slice>
            </slice>
            <slice>
              <time_slice>27:45</time_slice>
              <text_slice>But each time that you measure
them, your measurements have</text_slice>
            </slice>
            <slice>
              <time_slice>27:49</time_slice>
              <text_slice>some extra noise in there.</text_slice>
            </slice>
            <slice>
              <time_slice>27:51</time_slice>
              <text_slice>And you want to kind of
get rid of that noise.</text_slice>
            </slice>
            <slice>
              <time_slice>27:54</time_slice>
              <text_slice>The way to try to get rid of
the measurement noise is to</text_slice>
            </slice>
            <slice>
              <time_slice>27:57</time_slice>
              <text_slice>collect lots of data and
average them out.</text_slice>
            </slice>
            <slice>
              <time_slice>28:01</time_slice>
              <text_slice>This is the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>And this is a very, very
reasonable way of trying to</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>estimate the unknown
mean of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>28:10</time_slice>
              <text_slice>So this is the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>28:12</time_slice>
              <text_slice>It's a reasonable, plausible,
in general, pretty good</text_slice>
            </slice>
            <slice>
              <time_slice>28:17</time_slice>
              <text_slice>estimator of the unknown mean
of a certain distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>28:22</time_slice>
              <text_slice>We can apply this estimator
without really knowing a lot</text_slice>
            </slice>
            <slice>
              <time_slice>28:26</time_slice>
              <text_slice>about the distribution
of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>28:28</time_slice>
              <text_slice>Actually, we don't need to
know anything about the</text_slice>
            </slice>
            <slice>
              <time_slice>28:31</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>28:32</time_slice>
              <text_slice>We can still apply it, because
the variance, for example,</text_slice>
            </slice>
            <slice>
              <time_slice>28:35</time_slice>
              <text_slice>does not show up here.</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>We don't need to know
the variance to</text_slice>
            </slice>
            <slice>
              <time_slice>28:38</time_slice>
              <text_slice>calculate that quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>28:40</time_slice>
              <text_slice>Does this estimator have
good properties?</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>Yes, it does.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>What's the expected value
of the sample mean?</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>If the expectation of this, it's
the expectation of this</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>sum divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>28:53</time_slice>
              <text_slice>The expected value for each
one of the X's is Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>28:56</time_slice>
              <text_slice>So the expected value
of the sample mean</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>is just Theta itself.</text_slice>
            </slice>
            <slice>
              <time_slice>29:00</time_slice>
              <text_slice>So our estimator is unbiased.</text_slice>
            </slice>
            <slice>
              <time_slice>29:03</time_slice>
              <text_slice>No matter what Theta is, our
estimator does not have a</text_slice>
            </slice>
            <slice>
              <time_slice>29:06</time_slice>
              <text_slice>systematic error in
either direction.</text_slice>
            </slice>
            <slice>
              <time_slice>29:11</time_slice>
              <text_slice>Furthermore, the weak law of
large numbers tells us that</text_slice>
            </slice>
            <slice>
              <time_slice>29:13</time_slice>
              <text_slice>this quantity converges to the
true parameter in probability.</text_slice>
            </slice>
            <slice>
              <time_slice>29:18</time_slice>
              <text_slice>So it's a consistent
estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>This is good.</text_slice>
            </slice>
            <slice>
              <time_slice>29:21</time_slice>
              <text_slice>And if you want to calculate
the mean squared error</text_slice>
            </slice>
            <slice>
              <time_slice>29:26</time_slice>
              <text_slice>corresponding to
this estimator.</text_slice>
            </slice>
            <slice>
              <time_slice>29:28</time_slice>
              <text_slice>Remember how we defined the
mean squared error?</text_slice>
            </slice>
            <slice>
              <time_slice>29:31</time_slice>
              <text_slice>It's this quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>29:35</time_slice>
              <text_slice>Then it's a calculation that we
have done a fair number of</text_slice>
            </slice>
            <slice>
              <time_slice>29:38</time_slice>
              <text_slice>times by now.</text_slice>
            </slice>
            <slice>
              <time_slice>29:40</time_slice>
              <text_slice>The mean squared error is the
variance of the distribution</text_slice>
            </slice>
            <slice>
              <time_slice>29:43</time_slice>
              <text_slice>of the X's divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>29:46</time_slice>
              <text_slice>So as we get more and more data,
the mean squared error</text_slice>
            </slice>
            <slice>
              <time_slice>29:49</time_slice>
              <text_slice>goes down to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>29:52</time_slice>
              <text_slice>In some examples, it turns out
that the sample mean is also</text_slice>
            </slice>
            <slice>
              <time_slice>29:56</time_slice>
              <text_slice>the same as the maximum
likelihood estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>29:58</time_slice>
              <text_slice>For example, if the X's are
coming from a normal</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>distribution, you can write down
the likelihood, do the</text_slice>
            </slice>
            <slice>
              <time_slice>30:07</time_slice>
              <text_slice>maximization with respect to
Theta, you'll find that the</text_slice>
            </slice>
            <slice>
              <time_slice>30:10</time_slice>
              <text_slice>maximum likelihood estimate is
the same as the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>30:15</time_slice>
              <text_slice>In other cases, the sample mean
will be different from</text_slice>
            </slice>
            <slice>
              <time_slice>30:18</time_slice>
              <text_slice>the maximum likelihood.</text_slice>
            </slice>
            <slice>
              <time_slice>30:20</time_slice>
              <text_slice>And then you have a choice
about which one of the</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>two you would use.</text_slice>
            </slice>
            <slice>
              <time_slice>30:24</time_slice>
              <text_slice>Probably, in most reasonable
situations, you would just use</text_slice>
            </slice>
            <slice>
              <time_slice>30:27</time_slice>
              <text_slice>the sample mean, because it's
simple, easy to compute, and</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>has nice properties.</text_slice>
            </slice>
            <slice>
              <time_slice>30:33</time_slice>
              <text_slice>All right.</text_slice>
            </slice>
            <slice>
              <time_slice>30:33</time_slice>
              <text_slice>So you go to your boss.</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>And you report and say,
OK, I did all my</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>experiments in the lab.</text_slice>
            </slice>
            <slice>
              <time_slice>30:39</time_slice>
              <text_slice>And the average value that I got
is a certain number, 2.37.</text_slice>
            </slice>
            <slice>
              <time_slice>30:49</time_slice>
              <text_slice>So is that the informative
to your boss?</text_slice>
            </slice>
            <slice>
              <time_slice>30:52</time_slice>
              <text_slice>Well your boss would like to
know how much they can trust</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>this number, 2.37.</text_slice>
            </slice>
            <slice>
              <time_slice>30:58</time_slice>
              <text_slice>Well, I know that the true
value is not going to be</text_slice>
            </slice>
            <slice>
              <time_slice>31:00</time_slice>
              <text_slice>exactly that.</text_slice>
            </slice>
            <slice>
              <time_slice>31:02</time_slice>
              <text_slice>But how close should it be?</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>So give me a range of
what you think are</text_slice>
            </slice>
            <slice>
              <time_slice>31:09</time_slice>
              <text_slice>possible values of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>31:12</time_slice>
              <text_slice>So the situation is like this.</text_slice>
            </slice>
            <slice>
              <time_slice>31:16</time_slice>
              <text_slice>So suppose that we observe X's
that are coming from a certain</text_slice>
            </slice>
            <slice>
              <time_slice>31:20</time_slice>
              <text_slice>distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>31:22</time_slice>
              <text_slice>And we're trying to
estimate the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>31:24</time_slice>
              <text_slice>We get our data.</text_slice>
            </slice>
            <slice>
              <time_slice>31:27</time_slice>
              <text_slice>Maybe our data looks something
like this.</text_slice>
            </slice>
            <slice>
              <time_slice>31:32</time_slice>
              <text_slice>You calculate the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>31:34</time_slice>
              <text_slice>You find the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>31:36</time_slice>
              <text_slice>So let's suppose that the sample
mean is a number, for</text_slice>
            </slice>
            <slice>
              <time_slice>31:40</time_slice>
              <text_slice>some reason take to be 2.37.</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>But you want to convey something
to your boss about</text_slice>
            </slice>
            <slice>
              <time_slice>31:48</time_slice>
              <text_slice>how spread out these
data were.</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>So the boss asks you to give
him or her some kind of</text_slice>
            </slice>
            <slice>
              <time_slice>31:56</time_slice>
              <text_slice>interval on which Theta, the
true parameter, might lie.</text_slice>
            </slice>
            <slice>
              <time_slice>32:05</time_slice>
              <text_slice>So the boss asked you
for an interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:07</time_slice>
              <text_slice>So what you do is you end up
reporting an interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>And you somehow use the data
that you have seen to</text_slice>
            </slice>
            <slice>
              <time_slice>32:14</time_slice>
              <text_slice>construct this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:17</time_slice>
              <text_slice>And you report to your
boss also the</text_slice>
            </slice>
            <slice>
              <time_slice>32:19</time_slice>
              <text_slice>endpoints of this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>Let's give names to
these endpoints,</text_slice>
            </slice>
            <slice>
              <time_slice>32:24</time_slice>
              <text_slice>Theta_n- and Theta_n+.</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>The ends here just play the role
of keeping track of how</text_slice>
            </slice>
            <slice>
              <time_slice>32:31</time_slice>
              <text_slice>many data we're using.</text_slice>
            </slice>
            <slice>
              <time_slice>32:33</time_slice>
              <text_slice>So what you report to your boss
is this interval as well.</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>Are these Theta's here, the
endpoints of the interval,</text_slice>
            </slice>
            <slice>
              <time_slice>32:42</time_slice>
              <text_slice>lowercase or uppercase?</text_slice>
            </slice>
            <slice>
              <time_slice>32:44</time_slice>
              <text_slice>What should they be?</text_slice>
            </slice>
            <slice>
              <time_slice>32:45</time_slice>
              <text_slice>Well you construct these
intervals after</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>you see your data.</text_slice>
            </slice>
            <slice>
              <time_slice>32:49</time_slice>
              <text_slice>You take the data into account
to construct your interval.</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>So these definitely should
depend on the data.</text_slice>
            </slice>
            <slice>
              <time_slice>32:57</time_slice>
              <text_slice>And therefore they are
random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>32:59</time_slice>
              <text_slice>Same thing with your estimator,
in general, it's</text_slice>
            </slice>
            <slice>
              <time_slice>33:03</time_slice>
              <text_slice>going to be a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>33:05</time_slice>
              <text_slice>Although, when you go and report
numbers to your boss,</text_slice>
            </slice>
            <slice>
              <time_slice>33:07</time_slice>
              <text_slice>you give the specific
realizations of the random</text_slice>
            </slice>
            <slice>
              <time_slice>33:10</time_slice>
              <text_slice>variables, given the
data that you got.</text_slice>
            </slice>
            <slice>
              <time_slice>33:15</time_slice>
              <text_slice>So instead of having
just a single box</text_slice>
            </slice>
            <slice>
              <time_slice>33:21</time_slice>
              <text_slice>that produces estimates.</text_slice>
            </slice>
            <slice>
              <time_slice>33:25</time_slice>
              <text_slice>So our previous picture was that
you have your estimator</text_slice>
            </slice>
            <slice>
              <time_slice>33:29</time_slice>
              <text_slice>that takes X's and produces
Theta hats.</text_slice>
            </slice>
            <slice>
              <time_slice>33:34</time_slice>
              <text_slice>Now our box will also be
producing Theta hats minus and</text_slice>
            </slice>
            <slice>
              <time_slice>33:40</time_slice>
              <text_slice>Theta hats plus.</text_slice>
            </slice>
            <slice>
              <time_slice>33:42</time_slice>
              <text_slice>It's going to produce
an interval as well.</text_slice>
            </slice>
            <slice>
              <time_slice>33:45</time_slice>
              <text_slice>The X's are random, therefore
these quantities are random.</text_slice>
            </slice>
            <slice>
              <time_slice>33:48</time_slice>
              <text_slice>Once you go and do the
experiment and obtain your</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>data, then your data
will be some</text_slice>
            </slice>
            <slice>
              <time_slice>33:55</time_slice>
              <text_slice>lowercase x, specific numbers.</text_slice>
            </slice>
            <slice>
              <time_slice>33:58</time_slice>
              <text_slice>And then your estimates
and estimator</text_slice>
            </slice>
            <slice>
              <time_slice>34:00</time_slice>
              <text_slice>become also lower case.</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>What would we like this
interval to do?</text_slice>
            </slice>
            <slice>
              <time_slice>34:08</time_slice>
              <text_slice>We would like it to be highly
likely to contain the true</text_slice>
            </slice>
            <slice>
              <time_slice>34:11</time_slice>
              <text_slice>value of the parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>34:13</time_slice>
              <text_slice>So we might impose some specs
of the following kind.</text_slice>
            </slice>
            <slice>
              <time_slice>34:17</time_slice>
              <text_slice>I pick a number, alpha.</text_slice>
            </slice>
            <slice>
              <time_slice>34:19</time_slice>
              <text_slice>Usually that alpha,
think of it as a</text_slice>
            </slice>
            <slice>
              <time_slice>34:21</time_slice>
              <text_slice>probability of a large error.</text_slice>
            </slice>
            <slice>
              <time_slice>34:23</time_slice>
              <text_slice>Typical value of alpha might
be 0.05, in which case this</text_slice>
            </slice>
            <slice>
              <time_slice>34:27</time_slice>
              <text_slice>number here is point 0.95.</text_slice>
            </slice>
            <slice>
              <time_slice>34:30</time_slice>
              <text_slice>And you're given specs that
say something like this.</text_slice>
            </slice>
            <slice>
              <time_slice>34:33</time_slice>
              <text_slice>I would like, with probability
at least 0.95, this to happen,</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>which says that the true
parameter lies inside the</text_slice>
            </slice>
            <slice>
              <time_slice>34:44</time_slice>
              <text_slice>confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>34:47</time_slice>
              <text_slice>Now let's try to interpret
this statement.</text_slice>
            </slice>
            <slice>
              <time_slice>34:50</time_slice>
              <text_slice>Suppose that you did the
experiment, and that you ended</text_slice>
            </slice>
            <slice>
              <time_slice>34:53</time_slice>
              <text_slice>up reporting to your boss
a confidence interval</text_slice>
            </slice>
            <slice>
              <time_slice>34:56</time_slice>
              <text_slice>from 1.97 to 2.56.</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>That's what you report
to your boss.</text_slice>
            </slice>
            <slice>
              <time_slice>35:06</time_slice>
              <text_slice>And suppose that the confidence</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>interval has this property.</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>Can you go to your boss and say,
with probability 95%, the</text_slice>
            </slice>
            <slice>
              <time_slice>35:16</time_slice>
              <text_slice>true value of Theta is between
these two numbers?</text_slice>
            </slice>
            <slice>
              <time_slice>35:20</time_slice>
              <text_slice>Is that a meaningful
statement?</text_slice>
            </slice>
            <slice>
              <time_slice>35:22</time_slice>
              <text_slice>So the statement is, the
tentative statement is, with</text_slice>
            </slice>
            <slice>
              <time_slice>35:26</time_slice>
              <text_slice>probability 95%, the true
value of Theta is</text_slice>
            </slice>
            <slice>
              <time_slice>35:30</time_slice>
              <text_slice>between 1.97 and 2.56.</text_slice>
            </slice>
            <slice>
              <time_slice>35:34</time_slice>
              <text_slice>Well, what is random
in that statement?</text_slice>
            </slice>
            <slice>
              <time_slice>35:38</time_slice>
              <text_slice>There's nothing random.</text_slice>
            </slice>
            <slice>
              <time_slice>35:40</time_slice>
              <text_slice>The true value of theta
is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>35:43</time_slice>
              <text_slice>1.97 is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>2.56 is a number.</text_slice>
            </slice>
            <slice>
              <time_slice>35:46</time_slice>
              <text_slice>So it doesn't make any sense to
talk about the probability</text_slice>
            </slice>
            <slice>
              <time_slice>35:52</time_slice>
              <text_slice>that theta is in
this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>Either theta happens to be
in that interval, or it</text_slice>
            </slice>
            <slice>
              <time_slice>35:57</time_slice>
              <text_slice>happens to not be.</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>But there are no probabilities
associated with this.</text_slice>
            </slice>
            <slice>
              <time_slice>36:01</time_slice>
              <text_slice>Because theta is not random.</text_slice>
            </slice>
            <slice>
              <time_slice>36:04</time_slice>
              <text_slice>Syntactically, you
can see this.</text_slice>
            </slice>
            <slice>
              <time_slice>36:06</time_slice>
              <text_slice>Because theta here
is a lower case.</text_slice>
            </slice>
            <slice>
              <time_slice>36:09</time_slice>
              <text_slice>So what kind of probabilities
are we talking about here?</text_slice>
            </slice>
            <slice>
              <time_slice>36:11</time_slice>
              <text_slice>Where's the randomness?</text_slice>
            </slice>
            <slice>
              <time_slice>36:13</time_slice>
              <text_slice>Well the random thing
is the interval.</text_slice>
            </slice>
            <slice>
              <time_slice>36:15</time_slice>
              <text_slice>It's not theta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>So the statement that is being
made here is that the</text_slice>
            </slice>
            <slice>
              <time_slice>36:21</time_slice>
              <text_slice>interval, that's being
constructed by our procedure,</text_slice>
            </slice>
            <slice>
              <time_slice>36:24</time_slice>
              <text_slice>should have the property that,
with probability 95%, it's</text_slice>
            </slice>
            <slice>
              <time_slice>36:28</time_slice>
              <text_slice>going to fall on top of the
true value of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>36:33</time_slice>
              <text_slice>So the right way of interpreting
what the 95%</text_slice>
            </slice>
            <slice>
              <time_slice>36:37</time_slice>
              <text_slice>confidence interval is, is
something like the following.</text_slice>
            </slice>
            <slice>
              <time_slice>36:42</time_slice>
              <text_slice>We have the true value of theta
that we don't know.</text_slice>
            </slice>
            <slice>
              <time_slice>36:45</time_slice>
              <text_slice>I get data.</text_slice>
            </slice>
            <slice>
              <time_slice>36:46</time_slice>
              <text_slice>Based on the data, I construct
a confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>I get my confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>36:51</time_slice>
              <text_slice>I got lucky.</text_slice>
            </slice>
            <slice>
              <time_slice>36:52</time_slice>
              <text_slice>And the true value of
theta is in here.</text_slice>
            </slice>
            <slice>
              <time_slice>36:54</time_slice>
              <text_slice>Next day, I do the same
experiment, take my data,</text_slice>
            </slice>
            <slice>
              <time_slice>36:57</time_slice>
              <text_slice>construct a confidence
interval.</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>And I get this confidence
interval, lucky once more.</text_slice>
            </slice>
            <slice>
              <time_slice>37:04</time_slice>
              <text_slice>Next day I get data.</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>I use my data to come up with
an estimate of theta and the</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>37:10</time_slice>
              <text_slice>That day, I was unlucky.</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>And I got a confidence
interval out there.</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>What the requirement here is, is
that 95% of the days, where</text_slice>
            </slice>
            <slice>
              <time_slice>37:20</time_slice>
              <text_slice>we use this certain procedure
for constructing confidence</text_slice>
            </slice>
            <slice>
              <time_slice>37:25</time_slice>
              <text_slice>intervals, 95% of those days,
we will be lucky.</text_slice>
            </slice>
            <slice>
              <time_slice>37:29</time_slice>
              <text_slice>And we will capture the correct
value of theta by your</text_slice>
            </slice>
            <slice>
              <time_slice>37:33</time_slice>
              <text_slice>confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>37:35</time_slice>
              <text_slice>So it's a statement about the
distribution of these random</text_slice>
            </slice>
            <slice>
              <time_slice>37:39</time_slice>
              <text_slice>confidence intervals, how likely
are they to fall on top</text_slice>
            </slice>
            <slice>
              <time_slice>37:42</time_slice>
              <text_slice>of the true theta, as opposed
to how likely</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>they are to fall outside.</text_slice>
            </slice>
            <slice>
              <time_slice>37:47</time_slice>
              <text_slice>So it's a statement about
probabilities associated with</text_slice>
            </slice>
            <slice>
              <time_slice>37:50</time_slice>
              <text_slice>a confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>37:52</time_slice>
              <text_slice>They're not probabilities about
theta, because theta,</text_slice>
            </slice>
            <slice>
              <time_slice>37:55</time_slice>
              <text_slice>itself, is not random.</text_slice>
            </slice>
            <slice>
              <time_slice>37:58</time_slice>
              <text_slice>So this is what the confidence
interval is, in general, and</text_slice>
            </slice>
            <slice>
              <time_slice>38:02</time_slice>
              <text_slice>how we interpret it.</text_slice>
            </slice>
            <slice>
              <time_slice>38:03</time_slice>
              <text_slice>How do we construct a 95%
confidence interval?</text_slice>
            </slice>
            <slice>
              <time_slice>38:07</time_slice>
              <text_slice>Let's go through this
exercise, in</text_slice>
            </slice>
            <slice>
              <time_slice>38:09</time_slice>
              <text_slice>a particular example.</text_slice>
            </slice>
            <slice>
              <time_slice>38:10</time_slice>
              <text_slice>The calculations are exactly the
same as the ones that you</text_slice>
            </slice>
            <slice>
              <time_slice>38:13</time_slice>
              <text_slice>did when we talked about laws
of large numbers and the</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>So there's nothing new
calculationally but it's,</text_slice>
            </slice>
            <slice>
              <time_slice>38:22</time_slice>
              <text_slice>perhaps, new in terms of the
language that we use and the</text_slice>
            </slice>
            <slice>
              <time_slice>38:25</time_slice>
              <text_slice>interpretation.</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>So we got our sample mean
from some distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>38:30</time_slice>
              <text_slice>And we would like to calculate
a 95% confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>38:39</time_slice>
              <text_slice>We know from the normal tables,
that the standard</text_slice>
            </slice>
            <slice>
              <time_slice>38:42</time_slice>
              <text_slice>normal has 2.5% on the tail,
that's after 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>Yes, by this time,
the number 1.96</text_slice>
            </slice>
            <slice>
              <time_slice>38:58</time_slice>
              <text_slice>should be pretty familiar.</text_slice>
            </slice>
            <slice>
              <time_slice>39:00</time_slice>
              <text_slice>So if this probability
here is 2.5%, this</text_slice>
            </slice>
            <slice>
              <time_slice>39:05</time_slice>
              <text_slice>number here is 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>Now look at this random
variable here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:12</time_slice>
              <text_slice>This is the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>39:15</time_slice>
              <text_slice>Difference, from the true mean,
normalized by the usual</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>normalizing factor.</text_slice>
            </slice>
            <slice>
              <time_slice>39:18</time_slice>
              <text_slice>By the central limit theorem,
this is approximately normal.</text_slice>
            </slice>
            <slice>
              <time_slice>39:22</time_slice>
              <text_slice>So it has probability 0.95
of being less than 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>39:26</time_slice>
              <text_slice>Now take this event here
and rewrite it.</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>This the event, well, that
Theta hat minus theta is</text_slice>
            </slice>
            <slice>
              <time_slice>39:36</time_slice>
              <text_slice>bigger than this number and
smaller than that number.</text_slice>
            </slice>
            <slice>
              <time_slice>39:40</time_slice>
              <text_slice>This event here is equivalent
to that event here.</text_slice>
            </slice>
            <slice>
              <time_slice>39:45</time_slice>
              <text_slice>And so this suggests a way of
constructing our 95% percent</text_slice>
            </slice>
            <slice>
              <time_slice>39:50</time_slice>
              <text_slice>confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>39:52</time_slice>
              <text_slice>I'm going to report the
interval, which gives this as</text_slice>
            </slice>
            <slice>
              <time_slice>39:56</time_slice>
              <text_slice>the lower end of the confidence
interval, and gives</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>this as the upper end of
the confidence interval</text_slice>
            </slice>
            <slice>
              <time_slice>40:05</time_slice>
              <text_slice>In other words, at the end of
the experiment, we report the</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>sample mean, which
is our estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>And we report also, an interval</text_slice>
            </slice>
            <slice>
              <time_slice>40:14</time_slice>
              <text_slice>around the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>40:16</time_slice>
              <text_slice>And this is our 95% confidence
interval.</text_slice>
            </slice>
            <slice>
              <time_slice>40:20</time_slice>
              <text_slice>The confidence interval becomes</text_slice>
            </slice>
            <slice>
              <time_slice>40:22</time_slice>
              <text_slice>smaller, when n is larger.</text_slice>
            </slice>
            <slice>
              <time_slice>40:26</time_slice>
              <text_slice>In some sense, we're more
certain that we're doing a</text_slice>
            </slice>
            <slice>
              <time_slice>40:28</time_slice>
              <text_slice>good estimation job, so we can
have a small interval and</text_slice>
            </slice>
            <slice>
              <time_slice>40:32</time_slice>
              <text_slice>still be quite confident that
our interval captures the true</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>value of the parameter.</text_slice>
            </slice>
            <slice>
              <time_slice>40:37</time_slice>
              <text_slice>Also, if our data have very
little noise, when you have</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>more accurate measurements,
you're more confident that</text_slice>
            </slice>
            <slice>
              <time_slice>40:45</time_slice>
              <text_slice>your estimate is pretty good.</text_slice>
            </slice>
            <slice>
              <time_slice>40:47</time_slice>
              <text_slice>And that results in a smaller
confidence interval, smaller</text_slice>
            </slice>
            <slice>
              <time_slice>40:51</time_slice>
              <text_slice>length of the confidence
interval.</text_slice>
            </slice>
            <slice>
              <time_slice>40:52</time_slice>
              <text_slice>And still you have 95%
probability of capturing the</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>true value of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>40:57</time_slice>
              <text_slice>So we did this exercise by
taking 95% confidence</text_slice>
            </slice>
            <slice>
              <time_slice>41:01</time_slice>
              <text_slice>intervals and the corresponding
value from the</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>normal tables, which is 1.96.</text_slice>
            </slice>
            <slice>
              <time_slice>41:06</time_slice>
              <text_slice>Of course, you can do it more
generally, if you set your</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>alpha to be some other number.</text_slice>
            </slice>
            <slice>
              <time_slice>41:13</time_slice>
              <text_slice>Again, you look at the
normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>41:16</time_slice>
              <text_slice>And you find the value here,
so that the tail has</text_slice>
            </slice>
            <slice>
              <time_slice>41:20</time_slice>
              <text_slice>probability alpha over 2.</text_slice>
            </slice>
            <slice>
              <time_slice>41:22</time_slice>
              <text_slice>And instead of using these 1.96,
you use whatever number</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>you get from the
normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>41:31</time_slice>
              <text_slice>And this tells you
how to construct</text_slice>
            </slice>
            <slice>
              <time_slice>41:33</time_slice>
              <text_slice>a confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>Well, to be exact, this
is not necessarily a</text_slice>
            </slice>
            <slice>
              <time_slice>41:42</time_slice>
              <text_slice>95% confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>41:44</time_slice>
              <text_slice>It's approximately a 95%
confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>41:47</time_slice>
              <text_slice>Why is this?</text_slice>
            </slice>
            <slice>
              <time_slice>41:48</time_slice>
              <text_slice>Because we've done
an approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>We have used the central
limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>41:53</time_slice>
              <text_slice>So it might turn out to be a
95.5% confidence interval</text_slice>
            </slice>
            <slice>
              <time_slice>41:59</time_slice>
              <text_slice>instead of 95%, because
our calculations are</text_slice>
            </slice>
            <slice>
              <time_slice>42:03</time_slice>
              <text_slice>not entirely accurate.</text_slice>
            </slice>
            <slice>
              <time_slice>42:04</time_slice>
              <text_slice>But for reasonable values of
n, using the central limit</text_slice>
            </slice>
            <slice>
              <time_slice>42:08</time_slice>
              <text_slice>theorem is a good
approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>42:10</time_slice>
              <text_slice>And that's what people
almost always do.</text_slice>
            </slice>
            <slice>
              <time_slice>42:13</time_slice>
              <text_slice>So just take the value from
the normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>42:17</time_slice>
              <text_slice>Okay, except for one catch.</text_slice>
            </slice>
            <slice>
              <time_slice>42:22</time_slice>
              <text_slice>I used the data.</text_slice>
            </slice>
            <slice>
              <time_slice>42:24</time_slice>
              <text_slice>I obtained my estimate.</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>And I want to go to my boss and
report this theta minus</text_slice>
            </slice>
            <slice>
              <time_slice>42:29</time_slice>
              <text_slice>and theta hat, which is the
confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>42:33</time_slice>
              <text_slice>What's the difficulty?</text_slice>
            </slice>
            <slice>
              <time_slice>42:35</time_slice>
              <text_slice>I know what n is.</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>But I don't know what sigma
is, in general.</text_slice>
            </slice>
            <slice>
              <time_slice>42:40</time_slice>
              <text_slice>So if I don't know sigma,
what am I going to do?</text_slice>
            </slice>
            <slice>
              <time_slice>42:44</time_slice>
              <text_slice>Here, there's a few options
for what you can do.</text_slice>
            </slice>
            <slice>
              <time_slice>42:48</time_slice>
              <text_slice>And the first option is familiar
from what we did when</text_slice>
            </slice>
            <slice>
              <time_slice>42:52</time_slice>
              <text_slice>we talked about the
pollster problem.</text_slice>
            </slice>
            <slice>
              <time_slice>42:55</time_slice>
              <text_slice>We don't know what sigma is,
but maybe we have an upper</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>bound on sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>43:00</time_slice>
              <text_slice>For example, if the Xi's
Bernoulli random variables, we</text_slice>
            </slice>
            <slice>
              <time_slice>43:03</time_slice>
              <text_slice>have seen that the standard
deviation is at most 1/2.</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>So use the most conservative
value for sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>43:10</time_slice>
              <text_slice>Using the most conservative
value means that you take</text_slice>
            </slice>
            <slice>
              <time_slice>43:13</time_slice>
              <text_slice>bigger confidence intervals
than necessary.</text_slice>
            </slice>
            <slice>
              <time_slice>43:17</time_slice>
              <text_slice>So that's one option.</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>Another option is to try to
estimate sigma from the data.</text_slice>
            </slice>
            <slice>
              <time_slice>43:25</time_slice>
              <text_slice>How do you do this estimation?</text_slice>
            </slice>
            <slice>
              <time_slice>43:27</time_slice>
              <text_slice>In special cases, for special
types of distributions, you</text_slice>
            </slice>
            <slice>
              <time_slice>43:31</time_slice>
              <text_slice>can think of heuristic ways
of doing this estimation.</text_slice>
            </slice>
            <slice>
              <time_slice>43:34</time_slice>
              <text_slice>For example, in the case of
Bernoulli random variables, we</text_slice>
            </slice>
            <slice>
              <time_slice>43:38</time_slice>
              <text_slice>know that the true value of
sigma, the standard deviation</text_slice>
            </slice>
            <slice>
              <time_slice>43:42</time_slice>
              <text_slice>of a Bernoulli random variable,
is the square root</text_slice>
            </slice>
            <slice>
              <time_slice>43:45</time_slice>
              <text_slice>of theta1 minus theta,
where theta is</text_slice>
            </slice>
            <slice>
              <time_slice>43:47</time_slice>
              <text_slice>the mean of the Bernoulli.</text_slice>
            </slice>
            <slice>
              <time_slice>43:50</time_slice>
              <text_slice>Try to use this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>But theta is the thing we're
trying to estimate in the</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>first place.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>We don't know it.</text_slice>
            </slice>
            <slice>
              <time_slice>43:55</time_slice>
              <text_slice>What do we do?</text_slice>
            </slice>
            <slice>
              <time_slice>43:57</time_slice>
              <text_slice>Well, we have an estimate for
theta, the estimate, produced</text_slice>
            </slice>
            <slice>
              <time_slice>44:00</time_slice>
              <text_slice>by our estimation procedure,
the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>44:04</time_slice>
              <text_slice>So I obtain my data.</text_slice>
            </slice>
            <slice>
              <time_slice>44:05</time_slice>
              <text_slice>I get my data.</text_slice>
            </slice>
            <slice>
              <time_slice>44:06</time_slice>
              <text_slice>I produce the estimate
theta hat.</text_slice>
            </slice>
            <slice>
              <time_slice>44:09</time_slice>
              <text_slice>It's an estimate of the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>44:10</time_slice>
              <text_slice>Use that estimate in this
formula to come up with an</text_slice>
            </slice>
            <slice>
              <time_slice>44:14</time_slice>
              <text_slice>estimate of my standard
deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>44:17</time_slice>
              <text_slice>And then use that standard
deviation, in the construction</text_slice>
            </slice>
            <slice>
              <time_slice>44:20</time_slice>
              <text_slice>of the confidence interval,
pretending</text_slice>
            </slice>
            <slice>
              <time_slice>44:22</time_slice>
              <text_slice>that this is correct.</text_slice>
            </slice>
            <slice>
              <time_slice>44:24</time_slice>
              <text_slice>Well the number of your data is
large, then we know, from</text_slice>
            </slice>
            <slice>
              <time_slice>44:29</time_slice>
              <text_slice>the law of large numbers, that
theta hat is a pretty good</text_slice>
            </slice>
            <slice>
              <time_slice>44:31</time_slice>
              <text_slice>estimate of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>44:33</time_slice>
              <text_slice>So sigma hat is going to be a
pretty good estimate of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>So we're not making large errors
by using this approach.</text_slice>
            </slice>
            <slice>
              <time_slice>44:42</time_slice>
              <text_slice>So in this scenario here, things
were simple, because we</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>had an analytical formula.</text_slice>
            </slice>
            <slice>
              <time_slice>44:49</time_slice>
              <text_slice>Sigma was determined by theta.</text_slice>
            </slice>
            <slice>
              <time_slice>44:52</time_slice>
              <text_slice>So we could come up
with a quick and</text_slice>
            </slice>
            <slice>
              <time_slice>44:54</time_slice>
              <text_slice>dirty estimate of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>In general, if you do not have
any nice formulas of this</text_slice>
            </slice>
            <slice>
              <time_slice>45:00</time_slice>
              <text_slice>kind, what could you do?</text_slice>
            </slice>
            <slice>
              <time_slice>45:03</time_slice>
              <text_slice>Well, you still need
to come up with an</text_slice>
            </slice>
            <slice>
              <time_slice>45:04</time_slice>
              <text_slice>estimate of sigma somehow.</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>What is a generic method for</text_slice>
            </slice>
            <slice>
              <time_slice>45:08</time_slice>
              <text_slice>estimating a standard deviation?</text_slice>
            </slice>
            <slice>
              <time_slice>45:11</time_slice>
              <text_slice>Equivalently, what could be a
generic method for estimating</text_slice>
            </slice>
            <slice>
              <time_slice>45:14</time_slice>
              <text_slice>a variance?</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>Well the variance is
an expected value</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>of some random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>45:20</time_slice>
              <text_slice>The variance is the mean of the
random variable inside of</text_slice>
            </slice>
            <slice>
              <time_slice>45:25</time_slice>
              <text_slice>those brackets.</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>How does one estimate the mean
of some random variable?</text_slice>
            </slice>
            <slice>
              <time_slice>45:33</time_slice>
              <text_slice>You obtain lots of measurements
of that random</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>variable and average them out.</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>So this would be a reasonable
way of estimating the variance</text_slice>
            </slice>
            <slice>
              <time_slice>45:45</time_slice>
              <text_slice>of a distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>45:47</time_slice>
              <text_slice>And again, the weak law of large
numbers tells us that</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>this average converges to the
expected value of this, which</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>is just the variance of
the distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>45:58</time_slice>
              <text_slice>So we got a nice and
consistent way</text_slice>
            </slice>
            <slice>
              <time_slice>46:01</time_slice>
              <text_slice>of estimating variances.</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>But now, we seem to be getting
in a vicious circle here,</text_slice>
            </slice>
            <slice>
              <time_slice>46:08</time_slice>
              <text_slice>because to estimate
the variance, we</text_slice>
            </slice>
            <slice>
              <time_slice>46:10</time_slice>
              <text_slice>need to know the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:12</time_slice>
              <text_slice>And the mean is something we're
trying to estimate in</text_slice>
            </slice>
            <slice>
              <time_slice>46:16</time_slice>
              <text_slice>the first place.</text_slice>
            </slice>
            <slice>
              <time_slice>46:18</time_slice>
              <text_slice>Okay.</text_slice>
            </slice>
            <slice>
              <time_slice>46:18</time_slice>
              <text_slice>But we do have an estimate
from the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>So a reasonable approximation,
once more, is to plug-in,</text_slice>
            </slice>
            <slice>
              <time_slice>46:24</time_slice>
              <text_slice>here, since we don't
know the mean, the</text_slice>
            </slice>
            <slice>
              <time_slice>46:27</time_slice>
              <text_slice>estimate of the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>46:29</time_slice>
              <text_slice>And so you get that expression,
but with a theta</text_slice>
            </slice>
            <slice>
              <time_slice>46:32</time_slice>
              <text_slice>hat instead of theta itself.</text_slice>
            </slice>
            <slice>
              <time_slice>46:35</time_slice>
              <text_slice>And this is another
reasonable way of</text_slice>
            </slice>
            <slice>
              <time_slice>46:37</time_slice>
              <text_slice>estimating the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>46:40</time_slice>
              <text_slice>It does have the same
consistency properties.</text_slice>
            </slice>
            <slice>
              <time_slice>46:42</time_slice>
              <text_slice>Why?</text_slice>
            </slice>
            <slice>
              <time_slice>46:44</time_slice>
              <text_slice>When n is large, this is going
to behave the same as that,</text_slice>
            </slice>
            <slice>
              <time_slice>46:51</time_slice>
              <text_slice>because theta hat converges
to theta.</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>And when n is large, this is
approximately the same as</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>46:58</time_slice>
              <text_slice>So for a large n, this quantity
also converges to</text_slice>
            </slice>
            <slice>
              <time_slice>47:02</time_slice>
              <text_slice>sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>47:03</time_slice>
              <text_slice>And we have a consistent
estimate of</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>the variance as well.</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>And we can take that consistent
estimate and use it</text_slice>
            </slice>
            <slice>
              <time_slice>47:09</time_slice>
              <text_slice>back in the construction
of confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>One little detail, here,
we're dividing by n.</text_slice>
            </slice>
            <slice>
              <time_slice>47:16</time_slice>
              <text_slice>Here, we're dividing by n-1.</text_slice>
            </slice>
            <slice>
              <time_slice>47:19</time_slice>
              <text_slice>Why do we do this?</text_slice>
            </slice>
            <slice>
              <time_slice>47:21</time_slice>
              <text_slice>Well, it turns out that's what
you need to do for these</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>estimates to be an unbiased
estimate of the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:28</time_slice>
              <text_slice>One has to do a little bit of
a calculation, and one finds</text_slice>
            </slice>
            <slice>
              <time_slice>47:32</time_slice>
              <text_slice>that that's the factor that you
need to have here in order</text_slice>
            </slice>
            <slice>
              <time_slice>47:36</time_slice>
              <text_slice>to be unbiased.</text_slice>
            </slice>
            <slice>
              <time_slice>47:37</time_slice>
              <text_slice>Of course, if you get 100 data
points, whether you divide by</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>100 or divided by 99, it's
going to make only a tiny</text_slice>
            </slice>
            <slice>
              <time_slice>47:46</time_slice>
              <text_slice>difference in your estimate
of your variance.</text_slice>
            </slice>
            <slice>
              <time_slice>47:48</time_slice>
              <text_slice>So it's going to make only
a tiny difference in your</text_slice>
            </slice>
            <slice>
              <time_slice>47:50</time_slice>
              <text_slice>estimate of the standard
deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>47:52</time_slice>
              <text_slice>It's not a big deal.</text_slice>
            </slice>
            <slice>
              <time_slice>47:54</time_slice>
              <text_slice>And it doesn't really matter.</text_slice>
            </slice>
            <slice>
              <time_slice>47:56</time_slice>
              <text_slice>But if you want to show off
about your deeper knowledge of</text_slice>
            </slice>
            <slice>
              <time_slice>48:00</time_slice>
              <text_slice>statistics, you throw in the
1 over n-1 factor in there.</text_slice>
            </slice>
            <slice>
              <time_slice>48:06</time_slice>
              <text_slice>So now one basically needs to
put together this story here,</text_slice>
            </slice>
            <slice>
              <time_slice>48:11</time_slice>
              <text_slice>how you estimate the variance.</text_slice>
            </slice>
            <slice>
              <time_slice>48:15</time_slice>
              <text_slice>You first estimate
the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>48:18</time_slice>
              <text_slice>And then you do some extra
work to come up with a</text_slice>
            </slice>
            <slice>
              <time_slice>48:21</time_slice>
              <text_slice>reasonable estimate of
the variance and</text_slice>
            </slice>
            <slice>
              <time_slice>48:23</time_slice>
              <text_slice>the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>48:24</time_slice>
              <text_slice>And then you use your estimate,
of the standard</text_slice>
            </slice>
            <slice>
              <time_slice>48:27</time_slice>
              <text_slice>deviation, to come up with a
confidence interval, which has</text_slice>
            </slice>
            <slice>
              <time_slice>48:32</time_slice>
              <text_slice>these two endpoints.</text_slice>
            </slice>
            <slice>
              <time_slice>48:35</time_slice>
              <text_slice>In doing this procedure, there's
basically a number of</text_slice>
            </slice>
            <slice>
              <time_slice>48:39</time_slice>
              <text_slice>approximations that
are involved.</text_slice>
            </slice>
            <slice>
              <time_slice>48:41</time_slice>
              <text_slice>There are two types
of approximations.</text_slice>
            </slice>
            <slice>
              <time_slice>48:43</time_slice>
              <text_slice>One approximation is that we're
pretending that the</text_slice>
            </slice>
            <slice>
              <time_slice>48:46</time_slice>
              <text_slice>sample mean has a normal
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>48:48</time_slice>
              <text_slice>That's something we're justified
to do, by the</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>48:52</time_slice>
              <text_slice>But it's not exact.</text_slice>
            </slice>
            <slice>
              <time_slice>48:53</time_slice>
              <text_slice>It's an approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>And the second approximation
that comes in is that, instead</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>of using the correct standard
deviation, in general, you</text_slice>
            </slice>
            <slice>
              <time_slice>49:01</time_slice>
              <text_slice>will have to use some
approximation of</text_slice>
            </slice>
            <slice>
              <time_slice>49:04</time_slice>
              <text_slice>the standard deviation.</text_slice>
            </slice>
            <slice>
              <time_slice>49:08</time_slice>
              <text_slice>Okay so you will be getting a
little bit of practice with</text_slice>
            </slice>
            <slice>
              <time_slice>49:11</time_slice>
              <text_slice>these concepts in recitation
and tutorial.</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>And we will move on to
new topics next week.</text_slice>
            </slice>
            <slice>
              <time_slice>49:18</time_slice>
              <text_slice>But the material that's going
to be covered in the final</text_slice>
            </slice>
            <slice>
              <time_slice>49:20</time_slice>
              <text_slice>exam is only up to this point.</text_slice>
            </slice>
            <slice>
              <time_slice>49:23</time_slice>
              <text_slice>So next week is just
general education.</text_slice>
            </slice>
            <slice>
              <time_slice>49:28</time_slice>
              <text_slice>Hopefully useful, but it's
not in the exam.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Classical Inference - III; Course Overview (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l25/</lecture_pdf_url>
      <lectureno>25</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 25 Simple binary hypothesis testing
Outline
n u l l h y p o t h e s i s H0:
Reference: Section 9.4 XpX(x;H0)[ o r fX(x;H0)]
a l t e r n a t i v e h y p o t h e s i s H:Course Evaluations (until 12/16)1
XpX(x;H1)[ o r fX(x;H1)]http://web.mit.edu/subjectevaluation
Choose a rejection region R;
reject H0idata R
Review of simple binary hypothesis tests
Likelihood ratio test: reject Hifexamples 0
pX(x;H1) fX(x;H1)
&gt;or &gt;Testing composite hypotheses pX(x;H0) fX(x;H0)
is my coin fair?x false rejection probability 
is my die fair? (e.g., =0.05)
goodness of t tests choose so that P(reject H0;H0)=
Example (test on normal mean) Example (test on normal variance)
ndata points, i.i.d. ndata points, i.i.d.
H0:XiN(0,1) H0:XiN(0,1)
H1:XiN(1,1) H1:XiN(0,4)
Likelihood ratio test; rejection region: Likelihood ratio test; rejection region:
(1n(2/
2)exp { iXi1)/2} (1/2
2)nexp {2i X /(2 4)}
(1/ &gt;i&gt;2)nexp {
2iX /2i} (1/
2)nexp {
2i
algebra: re ctX /2i}
jeH0if:
Xi&gt;algebra: reject
2H0if &gt;
i
Xi
i
Find such that Find such that
n
;
=n
2P Xi&gt;H0  P Xi&gt;;H0
i=1 i=1
=
use normal tables the distribution of2iXis knowni
(derived distribution
problem)
chi-square distribution;
tables are available
1</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Composite hypotheses Is my die fair?
GotS= 472 heads in n= 1000 tosses; Hypothesis H0:
is the coin fair? P(X=i)=p i=1/6,i=1,...,6
H0:p=1/2 versus H1:p=1/2Observed occurrences of i:Ni
Pick a statistic (e.g., S)Choose form of rejection region;
chi-square test:Pick shape of rejection region
(e.g., |S )2n/2|&gt;) (N npreject H0ifT=i i&gt;npi i
Choose signicance level (e.g., =0.05)
Choose so that:Pick critical value so that:
P(reject H0;H0)=0 .05P(reject H0;H0)=
Using the CLT: P(T&gt; ;H0)=0 .05
P(|S500|31;H0)0.95; = 31Need the distribution of T:
(CLT + derived distribution problem)
In our example: |S500| = 28 &lt;
for large n,Thas approximatelyH0not rejected (at the 5% level)
a chi-square distribution
available in tables
Do I have the correct pdf? What else is there?
Partition the range into binsSystematic methods for coming up with
npi: expected incidence of bin i shape of rejection regions
(from the pdf)
Ni: observed incidence of bin i Methods to estimate an unknown PDF(e.g., form a histogram and smooth itUse chi-square test (as in die problem)out)
Kolmogorov-Smirnov test:
form empirical CDF ,F, from data Ecient and recursive signal processing
X
Methods to select between less or more
complex models
(e.g., identify relevant explanatoryvariables in regression models)
Methods tailored to high-dimensionalunknown parameter vectors and hugenumber of data points (data mining)
(http://www.itl.nist.gov/div898/handbook/) etc. etc. ...
Dn=m a x x|FX(x)FX(x)|
P(nDn1.36) 0.05
2</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-24-classical-inference-ii/</video_url>
          <video_title>Lecture 24: Classical Inference II</video_title>
          <transcript>
            <slice>
              <time_slice>0:00</time_slice>
              <text_slice>The following content is
provided under a Creative</text_slice>
            </slice>
            <slice>
              <time_slice>0:02</time_slice>
              <text_slice>Commons license.</text_slice>
            </slice>
            <slice>
              <time_slice>0:03</time_slice>
              <text_slice>Your support will help MIT
OpenCourseWare continue to</text_slice>
            </slice>
            <slice>
              <time_slice>0:06</time_slice>
              <text_slice>offer high quality educational
resources for free.</text_slice>
            </slice>
            <slice>
              <time_slice>0:10</time_slice>
              <text_slice>To make a donation or view
additional materials from</text_slice>
            </slice>
            <slice>
              <time_slice>0:13</time_slice>
              <text_slice>hundreds of MIT courses, visit
MIT OpenCourseWare at</text_slice>
            </slice>
            <slice>
              <time_slice>0:19</time_slice>
              <text_slice>ocw.mit.edu.</text_slice>
            </slice>
            <slice>
              <time_slice>0:21</time_slice>
              <text_slice>JOHN TSITSIKLIS: And we're going
to continue today with</text_slice>
            </slice>
            <slice>
              <time_slice>0:24</time_slice>
              <text_slice>our discussion of classical
statistics.</text_slice>
            </slice>
            <slice>
              <time_slice>0:26</time_slice>
              <text_slice>We'll start with a quick review
of what we discussed</text_slice>
            </slice>
            <slice>
              <time_slice>0:29</time_slice>
              <text_slice>last time, and then talk about
two topics that cover a lot of</text_slice>
            </slice>
            <slice>
              <time_slice>0:34</time_slice>
              <text_slice>statistics that are happening
in the real world.</text_slice>
            </slice>
            <slice>
              <time_slice>0:37</time_slice>
              <text_slice>So two basic methods.</text_slice>
            </slice>
            <slice>
              <time_slice>0:39</time_slice>
              <text_slice>One is the method of linear
regression, and the other one</text_slice>
            </slice>
            <slice>
              <time_slice>0:43</time_slice>
              <text_slice>is the basic methods and
tools for how to</text_slice>
            </slice>
            <slice>
              <time_slice>0:46</time_slice>
              <text_slice>do hypothesis testing.</text_slice>
            </slice>
            <slice>
              <time_slice>0:49</time_slice>
              <text_slice>OK, so these two are topics
that any scientifically</text_slice>
            </slice>
            <slice>
              <time_slice>0:53</time_slice>
              <text_slice>literate person should
know something about.</text_slice>
            </slice>
            <slice>
              <time_slice>0:57</time_slice>
              <text_slice>So we're going to introduce
the basic ideas</text_slice>
            </slice>
            <slice>
              <time_slice>0:59</time_slice>
              <text_slice>and concepts involved.</text_slice>
            </slice>
            <slice>
              <time_slice>1:01</time_slice>
              <text_slice>So in classical statistics we
basically have essentially a</text_slice>
            </slice>
            <slice>
              <time_slice>1:07</time_slice>
              <text_slice>family of possible models
about the world.</text_slice>
            </slice>
            <slice>
              <time_slice>1:11</time_slice>
              <text_slice>So the world is the random
variable that we observe, and</text_slice>
            </slice>
            <slice>
              <time_slice>1:15</time_slice>
              <text_slice>we have a model for it, but
actually not just one model,</text_slice>
            </slice>
            <slice>
              <time_slice>1:19</time_slice>
              <text_slice>several candidate models.</text_slice>
            </slice>
            <slice>
              <time_slice>1:20</time_slice>
              <text_slice>And each candidate model
corresponds to a different</text_slice>
            </slice>
            <slice>
              <time_slice>1:24</time_slice>
              <text_slice>value of a parameter theta
that we do not know.</text_slice>
            </slice>
            <slice>
              <time_slice>1:28</time_slice>
              <text_slice>So in contrast to Bayesian
statistics, this theta is</text_slice>
            </slice>
            <slice>
              <time_slice>1:32</time_slice>
              <text_slice>assumed to be a constant
that we do not know.</text_slice>
            </slice>
            <slice>
              <time_slice>1:35</time_slice>
              <text_slice>It is not modeled as a random
variable, there's no</text_slice>
            </slice>
            <slice>
              <time_slice>1:38</time_slice>
              <text_slice>probabilities associated
with theta.</text_slice>
            </slice>
            <slice>
              <time_slice>1:40</time_slice>
              <text_slice>We only have probabilities
about the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>1:43</time_slice>
              <text_slice>So in this context what is a
reasonable way of choosing a</text_slice>
            </slice>
            <slice>
              <time_slice>1:47</time_slice>
              <text_slice>value for the parameter?</text_slice>
            </slice>
            <slice>
              <time_slice>1:49</time_slice>
              <text_slice>One general approach is the
maximum likelihood approach,</text_slice>
            </slice>
            <slice>
              <time_slice>1:53</time_slice>
              <text_slice>which chooses the
theta for which</text_slice>
            </slice>
            <slice>
              <time_slice>1:56</time_slice>
              <text_slice>this quantity is largest.</text_slice>
            </slice>
            <slice>
              <time_slice>1:58</time_slice>
              <text_slice>So what does that mean
intuitively?</text_slice>
            </slice>
            <slice>
              <time_slice>2:00</time_slice>
              <text_slice>I'm trying to find the value of
theta under which the data</text_slice>
            </slice>
            <slice>
              <time_slice>2:04</time_slice>
              <text_slice>that I observe are most likely
to have occurred.</text_slice>
            </slice>
            <slice>
              <time_slice>2:08</time_slice>
              <text_slice>So is the thinking is
essentially as follows.</text_slice>
            </slice>
            <slice>
              <time_slice>2:11</time_slice>
              <text_slice>Let's say I have to choose
between two choices of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>2:13</time_slice>
              <text_slice>Under this theta the
X that I observed</text_slice>
            </slice>
            <slice>
              <time_slice>2:16</time_slice>
              <text_slice>would be very unlikely.</text_slice>
            </slice>
            <slice>
              <time_slice>2:17</time_slice>
              <text_slice>Under that theta the X that I
observed would have a decent</text_slice>
            </slice>
            <slice>
              <time_slice>2:21</time_slice>
              <text_slice>probability of occurring.</text_slice>
            </slice>
            <slice>
              <time_slice>2:22</time_slice>
              <text_slice>So I chose the latter as
my estimate of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>2:28</time_slice>
              <text_slice>It's interesting to do the
comparison with the Bayesian</text_slice>
            </slice>
            <slice>
              <time_slice>2:31</time_slice>
              <text_slice>approach which we did discuss
last time, in the Bayesian</text_slice>
            </slice>
            <slice>
              <time_slice>2:34</time_slice>
              <text_slice>approach we also maximize over
theta, but we maximize a</text_slice>
            </slice>
            <slice>
              <time_slice>2:38</time_slice>
              <text_slice>quantity in which the relation
between X's and thetas run the</text_slice>
            </slice>
            <slice>
              <time_slice>2:43</time_slice>
              <text_slice>opposite way.</text_slice>
            </slice>
            <slice>
              <time_slice>2:44</time_slice>
              <text_slice>Here in the Bayesian world,
Theta is a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>2:47</time_slice>
              <text_slice>So it has a distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>2:48</time_slice>
              <text_slice>Once we observe the data, it has
a posterior distribution,</text_slice>
            </slice>
            <slice>
              <time_slice>2:53</time_slice>
              <text_slice>and we find the value of Theta,
which is most likely</text_slice>
            </slice>
            <slice>
              <time_slice>2:56</time_slice>
              <text_slice>under the posterior
distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>2:59</time_slice>
              <text_slice>As we discussed last time when
you do this maximization now</text_slice>
            </slice>
            <slice>
              <time_slice>3:03</time_slice>
              <text_slice>the posterior distribution is
given by this expression.</text_slice>
            </slice>
            <slice>
              <time_slice>3:05</time_slice>
              <text_slice>The denominator doesn't matter,
and if you were to</text_slice>
            </slice>
            <slice>
              <time_slice>3:09</time_slice>
              <text_slice>take a prior, which is flat--</text_slice>
            </slice>
            <slice>
              <time_slice>3:12</time_slice>
              <text_slice>that is a constant independent
of Theta, then that</text_slice>
            </slice>
            <slice>
              <time_slice>3:16</time_slice>
              <text_slice>term would go away.</text_slice>
            </slice>
            <slice>
              <time_slice>3:17</time_slice>
              <text_slice>And syntactically,
at least, the two</text_slice>
            </slice>
            <slice>
              <time_slice>3:19</time_slice>
              <text_slice>approaches look the same.</text_slice>
            </slice>
            <slice>
              <time_slice>3:21</time_slice>
              <text_slice>So syntactically, or formally,
maximum likelihood estimation</text_slice>
            </slice>
            <slice>
              <time_slice>3:28</time_slice>
              <text_slice>is the same as Bayesian
estimation in which you assume</text_slice>
            </slice>
            <slice>
              <time_slice>3:32</time_slice>
              <text_slice>a prior which is flat, so that
all possible values of Theta</text_slice>
            </slice>
            <slice>
              <time_slice>3:36</time_slice>
              <text_slice>are equally likely.</text_slice>
            </slice>
            <slice>
              <time_slice>3:37</time_slice>
              <text_slice>Philosophically, however,
they're very different things.</text_slice>
            </slice>
            <slice>
              <time_slice>3:40</time_slice>
              <text_slice>Here I'm picking the most
likely value of Theta.</text_slice>
            </slice>
            <slice>
              <time_slice>3:44</time_slice>
              <text_slice>Here I'm picking the value of
Theta under which the observed</text_slice>
            </slice>
            <slice>
              <time_slice>3:47</time_slice>
              <text_slice>data would have been more
likely to occur.</text_slice>
            </slice>
            <slice>
              <time_slice>3:51</time_slice>
              <text_slice>So maximum likelihood estimation
is a general</text_slice>
            </slice>
            <slice>
              <time_slice>3:53</time_slice>
              <text_slice>purpose method, so it's applied
all over the place in</text_slice>
            </slice>
            <slice>
              <time_slice>3:57</time_slice>
              <text_slice>many, many different types
of estimation problems.</text_slice>
            </slice>
            <slice>
              <time_slice>4:02</time_slice>
              <text_slice>There is a special kind of
estimation problem in which</text_slice>
            </slice>
            <slice>
              <time_slice>4:05</time_slice>
              <text_slice>you may forget about maximum
likelihood estimation, and</text_slice>
            </slice>
            <slice>
              <time_slice>4:08</time_slice>
              <text_slice>come up with an estimate in
a straightforward way.</text_slice>
            </slice>
            <slice>
              <time_slice>4:12</time_slice>
              <text_slice>And this is the case where
you're trying to estimate the</text_slice>
            </slice>
            <slice>
              <time_slice>4:15</time_slice>
              <text_slice>mean of the distribution of X,
where X is a random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>4:22</time_slice>
              <text_slice>You observe several independent
identically</text_slice>
            </slice>
            <slice>
              <time_slice>4:25</time_slice>
              <text_slice>distributed random variables
X1 up to Xn.</text_slice>
            </slice>
            <slice>
              <time_slice>4:30</time_slice>
              <text_slice>All of them have the same
distribution as this X.</text_slice>
            </slice>
            <slice>
              <time_slice>4:32</time_slice>
              <text_slice>So they have a common mean.</text_slice>
            </slice>
            <slice>
              <time_slice>4:34</time_slice>
              <text_slice>We do not know the mean we
want to estimate it.</text_slice>
            </slice>
            <slice>
              <time_slice>4:37</time_slice>
              <text_slice>What is more natural than just
taking the average of the</text_slice>
            </slice>
            <slice>
              <time_slice>4:40</time_slice>
              <text_slice>values that we have observed?</text_slice>
            </slice>
            <slice>
              <time_slice>4:42</time_slice>
              <text_slice>So you generate lots of X's,
take the average of them, and</text_slice>
            </slice>
            <slice>
              <time_slice>4:46</time_slice>
              <text_slice>you expect that this is going to
be a reasonable estimate of</text_slice>
            </slice>
            <slice>
              <time_slice>4:50</time_slice>
              <text_slice>the true mean of that
random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>4:53</time_slice>
              <text_slice>And indeed we know from the weak
law of large numbers that</text_slice>
            </slice>
            <slice>
              <time_slice>4:56</time_slice>
              <text_slice>this estimate converges in
probability to the true mean</text_slice>
            </slice>
            <slice>
              <time_slice>5:00</time_slice>
              <text_slice>of the random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>5:02</time_slice>
              <text_slice>The other thing that we talked
about last time is that</text_slice>
            </slice>
            <slice>
              <time_slice>5:04</time_slice>
              <text_slice>besides giving a point estimate
we may want to also</text_slice>
            </slice>
            <slice>
              <time_slice>5:07</time_slice>
              <text_slice>give an interval that tells us
something about where we might</text_slice>
            </slice>
            <slice>
              <time_slice>5:13</time_slice>
              <text_slice>believe theta to lie.</text_slice>
            </slice>
            <slice>
              <time_slice>5:16</time_slice>
              <text_slice>And 1-alpha confidence interval
is in interval</text_slice>
            </slice>
            <slice>
              <time_slice>5:21</time_slice>
              <text_slice>generated based on the data.</text_slice>
            </slice>
            <slice>
              <time_slice>5:24</time_slice>
              <text_slice>So it's an interval from this
value to that value.</text_slice>
            </slice>
            <slice>
              <time_slice>5:26</time_slice>
              <text_slice>These values are written with
capital letters because</text_slice>
            </slice>
            <slice>
              <time_slice>5:30</time_slice>
              <text_slice>they're random, because they
depend on the data</text_slice>
            </slice>
            <slice>
              <time_slice>5:32</time_slice>
              <text_slice>that we have seen.</text_slice>
            </slice>
            <slice>
              <time_slice>5:33</time_slice>
              <text_slice>And this gives us an interval,
and we would like this</text_slice>
            </slice>
            <slice>
              <time_slice>5:36</time_slice>
              <text_slice>interval to have the property
that theta is inside that</text_slice>
            </slice>
            <slice>
              <time_slice>5:40</time_slice>
              <text_slice>interval with high
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>5:42</time_slice>
              <text_slice>So typically we would take
1-alpha to be a quantity such</text_slice>
            </slice>
            <slice>
              <time_slice>5:46</time_slice>
              <text_slice>as 95% for example.</text_slice>
            </slice>
            <slice>
              <time_slice>5:49</time_slice>
              <text_slice>In which case we have a 95%
confidence interval.</text_slice>
            </slice>
            <slice>
              <time_slice>5:54</time_slice>
              <text_slice>As we discussed last time it's
important to have the right</text_slice>
            </slice>
            <slice>
              <time_slice>5:56</time_slice>
              <text_slice>interpretation of what's
95% means.</text_slice>
            </slice>
            <slice>
              <time_slice>6:00</time_slice>
              <text_slice>What it does not mean
is the following--</text_slice>
            </slice>
            <slice>
              <time_slice>6:04</time_slice>
              <text_slice>the unknown value has 95%
percent probability of being</text_slice>
            </slice>
            <slice>
              <time_slice>6:09</time_slice>
              <text_slice>in the interval that
we have generated.</text_slice>
            </slice>
            <slice>
              <time_slice>6:12</time_slice>
              <text_slice>That's because the unknown
value is not a random</text_slice>
            </slice>
            <slice>
              <time_slice>6:14</time_slice>
              <text_slice>variable, it's a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>6:15</time_slice>
              <text_slice>Once we generate the interval
either it's inside or it's</text_slice>
            </slice>
            <slice>
              <time_slice>6:18</time_slice>
              <text_slice>outside, but there's no
probabilities involved.</text_slice>
            </slice>
            <slice>
              <time_slice>6:22</time_slice>
              <text_slice>Rather the probabilities are
to be interpreted over the</text_slice>
            </slice>
            <slice>
              <time_slice>6:26</time_slice>
              <text_slice>random interval itself.</text_slice>
            </slice>
            <slice>
              <time_slice>6:28</time_slice>
              <text_slice>What a statement like this
says is that if I have a</text_slice>
            </slice>
            <slice>
              <time_slice>6:31</time_slice>
              <text_slice>procedure for generating 95%
confidence intervals, then</text_slice>
            </slice>
            <slice>
              <time_slice>6:37</time_slice>
              <text_slice>whenever I use that procedure
I'm going to get a random</text_slice>
            </slice>
            <slice>
              <time_slice>6:40</time_slice>
              <text_slice>interval, and it's going to
have 95% probability of</text_slice>
            </slice>
            <slice>
              <time_slice>6:44</time_slice>
              <text_slice>capturing the true
value of theta.</text_slice>
            </slice>
            <slice>
              <time_slice>6:48</time_slice>
              <text_slice>So most of the time when I use
this particular procedure for</text_slice>
            </slice>
            <slice>
              <time_slice>6:53</time_slice>
              <text_slice>generating confidence intervals
the true theta will</text_slice>
            </slice>
            <slice>
              <time_slice>6:56</time_slice>
              <text_slice>happen to lie inside that
confidence interval with</text_slice>
            </slice>
            <slice>
              <time_slice>6:59</time_slice>
              <text_slice>probability 95%.</text_slice>
            </slice>
            <slice>
              <time_slice>7:01</time_slice>
              <text_slice>So the randomness in this
statement is with respect to</text_slice>
            </slice>
            <slice>
              <time_slice>7:04</time_slice>
              <text_slice>my confidence interval, it's
not with respect to theta,</text_slice>
            </slice>
            <slice>
              <time_slice>7:09</time_slice>
              <text_slice>because theta is not random.</text_slice>
            </slice>
            <slice>
              <time_slice>7:11</time_slice>
              <text_slice>How does one construct
confidence intervals?</text_slice>
            </slice>
            <slice>
              <time_slice>7:14</time_slice>
              <text_slice>There's various ways of going
about it, but in the case</text_slice>
            </slice>
            <slice>
              <time_slice>7:17</time_slice>
              <text_slice>where we're dealing with the
estimation of the mean of a</text_slice>
            </slice>
            <slice>
              <time_slice>7:20</time_slice>
              <text_slice>random variable doing this is
straightforward using the</text_slice>
            </slice>
            <slice>
              <time_slice>7:23</time_slice>
              <text_slice>central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>7:25</time_slice>
              <text_slice>Basically we take our estimated
mean, that's the</text_slice>
            </slice>
            <slice>
              <time_slice>7:31</time_slice>
              <text_slice>sample mean, and we take a
symmetric interval to the left</text_slice>
            </slice>
            <slice>
              <time_slice>7:35</time_slice>
              <text_slice>and to the right of
the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>7:38</time_slice>
              <text_slice>And we choose the width of that
interval by looking at</text_slice>
            </slice>
            <slice>
              <time_slice>7:42</time_slice>
              <text_slice>the normal tables.</text_slice>
            </slice>
            <slice>
              <time_slice>7:43</time_slice>
              <text_slice>So if this quantity, 1-alpha is
95% percent, we're going to</text_slice>
            </slice>
            <slice>
              <time_slice>7:50</time_slice>
              <text_slice>look at the 97.5 percentile of
the normal distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>7:55</time_slice>
              <text_slice>Find the constant number that
corresponds to that value from</text_slice>
            </slice>
            <slice>
              <time_slice>7:59</time_slice>
              <text_slice>the normal tables, and construct
the confidence</text_slice>
            </slice>
            <slice>
              <time_slice>8:02</time_slice>
              <text_slice>intervals according
to this formula.</text_slice>
            </slice>
            <slice>
              <time_slice>8:07</time_slice>
              <text_slice>So that gives you a pretty
mechanical way of going about</text_slice>
            </slice>
            <slice>
              <time_slice>8:10</time_slice>
              <text_slice>constructing confidence
intervals when you're</text_slice>
            </slice>
            <slice>
              <time_slice>8:13</time_slice>
              <text_slice>estimating the sample mean.</text_slice>
            </slice>
            <slice>
              <time_slice>8:15</time_slice>
              <text_slice>So constructing confidence
intervals in this way involves</text_slice>
            </slice>
            <slice>
              <time_slice>8:18</time_slice>
              <text_slice>an approximation.</text_slice>
            </slice>
            <slice>
              <time_slice>8:19</time_slice>
              <text_slice>The approximation is the
central limit theorem.</text_slice>
            </slice>
            <slice>
              <time_slice>8:22</time_slice>
              <text_slice>We are pretending that
the sample mean is a</text_slice>
            </slice>
            <slice>
              <time_slice>8:24</time_slice>
              <text_slice>normal random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>8:26</time_slice>
              <text_slice>Which is, more or less,
right when n is large.</text_slice>
            </slice>
            <slice>
              <time_slice>8:30</time_slice>
              <text_slice>That's what the central limit
theorem tells us.</text_slice>
            </slice>
            <slice>
              <time_slice>8:32</time_slice>
              <text_slice>And sometimes we may need to
do some extra approximation</text_slice>
            </slice>
            <slice>
              <time_slice>8:36</time_slice>
              <text_slice>work, because quite often
we do not know the</text_slice>
            </slice>
            <slice>
              <time_slice>8:39</time_slice>
              <text_slice>true value of sigma.</text_slice>
            </slice>
            <slice>
              <time_slice>8:41</time_slice>
              <text_slice>So we need to do some work
either to estimate</text_slice>
            </slice>
            <slice>
              <time_slice>8:43</time_slice>
              <text_slice>sigma from the data.</text_slice>
            </slice>
            <slice>
              <time_slice>8:45</time_slice>
              <text_slice>So sigma is, of course, the
standard deviation of the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>8:48</time_slice>
              <text_slice>We may want to estimate it from
the data, or we may have</text_slice>
            </slice>
            <slice>
              <time_slice>8:51</time_slice>
              <text_slice>an upper bound on sigma, and we
just use that upper bound.</text_slice>
            </slice>
            <slice>
              <time_slice>8:57</time_slice>
              <text_slice>So now let's move on
to a new topic.</text_slice>
            </slice>
            <slice>
              <time_slice>9:02</time_slice>
              <text_slice>A lot of statistics in the
real world are of the</text_slice>
            </slice>
            <slice>
              <time_slice>9:09</time_slice>
              <text_slice>following flavor.</text_slice>
            </slice>
            <slice>
              <time_slice>9:12</time_slice>
              <text_slice>So suppose that X is the SAT
score of a student in high</text_slice>
            </slice>
            <slice>
              <time_slice>9:16</time_slice>
              <text_slice>school, and Y is the MIT GPA
of that same student.</text_slice>
            </slice>
            <slice>
              <time_slice>9:23</time_slice>
              <text_slice>So you expect that there is a
relation between these two.</text_slice>
            </slice>
            <slice>
              <time_slice>9:27</time_slice>
              <text_slice>So you go and collect data for
different students, and you</text_slice>
            </slice>
            <slice>
              <time_slice>9:31</time_slice>
              <text_slice>record for a typical student
this would be their SAT score,</text_slice>
            </slice>
            <slice>
              <time_slice>9:35</time_slice>
              <text_slice>that could be their MIT GPA.</text_slice>
            </slice>
            <slice>
              <time_slice>9:37</time_slice>
              <text_slice>And you plot all this data
on an (X,Y) diagram.</text_slice>
            </slice>
            <slice>
              <time_slice>9:43</time_slice>
              <text_slice>Now it's reasonable to believe
that there is some systematic</text_slice>
            </slice>
            <slice>
              <time_slice>9:48</time_slice>
              <text_slice>relation between the two.</text_slice>
            </slice>
            <slice>
              <time_slice>9:49</time_slice>
              <text_slice>So people who had higher SAT
scores in high school may have</text_slice>
            </slice>
            <slice>
              <time_slice>9:54</time_slice>
              <text_slice>higher GPA in college.</text_slice>
            </slice>
            <slice>
              <time_slice>9:57</time_slice>
              <text_slice>Well that may or may
not be true.</text_slice>
            </slice>
            <slice>
              <time_slice>10:00</time_slice>
              <text_slice>You want to construct a model of
this kind, and see to what</text_slice>
            </slice>
            <slice>
              <time_slice>10:05</time_slice>
              <text_slice>extent a relation of
this type is true.</text_slice>
            </slice>
            <slice>
              <time_slice>10:08</time_slice>
              <text_slice>So you might hypothesize that
the real world is described by</text_slice>
            </slice>
            <slice>
              <time_slice>10:15</time_slice>
              <text_slice>a model of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>10:17</time_slice>
              <text_slice>That there is a linear relation
between the SAT</text_slice>
            </slice>
            <slice>
              <time_slice>10:22</time_slice>
              <text_slice>score, and the college GPA.</text_slice>
            </slice>
            <slice>
              <time_slice>10:27</time_slice>
              <text_slice>So it's a linear relation with
some parameters, theta0 and</text_slice>
            </slice>
            <slice>
              <time_slice>10:30</time_slice>
              <text_slice>theta1 that we do not know.</text_slice>
            </slice>
            <slice>
              <time_slice>10:33</time_slice>
              <text_slice>So we assume a linear relation
for the data, and depending on</text_slice>
            </slice>
            <slice>
              <time_slice>10:37</time_slice>
              <text_slice>the choices of theta0 and theta1
it could be a different</text_slice>
            </slice>
            <slice>
              <time_slice>10:41</time_slice>
              <text_slice>line through those data.</text_slice>
            </slice>
            <slice>
              <time_slice>10:43</time_slice>
              <text_slice>Now we would like to find the
best model of this kind to</text_slice>
            </slice>
            <slice>
              <time_slice>10:47</time_slice>
              <text_slice>explain the data.</text_slice>
            </slice>
            <slice>
              <time_slice>10:49</time_slice>
              <text_slice>Of course there's going
to be some randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>10:52</time_slice>
              <text_slice>So in general it's going to be
impossible to find a line that</text_slice>
            </slice>
            <slice>
              <time_slice>10:55</time_slice>
              <text_slice>goes through all of
the data points.</text_slice>
            </slice>
            <slice>
              <time_slice>10:57</time_slice>
              <text_slice>So let's try to find the best
line that comes closest to</text_slice>
            </slice>
            <slice>
              <time_slice>11:04</time_slice>
              <text_slice>explaining those data.</text_slice>
            </slice>
            <slice>
              <time_slice>11:05</time_slice>
              <text_slice>And here's how we go about it.</text_slice>
            </slice>
            <slice>
              <time_slice>11:08</time_slice>
              <text_slice>Suppose we try some particular
values of theta0 and theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>11:13</time_slice>
              <text_slice>These give us a certain line.</text_slice>
            </slice>
            <slice>
              <time_slice>11:15</time_slice>
              <text_slice>Given that line, we can
make predictions.</text_slice>
            </slice>
            <slice>
              <time_slice>11:20</time_slice>
              <text_slice>For a student who had this x,
the model that we have would</text_slice>
            </slice>
            <slice>
              <time_slice>11:24</time_slice>
              <text_slice>predict that y would
be this value.</text_slice>
            </slice>
            <slice>
              <time_slice>11:27</time_slice>
              <text_slice>The actual y is something else,
and so this quantity is</text_slice>
            </slice>
            <slice>
              <time_slice>11:32</time_slice>
              <text_slice>the error that our model would
make in predicting the y of</text_slice>
            </slice>
            <slice>
              <time_slice>11:37</time_slice>
              <text_slice>that particular student.</text_slice>
            </slice>
            <slice>
              <time_slice>11:39</time_slice>
              <text_slice>We would like to choose a line
for which the predictions are</text_slice>
            </slice>
            <slice>
              <time_slice>11:43</time_slice>
              <text_slice>as good as possible.</text_slice>
            </slice>
            <slice>
              <time_slice>11:45</time_slice>
              <text_slice>And what do we mean by
as good as possible?</text_slice>
            </slice>
            <slice>
              <time_slice>11:47</time_slice>
              <text_slice>As our criteria we're going
to take the following.</text_slice>
            </slice>
            <slice>
              <time_slice>11:51</time_slice>
              <text_slice>We are going to look at the
prediction error that our</text_slice>
            </slice>
            <slice>
              <time_slice>11:54</time_slice>
              <text_slice>model makes for each
particular student.</text_slice>
            </slice>
            <slice>
              <time_slice>11:56</time_slice>
              <text_slice>Take the square of that, and
then add them up over all of</text_slice>
            </slice>
            <slice>
              <time_slice>12:01</time_slice>
              <text_slice>our data points.</text_slice>
            </slice>
            <slice>
              <time_slice>12:02</time_slice>
              <text_slice>So what we're looking at is
the sum of this quantity</text_slice>
            </slice>
            <slice>
              <time_slice>12:06</time_slice>
              <text_slice>squared, that quantity squared,
that quantity</text_slice>
            </slice>
            <slice>
              <time_slice>12:08</time_slice>
              <text_slice>squared, and so on.</text_slice>
            </slice>
            <slice>
              <time_slice>12:09</time_slice>
              <text_slice>We add all of these squares, and
we would like to find the</text_slice>
            </slice>
            <slice>
              <time_slice>12:13</time_slice>
              <text_slice>line for which the sum of
these squared prediction</text_slice>
            </slice>
            <slice>
              <time_slice>12:17</time_slice>
              <text_slice>errors are as small
as possible.</text_slice>
            </slice>
            <slice>
              <time_slice>12:20</time_slice>
              <text_slice>So that's the procedure.</text_slice>
            </slice>
            <slice>
              <time_slice>12:23</time_slice>
              <text_slice>We have our data, the
X's and the Y's.</text_slice>
            </slice>
            <slice>
              <time_slice>12:27</time_slice>
              <text_slice>And we're going to find theta's
the best model of this</text_slice>
            </slice>
            <slice>
              <time_slice>12:31</time_slice>
              <text_slice>type, the best possible model,
by minimizing this sum of</text_slice>
            </slice>
            <slice>
              <time_slice>12:35</time_slice>
              <text_slice>squared errors.</text_slice>
            </slice>
            <slice>
              <time_slice>12:38</time_slice>
              <text_slice>So that's a method that one
could pull out of the hat and</text_slice>
            </slice>
            <slice>
              <time_slice>12:41</time_slice>
              <text_slice>say OK, that's how I'm going
to build my model.</text_slice>
            </slice>
            <slice>
              <time_slice>12:44</time_slice>
              <text_slice>And it sounds pretty
reasonable.</text_slice>
            </slice>
            <slice>
              <time_slice>12:46</time_slice>
              <text_slice>And it sounds pretty reasonable
even if you don't</text_slice>
            </slice>
            <slice>
              <time_slice>12:49</time_slice>
              <text_slice>know anything about
probability.</text_slice>
            </slice>
            <slice>
              <time_slice>12:51</time_slice>
              <text_slice>But does it have some
probabilistic justification?</text_slice>
            </slice>
            <slice>
              <time_slice>12:55</time_slice>
              <text_slice>It turns out that yes, you can
motivate this method with</text_slice>
            </slice>
            <slice>
              <time_slice>12:59</time_slice>
              <text_slice>probabilistic considerations
under certain assumptions.</text_slice>
            </slice>
            <slice>
              <time_slice>13:03</time_slice>
              <text_slice>So let's make a probabilistic
model that's going to lead us</text_slice>
            </slice>
            <slice>
              <time_slice>13:07</time_slice>
              <text_slice>to these particular way of
estimating the parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>13:10</time_slice>
              <text_slice>So here's a probabilistic
model.</text_slice>
            </slice>
            <slice>
              <time_slice>13:12</time_slice>
              <text_slice>I pick a student who had
a specific SAT score.</text_slice>
            </slice>
            <slice>
              <time_slice>13:18</time_slice>
              <text_slice>And that could be done at
random, but also could be done</text_slice>
            </slice>
            <slice>
              <time_slice>13:21</time_slice>
              <text_slice>in a systematic way.</text_slice>
            </slice>
            <slice>
              <time_slice>13:22</time_slice>
              <text_slice>That is, I pick a student who
had an SAT of 600, a student</text_slice>
            </slice>
            <slice>
              <time_slice>13:25</time_slice>
              <text_slice>of 610 all the way to 1,400
or 1,600, whatever the</text_slice>
            </slice>
            <slice>
              <time_slice>13:33</time_slice>
              <text_slice>right number is.</text_slice>
            </slice>
            <slice>
              <time_slice>13:34</time_slice>
              <text_slice>I pick all those students.</text_slice>
            </slice>
            <slice>
              <time_slice>13:36</time_slice>
              <text_slice>And I assume that for a student
of this kind there's a</text_slice>
            </slice>
            <slice>
              <time_slice>13:40</time_slice>
              <text_slice>true model that tells me that
their GPA is going to be a</text_slice>
            </slice>
            <slice>
              <time_slice>13:44</time_slice>
              <text_slice>random variable, which is
something predicted by their</text_slice>
            </slice>
            <slice>
              <time_slice>13:48</time_slice>
              <text_slice>SAT score plus some randomness,
some random noise.</text_slice>
            </slice>
            <slice>
              <time_slice>13:52</time_slice>
              <text_slice>And I model that random noise
by independent normal random</text_slice>
            </slice>
            <slice>
              <time_slice>13:56</time_slice>
              <text_slice>variables with 0 mean and
a certain variance.</text_slice>
            </slice>
            <slice>
              <time_slice>14:00</time_slice>
              <text_slice>So this is a specific
probabilistic model, and now I</text_slice>
            </slice>
            <slice>
              <time_slice>14:04</time_slice>
              <text_slice>can think about doing maximum
likelihood estimation for this</text_slice>
            </slice>
            <slice>
              <time_slice>14:09</time_slice>
              <text_slice>particular model.</text_slice>
            </slice>
            <slice>
              <time_slice>14:10</time_slice>
              <text_slice>So to do maximum likelihood
estimation here I need to</text_slice>
            </slice>
            <slice>
              <time_slice>14:14</time_slice>
              <text_slice>write down the likelihood of the
y's that I have observed.</text_slice>
            </slice>
            <slice>
              <time_slice>14:19</time_slice>
              <text_slice>What's the likelihood of the
y's that I have observed?</text_slice>
            </slice>
            <slice>
              <time_slice>14:23</time_slice>
              <text_slice>Well, a particular w has a
likelihood of the form e to</text_slice>
            </slice>
            <slice>
              <time_slice>14:28</time_slice>
              <text_slice>the minus w squared over
(2 sigma-squared).</text_slice>
            </slice>
            <slice>
              <time_slice>14:33</time_slice>
              <text_slice>That's the likelihood
of a particular w.</text_slice>
            </slice>
            <slice>
              <time_slice>14:37</time_slice>
              <text_slice>The probability, or the
likelihood of observing a</text_slice>
            </slice>
            <slice>
              <time_slice>14:40</time_slice>
              <text_slice>particular value of y, that's
the same as the likelihood</text_slice>
            </slice>
            <slice>
              <time_slice>14:43</time_slice>
              <text_slice>that w takes a value of y
minus this, minus that.</text_slice>
            </slice>
            <slice>
              <time_slice>14:49</time_slice>
              <text_slice>So the likelihood of the
y's is of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>14:52</time_slice>
              <text_slice>Think of this as just being
the w_i-squared.</text_slice>
            </slice>
            <slice>
              <time_slice>14:57</time_slice>
              <text_slice>So this is the density --</text_slice>
            </slice>
            <slice>
              <time_slice>15:01</time_slice>
              <text_slice>and if we have multiple data you
multiply the likelihoods</text_slice>
            </slice>
            <slice>
              <time_slice>15:06</time_slice>
              <text_slice>of the different y's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:07</time_slice>
              <text_slice>So you have to write something
like this.</text_slice>
            </slice>
            <slice>
              <time_slice>15:12</time_slice>
              <text_slice>Since the w's are independent
that means that the y's are</text_slice>
            </slice>
            <slice>
              <time_slice>15:16</time_slice>
              <text_slice>also independent.</text_slice>
            </slice>
            <slice>
              <time_slice>15:17</time_slice>
              <text_slice>The likelihood of a y vector
is the product of the</text_slice>
            </slice>
            <slice>
              <time_slice>15:21</time_slice>
              <text_slice>likelihoods of the
individual y's.</text_slice>
            </slice>
            <slice>
              <time_slice>15:24</time_slice>
              <text_slice>The likelihood of every
individual y is of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>15:27</time_slice>
              <text_slice>Where w is y_i minus these
two quantities.</text_slice>
            </slice>
            <slice>
              <time_slice>15:33</time_slice>
              <text_slice>So this is the form that the
likelihood function is going</text_slice>
            </slice>
            <slice>
              <time_slice>15:36</time_slice>
              <text_slice>to take under this
particular model.</text_slice>
            </slice>
            <slice>
              <time_slice>15:38</time_slice>
              <text_slice>And under the maximum likelihood
methodology we want</text_slice>
            </slice>
            <slice>
              <time_slice>15:42</time_slice>
              <text_slice>to maximize this quantity with
respect to theta0 and theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>15:49</time_slice>
              <text_slice>Now to do this maximization you
might as well consider the</text_slice>
            </slice>
            <slice>
              <time_slice>15:56</time_slice>
              <text_slice>logarithm and maximize the
logarithm, which is just the</text_slice>
            </slice>
            <slice>
              <time_slice>16:00</time_slice>
              <text_slice>exponent up here.</text_slice>
            </slice>
            <slice>
              <time_slice>16:02</time_slice>
              <text_slice>Maximizing this exponent because
we have a minus sign</text_slice>
            </slice>
            <slice>
              <time_slice>16:05</time_slice>
              <text_slice>is the same as minimizing
the exponent</text_slice>
            </slice>
            <slice>
              <time_slice>16:08</time_slice>
              <text_slice>without the minus sign.</text_slice>
            </slice>
            <slice>
              <time_slice>16:10</time_slice>
              <text_slice>Sigma squared is a constant.</text_slice>
            </slice>
            <slice>
              <time_slice>16:12</time_slice>
              <text_slice>So what you end up doing is
minimizing this quantity here,</text_slice>
            </slice>
            <slice>
              <time_slice>16:17</time_slice>
              <text_slice>which is the same as
what we had in our</text_slice>
            </slice>
            <slice>
              <time_slice>16:20</time_slice>
              <text_slice>linear regression methods.</text_slice>
            </slice>
            <slice>
              <time_slice>16:23</time_slice>
              <text_slice>So in conclusion you might
choose to do linear regression</text_slice>
            </slice>
            <slice>
              <time_slice>16:29</time_slice>
              <text_slice>in this particular way,
just because it looks</text_slice>
            </slice>
            <slice>
              <time_slice>16:34</time_slice>
              <text_slice>reasonable or plausible.</text_slice>
            </slice>
            <slice>
              <time_slice>16:36</time_slice>
              <text_slice>Or you might interpret what
you're doing as maximum</text_slice>
            </slice>
            <slice>
              <time_slice>16:41</time_slice>
              <text_slice>likelihood estimation, in which
you assume a model of</text_slice>
            </slice>
            <slice>
              <time_slice>16:45</time_slice>
              <text_slice>this kind where the noise
terms are normal random</text_slice>
            </slice>
            <slice>
              <time_slice>16:49</time_slice>
              <text_slice>variables with the same
distribution --</text_slice>
            </slice>
            <slice>
              <time_slice>16:51</time_slice>
              <text_slice>independent identically
distributed.</text_slice>
            </slice>
            <slice>
              <time_slice>16:54</time_slice>
              <text_slice>So linear regression implicitly
makes an assumption</text_slice>
            </slice>
            <slice>
              <time_slice>17:01</time_slice>
              <text_slice>of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>17:02</time_slice>
              <text_slice>It's doing maximum likelihood
estimation as if the world was</text_slice>
            </slice>
            <slice>
              <time_slice>17:07</time_slice>
              <text_slice>really described by a model of
this form, and with the W's</text_slice>
            </slice>
            <slice>
              <time_slice>17:11</time_slice>
              <text_slice>being random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>17:12</time_slice>
              <text_slice>So this gives us at least some
justification that this</text_slice>
            </slice>
            <slice>
              <time_slice>17:17</time_slice>
              <text_slice>particular approach to fitting
lines to data is not so</text_slice>
            </slice>
            <slice>
              <time_slice>17:21</time_slice>
              <text_slice>arbitrary, but it has
a sound footing.</text_slice>
            </slice>
            <slice>
              <time_slice>17:25</time_slice>
              <text_slice>OK so then once you accept this
formulation as being a</text_slice>
            </slice>
            <slice>
              <time_slice>17:30</time_slice>
              <text_slice>reasonable one what's
the next step?</text_slice>
            </slice>
            <slice>
              <time_slice>17:32</time_slice>
              <text_slice>The next step is to see how to
carry out this minimization.</text_slice>
            </slice>
            <slice>
              <time_slice>17:37</time_slice>
              <text_slice>This is not a very difficult
minimization to do.</text_slice>
            </slice>
            <slice>
              <time_slice>17:42</time_slice>
              <text_slice>The way it's done is by setting
the derivatives of</text_slice>
            </slice>
            <slice>
              <time_slice>17:48</time_slice>
              <text_slice>this expression to 0.</text_slice>
            </slice>
            <slice>
              <time_slice>17:50</time_slice>
              <text_slice>Now because this is a quadratic
function of theta0</text_slice>
            </slice>
            <slice>
              <time_slice>17:54</time_slice>
              <text_slice>and theta1--</text_slice>
            </slice>
            <slice>
              <time_slice>17:55</time_slice>
              <text_slice>when you take the derivatives
with respect</text_slice>
            </slice>
            <slice>
              <time_slice>17:57</time_slice>
              <text_slice>to theta0 and theta1--</text_slice>
            </slice>
            <slice>
              <time_slice>17:58</time_slice>
              <text_slice>you get linear functions
of theta0 and theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>18:03</time_slice>
              <text_slice>And you end up solving a system
of linear equations in</text_slice>
            </slice>
            <slice>
              <time_slice>18:08</time_slice>
              <text_slice>theta0 and theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>18:09</time_slice>
              <text_slice>And it turns out that there's
very nice and simple formulas</text_slice>
            </slice>
            <slice>
              <time_slice>18:15</time_slice>
              <text_slice>for the optimal estimates
of the parameters in</text_slice>
            </slice>
            <slice>
              <time_slice>18:18</time_slice>
              <text_slice>terms of the data.</text_slice>
            </slice>
            <slice>
              <time_slice>18:20</time_slice>
              <text_slice>And the formulas
are these ones.</text_slice>
            </slice>
            <slice>
              <time_slice>18:23</time_slice>
              <text_slice>I said that these are nice
and simple formulas.</text_slice>
            </slice>
            <slice>
              <time_slice>18:28</time_slice>
              <text_slice>Let's see why.</text_slice>
            </slice>
            <slice>
              <time_slice>18:29</time_slice>
              <text_slice>How can we interpret them?</text_slice>
            </slice>
            <slice>
              <time_slice>18:34</time_slice>
              <text_slice>So suppose that the world is
described by a model of this</text_slice>
            </slice>
            <slice>
              <time_slice>18:42</time_slice>
              <text_slice>kind, where the X's and Y's
are random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>18:48</time_slice>
              <text_slice>And where W is a noise term
that's independent of X. So</text_slice>
            </slice>
            <slice>
              <time_slice>18:53</time_slice>
              <text_slice>we're assuming that a linear
model is indeed true, but not</text_slice>
            </slice>
            <slice>
              <time_slice>18:57</time_slice>
              <text_slice>exactly true.</text_slice>
            </slice>
            <slice>
              <time_slice>18:58</time_slice>
              <text_slice>There's always some noise
associated with any particular</text_slice>
            </slice>
            <slice>
              <time_slice>19:01</time_slice>
              <text_slice>data point that we obtain.</text_slice>
            </slice>
            <slice>
              <time_slice>19:04</time_slice>
              <text_slice>So if a model of this kind is
true, and the W's have 0 mean</text_slice>
            </slice>
            <slice>
              <time_slice>19:10</time_slice>
              <text_slice>then we have that the expected
value of Y would be theta0</text_slice>
            </slice>
            <slice>
              <time_slice>19:15</time_slice>
              <text_slice>plus theta1 expected value of
X. And because W has 0 mean</text_slice>
            </slice>
            <slice>
              <time_slice>19:23</time_slice>
              <text_slice>there's no extra term.</text_slice>
            </slice>
            <slice>
              <time_slice>19:26</time_slice>
              <text_slice>So in particular, theta0 would
be equal to expected value of</text_slice>
            </slice>
            <slice>
              <time_slice>19:31</time_slice>
              <text_slice>Y minus theta1 expected
value of X.</text_slice>
            </slice>
            <slice>
              <time_slice>19:37</time_slice>
              <text_slice>So let's use this equation
to try to come up with a</text_slice>
            </slice>
            <slice>
              <time_slice>19:40</time_slice>
              <text_slice>reasonable estimate of theta0.</text_slice>
            </slice>
            <slice>
              <time_slice>19:44</time_slice>
              <text_slice>I do not know the expected
value of Y, but I</text_slice>
            </slice>
            <slice>
              <time_slice>19:47</time_slice>
              <text_slice>can estimate it.</text_slice>
            </slice>
            <slice>
              <time_slice>19:48</time_slice>
              <text_slice>How do I estimate it?</text_slice>
            </slice>
            <slice>
              <time_slice>19:49</time_slice>
              <text_slice>I look at the average of all the
y's that I have obtained.</text_slice>
            </slice>
            <slice>
              <time_slice>19:53</time_slice>
              <text_slice>so I replace this, I estimate
it with the average of the</text_slice>
            </slice>
            <slice>
              <time_slice>19:57</time_slice>
              <text_slice>data I have seen.</text_slice>
            </slice>
            <slice>
              <time_slice>19:59</time_slice>
              <text_slice>Here, similarly with the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>20:02</time_slice>
              <text_slice>I might not know the expected
value of X's, but I have data</text_slice>
            </slice>
            <slice>
              <time_slice>20:06</time_slice>
              <text_slice>points for the x's.</text_slice>
            </slice>
            <slice>
              <time_slice>20:08</time_slice>
              <text_slice>I look at the average of all my
data points, I come up with</text_slice>
            </slice>
            <slice>
              <time_slice>20:13</time_slice>
              <text_slice>an estimate of this
expectation.</text_slice>
            </slice>
            <slice>
              <time_slice>20:16</time_slice>
              <text_slice>Now I don't know what theta1 is,
but my procedure is going</text_slice>
            </slice>
            <slice>
              <time_slice>20:21</time_slice>
              <text_slice>to generate an estimate of
theta1 called theta1 hat.</text_slice>
            </slice>
            <slice>
              <time_slice>20:25</time_slice>
              <text_slice>And once I have this estimate,
then a reasonable person would</text_slice>
            </slice>
            <slice>
              <time_slice>20:29</time_slice>
              <text_slice>estimate theta0 in this
particular way.</text_slice>
            </slice>
            <slice>
              <time_slice>20:33</time_slice>
              <text_slice>So that's how my estimate
of theta0 is going to be</text_slice>
            </slice>
            <slice>
              <time_slice>20:37</time_slice>
              <text_slice>constructed.</text_slice>
            </slice>
            <slice>
              <time_slice>20:38</time_slice>
              <text_slice>It's this formula here.</text_slice>
            </slice>
            <slice>
              <time_slice>20:41</time_slice>
              <text_slice>We have not yet addressed the
harder question, which is how</text_slice>
            </slice>
            <slice>
              <time_slice>20:44</time_slice>
              <text_slice>to estimate theta1 in
the first place.</text_slice>
            </slice>
            <slice>
              <time_slice>20:47</time_slice>
              <text_slice>So to estimate theta0 I assumed
that I already had an</text_slice>
            </slice>
            <slice>
              <time_slice>20:50</time_slice>
              <text_slice>estimate for a theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>20:55</time_slice>
              <text_slice>OK, the right formula for the
estimate of theta1 happens to</text_slice>
            </slice>
            <slice>
              <time_slice>21:02</time_slice>
              <text_slice>be this one.</text_slice>
            </slice>
            <slice>
              <time_slice>21:03</time_slice>
              <text_slice>It looks messy, but let's
try to interpret it.</text_slice>
            </slice>
            <slice>
              <time_slice>21:08</time_slice>
              <text_slice>What I'm going to do is I'm
going to take this model for</text_slice>
            </slice>
            <slice>
              <time_slice>21:12</time_slice>
              <text_slice>simplicity let's assume that
they're the random variables</text_slice>
            </slice>
            <slice>
              <time_slice>21:18</time_slice>
              <text_slice>have 0 means.</text_slice>
            </slice>
            <slice>
              <time_slice>21:22</time_slice>
              <text_slice>And see how we might estimate
how we might</text_slice>
            </slice>
            <slice>
              <time_slice>21:28</time_slice>
              <text_slice>try to estimate theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>21:30</time_slice>
              <text_slice>Let's multiply both sides of
this equation by X. So we get</text_slice>
            </slice>
            <slice>
              <time_slice>21:36</time_slice>
              <text_slice>Y times X equals theta0 plus
theta0 times X plus theta1</text_slice>
            </slice>
            <slice>
              <time_slice>21:48</time_slice>
              <text_slice>times X-squared, plus X times
W. And now take expectations</text_slice>
            </slice>
            <slice>
              <time_slice>21:54</time_slice>
              <text_slice>of both sides.</text_slice>
            </slice>
            <slice>
              <time_slice>21:56</time_slice>
              <text_slice>If I have 0 mean random
variables the expected value</text_slice>
            </slice>
            <slice>
              <time_slice>22:00</time_slice>
              <text_slice>of Y times X is just the
covariance of X with Y.</text_slice>
            </slice>
            <slice>
              <time_slice>22:07</time_slice>
              <text_slice>I have assumed that my random
variables have 0 means, so the</text_slice>
            </slice>
            <slice>
              <time_slice>22:10</time_slice>
              <text_slice>expectation of this is 0.</text_slice>
            </slice>
            <slice>
              <time_slice>22:13</time_slice>
              <text_slice>This one is going to be the
variance of X, so I have</text_slice>
            </slice>
            <slice>
              <time_slice>22:17</time_slice>
              <text_slice>theta1 times variance of X. And
since I'm assuming that my</text_slice>
            </slice>
            <slice>
              <time_slice>22:23</time_slice>
              <text_slice>random variables have 0 mean,
and I'm also assuming that W</text_slice>
            </slice>
            <slice>
              <time_slice>22:26</time_slice>
              <text_slice>is independent of X this last
term also has 0 mean.</text_slice>
            </slice>
            <slice>
              <time_slice>22:32</time_slice>
              <text_slice>So under such a probabilistic
model this equation is true.</text_slice>
            </slice>
            <slice>
              <time_slice>22:39</time_slice>
              <text_slice>If we knew the variance and the
covariance then we would</text_slice>
            </slice>
            <slice>
              <time_slice>22:43</time_slice>
              <text_slice>know the value of theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>22:45</time_slice>
              <text_slice>But we only have data, we do
not necessarily know the</text_slice>
            </slice>
            <slice>
              <time_slice>22:49</time_slice>
              <text_slice>variance and the covariance,
but we can estimate it.</text_slice>
            </slice>
            <slice>
              <time_slice>22:53</time_slice>
              <text_slice>What's a reasonable estimate
of the variance?</text_slice>
            </slice>
            <slice>
              <time_slice>22:55</time_slice>
              <text_slice>The reasonable estimate of the
variance is this quantity here</text_slice>
            </slice>
            <slice>
              <time_slice>22:59</time_slice>
              <text_slice>divided by n, and the reasonable
estimate of the</text_slice>
            </slice>
            <slice>
              <time_slice>23:03</time_slice>
              <text_slice>covariance is that numerator
divided by n.</text_slice>
            </slice>
            <slice>
              <time_slice>23:09</time_slice>
              <text_slice>So this is my estimate
of the mean.</text_slice>
            </slice>
            <slice>
              <time_slice>23:11</time_slice>
              <text_slice>I'm looking at the squared
distances from the mean, and I</text_slice>
            </slice>
            <slice>
              <time_slice>23:15</time_slice>
              <text_slice>average them over lots
and lots of data.</text_slice>
            </slice>
            <slice>
              <time_slice>23:18</time_slice>
              <text_slice>This is the most reasonable way
of estimating the variance</text_slice>
            </slice>
            <slice>
              <time_slice>23:23</time_slice>
              <text_slice>of our distribution.</text_slice>
            </slice>
            <slice>
              <time_slice>23:26</time_slice>
              <text_slice>And similarly the expected value
of this quantity is the</text_slice>
            </slice>
            <slice>
              <time_slice>23:31</time_slice>
              <text_slice>covariance of X with Y, and then
we have lots and lots of</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>data points.</text_slice>
            </slice>
            <slice>
              <time_slice>23:35</time_slice>
              <text_slice>This quantity here is going to
be a very good estimate of the</text_slice>
            </slice>
            <slice>
              <time_slice>23:38</time_slice>
              <text_slice>covariance.</text_slice>
            </slice>
            <slice>
              <time_slice>23:40</time_slice>
              <text_slice>So basically what this
formula does is--</text_slice>
            </slice>
            <slice>
              <time_slice>23:44</time_slice>
              <text_slice>one way of thinking about it--</text_slice>
            </slice>
            <slice>
              <time_slice>23:46</time_slice>
              <text_slice>is that it starts from this
relation which is true</text_slice>
            </slice>
            <slice>
              <time_slice>23:50</time_slice>
              <text_slice>exactly, but estimates the
covariance and the variance on</text_slice>
            </slice>
            <slice>
              <time_slice>23:57</time_slice>
              <text_slice>the basis of the data, and then
using these estimates to</text_slice>
            </slice>
            <slice>
              <time_slice>24:00</time_slice>
              <text_slice>come up with an estimate
of theta1.</text_slice>
            </slice>
            <slice>
              <time_slice>24:05</time_slice>
              <text_slice>So this gives us a probabilistic
interpretation</text_slice>
            </slice>
            <slice>
              <time_slice>24:09</time_slice>
              <text_slice>of the formulas that we have for
the way that the estimates</text_slice>
            </slice>
            <slice>
              <time_slice>24:13</time_slice>
              <text_slice>are constructed.</text_slice>
            </slice>
            <slice>
              <time_slice>24:14</time_slice>
              <text_slice>If you're willing to assume that
this is the true model of</text_slice>
            </slice>
            <slice>
              <time_slice>24:19</time_slice>
              <text_slice>the world, the structure of the
true model of the world,</text_slice>
            </slice>
            <slice>
              <time_slice>24:22</time_slice>
              <text_slice>except that you do not
know means and</text_slice>
            </slice>
            <slice>
              <time_slice>24:24</time_slice>
              <text_slice>covariances, and variances.</text_slice>
            </slice>
            <slice>
              <time_slice>24:27</time_slice>
              <text_slice>Then this is a natural way of
estimating those unknown</text_slice>
            </slice>
            <slice>
              <time_slice>24:33</time_slice>
              <text_slice>parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>24:36</time_slice>
              <text_slice>All right, so we have a
closed-form formula, we can</text_slice>
            </slice>
            <slice>
              <time_slice>24:39</time_slice>
              <text_slice>apply it whenever
we have data.</text_slice>
            </slice>
            <slice>
              <time_slice>24:43</time_slice>
              <text_slice>Now linear regression is a
subject on which there are</text_slice>
            </slice>
            <slice>
              <time_slice>24:47</time_slice>
              <text_slice>whole courses, and whole
books that are given.</text_slice>
            </slice>
            <slice>
              <time_slice>24:51</time_slice>
              <text_slice>And the reason for that is that
there's a lot more that</text_slice>
            </slice>
            <slice>
              <time_slice>24:54</time_slice>
              <text_slice>you can bring into the topic,
and many ways that you can</text_slice>
            </slice>
            <slice>
              <time_slice>24:58</time_slice>
              <text_slice>elaborate on the simple solution
that we got for the</text_slice>
            </slice>
            <slice>
              <time_slice>25:02</time_slice>
              <text_slice>case of two parameters and only
two random variables.</text_slice>
            </slice>
            <slice>
              <time_slice>25:05</time_slice>
              <text_slice>So let me give you a little bit
of flavor of what are the</text_slice>
            </slice>
            <slice>
              <time_slice>25:09</time_slice>
              <text_slice>topics that come up when you
start looking into linear</text_slice>
            </slice>
            <slice>
              <time_slice>25:12</time_slice>
              <text_slice>regression in more depth.</text_slice>
            </slice>
            <slice>
              <time_slice>25:16</time_slice>
              <text_slice>So in our discussions so far
we made the linear model in</text_slice>
            </slice>
            <slice>
              <time_slice>25:24</time_slice>
              <text_slice>which we're trying to explain
the values of one variable in</text_slice>
            </slice>
            <slice>
              <time_slice>25:28</time_slice>
              <text_slice>terms of the values of
another variable.</text_slice>
            </slice>
            <slice>
              <time_slice>25:30</time_slice>
              <text_slice>We're trying to explain GPAs
in terms of SAT scores, or</text_slice>
            </slice>
            <slice>
              <time_slice>25:35</time_slice>
              <text_slice>we're trying to predict GPAs
in terms of SAT scores.</text_slice>
            </slice>
            <slice>
              <time_slice>25:39</time_slice>
              <text_slice>But maybe your GPA is affected
by several factors.</text_slice>
            </slice>
            <slice>
              <time_slice>25:47</time_slice>
              <text_slice>For example maybe your GPA is
affected by your SAT score,</text_slice>
            </slice>
            <slice>
              <time_slice>25:56</time_slice>
              <text_slice>also the income of your family,
the years of education</text_slice>
            </slice>
            <slice>
              <time_slice>26:01</time_slice>
              <text_slice>of your grandmother, and many
other factors like that.</text_slice>
            </slice>
            <slice>
              <time_slice>26:06</time_slice>
              <text_slice>So you might write down a model
in which I believe that</text_slice>
            </slice>
            <slice>
              <time_slice>26:11</time_slice>
              <text_slice>GPA has a relation, which is a
linear function of all these</text_slice>
            </slice>
            <slice>
              <time_slice>26:17</time_slice>
              <text_slice>other variables that
I mentioned.</text_slice>
            </slice>
            <slice>
              <time_slice>26:20</time_slice>
              <text_slice>So perhaps you have a theory of
what determines performance</text_slice>
            </slice>
            <slice>
              <time_slice>26:24</time_slice>
              <text_slice>at college, and you want to
build a model of that type.</text_slice>
            </slice>
            <slice>
              <time_slice>26:29</time_slice>
              <text_slice>How do we go about
in this case?</text_slice>
            </slice>
            <slice>
              <time_slice>26:31</time_slice>
              <text_slice>Well, again we collect
the data points.</text_slice>
            </slice>
            <slice>
              <time_slice>26:33</time_slice>
              <text_slice>We look at the i-th student,
who has a college GPA.</text_slice>
            </slice>
            <slice>
              <time_slice>26:37</time_slice>
              <text_slice>We record their SAT score,
their family income, and</text_slice>
            </slice>
            <slice>
              <time_slice>26:42</time_slice>
              <text_slice>grandmother's years
of education.</text_slice>
            </slice>
            <slice>
              <time_slice>26:45</time_slice>
              <text_slice>So this is one data point that
is for one particular student.</text_slice>
            </slice>
            <slice>
              <time_slice>26:50</time_slice>
              <text_slice>We postulate the model
of this form.</text_slice>
            </slice>
            <slice>
              <time_slice>26:52</time_slice>
              <text_slice>For the i-th student this would
be the mistake that our</text_slice>
            </slice>
            <slice>
              <time_slice>26:56</time_slice>
              <text_slice>model makes if we have chosen
specific values for those</text_slice>
            </slice>
            <slice>
              <time_slice>26:59</time_slice>
              <text_slice>parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>27:01</time_slice>
              <text_slice>And then we go and choose the
parameters that are going to</text_slice>
            </slice>
            <slice>
              <time_slice>27:05</time_slice>
              <text_slice>give us, again, the
smallest possible</text_slice>
            </slice>
            <slice>
              <time_slice>27:07</time_slice>
              <text_slice>sum of squared errors.</text_slice>
            </slice>
            <slice>
              <time_slice>27:10</time_slice>
              <text_slice>So philosophically it's exactly
the same as what we</text_slice>
            </slice>
            <slice>
              <time_slice>27:12</time_slice>
              <text_slice>were discussing before, except
that now we're including</text_slice>
            </slice>
            <slice>
              <time_slice>27:15</time_slice>
              <text_slice>multiple explanatory variables
in our model instead of a</text_slice>
            </slice>
            <slice>
              <time_slice>27:19</time_slice>
              <text_slice>single explanatory variable.</text_slice>
            </slice>
            <slice>
              <time_slice>27:22</time_slice>
              <text_slice>So that's the formulation.</text_slice>
            </slice>
            <slice>
              <time_slice>27:24</time_slice>
              <text_slice>What do you do next?</text_slice>
            </slice>
            <slice>
              <time_slice>27:26</time_slice>
              <text_slice>Well, to do this minimization
you're going to take</text_slice>
            </slice>
            <slice>
              <time_slice>27:29</time_slice>
              <text_slice>derivatives once you have your
data, you have a function of</text_slice>
            </slice>
            <slice>
              <time_slice>27:32</time_slice>
              <text_slice>these three parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>27:34</time_slice>
              <text_slice>You take the derivative with
respect to the parameter, set</text_slice>
            </slice>
            <slice>
              <time_slice>27:37</time_slice>
              <text_slice>the derivative equal
to 0, you get the</text_slice>
            </slice>
            <slice>
              <time_slice>27:39</time_slice>
              <text_slice>system of linear equations.</text_slice>
            </slice>
            <slice>
              <time_slice>27:41</time_slice>
              <text_slice>You throw that system of
linear equations to the</text_slice>
            </slice>
            <slice>
              <time_slice>27:43</time_slice>
              <text_slice>computer, and you get numerical
values for the</text_slice>
            </slice>
            <slice>
              <time_slice>27:46</time_slice>
              <text_slice>optimal parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>27:48</time_slice>
              <text_slice>There are no nice closed-form
formulas of the type that we</text_slice>
            </slice>
            <slice>
              <time_slice>27:52</time_slice>
              <text_slice>had in the previous slide
when you're dealing</text_slice>
            </slice>
            <slice>
              <time_slice>27:54</time_slice>
              <text_slice>with multiple variables.</text_slice>
            </slice>
            <slice>
              <time_slice>27:56</time_slice>
              <text_slice>Unless you're willing to go
into matrix notation.</text_slice>
            </slice>
            <slice>
              <time_slice>28:02</time_slice>
              <text_slice>In that case you can again
write down closed-form</text_slice>
            </slice>
            <slice>
              <time_slice>28:04</time_slice>
              <text_slice>formulas, but they will be a
little less intuitive than</text_slice>
            </slice>
            <slice>
              <time_slice>28:07</time_slice>
              <text_slice>what we had before.</text_slice>
            </slice>
            <slice>
              <time_slice>28:09</time_slice>
              <text_slice>But the moral of the story is
that numerically this is a</text_slice>
            </slice>
            <slice>
              <time_slice>28:13</time_slice>
              <text_slice>procedure that's very easy.</text_slice>
            </slice>
            <slice>
              <time_slice>28:16</time_slice>
              <text_slice>It's a problem, an optimization
problem that the</text_slice>
            </slice>
            <slice>
              <time_slice>28:18</time_slice>
              <text_slice>computer can solve for you.</text_slice>
            </slice>
            <slice>
              <time_slice>28:20</time_slice>
              <text_slice>And it can solve it for
you very quickly.</text_slice>
            </slice>
            <slice>
              <time_slice>28:23</time_slice>
              <text_slice>Because all that it involves
is solving a</text_slice>
            </slice>
            <slice>
              <time_slice>28:25</time_slice>
              <text_slice>system of linear equations.</text_slice>
            </slice>
            <slice>
              <time_slice>28:29</time_slice>
              <text_slice>Now when you choose your
explanatory variables you may</text_slice>
            </slice>
            <slice>
              <time_slice>28:34</time_slice>
              <text_slice>have some choices.</text_slice>
            </slice>
            <slice>
              <time_slice>28:37</time_slice>
              <text_slice>One person may think that your
GPA a has something to do with</text_slice>
            </slice>
            <slice>
              <time_slice>28:43</time_slice>
              <text_slice>your SAT score.</text_slice>
            </slice>
            <slice>
              <time_slice>28:45</time_slice>
              <text_slice>Some other person may think that
your GPA has something to</text_slice>
            </slice>
            <slice>
              <time_slice>28:48</time_slice>
              <text_slice>do with the square of
your SAT score.</text_slice>
            </slice>
            <slice>
              <time_slice>28:51</time_slice>
              <text_slice>And that other person may
want to try to build a</text_slice>
            </slice>
            <slice>
              <time_slice>28:55</time_slice>
              <text_slice>model of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>28:58</time_slice>
              <text_slice>Now when would you want
to do this? ?</text_slice>
            </slice>
            <slice>
              <time_slice>29:01</time_slice>
              <text_slice>Suppose that the data that
you have looks like this.</text_slice>
            </slice>
            <slice>
              <time_slice>29:12</time_slice>
              <text_slice>If the data looks like this then
you might be tempted to</text_slice>
            </slice>
            <slice>
              <time_slice>29:15</time_slice>
              <text_slice>say well a linear model does
not look right, but maybe a</text_slice>
            </slice>
            <slice>
              <time_slice>29:20</time_slice>
              <text_slice>quadratic model will give me
a better fit for the data.</text_slice>
            </slice>
            <slice>
              <time_slice>29:25</time_slice>
              <text_slice>So if you want to fit a
quadratic model to the data</text_slice>
            </slice>
            <slice>
              <time_slice>29:30</time_slice>
              <text_slice>then what you do is you take
X-squared as your explanatory</text_slice>
            </slice>
            <slice>
              <time_slice>29:35</time_slice>
              <text_slice>variable instead of X, and you
build a model of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>29:42</time_slice>
              <text_slice>There's nothing really different
in models of this</text_slice>
            </slice>
            <slice>
              <time_slice>29:45</time_slice>
              <text_slice>kind compared to models
of that kind.</text_slice>
            </slice>
            <slice>
              <time_slice>29:48</time_slice>
              <text_slice>They are still linear models
because we have theta's</text_slice>
            </slice>
            <slice>
              <time_slice>29:54</time_slice>
              <text_slice>showing up in a linear
fashion.</text_slice>
            </slice>
            <slice>
              <time_slice>29:57</time_slice>
              <text_slice>What you take as your
explanatory variables, whether</text_slice>
            </slice>
            <slice>
              <time_slice>30:00</time_slice>
              <text_slice>it's X, whether it's X-squared,
or whether it's</text_slice>
            </slice>
            <slice>
              <time_slice>30:02</time_slice>
              <text_slice>some other function
that you chose.</text_slice>
            </slice>
            <slice>
              <time_slice>30:05</time_slice>
              <text_slice>Some general function h of X,
doesn't make a difference.</text_slice>
            </slice>
            <slice>
              <time_slice>30:09</time_slice>
              <text_slice>So think of you h of X as being
your new X. So you can</text_slice>
            </slice>
            <slice>
              <time_slice>30:14</time_slice>
              <text_slice>formulate the problem exactly
the same way, except that</text_slice>
            </slice>
            <slice>
              <time_slice>30:17</time_slice>
              <text_slice>instead of using X's you
choose h of X's.</text_slice>
            </slice>
            <slice>
              <time_slice>30:23</time_slice>
              <text_slice>So it's basically a question
do I want to build a model</text_slice>
            </slice>
            <slice>
              <time_slice>30:26</time_slice>
              <text_slice>that explains Y's based on the
values of X, or do I want to</text_slice>
            </slice>
            <slice>
              <time_slice>30:31</time_slice>
              <text_slice>build a model that explains Y's
on the basis of the values</text_slice>
            </slice>
            <slice>
              <time_slice>30:35</time_slice>
              <text_slice>of h of X. Which is the
right value to use?</text_slice>
            </slice>
            <slice>
              <time_slice>30:38</time_slice>
              <text_slice>And with this picture here,
we see that it can make a</text_slice>
            </slice>
            <slice>
              <time_slice>30:42</time_slice>
              <text_slice>difference.</text_slice>
            </slice>
            <slice>
              <time_slice>30:43</time_slice>
              <text_slice>A linear model in X might be
a poor fit, but a quadratic</text_slice>
            </slice>
            <slice>
              <time_slice>30:47</time_slice>
              <text_slice>model might give us
a better fit.</text_slice>
            </slice>
            <slice>
              <time_slice>30:49</time_slice>
              <text_slice>So this brings to the topic of
how to choose your functions h</text_slice>
            </slice>
            <slice>
              <time_slice>30:55</time_slice>
              <text_slice>of X if you're dealing with
a real world problem.</text_slice>
            </slice>
            <slice>
              <time_slice>30:59</time_slice>
              <text_slice>So in a real world problem
you're just given X's and Y's.</text_slice>
            </slice>
            <slice>
              <time_slice>31:03</time_slice>
              <text_slice>And you have the freedom
of building models of</text_slice>
            </slice>
            <slice>
              <time_slice>31:05</time_slice>
              <text_slice>any kind you want.</text_slice>
            </slice>
            <slice>
              <time_slice>31:07</time_slice>
              <text_slice>You have the freedom of choosing
a function h of X of</text_slice>
            </slice>
            <slice>
              <time_slice>31:11</time_slice>
              <text_slice>any type that you want.</text_slice>
            </slice>
            <slice>
              <time_slice>31:13</time_slice>
              <text_slice>So this turns out to be a quite</text_slice>
            </slice>
            <slice>
              <time_slice>31:14</time_slice>
              <text_slice>difficult and tricky topic.</text_slice>
            </slice>
            <slice>
              <time_slice>31:18</time_slice>
              <text_slice>Because you may be tempted
to overdo it.</text_slice>
            </slice>
            <slice>
              <time_slice>31:22</time_slice>
              <text_slice>For example, I got my 10 data
points, and I could say OK,</text_slice>
            </slice>
            <slice>
              <time_slice>31:28</time_slice>
              <text_slice>I'm going to choose an h of X.
I'm going to choose h of X and</text_slice>
            </slice>
            <slice>
              <time_slice>31:35</time_slice>
              <text_slice>actually multiple h's of X
to do a multiple linear</text_slice>
            </slice>
            <slice>
              <time_slice>31:40</time_slice>
              <text_slice>regression in which I'm going to
build a model that's uses a</text_slice>
            </slice>
            <slice>
              <time_slice>31:45</time_slice>
              <text_slice>10th degree polynomial.</text_slice>
            </slice>
            <slice>
              <time_slice>31:47</time_slice>
              <text_slice>If I choose to fit my data with
a 10th degree polynomial</text_slice>
            </slice>
            <slice>
              <time_slice>31:51</time_slice>
              <text_slice>I'm going to fit my data
perfectly, but I may obtain a</text_slice>
            </slice>
            <slice>
              <time_slice>31:54</time_slice>
              <text_slice>model is does something like
this, and goes through all my</text_slice>
            </slice>
            <slice>
              <time_slice>31:58</time_slice>
              <text_slice>data points.</text_slice>
            </slice>
            <slice>
              <time_slice>31:59</time_slice>
              <text_slice>So I can make my prediction
errors extremely small if I</text_slice>
            </slice>
            <slice>
              <time_slice>32:03</time_slice>
              <text_slice>use lots of parameters, and
if I choose my h functions</text_slice>
            </slice>
            <slice>
              <time_slice>32:08</time_slice>
              <text_slice>appropriately.</text_slice>
            </slice>
            <slice>
              <time_slice>32:09</time_slice>
              <text_slice>But clearly this would
be garbage.</text_slice>
            </slice>
            <slice>
              <time_slice>32:11</time_slice>
              <text_slice>If you get those data points,
and you say here's my model</text_slice>
            </slice>
            <slice>
              <time_slice>32:15</time_slice>
              <text_slice>that explains them.</text_slice>
            </slice>
            <slice>
              <time_slice>32:16</time_slice>
              <text_slice>That has a polynomial going up
and down, then you're probably</text_slice>
            </slice>
            <slice>
              <time_slice>32:21</time_slice>
              <text_slice>doing something wrong.</text_slice>
            </slice>
            <slice>
              <time_slice>32:22</time_slice>
              <text_slice>So choosing how complicated
those functions,</text_slice>
            </slice>
            <slice>
              <time_slice>32:26</time_slice>
              <text_slice>the h's, should be.</text_slice>
            </slice>
            <slice>
              <time_slice>32:27</time_slice>
              <text_slice>And how many explanatory
variables to use is a very</text_slice>
            </slice>
            <slice>
              <time_slice>32:32</time_slice>
              <text_slice>delicate and deep topic on which
there's deep theory that</text_slice>
            </slice>
            <slice>
              <time_slice>32:36</time_slice>
              <text_slice>tells you what you should do,
and what you shouldn't do.</text_slice>
            </slice>
            <slice>
              <time_slice>32:39</time_slice>
              <text_slice>But the main thing that one
should avoid doing is having</text_slice>
            </slice>
            <slice>
              <time_slice>32:43</time_slice>
              <text_slice>too many parameters in
your model when you</text_slice>
            </slice>
            <slice>
              <time_slice>32:46</time_slice>
              <text_slice>have too few data.</text_slice>
            </slice>
            <slice>
              <time_slice>32:48</time_slice>
              <text_slice>So if you only have 10 data
points, you shouldn't have 10</text_slice>
            </slice>
            <slice>
              <time_slice>32:52</time_slice>
              <text_slice>free parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>32:53</time_slice>
              <text_slice>With 10 free parameters you will
be able to fit your data</text_slice>
            </slice>
            <slice>
              <time_slice>32:56</time_slice>
              <text_slice>perfectly, but you wouldn't be
able to really rely on the</text_slice>
            </slice>
            <slice>
              <time_slice>33:00</time_slice>
              <text_slice>results that you are seeing.</text_slice>
            </slice>
            <slice>
              <time_slice>33:06</time_slice>
              <text_slice>OK, now in practice, when people
run linear regressions</text_slice>
            </slice>
            <slice>
              <time_slice>33:12</time_slice>
              <text_slice>they do not just give
point estimates for</text_slice>
            </slice>
            <slice>
              <time_slice>33:15</time_slice>
              <text_slice>the parameters theta.</text_slice>
            </slice>
            <slice>
              <time_slice>33:17</time_slice>
              <text_slice>But similar to what we did for
the case of estimating the</text_slice>
            </slice>
            <slice>
              <time_slice>33:20</time_slice>
              <text_slice>mean of a random variable you
might want to give confidence</text_slice>
            </slice>
            <slice>
              <time_slice>33:23</time_slice>
              <text_slice>intervals that sort of tell you
how much randomness there</text_slice>
            </slice>
            <slice>
              <time_slice>33:27</time_slice>
              <text_slice>is when you estimate each one of
the particular parameters.</text_slice>
            </slice>
            <slice>
              <time_slice>33:30</time_slice>
              <text_slice>There are formulas for building
confidence intervals</text_slice>
            </slice>
            <slice>
              <time_slice>33:33</time_slice>
              <text_slice>for the estimates
of the theta's.</text_slice>
            </slice>
            <slice>
              <time_slice>33:36</time_slice>
              <text_slice>We're not going to look
at them, it would</text_slice>
            </slice>
            <slice>
              <time_slice>33:38</time_slice>
              <text_slice>take too much time.</text_slice>
            </slice>
            <slice>
              <time_slice>33:39</time_slice>
              <text_slice>Also you might want to estimate
the variance in the</text_slice>
            </slice>
            <slice>
              <time_slice>33:44</time_slice>
              <text_slice>noise that you have
in your model.</text_slice>
            </slice>
            <slice>
              <time_slice>33:47</time_slice>
              <text_slice>That is if you are pretending
that your true model is of the</text_slice>
            </slice>
            <slice>
              <time_slice>33:52</time_slice>
              <text_slice>kind we were discussing before,
namely Y equals theta1</text_slice>
            </slice>
            <slice>
              <time_slice>33:57</time_slice>
              <text_slice>times X plus W, and W has a
variance sigma squared.</text_slice>
            </slice>
            <slice>
              <time_slice>34:02</time_slice>
              <text_slice>You might want to estimate this,
because it tells you</text_slice>
            </slice>
            <slice>
              <time_slice>34:05</time_slice>
              <text_slice>something about the model, and
this is called standard error.</text_slice>
            </slice>
            <slice>
              <time_slice>34:09</time_slice>
              <text_slice>It puts a limit on how
good predictions</text_slice>
            </slice>
            <slice>
              <time_slice>34:11</time_slice>
              <text_slice>your model can make.</text_slice>
            </slice>
            <slice>
              <time_slice>34:14</time_slice>
              <text_slice>Even if you have the correct
theta0 and theta1, and</text_slice>
            </slice>
            <slice>
              <time_slice>34:18</time_slice>
              <text_slice>somebody tells you X you can
make a prediction about Y, but</text_slice>
            </slice>
            <slice>
              <time_slice>34:22</time_slice>
              <text_slice>that prediction will
not be accurate.</text_slice>
            </slice>
            <slice>
              <time_slice>34:24</time_slice>
              <text_slice>Because there's this additional
randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>34:26</time_slice>
              <text_slice>And if that additional
randomness is big, then your</text_slice>
            </slice>
            <slice>
              <time_slice>34:29</time_slice>
              <text_slice>predictions will also have a
substantial error in them.</text_slice>
            </slice>
            <slice>
              <time_slice>34:33</time_slice>
              <text_slice>There's another quantity that
gets reported usually.</text_slice>
            </slice>
            <slice>
              <time_slice>34:38</time_slice>
              <text_slice>This is part of the computer
output that you get when you</text_slice>
            </slice>
            <slice>
              <time_slice>34:41</time_slice>
              <text_slice>use a statistical package which
is called R-square.</text_slice>
            </slice>
            <slice>
              <time_slice>34:45</time_slice>
              <text_slice>And its a measure of the
explanatory power of the model</text_slice>
            </slice>
            <slice>
              <time_slice>34:49</time_slice>
              <text_slice>that you have built
linear regression.</text_slice>
            </slice>
            <slice>
              <time_slice>34:52</time_slice>
              <text_slice>Using linear regression.</text_slice>
            </slice>
            <slice>
              <time_slice>34:55</time_slice>
              <text_slice>Instead of defining R-square
exactly, let me give you a</text_slice>
            </slice>
            <slice>
              <time_slice>35:01</time_slice>
              <text_slice>sort of analogous quantity
that's involved.</text_slice>
            </slice>
            <slice>
              <time_slice>35:05</time_slice>
              <text_slice>After you do your linear
regression you can look at the</text_slice>
            </slice>
            <slice>
              <time_slice>35:08</time_slice>
              <text_slice>following quantity.</text_slice>
            </slice>
            <slice>
              <time_slice>35:10</time_slice>
              <text_slice>You look at the variance of Y,
which is something that you</text_slice>
            </slice>
            <slice>
              <time_slice>35:15</time_slice>
              <text_slice>can estimate from data.</text_slice>
            </slice>
            <slice>
              <time_slice>35:17</time_slice>
              <text_slice>This is how much randomness
there is in Y. And compare it</text_slice>
            </slice>
            <slice>
              <time_slice>35:23</time_slice>
              <text_slice>with the randomness that you
have in Y, but conditioned on</text_slice>
            </slice>
            <slice>
              <time_slice>35:28</time_slice>
              <text_slice>X. So this quantity tells
me if I knew X how much</text_slice>
            </slice>
            <slice>
              <time_slice>35:35</time_slice>
              <text_slice>randomness would there
still be in my Y?</text_slice>
            </slice>
            <slice>
              <time_slice>35:39</time_slice>
              <text_slice>So if I know X, I have more
information, so Y is more</text_slice>
            </slice>
            <slice>
              <time_slice>35:43</time_slice>
              <text_slice>constrained.</text_slice>
            </slice>
            <slice>
              <time_slice>35:44</time_slice>
              <text_slice>There's less randomness in Y.
This is the randomness in Y if</text_slice>
            </slice>
            <slice>
              <time_slice>35:48</time_slice>
              <text_slice>I don't know anything about X.</text_slice>
            </slice>
            <slice>
              <time_slice>35:50</time_slice>
              <text_slice>So naturally this quantity would
be less than 1, and if</text_slice>
            </slice>
            <slice>
              <time_slice>35:54</time_slice>
              <text_slice>this quantity is small it would
mean that whenever I</text_slice>
            </slice>
            <slice>
              <time_slice>35:58</time_slice>
              <text_slice>know X then Y is very
well known.</text_slice>
            </slice>
            <slice>
              <time_slice>36:03</time_slice>
              <text_slice>Which essentially tells me that
knowing x allows me to</text_slice>
            </slice>
            <slice>
              <time_slice>36:07</time_slice>
              <text_slice>make very good predictions about
Y. Knowing X means that</text_slice>
            </slice>
            <slice>
              <time_slice>36:12</time_slice>
              <text_slice>I'm explaining away most
of the randomness in Y.</text_slice>
            </slice>
            <slice>
              <time_slice>36:17</time_slice>
              <text_slice>So if you read a statistical
study that uses linear</text_slice>
            </slice>
            <slice>
              <time_slice>36:22</time_slice>
              <text_slice>regression you might encounter
statements of the form 60% of</text_slice>
            </slice>
            <slice>
              <time_slice>36:29</time_slice>
              <text_slice>a student's GPA is explained
by the family income.</text_slice>
            </slice>
            <slice>
              <time_slice>36:36</time_slice>
              <text_slice>If you read the statements of
this kind it's really refers</text_slice>
            </slice>
            <slice>
              <time_slice>36:40</time_slice>
              <text_slice>to quantities of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>36:43</time_slice>
              <text_slice>Out of the total variance in Y,
how much variance is left</text_slice>
            </slice>
            <slice>
              <time_slice>36:47</time_slice>
              <text_slice>after we build our model?</text_slice>
            </slice>
            <slice>
              <time_slice>36:50</time_slice>
              <text_slice>So if only 40% of the variance
of Y is left after we build</text_slice>
            </slice>
            <slice>
              <time_slice>36:56</time_slice>
              <text_slice>our model, that means that
X explains 60% of the</text_slice>
            </slice>
            <slice>
              <time_slice>37:00</time_slice>
              <text_slice>variations in Y's.</text_slice>
            </slice>
            <slice>
              <time_slice>37:02</time_slice>
              <text_slice>So the idea is that
randomness in Y is</text_slice>
            </slice>
            <slice>
              <time_slice>37:06</time_slice>
              <text_slice>caused by multiple sources.</text_slice>
            </slice>
            <slice>
              <time_slice>37:09</time_slice>
              <text_slice>Our explanatory variable
and random noise.</text_slice>
            </slice>
            <slice>
              <time_slice>37:12</time_slice>
              <text_slice>And we ask the question what
percentage of the total</text_slice>
            </slice>
            <slice>
              <time_slice>37:15</time_slice>
              <text_slice>randomness in Y is explained by</text_slice>
            </slice>
            <slice>
              <time_slice>37:19</time_slice>
              <text_slice>variations in the X parameter?</text_slice>
            </slice>
            <slice>
              <time_slice>37:23</time_slice>
              <text_slice>And how much of the total
randomness in Y is attributed</text_slice>
            </slice>
            <slice>
              <time_slice>37:26</time_slice>
              <text_slice>just to random effects?</text_slice>
            </slice>
            <slice>
              <time_slice>37:30</time_slice>
              <text_slice>So if you have a model that
explains most of the variation</text_slice>
            </slice>
            <slice>
              <time_slice>37:34</time_slice>
              <text_slice>in Y then you can think that
you have a good model that</text_slice>
            </slice>
            <slice>
              <time_slice>37:37</time_slice>
              <text_slice>tells you something useful
about the real world.</text_slice>
            </slice>
            <slice>
              <time_slice>37:42</time_slice>
              <text_slice>Now there's lots of things that
can go wrong when you use</text_slice>
            </slice>
            <slice>
              <time_slice>37:45</time_slice>
              <text_slice>linear regression, and there's
many pitfalls.</text_slice>
            </slice>
            <slice>
              <time_slice>37:50</time_slice>
              <text_slice>One pitfall happens when you
have this situation that's</text_slice>
            </slice>
            <slice>
              <time_slice>37:56</time_slice>
              <text_slice>called heteroskedacisity.</text_slice>
            </slice>
            <slice>
              <time_slice>37:58</time_slice>
              <text_slice>So suppose your data
are of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>38:06</time_slice>
              <text_slice>So what's happening here?</text_slice>
            </slice>
            <slice>
              <time_slice>38:09</time_slice>
              <text_slice>You seem to have a linear model,
but when X is small you</text_slice>
            </slice>
            <slice>
              <time_slice>38:17</time_slice>
              <text_slice>have a very good model.</text_slice>
            </slice>
            <slice>
              <time_slice>38:19</time_slice>
              <text_slice>So this means that W has a small
variance when X is here.</text_slice>
            </slice>
            <slice>
              <time_slice>38:23</time_slice>
              <text_slice>On the other hand, when X is
there you have a lot of</text_slice>
            </slice>
            <slice>
              <time_slice>38:26</time_slice>
              <text_slice>randomness.</text_slice>
            </slice>
            <slice>
              <time_slice>38:27</time_slice>
              <text_slice>This would be a situation
in which the W's are not</text_slice>
            </slice>
            <slice>
              <time_slice>38:32</time_slice>
              <text_slice>identically distributed, but
the variance of the W's, of</text_slice>
            </slice>
            <slice>
              <time_slice>38:35</time_slice>
              <text_slice>the noise, has something
to do with the X's.</text_slice>
            </slice>
            <slice>
              <time_slice>38:40</time_slice>
              <text_slice>So with different regions of our
x-space we have different</text_slice>
            </slice>
            <slice>
              <time_slice>38:43</time_slice>
              <text_slice>amounts of noise.</text_slice>
            </slice>
            <slice>
              <time_slice>38:45</time_slice>
              <text_slice>What will go wrong in
this situation?</text_slice>
            </slice>
            <slice>
              <time_slice>38:47</time_slice>
              <text_slice>Since we're trying to minimize
sum of squared errors, we're</text_slice>
            </slice>
            <slice>
              <time_slice>38:51</time_slice>
              <text_slice>really paying attention
to the biggest errors.</text_slice>
            </slice>
            <slice>
              <time_slice>38:54</time_slice>
              <text_slice>Which will mean that we are
going to pay attention to</text_slice>
            </slice>
            <slice>
              <time_slice>38:57</time_slice>
              <text_slice>these data points, because
that's where the big errors</text_slice>
            </slice>
            <slice>
              <time_slice>38:59</time_slice>
              <text_slice>are going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>39:01</time_slice>
              <text_slice>So the linear regression
formulas will end up building</text_slice>
            </slice>
            <slice>
              <time_slice>39:04</time_slice>
              <text_slice>a model based on these data,
which are the most noisy ones.</text_slice>
            </slice>
            <slice>
              <time_slice>39:09</time_slice>
              <text_slice>Instead of those data that are
nicely stacked in order.</text_slice>
            </slice>
            <slice>
              <time_slice>39:14</time_slice>
              <text_slice>Clearly that's not to the
right thing to do.</text_slice>
            </slice>
            <slice>
              <time_slice>39:17</time_slice>
              <text_slice>So you need to change something,
and use the fact</text_slice>
            </slice>
            <slice>
              <time_slice>39:21</time_slice>
              <text_slice>that the variance of W changes
with the X's, and there are</text_slice>
            </slice>
            <slice>
              <time_slice>39:25</time_slice>
              <text_slice>ways of dealing with it.</text_slice>
            </slice>
            <slice>
              <time_slice>39:27</time_slice>
              <text_slice>It's something that one needs
to be careful about.</text_slice>
            </slice>
            <slice>
              <time_slice>39:31</time_slice>
              <text_slice>Another possibility of getting
into trouble is if you're</text_slice>
            </slice>
            <slice>
              <time_slice>39:34</time_slice>
              <text_slice>using multiple explanatory
variables that are very</text_slice>
            </slice>
            <slice>
              <time_slice>39:38</time_slice>
              <text_slice>closely related to each other.</text_slice>
            </slice>
            <slice>
              <time_slice>39:41</time_slice>
              <text_slice>So for example, suppose that I
tried to predict your GPA by</text_slice>
            </slice>
            <slice>
              <time_slice>39:47</time_slice>
              <text_slice>looking at your SAT the first
time that you took it plus</text_slice>
            </slice>
            <slice>
              <time_slice>39:54</time_slice>
              <text_slice>your SAT the second time that
you took your SATs.</text_slice>
            </slice>
            <slice>
              <time_slice>39:58</time_slice>
              <text_slice>I'm assuming that almost
everyone takes the</text_slice>
            </slice>
            <slice>
              <time_slice>40:00</time_slice>
              <text_slice>SAT more than once.</text_slice>
            </slice>
            <slice>
              <time_slice>40:02</time_slice>
              <text_slice>So suppose that you had
a model of this kind.</text_slice>
            </slice>
            <slice>
              <time_slice>40:05</time_slice>
              <text_slice>Well, SAT on your first try and
SAT on your second try are</text_slice>
            </slice>
            <slice>
              <time_slice>40:09</time_slice>
              <text_slice>very likely to be
fairly close.</text_slice>
            </slice>
            <slice>
              <time_slice>40:12</time_slice>
              <text_slice>And you could think of coming
up with estimates in which</text_slice>
            </slice>
            <slice>
              <time_slice>40:17</time_slice>
              <text_slice>this is ignored.</text_slice>
            </slice>
            <slice>
              <time_slice>40:19</time_slice>
              <text_slice>And you build a model based on
this, or an alternative model</text_slice>
            </slice>
            <slice>
              <time_slice>40:22</time_slice>
              <text_slice>in which this term is ignored,
and you make predictions based</text_slice>
            </slice>
            <slice>
              <time_slice>40:25</time_slice>
              <text_slice>on the second SAT.</text_slice>
            </slice>
            <slice>
              <time_slice>40:27</time_slice>
              <text_slice>And both models are likely to be
essentially as good as the</text_slice>
            </slice>
            <slice>
              <time_slice>40:31</time_slice>
              <text_slice>other one, because these
two quantities are</text_slice>
            </slice>
            <slice>
              <time_slice>40:34</time_slice>
              <text_slice>essentially the same.</text_slice>
            </slice>
            <slice>
              <time_slice>40:36</time_slice>
              <text_slice>So in that case, your theta's
that you estimate are going to</text_slice>
            </slice>
            <slice>
              <time_slice>40:41</time_slice>
              <text_slice>be very sensitive to little
details of the data.</text_slice>
            </slice>
            <slice>
              <time_slice>40:44</time_slice>
              <text_slice>You change your data, you have
your data, and your data tell</text_slice>
            </slice>
            <slice>
              <time_slice>40:48</time_slice>
              <text_slice>you that this coefficient
is big and that</text_slice>
            </slice>
            <slice>
              <time_slice>40:52</time_slice>
              <text_slice>coefficient is small.</text_slice>
            </slice>
            <slice>
              <time_slice>40:52</time_slice>
              <text_slice>You change your data just a
tiny bit, and your theta's</text_slice>
            </slice>
            <slice>
              <time_slice>40:56</time_slice>
              <text_slice>would drastically change.</text_slice>
            </slice>
            <slice>
              <time_slice>40:57</time_slice>
              <text_slice>So this is a case in which you
have multiple explanatory</text_slice>
            </slice>
            <slice>
              <time_slice>41:00</time_slice>
              <text_slice>variables, but they're redundant
in the sense that</text_slice>
            </slice>
            <slice>
              <time_slice>41:04</time_slice>
              <text_slice>they're very closely related
to each other, and perhaps</text_slice>
            </slice>
            <slice>
              <time_slice>41:07</time_slice>
              <text_slice>with a linear relation.</text_slice>
            </slice>
            <slice>
              <time_slice>41:08</time_slice>
              <text_slice>So one must be careful about the
situation, and do special</text_slice>
            </slice>
            <slice>
              <time_slice>41:11</time_slice>
              <text_slice>tests to make sure that
this doesn't happen.</text_slice>
            </slice>
            <slice>
              <time_slice>41:15</time_slice>
              <text_slice>Finally the biggest and most
common blunder is that you run</text_slice>
            </slice>
            <slice>
              <time_slice>41:20</time_slice>
              <text_slice>your linear regression, you
get your linear model, and</text_slice>
            </slice>
            <slice>
              <time_slice>41:24</time_slice>
              <text_slice>then you say oh, OK.</text_slice>
            </slice>
            <slice>
              <time_slice>41:26</time_slice>
              <text_slice>Y is caused by X according to
this particular formula.</text_slice>
            </slice>
            <slice>
              <time_slice>41:33</time_slice>
              <text_slice>Well, all that we did was to
identify a linear relation</text_slice>
            </slice>
            <slice>
              <time_slice>41:36</time_slice>
              <text_slice>between X and Y. This doesn't
tell us anything.</text_slice>
            </slice>
            <slice>
              <time_slice>41:40</time_slice>
              <text_slice>Whether it's Y that causes X, or
whether it's X that causes</text_slice>
            </slice>
            <slice>
              <time_slice>41:44</time_slice>
              <text_slice>Y, or maybe both X and Y are
caused by some other variable</text_slice>
            </slice>
            <slice>
              <time_slice>41:48</time_slice>
              <text_slice>that we didn't think about.</text_slice>
            </slice>
            <slice>
              <time_slice>41:51</time_slice>
              <text_slice>So building a good linear model
that has small errors</text_slice>
            </slice>
            <slice>
              <time_slice>41:56</time_slice>
              <text_slice>does not tell us anything about
causal relations between</text_slice>
            </slice>
            <slice>
              <time_slice>42:00</time_slice>
              <text_slice>the two variables.</text_slice>
            </slice>
            <slice>
              <time_slice>42:02</time_slice>
              <text_slice>It only tells us that there's
a close association between</text_slice>
            </slice>
            <slice>
              <time_slice>42:05</time_slice>
              <text_slice>the two variables.</text_slice>
            </slice>
            <slice>
              <time_slice>42:06</time_slice>
              <text_slice>If you know one you can make
predictions about the other.</text_slice>
            </slice>
            <slice>
              <time_slice>42:10</time_slice>
              <text_slice>But it doesn't tell you anything
about the underlying</text_slice>
            </slice>
            <slice>
              <time_slice>42:13</time_slice>
              <text_slice>physics, that there's some
physical mechanism that</text_slice>
            </slice>
            <slice>
              <time_slice>42:18</time_slice>
              <text_slice>introduces the relation between
those variables.</text_slice>
            </slice>
            <slice>
              <time_slice>42:22</time_slice>
              <text_slice>OK, that's it about
linear regression.</text_slice>
            </slice>
            <slice>
              <time_slice>42:26</time_slice>
              <text_slice>Let us start the next topic,
which is hypothesis testing.</text_slice>
            </slice>
            <slice>
              <time_slice>42:30</time_slice>
              <text_slice>And we're going to continue
with it next time.</text_slice>
            </slice>
            <slice>
              <time_slice>42:35</time_slice>
              <text_slice>So here, instead of trying
to estimate continuous</text_slice>
            </slice>
            <slice>
              <time_slice>42:37</time_slice>
              <text_slice>parameters, we have two
alternative hypotheses about</text_slice>
            </slice>
            <slice>
              <time_slice>42:41</time_slice>
              <text_slice>the distribution of the
X random variable.</text_slice>
            </slice>
            <slice>
              <time_slice>42:46</time_slice>
              <text_slice>So for example our random
variable could be either</text_slice>
            </slice>
            <slice>
              <time_slice>42:53</time_slice>
              <text_slice>distributed according to this
distribution, under H0, or it</text_slice>
            </slice>
            <slice>
              <time_slice>42:58</time_slice>
              <text_slice>might be distributed according
to this distribution under H1.</text_slice>
            </slice>
            <slice>
              <time_slice>43:02</time_slice>
              <text_slice>And we want to make a decision
which distribution is the</text_slice>
            </slice>
            <slice>
              <time_slice>43:06</time_slice>
              <text_slice>correct one?</text_slice>
            </slice>
            <slice>
              <time_slice>43:07</time_slice>
              <text_slice>So we're given those two
distributions, and some common</text_slice>
            </slice>
            <slice>
              <time_slice>43:10</time_slice>
              <text_slice>terminologies that one of them
is the null hypothesis--</text_slice>
            </slice>
            <slice>
              <time_slice>43:14</time_slice>
              <text_slice>sort of the default hypothesis,
and we have some</text_slice>
            </slice>
            <slice>
              <time_slice>43:16</time_slice>
              <text_slice>alternative hypotheses--</text_slice>
            </slice>
            <slice>
              <time_slice>43:18</time_slice>
              <text_slice>and we want to check whether
this one is true,</text_slice>
            </slice>
            <slice>
              <time_slice>43:20</time_slice>
              <text_slice>or that one is true.</text_slice>
            </slice>
            <slice>
              <time_slice>43:21</time_slice>
              <text_slice>So you obtain a data
point, and you</text_slice>
            </slice>
            <slice>
              <time_slice>43:24</time_slice>
              <text_slice>want to make a decision.</text_slice>
            </slice>
            <slice>
              <time_slice>43:26</time_slice>
              <text_slice>In this picture what would
a reasonable person</text_slice>
            </slice>
            <slice>
              <time_slice>43:28</time_slice>
              <text_slice>do to make a decision?</text_slice>
            </slice>
            <slice>
              <time_slice>43:30</time_slice>
              <text_slice>They would probably choose a
certain threshold, Xi, and</text_slice>
            </slice>
            <slice>
              <time_slice>43:35</time_slice>
              <text_slice>decide that H1 is true if your
data falls in this interval.</text_slice>
            </slice>
            <slice>
              <time_slice>43:43</time_slice>
              <text_slice>And decide that H0 is true
if you fall on the side.</text_slice>
            </slice>
            <slice>
              <time_slice>43:49</time_slice>
              <text_slice>So that would be a
reasonable way of</text_slice>
            </slice>
            <slice>
              <time_slice>43:51</time_slice>
              <text_slice>approaching the problem.</text_slice>
            </slice>
            <slice>
              <time_slice>43:54</time_slice>
              <text_slice>More generally you take the set
of all possible X's, and</text_slice>
            </slice>
            <slice>
              <time_slice>43:59</time_slice>
              <text_slice>you divide the set of possible
X's into two regions.</text_slice>
            </slice>
            <slice>
              <time_slice>44:03</time_slice>
              <text_slice>One is the rejection region,
in which you decide H1,</text_slice>
            </slice>
            <slice>
              <time_slice>44:11</time_slice>
              <text_slice>or you reject H0.</text_slice>
            </slice>
            <slice>
              <time_slice>44:15</time_slice>
              <text_slice>And the complement of that
region is where you decide H0.</text_slice>
            </slice>
            <slice>
              <time_slice>44:21</time_slice>
              <text_slice>So this is the x-space
of your data.</text_slice>
            </slice>
            <slice>
              <time_slice>44:25</time_slice>
              <text_slice>In this example here, x
was one-dimensional.</text_slice>
            </slice>
            <slice>
              <time_slice>44:28</time_slice>
              <text_slice>But in general X is going to
be a vector, where all the</text_slice>
            </slice>
            <slice>
              <time_slice>44:31</time_slice>
              <text_slice>possible data vectors that
you can get, they're</text_slice>
            </slice>
            <slice>
              <time_slice>44:34</time_slice>
              <text_slice>divided into two types.</text_slice>
            </slice>
            <slice>
              <time_slice>44:36</time_slice>
              <text_slice>If it falls in this set you'd
make one decision.</text_slice>
            </slice>
            <slice>
              <time_slice>44:40</time_slice>
              <text_slice>If it falls in that set, you
make the other decision.</text_slice>
            </slice>
            <slice>
              <time_slice>44:43</time_slice>
              <text_slice>OK, so how would you
characterize the performance</text_slice>
            </slice>
            <slice>
              <time_slice>44:47</time_slice>
              <text_slice>of the particular way of
making a decision?</text_slice>
            </slice>
            <slice>
              <time_slice>44:49</time_slice>
              <text_slice>Suppose I chose my threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>44:53</time_slice>
              <text_slice>I may make mistakes of
two possible types.</text_slice>
            </slice>
            <slice>
              <time_slice>44:57</time_slice>
              <text_slice>Perhaps H0 is true, but my data
happens to fall here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:03</time_slice>
              <text_slice>In which case I make a mistake,
and this would be a</text_slice>
            </slice>
            <slice>
              <time_slice>45:07</time_slice>
              <text_slice>false rejection of H0.</text_slice>
            </slice>
            <slice>
              <time_slice>45:10</time_slice>
              <text_slice>If my data falls here
I reject H0.</text_slice>
            </slice>
            <slice>
              <time_slice>45:15</time_slice>
              <text_slice>I decide H1.</text_slice>
            </slice>
            <slice>
              <time_slice>45:16</time_slice>
              <text_slice>Whereas H0 was true.</text_slice>
            </slice>
            <slice>
              <time_slice>45:19</time_slice>
              <text_slice>The probability of
this happening?</text_slice>
            </slice>
            <slice>
              <time_slice>45:21</time_slice>
              <text_slice>Let's call it alpha.</text_slice>
            </slice>
            <slice>
              <time_slice>45:24</time_slice>
              <text_slice>But there's another kind of
error that can be made.</text_slice>
            </slice>
            <slice>
              <time_slice>45:28</time_slice>
              <text_slice>Suppose that H1 was true, but by
accident my data happens to</text_slice>
            </slice>
            <slice>
              <time_slice>45:32</time_slice>
              <text_slice>falls on that side.</text_slice>
            </slice>
            <slice>
              <time_slice>45:34</time_slice>
              <text_slice>Then I'm going to make
an error again.</text_slice>
            </slice>
            <slice>
              <time_slice>45:36</time_slice>
              <text_slice>I'm going to decide H0 even
though H1 was true.</text_slice>
            </slice>
            <slice>
              <time_slice>45:40</time_slice>
              <text_slice>How likely is this to occur?</text_slice>
            </slice>
            <slice>
              <time_slice>45:42</time_slice>
              <text_slice>This would be the area under
this curve here.</text_slice>
            </slice>
            <slice>
              <time_slice>45:46</time_slice>
              <text_slice>And that's the other type of
error than can be made, and</text_slice>
            </slice>
            <slice>
              <time_slice>45:50</time_slice>
              <text_slice>beta is the probability of this
particular type of error.</text_slice>
            </slice>
            <slice>
              <time_slice>45:55</time_slice>
              <text_slice>Both of these are errors.</text_slice>
            </slice>
            <slice>
              <time_slice>45:57</time_slice>
              <text_slice>Alpha is the probability
of error of one kind.</text_slice>
            </slice>
            <slice>
              <time_slice>45:59</time_slice>
              <text_slice>Beta is the probability of an
error of the other kind.</text_slice>
            </slice>
            <slice>
              <time_slice>46:02</time_slice>
              <text_slice>You would like the
probabilities</text_slice>
            </slice>
            <slice>
              <time_slice>46:03</time_slice>
              <text_slice>of error to be small.</text_slice>
            </slice>
            <slice>
              <time_slice>46:05</time_slice>
              <text_slice>So you would like to
make both alpha and</text_slice>
            </slice>
            <slice>
              <time_slice>46:07</time_slice>
              <text_slice>beta as small as possible.</text_slice>
            </slice>
            <slice>
              <time_slice>46:09</time_slice>
              <text_slice>Unfortunately that's not
possible, there's a trade-off.</text_slice>
            </slice>
            <slice>
              <time_slice>46:13</time_slice>
              <text_slice>If I go to my threshold it this
way, then alpha become</text_slice>
            </slice>
            <slice>
              <time_slice>46:17</time_slice>
              <text_slice>smaller, but beta
becomes bigger.</text_slice>
            </slice>
            <slice>
              <time_slice>46:20</time_slice>
              <text_slice>So there's a trade-off.</text_slice>
            </slice>
            <slice>
              <time_slice>46:22</time_slice>
              <text_slice>If I make my rejection region
smaller one kind of error is</text_slice>
            </slice>
            <slice>
              <time_slice>46:29</time_slice>
              <text_slice>less likely, but the
other kind of error</text_slice>
            </slice>
            <slice>
              <time_slice>46:31</time_slice>
              <text_slice>becomes more likely.</text_slice>
            </slice>
            <slice>
              <time_slice>46:34</time_slice>
              <text_slice>So we got this trade-off.</text_slice>
            </slice>
            <slice>
              <time_slice>46:38</time_slice>
              <text_slice>So what do we do about it?</text_slice>
            </slice>
            <slice>
              <time_slice>46:39</time_slice>
              <text_slice>How do we move systematically?</text_slice>
            </slice>
            <slice>
              <time_slice>46:41</time_slice>
              <text_slice>How do we come up with
rejection regions?</text_slice>
            </slice>
            <slice>
              <time_slice>46:45</time_slice>
              <text_slice>Well, what the theory basically
tells you is it</text_slice>
            </slice>
            <slice>
              <time_slice>46:48</time_slice>
              <text_slice>tells you how you should
create those regions.</text_slice>
            </slice>
            <slice>
              <time_slice>46:53</time_slice>
              <text_slice>But it doesn't tell
you exactly how.</text_slice>
            </slice>
            <slice>
              <time_slice>46:57</time_slice>
              <text_slice>It tells you the general
shape of those regions.</text_slice>
            </slice>
            <slice>
              <time_slice>47:00</time_slice>
              <text_slice>For example here, the theory
who tells us that the right</text_slice>
            </slice>
            <slice>
              <time_slice>47:05</time_slice>
              <text_slice>thing to do would be to put
the threshold and make</text_slice>
            </slice>
            <slice>
              <time_slice>47:07</time_slice>
              <text_slice>decisions one way to the right,
one way to the left.</text_slice>
            </slice>
            <slice>
              <time_slice>47:10</time_slice>
              <text_slice>But it might not necessarily
tell us</text_slice>
            </slice>
            <slice>
              <time_slice>47:12</time_slice>
              <text_slice>where to put the threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>47:15</time_slice>
              <text_slice>Still, it's useful enough to
know that the way to make a</text_slice>
            </slice>
            <slice>
              <time_slice>47:18</time_slice>
              <text_slice>good decision would
be in terms of</text_slice>
            </slice>
            <slice>
              <time_slice>47:20</time_slice>
              <text_slice>a particular threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>47:22</time_slice>
              <text_slice>Let me make this
more specific.</text_slice>
            </slice>
            <slice>
              <time_slice>47:24</time_slice>
              <text_slice>We can take our inspiration
from the solution of the</text_slice>
            </slice>
            <slice>
              <time_slice>47:27</time_slice>
              <text_slice>hypothesis testing problem
that we had in</text_slice>
            </slice>
            <slice>
              <time_slice>47:29</time_slice>
              <text_slice>the Bayesian case.</text_slice>
            </slice>
            <slice>
              <time_slice>47:31</time_slice>
              <text_slice>In the Bayesian case we just
pick the hypothesis which is</text_slice>
            </slice>
            <slice>
              <time_slice>47:34</time_slice>
              <text_slice>more likely given the data.</text_slice>
            </slice>
            <slice>
              <time_slice>47:37</time_slice>
              <text_slice>The produced posterior
probabilities using Bayesian</text_slice>
            </slice>
            <slice>
              <time_slice>47:40</time_slice>
              <text_slice>rule, they're written
this way.</text_slice>
            </slice>
            <slice>
              <time_slice>47:42</time_slice>
              <text_slice>And this term is the
same as that term.</text_slice>
            </slice>
            <slice>
              <time_slice>47:45</time_slice>
              <text_slice>They cancel out, then let me
collect terms here and there.</text_slice>
            </slice>
            <slice>
              <time_slice>47:52</time_slice>
              <text_slice>I get an expression here.</text_slice>
            </slice>
            <slice>
              <time_slice>47:54</time_slice>
              <text_slice>I think the version you
have in your handout</text_slice>
            </slice>
            <slice>
              <time_slice>47:56</time_slice>
              <text_slice>is the correct one.</text_slice>
            </slice>
            <slice>
              <time_slice>47:59</time_slice>
              <text_slice>The one on the slide was
not the correct one, so</text_slice>
            </slice>
            <slice>
              <time_slice>48:02</time_slice>
              <text_slice>I'm fixing it here.</text_slice>
            </slice>
            <slice>
              <time_slice>48:03</time_slice>
              <text_slice>OK, so this is the form of how
you make decisions in the</text_slice>
            </slice>
            <slice>
              <time_slice>48:06</time_slice>
              <text_slice>Bayesian case.</text_slice>
            </slice>
            <slice>
              <time_slice>48:08</time_slice>
              <text_slice>What you do in the Bayesian
case, you</text_slice>
            </slice>
            <slice>
              <time_slice>48:10</time_slice>
              <text_slice>calculate this ratio.</text_slice>
            </slice>
            <slice>
              <time_slice>48:13</time_slice>
              <text_slice>Let's call it the likelihood
ratio.</text_slice>
            </slice>
            <slice>
              <time_slice>48:17</time_slice>
              <text_slice>And compare that ratio
to a threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>48:20</time_slice>
              <text_slice>And the threshold that you
should be using in the</text_slice>
            </slice>
            <slice>
              <time_slice>48:22</time_slice>
              <text_slice>Bayesian case has something
to do with the prior</text_slice>
            </slice>
            <slice>
              <time_slice>48:25</time_slice>
              <text_slice>probabilities of the
two hypotheses.</text_slice>
            </slice>
            <slice>
              <time_slice>48:28</time_slice>
              <text_slice>In the non-Bayesian case we do
not have prior probabilities,</text_slice>
            </slice>
            <slice>
              <time_slice>48:31</time_slice>
              <text_slice>so we do not know how to
set this threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>48:34</time_slice>
              <text_slice>But we're going to do is we're
going to keep this particular</text_slice>
            </slice>
            <slice>
              <time_slice>48:38</time_slice>
              <text_slice>structure anyway, and maybe use
some other considerations</text_slice>
            </slice>
            <slice>
              <time_slice>48:42</time_slice>
              <text_slice>to pick the threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>48:44</time_slice>
              <text_slice>So we're going to use a
likelihood ratio test, that's</text_slice>
            </slice>
            <slice>
              <time_slice>48:51</time_slice>
              <text_slice>how it's called in which we
calculate a quantity of this</text_slice>
            </slice>
            <slice>
              <time_slice>48:54</time_slice>
              <text_slice>kind that we call the
likelihood, and compare it</text_slice>
            </slice>
            <slice>
              <time_slice>48:56</time_slice>
              <text_slice>with a threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>48:58</time_slice>
              <text_slice>So what's the interpretation
of this likelihood?</text_slice>
            </slice>
            <slice>
              <time_slice>49:03</time_slice>
              <text_slice>We ask--</text_slice>
            </slice>
            <slice>
              <time_slice>49:04</time_slice>
              <text_slice>the X's that I have observed,
how likely were they to occur</text_slice>
            </slice>
            <slice>
              <time_slice>49:08</time_slice>
              <text_slice>if H1 was true?</text_slice>
            </slice>
            <slice>
              <time_slice>49:10</time_slice>
              <text_slice>And how likely were they to
occur if H0 was true?</text_slice>
            </slice>
            <slice>
              <time_slice>49:14</time_slice>
              <text_slice>This ratio could be big if my
data are plausible they might</text_slice>
            </slice>
            <slice>
              <time_slice>49:20</time_slice>
              <text_slice>occur under H1.</text_slice>
            </slice>
            <slice>
              <time_slice>49:22</time_slice>
              <text_slice>But they're very implausible,
extremely unlikely</text_slice>
            </slice>
            <slice>
              <time_slice>49:25</time_slice>
              <text_slice>to occur under H0.</text_slice>
            </slice>
            <slice>
              <time_slice>49:27</time_slice>
              <text_slice>Then my thinking would be well
the data that I saw are</text_slice>
            </slice>
            <slice>
              <time_slice>49:30</time_slice>
              <text_slice>extremely unlikely to have
occurred under H0.</text_slice>
            </slice>
            <slice>
              <time_slice>49:33</time_slice>
              <text_slice>So H0 is probably not true.</text_slice>
            </slice>
            <slice>
              <time_slice>49:36</time_slice>
              <text_slice>I'm going to go for
H1 and choose H1.</text_slice>
            </slice>
            <slice>
              <time_slice>49:39</time_slice>
              <text_slice>So when this ratio is big it
tells us that the data that</text_slice>
            </slice>
            <slice>
              <time_slice>49:43</time_slice>
              <text_slice>we're seeing are better
explained if we assume H1 to</text_slice>
            </slice>
            <slice>
              <time_slice>49:47</time_slice>
              <text_slice>be true rather than
H0 to be true.</text_slice>
            </slice>
            <slice>
              <time_slice>49:50</time_slice>
              <text_slice>So I calculate this quantity,
compare it with a threshold,</text_slice>
            </slice>
            <slice>
              <time_slice>49:53</time_slice>
              <text_slice>and that's how I make
my decision.</text_slice>
            </slice>
            <slice>
              <time_slice>49:56</time_slice>
              <text_slice>So in this particular picture,
for example the way it would</text_slice>
            </slice>
            <slice>
              <time_slice>49:59</time_slice>
              <text_slice>go would be the likelihood ratio
in this picture goes</text_slice>
            </slice>
            <slice>
              <time_slice>50:02</time_slice>
              <text_slice>monotonically with my X. So
comparing the likelihood ratio</text_slice>
            </slice>
            <slice>
              <time_slice>50:07</time_slice>
              <text_slice>to the threshold would be the
same as comparing my x to the</text_slice>
            </slice>
            <slice>
              <time_slice>50:10</time_slice>
              <text_slice>threshold, and we've got
the question of how</text_slice>
            </slice>
            <slice>
              <time_slice>50:12</time_slice>
              <text_slice>to choose the threshold.</text_slice>
            </slice>
            <slice>
              <time_slice>50:13</time_slice>
              <text_slice>The way that the threshold is
chosen is usually done by</text_slice>
            </slice>
            <slice>
              <time_slice>50:17</time_slice>
              <text_slice>fixing one of the two
probabilities of error.</text_slice>
            </slice>
            <slice>
              <time_slice>50:21</time_slice>
              <text_slice>That is, I say, that I want my
error of one particular type</text_slice>
            </slice>
            <slice>
              <time_slice>50:26</time_slice>
              <text_slice>to be a given number,
so I fix this alpha.</text_slice>
            </slice>
            <slice>
              <time_slice>50:30</time_slice>
              <text_slice>And then I try to find where
my threshold should be.</text_slice>
            </slice>
            <slice>
              <time_slice>50:33</time_slice>
              <text_slice>So that this probability theta,
probability out there,</text_slice>
            </slice>
            <slice>
              <time_slice>50:36</time_slice>
              <text_slice>is just equal to alpha.</text_slice>
            </slice>
            <slice>
              <time_slice>50:39</time_slice>
              <text_slice>And then the other probability
of error, beta, will be</text_slice>
            </slice>
            <slice>
              <time_slice>50:42</time_slice>
              <text_slice>whatever it turns out to be.</text_slice>
            </slice>
            <slice>
              <time_slice>50:44</time_slice>
              <text_slice>So somebody picks alpha
ahead of time.</text_slice>
            </slice>
            <slice>
              <time_slice>50:48</time_slice>
              <text_slice>Based on the probability of
a false rejection based on</text_slice>
            </slice>
            <slice>
              <time_slice>50:52</time_slice>
              <text_slice>alpha, I find where my threshold
is going to be.</text_slice>
            </slice>
            <slice>
              <time_slice>50:55</time_slice>
              <text_slice>I choose my threshold, and that
determines subsequently</text_slice>
            </slice>
            <slice>
              <time_slice>50:59</time_slice>
              <text_slice>the value of beta.</text_slice>
            </slice>
            <slice>
              <time_slice>51:01</time_slice>
              <text_slice>So we're going to continue with
this story next time, and</text_slice>
            </slice>
            <slice>
              <time_slice>51:07</time_slice>
              <text_slice>we'll stop here.</text_slice>
            </slice>
          </transcript>
        </video>
      </videos>
    </lecture>
    <lecture>
      <lecture_title>Discrete Random Variable Examples; Joint PMFs (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/mit6_041f10_l06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>LECTURE 6 Review
Random variable X: function fromReadings: Sections 2.4-2.6
sample space to the real numbers
Lecture outlinePMF (for discrete random variables):
pX(x)= P(X=x)
Review: PMF, expectation, varianceExpectation:
Conditional PMF E[X]=/summationdisplay
xpX(x)
x
Geometric PMF
E[g(X)] = g(x)p (x)
XTotal expectation theorem/summationdisplay
x
Joint PMF of two random variables E[X+]=E[X]+
E/bracketleftbigg
XE[X]/bracketrightbigg
=
var(X)= E
=/bracketleftBig
(2XE[X])
(xE[X])2p/bracketrightBig
=/summationdisplay
x X(x)
E[2X](2E[X])
Standard deviation: X=/radicalBig
var(X)
Random speed Average speed vs. average time
Traverse a 200 mile distance at constant Traverse a 200 mile distance at constant
but random speed V but random speed V
p (v ) 1/2 1/2 p (v ) 1/2 1/2 V V
1 200 v 1 200 v
d= 200, T=t(V) = 200 /V time in hours = T=t(V)=
E[T]=E[t(V)] =/summationtext
vt(v)pV(v)=
E[V]=
E[TV] = 200 = E[T]E[V]
var(V)=E[200/V ]=E[T] = 200/ E[V].
V=/negationslash
/negationslash
1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu
6.041 / 6.431 Probabilistic Systems Analysis and Applied Probability
Fall 2010
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Conditional PMF and expectation Geometric PMF
X: number of independent coin tossespX|A(x)= P(X=x|A)until rst head
E[X|A]=/summationdisplay
xp (x) ( ) = (1 )k1X p
x|A pXk p, k =1,2,...
 
p (x ) E[X]= (1kkp(1X/summationdisplay
Xk)= p
k/summationdisplay
k
=1p)
k=1
Memoryless property: Given that X&gt; 2,1/4the r.v. X2 has same geometric PMF
p   p (k) p (k)X X |X&gt;2
2 p(1-p)p  1 23 4x  
... ...LetA={X2} k 1 3 k
p (k)X- 2|X&gt;2
pX|A(x)=
p   
E[X|A]=
...
1 k
Total Expectation theorem Joint PMFs
Partition of sample space pX,Y(x, y)=P(X=xand Y=y)
into disjoint events A1,A2, . . . , A n
y
A1 41/20 2/20 2/20B
32/20 4/20 1/20 2/20
2 1/20 3/20 1/20
A A 12 31/20
x12 3 4
P(B)=P(A1)P(B|A1)+ +P(An)P(B|An) pX,Y(x, y)=
pX(x)= P(A1)pX A(x)+ +P(An)pX A(x)n/summationdisplay
x/summationdisplay
y|1 |
E[X]=P(A1)E[X|A1]++P(An)E[X|An] pX(x)=/summationdisplay
pX,Y(x, y)
y
Geometric example: pX,Y(x, y)pX Y(x y )=P(X=x Y =y)=A1:{X=1 },A2:{X&gt; 1}| | |pY(y)
E[X]= P(X= 1)E[X|X= 1] /summationdisplay
pX Y(x|y)=|
+P(X&gt; 1)E[X|X&gt; 1]x
Solve to get E[X]=1 /p
2</text>
        </slide>
      </slides>
      <videos>
        <video>
          <video_url>https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/resources/lecture-25-classical-inference-iii-course-overview/</video_url>
          <video_title>Lecture 25: Classical Inference III; Course Overview</video_title>
          <transcript/>
        </video>
      </videos>
    </lecture>
  </lectures>
</doc>
