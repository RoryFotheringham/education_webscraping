<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/</course_url>
    <course_title>Topics in Linguistic Theory: Laboratory Phonology</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Humanities </list>
      <list>Electrical Engineering </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Source-filter theory
Acoustics of vowels</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec3_src_filterb/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>37</slideno>
          <text>Standing waves and resonance in tubes  
 The air in a tube which is ope n at one end naturally vibrates 
in standing waves.
 There must be a velocity node at the closed end since the 
fixed end prevents particle movement.
 There must be a pressure node at the open end - pressure is 
always equal to external pressure.
 The simplest wave that meets these conditions is a standing 
wave that goes from maximum pressure at the glottis to zero 
(atmospheric pressure) at the lips.
velocity
pressure</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Perturbation Theory vs. two-tube models
 Our simple tube models ignore acoustic coupling and are 
therefore most valid where constrictions are narrow.
 Perturbation theory accounts for the effects of small 
perturbations of a uniform tube, and thus is most accurate 
for open constrictions.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Resonance in a uniform tube, open at one end
 For resonance to occur, compressions must reach the closed 
end at the same time as the excitation source produces 
another compression peak, and rarefactions must reach the closed end at the same time as the source produces a rarefaction.
 In a tube which is open at one end:
i. a compression is reflected from the open end as a 
rarefaction. 
ii. This rarefaction is reflected back off the closed end.
iii. The rarefaction is then re flected from the open end as a 
compression.
rarefactions do the opposite.
 So source and reflected wave will interfere constructively if, 
e.g., the time it takes for the wave to travel 2 times up and down the tube is equal to the half the period of the wave.</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Helmholtz resonators
 The back cavity and the constriction together form a 
resonant system called a Helmholtz resonator.
 If the length of the constriction is short, the air in it 
vibrates as a mass on the spring formed by the air in the 
back cavity.
 Resonant frequency, f=c
2Ac
Vlc=c
2Ac
Ablblclb lcAb Ac Af
lf
Figure by MIT O penC ourseW are. Adap ted from Johnson, Keith. Acoustic and Auditory 
Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483. aabb
cc
d
d
Figure by MIT OpenCourseWare. Adapted from Ladefoged, P.
Elements of Acoustic Phonetics. 2nd ed. Chicago, IL: University
of Chicago Press, 1996.</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Perturbation Theory (Chiba and Kajiyama 1941)
A nice story about American 
[]:
 Three constrictions: labial 
(lip protrusion/rounding), 
palatal (bunching or retroflexion), and 
pharyngeal.
 All 3 are near velocity 
maxima for F3, hence very 
low F3.
 But see Espy-Wilson et al 
(2000).V1
V2
V2F2 F4V1 V3
V4
V4F1 F3V3 V3'
V2'
V2'V3'
V4'
V4'V4''
V4''V4'''
V4'''V3''
V3''
Figure by MIT  OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics.
Malden, MA: Blackwell Publishers, 1997. Based on Chiba and Kajiyama 1941.</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Tubes models of the vocal tract
L = n/2
= 2L /n
Fn= c/
Fn=nc
2L
Diagrams of the standing waves in a tube closed at both ends (almost)
Figure by MIT OpenC ourseWare. Ada pted from Ladefoged, Peter. L104/L204 Phonetic Theory C ourse Notes, University of California, Los Angeles.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Noise source
 Air turbulence generated at an obstruction involves random 
(aperiodic) pressure fluctuations over a wide range of 
frequencies. 
 Noise generated at the glottis is called aspiration, while 
noise generated elsewhere is called frication. 
0
0 2 4 6 8 105 10 15 20 2530
25
20
15
10
5
0kHzBarkAmplitude (dB)
Acoustic frequency (kHz) and Auditory frequency (Bark)Power Spectrum of Turbulent Noise (Shadle 1985)
 Image by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics . 
Malden, MA: Blackwell Publishers, 1997.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Resonance in a tube open at one end
source emits a rarefaction</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Sources
 A source is an input of acoustic energy into 
the speech production system. 
 There are two basic types:
 Voicing
N o i s e</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>The vocal tract is a filter
 In speech the input has energy at a range of frequencies 
(voicing or noise, or both) and the resonator has multiple 
resonant frequencies of varying bandwidths.
 The amplitude in the output wave of each frequency 
component of the source depends on the extent to which the 
air in the vocal tract resonate s at that frequency. Thus the 
vocal tract filters the source.
 The resonances of the vocal tract are called formants.
Image by MIT OpenC ourseW are.30
20
10
030
20
10
030
20
10
0
0 1000 2000 30000 1000 2000 3000 500 2500 1500
Source SpectrumFrequency (Hz)Frequency (Hz) Frequency (Hz)
Output SpectrumResonances =
Formant Frequencies</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>The vocal tract as a filter
 The sound wave at some distance from the speaker is the 
result of filtering the source with the vocal tract filter, plus
the radiation characteristics of the lips/nose.
 How does the vocal tract operate as a filter?
 What determines the characteristics of the filter?
Outline
 Resonators are filters.
 The column of air in the vocal tract is a resonator, hence a 
filter.
 The characteristics of the filter depend on the shape of the 
vocal tract - we will explore the relationship by considering simple cases.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Voicing source
Video removed due to 
copyright restrictions.  
Please see 
The vibrating vocal folds 
in Peter Ladefogeds
Vowels and Consonants.Video removed due to 
copyright restrictions.  
Please see 
The vocal tract and larynx 
to view.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Resonators are filters
A resonator acts as an acoustic filter:
 Consider the vibration of a column of  air driven by a vibrating source. 
 The source is the input to the filter. This input produces vibration in the 
column of air. 
 This vibration produces a sound wave in  the external air - the output of 
the filter.
 The strength with which an input fr equency is output from the filter will 
depend on the resonance characteristics of the air column - if it is near a 
resonant frequency, it will be passed through to the output at full 
strength, if it is not near a resonant  frequency, it will have low amplitude 
in the output.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Resonance in a tube open at one end
source emits a compression</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Voicing source
 vocal fold vibration produces a complex periodic wave, 
whose spectrum contains energy at the fundamental 
frequency of laryngeal vibration and multiples of the 
fundamental frequency (harmonics).
Figure by MIT OpenC ourseWare. Adapted from Rothenberg, M. 
"The Glottal Volume Velocity Waveform During Loose and Tight Gl ottal Adjustments." 
Proceedings of the VII International Congress of Phonetic Sciences (1971): 380-388. 00.5
00.5
00.5
00.5
0
0msec 100.5LITERS SECSPEAKER P.B.FUNDAMENTAL
FREQUENCYSUBGLOTTALPRESSURE
107 Hz
97 Hz
94 Hz
89 Hz
83 Hz8.6 cm H
2O
8.4 cm H 2O
8.1 cm H 2O
7.3 cm H 2O
6.9 cm H 2O
Glottal volume velocity waveforms obtained by inverse-filtering the volume velocity at the mouth.
From five successive repetitions of the phrase /bap/ (from top to bottom). Time in each trace runsfrom left to right. Waveforms traced from original.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Voicing source
 Voicing is a periodic source produced by modulation of the 
airflow from the lungs by the vocal folds.
(1)
(2)
(3)
(4)
(5)(6)
(7)
(8)
(9)
(10)
TracheaFalse
vocal
foldsEpiglottis
Vocal folds
Figure by MIT OpenCourseWare. Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Standing waves and resonance in tubes  
What is the frequency of this wave?
 The transition from maximum to zero pressure is only one 
quarter of a cycle.
 For a tube of length L, L = /4
= 4L
f= c/= c/4L
velocity
pressure</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Relating frequency to wavelength  
 The distribution of pressure fluctuations in space due to a 
sound wave depends on the frequency of the wave and the 
speed at which the wavefront travels.
 Consider a wave with frequency fHz and speed c cm/s.
 In one second it travels c cm.
 In one second it goes through fcycles.
 So there are f cycles in ccm.
 So the length of one cycle (the wavelength) is c/f.
 Usual formulation: c= f Where is wavelength</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Filters
 The vocal tract acts as a filter, modifying the source 
waveform.
 An acoustic filter is a device which passes or amplifies 
certain frequencies and attenuates others. 
 An important characteristic of a filter is its transfer 
function - the ratio of the output to the input depending on 
frequency.
1.4
1.2
0.8
0.6
0.40.21
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Magnitude
Normalized frequencyFIR High Pass Filter
1.4
1.2
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.01Magnitude
Normalized frequencyStopband Filter
Figure by MIT OpenCourseWare. Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Source
A sound may involve more than one source. 
 E.g. a voiced fricative combines voicing and 
frication noise
 breathy voice can combine voicing and aspiration 
noise. 
 voiceless fricatives can have noise generated at the 
glottis and at the supralaryngeal constriction.</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Shepard (1972): MDS analysis of vowel confusions
HEED
HID
HEAD
HAD
HEARD
HUD HOD
HOOD HAWEDWHO'D uUu3a
Q
VV
Ii
e
  
 
Figure by MIT OpenCourseWare. Adapted from Roger Shepard. "Psychological Representation of Speech Sounds." 
In Human Communication: A Unified View . Edited by E. E. David and P. B. Denes. New York, 
NY: McGraw-Hill, 1972, pp. 67-113.Q

AcWI</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Low vowels [, a, ]
 Pharyngeal constriction
 Since the back tube is much narrower than the front tube, each can 
reasonably be approximated by a tube  closed at one end and open at 
the other.
 The resonances of the combined tubes deviate from the values we 
would calculate for these configur ations in isolation because the 
resonato rs are acoustically coupled .
 The degree of coupling depends on the difference in cross-sectional 
areas. The shape of the vocal tract in the vowel [   ] as in father schematized as two tubes.
___________________________Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Cues to vowel quality
 The main cues to vowel quality are related to the frequencies of the 
first two or three formants.
Evidence:
 Multi-Dimensional Scaling studies: Shepard (1972) presents a three-
dimensional MDS analysis of confusion data on American English 
vowels from Peterson and Barney (1 952). Two nearly orthogonal axes 
correlate well (but non-linearly) wi th F1 and F2. It is possible to 
identify an axis which correlates well with F3, but it is also correlated 
with the F2 axis (not orthogonal).
 Many other MDS studies of vowel  confusion/similarity have 
found that the first two dimensio ns identified correspond well to 
F1, F2.
 It seems that a third dimension is required to accommodate rhotic
vowels, but MDS analyses do not provide clear evidence that thisdimension corresponds to F3.</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Perturbation Theory (Chiba and Kajiyama 1941)
 Constriction near a 
point of maximum 
velocity (Vn) lowers th e 
associated formant 
frequency.
 Constriction near a 
point of maximum 
pressure raises the associated formant frequency. V1
V2
V2F2 F4V1 V3
V4
V4F1 F3V3 V3'
V2'
V2'V3'
V4'
V4'V4''
V4''V4'''
V4'''V3''
V3''
Figure by MIT  OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics.
Malden, MA: Blackwell Publishers, 1997. Based on Chiba and Kajiyama 1941.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Perturbation Theory (Chiba and Kajiyama 1941)
 What is the effect of a 
pharyngeal constriction?
 Does this correspond to the 
tube model above?
 How do you raise F2 
maximally?V1
V2
V2F2 F4V1 V3
V4
V4F1 F3V3 V3'
V2'
V2'V3'
V4'
V4'V4''
V4''V4'''
V4'''V3''
V3''
Figure by MIT  OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics.
Malden, MA: Blackwell Publishers, 1997. Based on Chiba and Kajiyama 1941.</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Tubes models of the vocal tract
 Another simple tube model that approximates a vocal tract 
shape is a tube (almost) closed at both ends.
 This is similar to a sound made with a narrow constriction at 
the lips, or the cavity behind an interior constriction.
 Air in a tube closed at both ends naturally vibrates in 
standing waves with nodes at both ends.
Diagrams of the standing waves in a tube closed at both ends (almost)
Figure by MIT O penCourse Ware. Ada pted from Ladefoged, Peter. L104/L204 Phonetic Theory C ourse Notes, University of California, Los Angeles.</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Common vowel inventories:I. Vowel inventories
 i     u   i     u   i     u  
       e    o  e  
    o  
   a       a       a    
Arabic, 
Nyangumata, 
Aleut, etc. Spanish, 
Swahili, 
Cherokee, etc.  Italian, 
Yoruba, 
Tunica, etc. 
 
 Unattested vowel inventories:
 i       i        i     u  
 e      e        
      
 
   a       a          
 Why?</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Resonators
 We can plot the response of a resonator to a range of input 
frequencies. Peaks mark the resonant frequencies.
 Resonators vary in the range of frequencies they will 
respond strongly to
a sharply tuned resonator only responds strongly to 
frequencies very close to the resonant frequency (it has a narrow bandwidth).
A resonator with a wide bandwidth will vibrate strongly in response to a wider range of input frequencies.
2000300 400 500 600
95-105 175-225 350-550
Figure by MIT O penCourse Ware. Adapted from Ladefoged, P . Elements of Acoustic Phonetics. 2nd ed. University of Chicago Press, 1996.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Standing waves
 Under the conditions for resonance, the air in a tube vibrates 
in a standing wave.
 Standing waves arise when identical waves are travelling in 
opposite directions - e.g. a wave and its reflection from a 
surface. standing wave demo
 A standing wave contains fixed points at which air particles 
do not move (velocity nodes). Movement is maximal between the nodes (at the velocity anti-nodes).
 Velocity anti-nodes correspond to pressure nodes - pressure 
does not change - and velocity nodes correspond to pressure anti-nodes.</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Cues to vowel quality
 The main cues to vowel quality are related to the 
frequencies of the first two or three formants.
Evidence:
 Synthesis experiments: Varying formant frequencies in 
synthetic vowels can change the vowel percept (Delattre, 
Liberman, Cooper, and Gerstman 1952, and many others).
http://www.asel.udel.edu/speech/tutorials/synthesis/vowels.html</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Vowel quality
 The main cues to vowel quality are related to the frequencies of the 
first two or three formants.
 So in trying to understand vowel acoustics, we will focus on the
frequencies of formants.
Questions to keep in mind:
 Why are the first 2-3 formant frequen cies the primary cues to vowel 
quality?
 What is the range of formant patterns that we can produce?
 We can get a basic understanding of vowel acoustics using very simple 
tube models of the vocal tract.</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>How air in a tube vibrates
(1)
(2)
Reflection of a compressinoal pulse in a slinky spring off a fixed end (1) and a
free end (2). Reflection from a free end = phase change in pressure or density.Transverse Representation
Figure by MIT OpenC ourseWare. Adapted from Berg, Richard E., and David G. Stork. The Physics of Sound. New York, NY: Prentice-Hall, 1982.</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Resonance in a tube open at one end
 This is only one resonant frequency (the lowest, or first, 
resonance)
 The general requirement is th at compressions emitted from 
the source must coincide with reflected compressions 
(likewise for rarefactions) - they dont have to coincide with 
every reflected compression.
 So the time it takes the wave to  travel 2L must be equal to 
1/2, 3/2, 5/2 .... i.e. (2n-1)/2 periods of the source wave.
(2n1)T
2=2L
c
T=4L
(2n1)cFn=(2n1)c
4Lso:</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Spectrogram image removed due to copyright restrictions.
See: http://hctv.humnet.ucla.edu/departments/linguistics/VowelsandConsonants/course/chapter8/8.3.htm</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings for next week: 
 Ladefoged Elements of Acoustic Phonetics (2nd edn) 
ch. 10 from p.160, ch. 11 to p.183.
 Liljencrants &amp; Lindblom (1972).
 Note: Mel is an auditory fre quency scale like the Bark scale.
 Assignments (due 3/6):
 Write up analysis of mandarin affricates.
 Acoustics assignment.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Lip rounding
 Lip-rounding also involves lip protrusion so it both 
lengthens the vocal tract and introduces a constriction at 
the lips. 
 Perturbation theory: All formants have a velocity 
maximum at the lips, so a constriction at the lips should lower all formants.
 Lengthening the vocal tract also lowers formants.
 Tube models + perturbation theory: A constriction at the 
lips lowers the resonant frequencies of the front cavity.
 In most vowels the lowest front cavity resonance is F2, but in a
front vowel it may be F3.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Resonant frequencies of air in a tube
What determines the resonant frequencies of a mass of air in a 
tube?
 The resonant frequencies (formants) of air in the vocal tract 
depend on how the cross-sectional area of the vocal tract 
varies over its length. 
 The relationship between vocal tract shape and transfer 
function is complex - we will consider the simple case of a uniform tube.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
The source-filter model of speech 
production</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>How air in a uniform tube vibrates
 A compression or rarefaction travelling down a tube is 
reflected off the ends of the tube. 
 There is reflection even at th e open end of a tube, but with a 
change of phase - a compression is reflected as a rarefaction, 
and vice versa.
 The vocal tract in a vowel can be approximated by a tube 
which is closed at one end (the glottis) and open at the other 
(the lips).
Image by MIT OpenCourseWare. Adapted from Ladefoged, P. Elements of Acoustic Phonetics . 
2nd ed. Chicago, IL: University of Chicago Press, 1996.1 cm
1 cm17.5 cm17.5 cm
Closed (with s lit) Open</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Resonance in a tube open at one end</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Low vowels [, a, ]
(2n1)cFn=4L
nomogramlbAb Af
lfFigure by M IT OpenCourseWare. Adapted from Johnson, Keith. 
Acoustic and Auditory Phonetics. Malden, 
MA: Blackwell Publishers, 1997. ISBN: 9780631188483. 
Front cavity resonances Back cavity resonancesFrequency (kHz)
Back cavity length (cm)5
4
3
2
1
0
0 2 4 6 8 10 12 14 16F3
F2
F1
Image by MIT OpenCourse Ware . Adapted from Johnson, Keith. Acoustic and Auditory Phonetics . Malden, MA: Blackwell Publishers, 1997.</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Tubes models of the vocal tract
 The vocal tract in vowels can be approximated by a tube that 
is closed at one end and open at the other.
Fn=(2n
 The cross-sectional area of the vocal tract is closest to being 
uniform in a mid central vowel [] as in RP English bird 
[bd], or at the end of sofa [sof].
 Caution: Most things that are transcribed as schwa do not 
have a uniform vocal tract.1)c
4L
Image by MIT OpenCourseWare. Adapted from Ladefoged, P. Elements of Acoustic Phonetics . 
2nd ed. Chicago, IL: University of Chicago Press, 1996.1 cm
1 cm17.5 cm17.5 cm
Closed (with s lit) Open</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Resonators
 A body, like a mass of air in a tube, naturally vibrates at one 
or more frequencies. 
 If a pulse of energy is imparted to the air, it will vibrate at 
these natural frequencies. 
 If a source (driving force) which is vibrating at a natural 
frequency of the body is applied to it (e.g. a tuning fork is 
held over the tube of air), the body will resonate with the 
source, i.e. vibrate strongly at the same frequency. 
 If a source of a different frequency is applied to the body it 
will vibrate with less amplitude.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Resonance in a tube open at one end
 Source and reflected wave will interfere constructively if, 
e.g., the time it takes for the wave to travel 2 times up and 
down the tube is equal to the half the period of the wave.
 For a tube of length L, for a wave with period T, travelling
with speed c,T
2=2L
c
f= 1/Tso this resonant frequency F= c/4L
 17.5cm is a reasonable figure for length of a (large) vocal 
tract, speed of sound in air is approximately 35000cm/s.</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Non-low vowels - nomogram
f=c
2Ac
AblblcFn=nc
2LFn=(2n1)c
4Lfront cavity
back cavity
back cavity + constriction
 How would you model a mid vowel?lb lcAb Ac Af
lf
Figure by MIT O penCourse Ware . Adap ted from Johnson, Keith.
Acoustic and Auditory Phonetics. Malden, MA: Blackwell
Publishers, 1997. ISBN: 9780631188483.  
Figure by MIT OpenCourse Ware . Adap ted from Johnson, Keith.
Acoustic and Auditory Phonetics. Malden, MA: Blackwell
Publishers, 1997. ISBN: 9780631188483.  5
4
3
2
1
0
1 3 5 7 9 11 13 15
Constriction location (cm from glottis)F1Frequency (kHz)
Resonant frequencies of the back tube (light lines), front tube (heavy lines) and Helmholtz resonance 
(dashed line) in the tube model. Frequency is plotted as function of different back tube lengths (l b), 
with the length of the constriction fixed at 2 cm and the total length of the model fixed at 16 cm.F3
F2front cavity 
resonancesback cavity 
resonances</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Source-Filter Model of Speech Production
400
200
0.1 0.2 0.3Output from lips Glottal airflow
Time (in secs)
3030 30
2020 20
1010 10
00 0
0 1000 2000 3000500 2500 0 1000 2000 3000 1500Frequency (Hz)Frequency (Hz) Frequency (Hz)
Source Spectrum Output SpectrumResonances =
Formant FrequenciesVocal Tract
Filter Function
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Non-low vowels (e.g. [i, e])
 Short constriction in the mouth
 The back cavity can be approximated by a tube closed at Fn=
both ends. 2L
 The front cavity is approximate d by a tube closed at one 
end.
 Neglects coupling. The degree of coupling depends on the 
cross-sectional area of the constriction.
 How do we account for the F1 of high vowels?(2n1)cFn=4Lnclb lcAb Ac Af
lf
Figure by MIT OpenCourse Ware . Adap ted from Johnson, Keith. Acoustic and Auditory
Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483. aabb
cc
d
d
Figure by MIT OpenCourseWare. Adapted from Ladefoged, P.
Elements of Acoustic Phonetics. 2nd ed. Chicago, IL: University
of Chicago Press, 1996.</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Standing waves and resonance in tubes  
What is the frequency of this wave?
velocity
pressure</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Source-Filter Model of Speech Production
400
200
0.1 0.2 0.3Output from lips Glottal airflow
Time (in secs)
3030 30
2020 20
1010 10
00 0
0 1000 2000 3000500 2500 0 1000 2000 3000 1500Frequency (Hz)Frequency (Hz) Frequency (Hz)
Source Spectrum Output SpectrumResonances =
Formant FrequenciesVocal Tract
Filter Function
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Standing waves and resonance in tubes
 Other standing waves that meet the boundary conditions:
 In general, air in a tube of length L, closed at one end, will 
have resonant frequencies (formants) at: Fn=(2n1)c
4LF1
F2
F3
glottismaxAir Pressure
lipsl
lVT length = l1
4
VT length = l34
VT length = l54
17.5 cm
l = wavelength of a vocal tract resonance.0
Figure by MIT OpenC ourseW are. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Basic statistics
Effects of the lexicon and context on speech perception</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec9_1_stats/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>11</slideno>
          <text>Hypothesis Testing: t-test
T h e  t-test allows us to test hypotheses concerning means and 
differences between means.
1. Mean F2 onset in [br] differs from mean F2 onset in [gr] in 
English.
2. Mean F2 onset in [br] is 1250 Hz (unlikely, but a simpler case 
- cf. [afva] is identified as [afa] &gt; 50%).
 We actually evaluate two exha ustive and mutually exclusive 
hypotheses, a null hypothesis that the mean has a particular value, 
and the alternative hypothesis that the mean does not have that value.
1. The mean F2 onset in [br] is th e same as the mean F2 onset in 
[gr] (Null).
2. The mean F2 onset in [bt] 1250 Hz (Alternative).
 Statistical tests allow us to asse ss the probability of obtaining the 
observed data if the null hypothesis were true.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Hypothesis Testing
 The mean of the distributi on is determined by hypothesis.
 E.g. mean = 1250 Hz or mean difference = 0.
 Population variance is estimated from  the sample variance. Unbiased 
estimate of the population variance:
 N-1 is the number of degrees of freedom of the sample.
 So estimated variance of distribution of sample means, SM2= S2/N
tscore:S2 =  (xi-)2
 N-1 
t  = M-
SM</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>300350400450500550600x
300350400450500550600x
300350400450500550600
300350400450500550600(a) Parent population
(b) Sampling distribution of
the mean based on samples
of size n = 4
(b) Sampling distribution of
the mean based on samples
of size n = 16
(b) Sampling distribution of
the mean based on samples
of size n = 64s = 50
x
xs1 = s
n50
4= =25
s1 = s
n50
16= =12.5
s1 = s
n50
64= =6.25
Figure by MIT OpenC ourseW are. Adapted from Kachigan, S. K . 2nd ed. New York, NY: Radius, 1991. Multivariate Statistical Analysis</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Hypothesis Testing
110 120 130 140 150 160 170 180 190= 10 ms
if this were the 
population meanit is unlikely that we would get a sample mean 
of this valueDistribution of sample means</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>ttest for independent means
 When we compare means, we are actually sampling a population of 
differences (e.g. differences in durations of vowels in open and
closed syllables).
 If the null hypothesis is correct, then the mean difference is 0.
 Variance of the distribution of me an differences is estimated based 
on the variances of the two samples.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>bl(ow)
Time (s)0.350678 0.4700905000
Time (s)0.312515 0.48722205000
Time (s)0.322387 0.52260705000
Time (s)0.337107 0.53741105000
Time (s)0.352544 0.55242705000gl(ow)
br(ew) dr(ew) gr(ew)</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Descriptive statistics
A measure of central tendency:
M e a n :
 M is used for sample mean, for population mean.
A measure of dispersion: Variance: mean of the squared deviations from the meanM = xi
N 
2 = (xi-)2
 N  
 Standard deviation:  (square root of the variance).</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Hypothesis Testing
tscores follow a t-distribution - similar to a normal distribution, but 
with slightly fatter tails (mor e extreme values) because S may 
underestimate .
t-distribution is actually a fam ily of distributions, one for each
number of degrees of freedom.
 Calculate t-score then consult relevant tdistribution to determine the 
probability of obtaining that t-score or greater (more extreme).
Figure by MIT OpenCourse Ware . Normal
t (df = 12)
t (df = 5)</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Topics: 
 Statistics
 The lexicon and context in speech 
perception.
 The lexicon and context in speech 
production.
 Phonology in speech perception.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Hypothesis Testing
 F2 onset (Hz)
 Are these differences in means significant?
 Could the apparent differences have arisen by chance, 
although the true (population) means of F2 onsets are the 
same?
 I.e. given that F2 onsets vary, we might happen to sample 
most of our [br] onsets from the low end of the 
distribution, and most of our [gr] onsets from the high end.
 Statistical tests allow us to assess the probability that this is 
the case. br dr gr 
mean 1225 1641 1272 
s.d. 150 177 215</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Fitting models
 Statistical analyses generally involve fitting a model to the experimental 
data.
 The model in a t-test is fairly trivial, e.g.
duration = + syllable_type (syllable_type is open or  closed)
durationij= + syllable_typei+ errorij
 Analysis of Variance (ANOVA) involves more complex models, e.g.
durijk= + voweli+ syll_typej+ errorijk
durijk= + voweli+ syll_typej+ vowel*syll_typeij+ errorijk
 Model fitting involves finding values for the model parameters that yield the 
best fit between model and data (e.g. minimize the squared errors).
 Hypothesis testing generally involves testing whether some term or 
coefficient in the model is significantly different from zero.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Hypothesis testing
 In most experiments we need more complex statistical analyses than 
the ttest (e.g. ANOVA), but the logic is the same: Given certain 
assumptions,  the test allows us to  determine the probability that our 
results could have arisen by chance  in the absence of the 
hypothesized effect (i.e. if th e null hypothesis were true).</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Writing up an experiment
1. Introduction
 Outline of the purpose of the experiment
 state hypotheses tested etc
 provide background information (possibly including 
descriptions of relevant previous results, theoretical 
issues etc).
2. Procedure - what was done and how.
 instructions for replication, e.g.
 Experimental materials
 Subjects
 Recording procedure
 Measurement procedures (especially measurement 
criteria ).</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Hypothesis Testing
 Basic assumption: The samples are drawn from normal populations.
+3S2.5%
68%
95%
99.7%+2S +S -S -2S -3S X
Figure by MIT OpenCourse Ware. Adapted from Kachigan, S. K. Multivariate Statistical Analysis. 2nd ed. New York, NY: Radius, 1991 .</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Hypothesis Testing
 Basic assumption: The samples are drawn from normal populations.
 Properties of distribution of  means of samples of size Ndrawn from a 
normal population:
 The sample means are normally distributed.
 Mean is the same as the population mean.
 The variance is less than the population variance:
M2 = 2
N</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Hypothesis Testing: t-test
 Basic concept: If we know what the distribution 
of sample means would be if the null hypothesis 
were true, then we can calculate the probability 
of obtaining the observed mean, given the null 
hypothesis.
 We arrive at the parameters of the distribution of 
sample means through assumptions and 
estimation.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
Basic statistics</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Some Statistics
Two uses of statistics in experiments:
 Summarize properties of the results (descriptive statistics).
 Test the significance of results (hypothesis testing).</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Reading: 
 Fowler &amp; Housum 1987.
Assignments:
 Write up voicing perception experiment 
(due in two weeks 5/8).
 Progress report on your project (5/1).
 Project draft + presentation 5/15.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Writing up an experiment
The report on an experiment usually consists 
of four basic parts:
1. Introduction
2. Procedure
3. Results
4. Discussion</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Fitting models
 Statistical analyses generally involve fitting a model to the 
experimental data.
 The model in a t-test is fairly trivial, e.g.
duration = + syllable_type (syllable_type  is open or  closed)</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Writing up an experiment
3. Results
 Presentation of results, including descriptive statistics 
(means etc) and statistical tests of hypotheses.
4. Discussion
 Discuss the interpretation and significance of the results</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Hypothesis testing
 Statistical tests like the ttest give us the proba bility of obtaining the 
observed results if the null hypoth esis were correct - the p value. 
E.g. p &lt; 0.01, p = 0.334.
 We reject the null hypothesis if the experimental results would be 
very unlikely to have arisen if  the null hypothesis were true.
 How should we set the threshold for rejecting the null hypothesis?
 Choosing a lower threshold increa ses the chance of incorrectly 
accepting the null hypothesis.
 Choosing a higher threshold increa ses the chance of incorrectly 
rejecting the null hypothesis.
 A common compromise is to reje ct the null hypothesis if p &lt; 
0.05, but there is nothing magical about this number.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Adaptive dispersion</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec4_dispersion/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>Recent work by Diehl, Lindblom and Creeger (2003) 
suggests that the greater perceptual significance of F1 
probably follows from the higher intensity of F1 relative to F2.
 F1 should be more salient audito rily and more robust to noise.Too many high non-peripheral vowels 
0
Frequency (Hz)350002040
0
Frequency (Hz)350002040 level (dB/Hz)
 level (dB/Hz)Sound pressure
Sound pressure</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Why does the range of possible F2 values taper as F1 
increases?
 How do you achieve maximum and minimum F1?
 How do you achieve maximum and minimum F2?The vowel space
Figures by MIT Open Course Ware . Adapted from Liljencrants, Johan, and Bjorn Lindblom. 
"Numerical Simulation of Vowel Quality Systems: The Role of Perceptual Contrast." 
Lang uage 48, no. 4 (December 1972): 839-862. MEL
MELkHz
500 1000 15001500
2000
4.03.02.01.52.5 1.0 1.5 .5Second Formant (M 2)
Third Formant (F 3)Third Formant (F 3)
MEL
MELkHz
500 1000 15007505002502.52.5
.51.0 1.5 .5
Second Formant (M2)
First Formant (F1)First Formant (F1)
.75</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
The Theory of Adaptive 
Dispersion
Image by MIT OpenCourseWare. Adapted from Liljencrants, Johan, and Bjorn Lindblom. "Numerical Simulation of Vowel Quality Systems: 
The Role of Perceptual Contrast." Language  48, no. 4 (December 1972): 839-862.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>All inventories larger than 5 are predicted to contain one or 
more high vowels between [i] and [u], e.g. [y, , ].
 E.g. prediction for 7 vowels (unattested):
 Common 7 vowel inventories:Too many high non-peripheral vowels 
Figure by  MIT O penC ourseW are.Figure by  MIT Open Course Ware .
Figure by  MIT O penC ourseW are.i
ecu
a 
i i y u u
e e o o
a aec</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>New simulations of 7 vowel system by Diehl, Lindblom 
and Creeger (2003)
 incorporate background noise
 perceptual distance is calculate d as difference between auditory
spectra.Too many high non-peripheral vowels 
2.5
2.01.51.0
.5
.2 .4 .6 .8 .2 .4 .6 .87 7Second formant frequency (kHz)
First formant frequency (kHz)
Figure by MI T OpenC ourseW are. Adatped from Diehl, R.  L., B. Lindblom, and C. P. Creeger. "Increasing Realism of  
Auditory Representations Y ields F urther I nsights I nto V owel P honetics." Proceedings of the 15th  International 
Congress of Phonetic Sciences. Vol. 2.  Adelaide, Australia: Causal Publications, 2003, pp.1381-1384.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Higher formants are not 
important in vowel quality 
because they are insufficiently perceptible 
(especially in noise).
 There is less energy in the 
voice source at higher 
frequencies.
 Our ears are less sensitive to 
higher frequencies.Why do the perceptual dimensions of vowel quality 
correspond to F1, F2 (&amp;F3)?
0.005 0.01 s
Frequency in HzIntensity Level in dB
Lower limit of audibilityUpper limit of hearing
-20-100102030405060708090100110120130140150
20 100 1K 10K 20KFigure by  MIT Open Course Ware .
Figure by  MIT Open Course Ware .
Figure by  MIT Open Course Ware .0
-10
-20
-30
-40dB
Hz0 1000 2000 3000Hz</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Approach to exploring dispersion hypothesis:
 Modeling
 Simulation
 Comparison of simulation results to impressionistic 
descriptions of a large sample of vowel inventories.Liljencrants &amp; Lindblom (1972)</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>(cf. Pierrehumbert 2000)
Production - we can control formant frequencies.
 Given that vowels are produced with a relatively open vocal 
tract, the properties of these sounds that we can manipulate 
most easily are:
 f0 (pitch) - a source property. The basis for tone contrasts.
 formants - filter property - the resonant frequencies of the vocal 
tract.
 Bandwidths and formant intensities generally covary with 
formant frequencies (Fant 1956).
 Varying bandwiths independently w ould involve changing the stiffness 
of the vocal tract walls, or the m ode of vocal fold vibration. (NB 
nasalization affects formant bandwidths).Why do the perceptual dimensions of vowel quality 
correspond to formant frequencies?</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Perceptual distinctiveness of contrast between Viand Vj: 
distance between vowels in perceptual vowel space
where xnis F2 of Vnin mel
ynis F1 of Vnin mel
 Maximize distinctiveness: select N vowels so as to 
minimize Erij=(xixj)2+(yiyj)2Liljencrants and Lindblom (1972)
E=1
rij2
j=0i1

i=1n1</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Perception - we can perceive formant peaks.
 f0 is (usually) much lower than formant frequencies.
 Resonant frequencies are well represented as peaks in 
the ouput spectrum.
 Exception: soprano singing.
 Formant peaks are more robustly perceptible than valleys
because they can rise above background noise.Why do the perceptual dimensions of vowel quality 
correspond to formant frequencies?</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>When an inventory has mid vowels [e , o] and front rounded vowel [y], 
it often has mid front [] as well (Finnish, German, French, etc)
 L&amp;L predict that interior vowels only appear with 10 or more vowels.
 The absence of interior vowels [ , ] is a result of the way in which 
overall distinctiveness is calculated.
 Each vowel contributes to Ebased on its distance from every other 
vowel.
 Interior vowels have a high cost because they are relatively close to all 
the peripheral vowels.
 Perhaps the measure of distinctiveness, E, can be improved on.Too few interior vowels
Figure by  MIT O penC ourseW are.i i y u u
e e o o
a a</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>L &amp; L  s  m e a s u r e  E is based on an analogy to dispersion of charged 
particles - it is not derived from anything based on vowel perception.
 It has the important property that distinctiveness cost increases more 
rapidly as two vowels become closer - 1/rij2
 I.e. vowels are only likely to be confused if they are quite similar. 
Likelihood of confusion drops of quickly as distance increases.
 But perhaps 1/rij2doesnt drop off quickly enoug h - the lack of interior 
vowels results from giving too much we ight to vowel pairs that are not 
very close.
 An alternative (Flemming 2005): onl y consider the closest pair of 
vowels in the inventory.
 Compromise (to be explored): 1/rijn, n&gt; 2.Alternative measures of distinctiveness</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>If perceptual distinctiveness is important in shaping vowel 
inventories, then it should play a similar role in shaping 
consonant inventories.
 It is harder to develop quan titative models in this area 
because it is less clear what the perceptual dimensions are.
 Especially because many consonants ca nnot be treated as static, e.g. 
stops.
 Note that this is an issue for vowels also - how do diphthongs and 
vowel duration contrasts fit into the model?Extending Adaptive Dispersion</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Specific instantiations of the model have made specific 
incorrect predictions (but some of the broad predictions are 
correct and models are improving).
 The model answers an inobvious question: Given N 
vowels, what should they be? - what determines the size of 
inventories?
 TAD predicts a single best inventory for each inventory 
size. Why would languages have sub-optimal inventories?
 The unattested inventories shown earlier are obviously very porrly
dispersed, but there are a variety of attested inventory patterns for 
any given number of vowels.Problems with Adaptive Dispersion</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Try to explain why vowel systems are the way they are.
 Observation: vowels in an inventory tend to be evenly 
dispersed through the vowel space (cf. Disner 1984).
 Hypothesis: this facilitates efficient communication by 
minimizing the likelihood of confusing vowels.Lindbloms Theory of Adaptive Dispersion
 i     u   i     u   i     u  
       e    o  e
    o  
   a       a       a    
 
Figure by MIT OpenC ourseWare. Adapted from Liljencrants, Johan, and Bjorn 
Lindblom. "Numerical Simulation of Vowel Quality Systems: The
Role of Perceptual Contrast." Lang uage 48, no. 4 (December 1972): 839-862. 
MEL
MELkHz
500 1000 15007505002502.52.5
.51.0 1.5 .5
Second Formant (M2)
First Formant (F1)First Formant (F1)
.75</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Start with vowels arranged in a circle  near the center of the vowel space. 
(Random arrangement might be better?)
 Pick a vowel at random.
 Try small movements of that vowel in 6 directions (within the vowel 
space)
 Select the direction that re sults in greatest reduction in E.
 Move vowel in that direction until Estops decreasing, or a boundary 
is reached.
 Repeat for all vowels.
 Cycle through the vowels until no further reduction in Ecan be 
achieved.
 Should be repeated multiple times, preferably with different starting 
configurations.
 More sophisticated search strategi es are possible, e.g. simulated 
annealing or more sophisticated pro cedures for identifying best change 
at each stage.Minimizing E- stochastic search</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Maximize the minimum distance (Flemming 2005)Alternative measures of distinctiveness
2
2.5
3
3.5
44.55
5.566 8 10 12 14F2 (Bark)
stressed unstressed</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Considerations of formant in tensity might also help to 
account for some exceptions to the generalization that every 
language includes the corner vowels [i, a, u].
 L&amp;L predict that this should be the case, and most 
languages do include all three, but a number of languages 
lack [u]:
 [i, a, o], e.g. Piraha, Axeninca Campa
[i, e, a, o], e.g. Navajo, Klamath
 [i, e, a, o, ], e.g. Tokyo Japanese 
 In general F1 is more intense where it is higher, and this 
also raises the intensity of all higher formants. In [u], both 
F1 and F2 are low, resulting in a low intensity vowel, with 
low intensity F2.The corner vowels [i, a, u]</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Why does the vowel space look like this?
Why do the dimensions correspond to formant 
frequencies?
Why just the first 2-3 formant frequencies?
Why does the F1-F2 space have this shape?The vowel space
Figures by MIT Open Course Ware . Adapted from Liljencrants, Johan, and Bjorn Lindblom. 
"Numerical Simulation of Vowel Quality Systems: The Role of Perceptual Contrast." 
Lang uage 48, no. 4 (December 1972): 839-862. MEL
MELkHz
500 1000 15001500
2000
4.03.02.01.52.5 1.0 1.5 .5Second Formant (M 2)
Third Formant (F 3)Third Formant (F 3)
MEL
MELkHz
500 1000 15007505002502.52.5
.51.0 1.5 .5
Second Formant (M2)
First Formant (F1)First Formant (F1)
.75</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Try to explain why vowel systems are the way they are.
 Observation: vowels in an inventory tend to be evenly 
dispersed through the vowel space (cf. Disner 1984).
 Hypothesis: this facilitates efficient communication by 
minimizing the likelihood of confusing vowels.
 Vowels that are closer in the perceptual space are more 
easily confused.
 Confusions between contrasting sounds impair 
communication.
 So contrasting vowels should be as far apart as possible 
(dispersion).Lindbloms Theory of Adaptive Dispersion</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Liljencrants and Lindblom 
(1972)
 The role of perceptual contrast 
in predicting vowel inventories.
 The perceptual space of 
articulatorily possible vowels:
Figures by MIT O penC ourseW are. Adapted from Liljencrants, Johan, and Bjorn Lindblom. 
"Numerical Simulation of Vowel Quality Systems: The Role of Perceptual Contrast." 
Lang uage 48, no. 4 (December 1972): 839-862. 
MEL
MELkHz
500 1000 15001500
2000
4.03.02.01.52.5 1.0 1.5 .5Second Formant (M 2)
Third Formant (F )Third Formant (F 3)
3MEL
MELkHz
500 1000 15007505002502.52.5
.51.0 1.5 .5
Second Formant (M 2)
First Formant (F 1)First Formant (F 1)
.75</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>The excess of central vowels arise because measuring 
distinctiveness in terms of di stance in formant space gives 
too much weight to differences in F2.
 In general, languages have more F1 contrasts than F2 contrasts.
 Why are F1 differences more distinct than F2 differences?
 One factor: auditory sensitivity to frequency (next slide).
 But L&amp;L already took this into  account - mel scaled formant 
frequencies.Too many high non-peripheral vowels</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Predicted optimal 
inventories
 Reasonable 
approximations to 
typical 3 and 5 
vowel inventories are derived.
 Preference for [i, a, 
u] is derived.
 Problem: Too 
many high, non-
peripheral vowels.
 Not enough mid 
non-peripheral 
vowels.2.5
2.0
1.5
1.0
.5
2.5
2.0
1.5
1.0
.5
2.5
2.0
1.5
1.0
.5
.2 .4 .6 .8 .2 .4 .6 .8 .2 .4 .6 .8Second Formant (kHz)
First Formant (kHz)3
6 7 84 5
9 10 11
.2 .4 .6 .812
Figure b y MIT O penC ourseW are. Adapted from Liljencrants, Johan, and Bjorn Lindblom. "Numerical Simulation of 
Vowel Quality Systems: The Role of Per ceptual Contrast." Language 48, no. 4 (December 1972):  839-862.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Italian vowels
i
e

aou200
400
600
800500 700 900 1100 1300 1500 1700 1900 2100 2300 2500F2 (Hz)
ERB scales
6
8
1012141615 17 19 21 23 25F2 (E)
E(F1)004812162024
1 2 3 4 5 6 7 8 9 10
Frequency (kHz)Frequency (Bark)
Figure by MIT O penCourse Ware. Adapted from Johnson, Keith.
Acoustic and Auditory Phonetics . Malden, MA: Blackwell
Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Common vowel inventories:
 i     u   i     u   i     u  Lindbloms Theory of Adaptive Dispersion
       e    o  e  
    o  
   a       a       a    
Arabic, 
Nyangumata, 
Aleut, etc. Spanish, 
Swahili, 
Cherokee, etc.  Italian, 
Yoruba, 
Tunica, etc. 
 
 Unattested vowel inventories:
 i       i        i     u  
 e      e        
      
 
   a       a</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Prediction: vowel inventories with a given number of 
vowels should arrange those vowels so as to minimize E.
 What are those predicted vowel arrangements?
 Optimization problem: For N vowe ls, find F1, F2 values that 
minimize E (objective function).
 Large search space, many local minima.Liljencrants and Lindblom (1972)
E=1
rij2
j=0i1

i=1n1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings for next week: 
 Steriade (1999), pp. 1-21
 Wright (2004).
Assignment:
 Waveform editing</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Spectral analysis
Licensing by cue</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec5_spectral/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>All-pole filter
-0.8-0.6-0.4-0.200.20.40.60.811.2
- 1 00 1 02 03 04 05 06 07 08 0u(n)
s(n)
s(n)=0.4s(n-1)-0.7s(n-2)+0.6s(n-3)-0.1s(n-4)+u(n)</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Pre-emphasis
-1.5-1-0.500.511.5
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73 76 79pre-emphasis=0
pre-emphasis=1
pre-emphasis=0.5y(n) = s(n) - ps(n-1 )</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Filter order
 In any case, try a range of filter orders and see what works 
best.
 Problems for this rule of thumb can arise if there are zeroes in
the speech signal. These can be introduced by nasalization, 
laterals, or breathiness.
 If you use too many coefficients, there may be spurious peaks 
in the LPC spectrum, if you use too few, some formants may 
not appear in the LPC spectrum.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Source-Filter Model of Speech Production
400
200
0.1 0.2 0.3Output from lips Glottal airflow
Time (in secs)
3030 30
2020 20
1010 10
00 0
0 1000 2000 3000500 2500 0 1000 2000 3000 1500Frequency (Hz)Frequency (Hz) Frequency (Hz)
Source Spectrum Output SpectrumResonances =
Formant FrequenciesVocal Tract
Filter Function
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Frequency resolution
 A spectrogram consists of a sequence of fourier
spectra.
 The bandwidth of a spectrogram depends on the 
window length used to calculate the spectra.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Reading: 
 Fujimura et al (1978)
 Johnson chapters 7 and 8.
Assignments:
 Measure formants of your vowels.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Different contrasts have different characteristic patterns of 
distribution (Steriade 1999):
(i) Obstruent voicing contrasts are permitted only before 
sonorants
(e.g. German, Lithuanian, Russian, Sanskrit).
(ii) Major place contrasts (labial vs. coronal vs. dorsal) are 
permitted only before vowels 
(e.g. Japanese, Luganda, Selayarese).
(iii) Retroflexion contrasts (retroflex vs. apical alveolar) are 
permitted only after vowels 
(e.g. Gooniyandi, Miriwung, Walmatjari).</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Steriade (1997) - obstruent voicing
 Implicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)
Totontepec Mixe
Lithuanian
French
Shilha
Khasi#_O, O_#
e.g. bsa vs. psaR_O
e.g. absa vs. apsaR_#
e.g. ab vs. ap_R
e.g. ba vs. paR_R
e.g. aba vs. apa
no
no
no
no
yesno
no
yes
yesyesno
yes
yes
yes
yes yesyesyesyesyes no
no
no
yes
n/a
O = obstruent, R = sonorant, inc. vowel
Figure by  MIT O penCourse Ware . Adapted from Steriade, Donca. Phonetics in Phonology: The Case of Laryngeal Neutralization. Manuscript, UCLA, 1997. (PDF)   ___</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Linear Predictive Coding
 The type of digital filter used to model the vocal tract filter in 
LPC (an all pole filter) can be expressed as a function of the 
form:
s(n)= aks(nk)+Gu(n)
k=1N

 So an LPC filter is specified by a set of coefficients ak
 The number of coefficients is called the order of the filter and
must be specified prior to analysis.
 Each pair of coefficients defi nes a resonance of the filter.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Linear Predictive Coding
 The source-filter theory of speech production analyzes speech 
sounds in terms of a source, vocal tract filter and radiation 
function.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Fourier Analysis
 For each analysis frequency, we calculate how well the 
sine and cosine waves of that frequency correlate with the 
speech wave.
 This is measured by multiplying the amplitude of each 
point of the speech wave by the amplitude of the corresponding point in the sinusoid and summing the 
results (dot product).
 Intuitively:
 if the waves are similar, they will  be positive at the same time and 
negative at the same time, so th e multiplications will yield large 
numbers.
 if the waves are moving in opposite  directions, the multiplications 
will yield negative numbers.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Zero padding
 FFT only works on windows of 2nsamples.
 If you select a different window length, most 
acoustic analysis software adds zero samples to 
the end of the signal to pad it out to 2nsamples.
 This does not alter the overall shape of the 
spectrum.
 PRAAT will do DFT (no zero padding) and FFt
(zero padding as required).</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Frequency resolution
 The interval between the frequencies of successive 
components of  the analysis depends on the window 
length.
 The first component of the analysis is a wave with period 
equal to the window length
= 1/window duration
= sampling rate/window length
 E.g. with window length of 25.6ms, the first component if 
the DFT analysis has a frequency of 1/0.0256 s = 39 Hz.
 The other components are at multiples of  this frequency: 
78 Hz, 117 Hz,...
 so the components of the analysis are 39 Hz apart.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Spectral analysis techniques
There are two major spectral analysis techniques used with 
speech:
 Fourier analysis
 Linear Predictive Coding (LPC)
 Fourier analysis is used to calculate the spectrum of an 
interval of a sound wave.
 LPC attempts to estimate the properties of the vocal tract 
filter that produced a given interval of speech sound.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Linear Predictive Coding
 The source-filter theory of speech production analyzes speech 
sounds in terms of a source, vocal tract filter and radiation 
function.
 Linear Predictive Coding (LPC) analysis attempts to 
determine the properties of the vocal tract filter through analysis by synthesis.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Practical considerations
What filter order should one use?
 Each pair of LPC coefficients specifies a resonance of the 
filter.
 The resonances of the filter should correspond to the formants 
of the vocal tract shape that generated the speech signal, so the 
number of coefficients we should use depends on the number of formants we expect to find.
 The number of formants we expect to find depends on the 
range of frequencies contained in the digitized speech signal -i.e. half the sampling rate.
 Generally we expect to find ~1 formant per 1000 Hz. So a general rule of thumb is to set the filter order to the 
sampling rate in kHz plus 2
 2 for each expected formant, plus two to account for the 
effects of higher formants and/or the glottal spectrum.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Window length
 Window length is often measured in points (1 
point = 1 sample).
 e.g. 256 points at a sampling rate of 10 kHz is 
0.0256s (25.6 ms).
 Most speech analysis software uses the Fast 
Fourier Transform algorithm to calculate DFTs.
 This algorithm only works with window lengths 
that are powers of 2 (e.g. 64, 128, 256 points).</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>LPC spectrum
Frequency (Hz)0 400020020
Frequency (Hz)0 400002040</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Fourier Analysis
 The degree of correlation indicates the relative 
amplitude of that frequency component in the 
complex wave.
 The correlation between two sinusoidal waves of 
different frequencies is always zero - i.e. the 
contribution of each frequency component to a 
complex wave is independent of the other 
frequency components.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>LPC: filter order
Frequency (Hz)0 400020020
Frequency (Hz)0 400002040
Frequency (Hz)0 4000204060N= 12
N= 10N= 12Frequency (Hz)0 4000204060
Frequency (Hz)0 400002040
Frequency (Hz)0 4000204060
N= 18</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Fourier Analysis
 The basic idea is to compare the speech wave with 
sinusoidal waves of different frequencies to 
determine the amplitude of that component 
frequency in the speech wave.
 What do we compare with what?
 A short interval (window) of a waveform with:
 Sine and cosine waves with a period equal to the 
window length and 
 sine and cosine waves with multiples of this first 
frequency.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>DFT - window length
Frequency (Hz)0 400020020
23 msFrequency (Hz)0 400020020
46 ms
12 msFrequency (Hz)0 400020020
Frequency (Hz)0 400020020
5 ms</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Window function
from mi.eng.cam.ac.uk/~ajr/SpeechAnalysis/FFT of rectangular and Hamming windowed sine wave in dB300 250 200 150 100 50 0-40-30-20203040
-1010
0
300 250 200 150 100 50 0-80-602040
-40-200600 500 400 300 200 100 0-11
-0.80.8
-0.60.6
-0.40.4
-0.20.2
0
600 500 400 300 200 100 0-11
-0.80.8
-0.60.6
-0.40.4
-0.20.2
0
Figure by MIT OpenC ourseWare.
Figure by MIT OpenC ourseWare.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Steriade (1997) - obstruent voicing
 Markedness of obstruent voicing contrast in 
context C depends on strength of cues to voicing in 
C.
Environment
*aV oice/ V_ [-son]Cues
clo voi, clo dur
clo voi, clo dur, V1 dur, F0, F1 in V1
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp, F0, F1 in V2*aV oice/ [-son] _ [-son], [-son]_#, #_[-son]
*aV oice/ V_ #
*aV oice/ V_ [+son]&gt;&gt;
&gt;&gt;
&gt;&gt;
&gt;&gt;
Figure by MIT OpenCourse Ware . Adapted from Steriade, Donca. Phonetic s in Phonology: The Case of Laryngeal N eutralization. 
Manuscript, UCLA, 1997. (PDF)    ___</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Window function
 If we take n samples directly from a waveform, it may begin 
and end abruptly.
 As a result, the spectrum of such a wave would include 
spurious high frequency components.
 To avoid this problem we multiply the signal by a window 
function that goes smoothly from 0 to 1 and back again.
 There are many such window functions (Hamming, Hanning
etc). It doesnt matter much which you use, but use one.
Hamming
Figure by MIT OpenC ourseWare.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>LPC analysis
 LPC analysis is based on a simple source-filter model of 
speech (the vocal tract is a lossless all-pole filter), so it should 
be well-suited to the analysis of speech as long as the 
assumptions of the model are met.
 However we have to specify the filter order, and it may be 
difficult to determine the correct order.
 This is especially problematic where the actual vocal tract 
filter contains zeroes, violating the assumptions of the model.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Hypothesized explanation: The likelihood that distinctive 
values of the feature F will occur in a given context is a 
function of the relative perceptibility of the F-contrast  in 
that context (Steriade 1999).
 Contrasts differ in their distribution of cues so they are 
subject to different patterns of neutralization.
 Obstruent voicing is best cued by Voice Onset Time - only 
realized with a following sonorant.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Pre-emphasis
 The spectrum of the voicing source falls off steadily as 
frequency increases.
 LPC analysis is trying to model vocal tract filter.
 This is often more successful if the spectral tilt of the glottal 
source is removed before LPC analysis.
 This is achieved by applying a simple high-pass filter (pre-
emphasis):
y(n) = s(n) - ps(n-1)
 where p is between 0 and 1. 
p= 1 yields the greatest high frequency emphasis. Typical 
values are between 0.9 and 0.98.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
Spectral Analysis
Frequency (Hz)0 400020020</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Linear Predictive Coding
 If we knew the form of the source and the output waveform, we could 
calculate the properties of the filter th at transformed that  source into that 
output.
 Since we dont know the properties of the source, we make some simple 
assumptions: There are two types of source; flat spectrum white noise
for voiceless sounds, and a flat sp ectrum pulse train for voiced sounds.
 The spectral shape of the source can then be modeled by an additional 
filter.
 Thus the filter calculated by LPC anal ysis includes the effects of source 
shaping, the vocal tract transfer functi on, and the radiation characteristics. 
 However, both of these typically affect  mainly spectral slope (for vowels, 
at least), so the locations of the peak s in the spectrum of the LPC filter 
still generally correspond to re sonances of the vocal tract.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Linear Predictive Coding
 The various techniques for calculating LPC spectra are based 
around minimizing the difference between the predicted 
(synthesized) signal and the actual signal (i.e. the error).
(Actually the squared difference is minimized).</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Frequency resolution
 A shorter window length implies that the first component 
has a higher frequency, so the interval between 
components is larger.
 So there is a trade-off between time resolution and 
frequency resolution in DFT analysis.
Window length   Interval between components
50 ms 20 Hz
25 ms 40 Hz
12.5 ms 80 Hz
6.4 ms 160 Hz</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Window function
 Tapering the window only reduc es the amplitude of spurious 
components, it does not eliminate them.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
 Phonological contrasts generally have restricted 
distributions.
 E.g. Lithuanian voicing contrasts
a. obstruent voicing is distinc tive before vocoids and consonantal 
sonorants:
ukle nuk niati au glingas dregna
silpnas ry tmetss k o bnis bd metys
b. obstruent voicing is neutralized (to voiceless) word-finally:
[dauk][ k a t]
c.obstruent voicing is neutralized be fore any obstruent (assimilating in 
voicing to following obstruent):
a[d-g]al m[ z-d]avau dr[p -t]i d[ k-t]i</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Fourier Analysis
 A complex wave can be analyzed as the 
sum of sinusoidal components.
 Fourier analysis determines what those 
components are for a given wave.
 The procedure we will use is the Discrete 
Fourier Transform.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Basic audition
Digital signal processing</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec2_audition/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Time course of auditory nerve response 
Response to a noise burst:
 Strong initial response
 Rapid adaptation (~5 ms)
 Slow adaptation (&gt;100ms)
 After tone offset, firing rate 
only gradually returns to 
spontaneous level.0128 -60256
0
0 64 128128 -40256
msec
Figure by MIT OpenCourse Ware. Adapted from Kiang et al.  (1965)</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Audition
AnatomyEustachian TubeOuter EarMiddle
EarInner Ear
Auditory
Nerve
Cochlea EardrumEar CanalEar Flap
HammerAnvil
Stirrup
Figure by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Quantization
 Quantizing an analog signal necessarily introduces 
quantization errors.
 If the signal level is lower, the degradation in signal-to-
noise ratio introduced by quantization noise will be greater, 
so digitize recordings at as high a level as possible without 
exceeding the maximum amplitude that can be represented (clipping).
 On the other hand, it is essential to avoid clipping.
Amplitude
0 5 10 15 20 25
Time (ms)
Figure by MIT OpenCourseWare. Adapted   from Johnson, Keith. 
Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Voicing and aspiration
 Voiced vs.voiceless [b vs. p]
 Russian, French, Dutch
 Unaspirated vs. aspirated [p vs. p ]
 Mandarin, Cantonese
 Voiced vs. voiceless unaspirated vs. aspirated [b vs. p vs. 
p]
 Hindi, Thai
 English shows contextual variation between 
voicing and aspiration.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Sampling
 In order to represent a wave 
component of a given frequency, it 
is necessary to sample the signal 
with at least twice that frequency 
(the Nyquist Theorem).
 The highest frequency that can be 
represented at a given sampling rate 
is called the Nyquist frequency.
 The wave at right has a significant 
harmonic at 300 Hz
 (a) sampling rate 1500 Hz
 (b) sampling rate 600 Hz
 (c) sampling rate 500 Hz0 .01 .02 .03 Time Sec
0 .01 .02 .03 Time Sec
0 .01 .02 .03 Time SecA wave with a fundamental frequency of 100 Hz and a major
component at 300Hz sampled at 1500Hz.
The same wave sampled at 600 Hz.(a)
(b)
(c)
Figure by MIT O penCourse Ware. Adapted from Ladefoged, Peter. 
L104/204 Phonetic Theory lecture notes, University of California, Los Angeles.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Masking - simultaneous
 Energy at one frequency can reduce audibility of 
simultaneous energy at another frequency (masking).
 One sound can also  mask a preceding or following sound.
4000 3600 3200 2600 2400 2000 1600 1200
Frequency of masked toneMasking
1000 800 600 40010
1102103104
Example of masking of a tone by a tone. The frequency of the masking tone is 1200 Hz. Each curve corresponds to a
different masker level, and gives the amount by which the threshold intensity of the masked tone is multiplied in the
presence of the masker , relative to its threshold in quiet. The dashed lines near 1200 Hz and its harmonics are estimates
of the masking functions in the absence of the ef fect of beats. 
Figure by MIT OpenCourse Ware. Adapted fr om Stevens, Kenneth N. Acoustic Phonetics. Cambridge, MA: MIT Press, 1999. ISBN; 9780262194044.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Intensity
 Perceived loudness is more closely related to intensity 
(power per unit area), which is proportional to the square 
of the amplitude.
 relative intensity in Bels = log10(x2/r2)
 relative intensity in dB   = 10 log10(x2/r2)
=  20 log10(x/r) 
 In absolute intensity measurements, the comparison 
amplitude is usually 20 Pa, the lowest audible pressure 
fluctuation of a 1000 Hz tone (dB SPL).</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Quantization
 The amplitude of the signal at each sampling point must be 
specified digitally - quantization.
 Divide the continuous amplitude scale into a finite number 
of steps. The more levels we use, the more accurately we 
approximate the analog signal.
Figure by MIT O penCourse Ware. Adapted from Johnson, Keith. 
Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483. 20 15 10 5 020 steps
200 steps
Time (ms)Amplitude</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Loudness
 The perceived loudness of a sound depends on the 
amplitude of the pressure fluctuations in the sound 
wave.
 Amplitude is usually measured in terms of root-
mean-square (rms amplitude):
 The square root of the mean of the squared amplitude 
over some time window.</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Time (s)547.195 547.4850.14680.2644
0
Time (s)547.195 547.48505000VOT, closure voicing
 English intervocalic stops can be fully voiced
 VOT is 0 ms in 2nd and 3rd stops
brigadoo(n)</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Loudness
 The relationship between intensity and perceived loudness 
is not exactly logarithmic.
500,000 1,000,000 1,500,000 2,000,000 00102030405060708090100
02468101214161820
Pressure ( mPa)
dB SPLSonesdB SPL
Sones
Figure by MIT OpenCourse Ware.  Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Aliasing
 Components if a signal which are above the Nyquist
frequency are misrepresented as lower frequency 
components ( aliasing ).
 To avoid aliasing, a signal must be filtered to eliminate 
frequencies above the Nyquist frequency.
 Since practical filters are not infinitely sharp, this will 
attenuate energy near to the Nyquist frequency also.
10 8 6 4 2 0
Time (ms)Amplitude
Figure by MIT O penC ourseWare. Adapted from Johnson, Keith. 
Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188 483.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Voice Onset Time
 English utterance-initial stops
Voiceless unaspirated Voiceless aspirated
Time (s)1.1154 1.275580.4710.3268
0
Time (s)1.1154 1.2755805000 Time (s)4.26614 4.427060.37180.1853
0
Time (s)4.26614 4.427060500022 ms 86 ms
die tie</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Voicing and aspiration
 Many languages make a contrast between two sets of stops 
with different laryngeal properties, loosely referred to as 
voiced and voiceless.
 The precise details of these laryngeal contrasts differ from 
language to language.
 Some broad distinctions:
 voiced [b]: vocal fold vibration during closure
bal(hair)
 voiceless unaspirated [p]: no vibration of the vocal 
folds, short VOT
pal(take care of)
 voiceless aspirated [p ]: no vibration of the vocal folds, 
long VOT (high airflow after release)
pal(knife blade)
Listen to all three sound files here.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>24.910
Linguistic Phonetics
Analog-to-digital conversion of 
speech signals
2.0
1.6
1.2
0.8
0.4
-0.4
-0.8
-1.20.0
0.00 0.01 0.02 0.03 0.04 0.05
The Results Of Sampling
2.0
1.6
1.2
0.8
0.4
-0.4
-0.8
-1.20.0
0.00 0.01 0.02 0.03 0.04 0.05
Figure by  MIT O penC ourseW are.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Square each sample in the analysis window.
 Calculate the mean value of the squared waveform: 
 Sum the values of the samples and divide by the number of 
samples.
 Take the square root of the mean.rms amplitude
-1.5-1-0.500.511.5
0 0.05 0.1 0.15 0.2
timepressure
pressure^2
rms amplitude</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>What sampling rate should you use?
 The highest frequency that (young, undamaged) ears can 
perceive is about 20 kHz, so to ensure that all audible 
frequencies are represented we must sample at 2 20 kHz = 
40 kHz.
 The ear is relatively insens itive to frequencies above 10 
kHz, and almost all of the information relevant to speech 
sounds is below 10 kHz, so high quality sound is still obtained at a sampling rate of 20 kHz.
 There is a practical trade-off between fidelity of the signal 
and memory, but memory is getting cheaper all the time.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>logarithmic scales
 log xn= n log x
00.20.40.60.811.21.41.61.8
0 1 02 03 04 05 0
x</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Analog-to-digital conversion
 Almost all acoustic analysis is now computer-based.
 Sound waves are analog (or continuous) signals, but digital 
computers require a digital representation - i.e. a series of 
numbers, each with a finite number of digits.
 There are two continuous scales that must be divided into 
discrete steps in analog-to-digital conversion of speech: time 
and pressure (or voltage).
 Dividing time into discrete chunks is called sampling .
 Dividing the amplitude scale into discrete steps is called 
quantization .</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Loudness
 Loudness also depends on frequency.
 equal loudness contours for pure tones:
Source: Wikimedia Commons.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Audition
 Loudness
P i t c h
 Auditory spectrograms</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Pitch
 Perceived pitch is approximately linear with respect to 
frequency from 100-1000 Hz, between 1000-10,000 Hz the 
relationship is approximately logarithmic.
004812162024
1 2 3 4 5 6 7 8 9 10
Frequency (kHz)Frequency (Bark)
Figure by MIT O penC ourseW are. Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Spectrogram images removed due to copyright restrictions.
Figure 3.8 in Johnson, Keith. "Comparison of Normal Acoustic Spectrogram and Auditory Spectrogram or Cochleagram."
Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>No class next week (Tuesday is a Monday)
 Readings for 2/27: Johnson chs 5 &amp; 6
 Assignments (due 2/27):
 Basic acoustics.
 VOT and laryngeal contrasts in Mandarin and 
English.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Quantization
 The number of levels is spec ified in terms of the number of 
bits used to encode the amplitude at each sample.
 Using n bits we can distinguish 2nlevels of amplitude.
 e.g. 8 bits, 256 levels.
 16 bits, 65536 levels.
 Now that memory is cheap, speech is almost always 
digitized at 16 bits (the CD standard).</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>What sampling rate should you use?
 For some purposes (e.g. measuring vowel formants), a high 
sampling rate can be a liability, but it is always possible to 
downsample before performing an analysis.
 Audio CD uses a sampling rate of 44.1 kHz.
 Many A-to-D systems only operate at fractions of this rate 
(22050 Hz, 11025 Hz).</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Pitch
 The non-linear frequency response of the auditory system is related to the 
physical structure of the basilar membrane.
 basilar membrane uncoiled:
Figure by MIT Open Course Ware . Adapted from Johnson, Keith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483. 100300 750
500 1,100 2,000 3,7008,700 4,900 2,700
6,5001,500
11,50015,400</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
Basic Audition</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Sampling
 The amplitude of the analog 
signal is sampled at regular 
intervals.
 The sampling rate is measured 
in Hz (samples per second).
 The higher the sampling rate, 
the more accurate the digital representation will be.0 .01 .02 .03 Time Sec
0 .01 .02 .03 Time Sec
0 .01 .02 .03 Time SecA wave with a fundamental frequency of 100 Hz and a major
component at 300Hz sampled at 1500Hz.
The same wave sampled at 600 Hz.(a)
(b)
(c)
Figure by MIT O penC ourseWare. Adapted from Ladefoged, Peter. 
L104/204 Phonetic Theory lecture notes, University of California, Los Angeles.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>26 24 22 20 18 16 14 12 1010203050
40607080
8 6 4 2
Number of ERBs, EExcitation Level, dBV owel /I/
The spectrum of a synthetic vowel /I/ (top) plotted on a linear frequency scale,
and the excitation patterns for that vowel (bottom) for two overall levels, 50 and80 dB. The excitation patterns are plotted on an ERB scale.80
50
Image by MIT OpenCourseWare. Adapted from Moore, Brian. The Handbook
of Phonetic Science . Edited by William J. Hardcastle and John Laver.10 8 6 4 2 0801001201401601802002202400 2 4 6 8 10 12 14 16 18 20 22 24Auditory Frequency (Bark)Amplitude (dB)
Acoustic Frequency (kHz)
A comparison of acoustic (light line) and auditory (heavy line) spectra of a complex wave
composed of sine waves at 500 at 1,500 Hz. Both spectra extend from 0 to 10 kHz, althoughon different frequency scales. The auditory spectrum was calculated from the acoustic spectrum
using the model described in Johnson (1989).
Image by MIT OpenCourseWare. Adapted from Johnson, Keith. Acoustic and Auditory
Phonetics . Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.4,000 3,500 3,000 2,500 2,000 1,500 1,000 500 0404550556070
65758085
Frequency, HzV owel /I/Level, dB
Image by MIT OpenCourseWare. Adapted from Moore, Brian. The
Handbook of Phonetic Science . Edited by William J. Hardcastle and John Laver.
Malden, MA: Blackwell, 1997. ISBN: 9780631188483.
Malden, MA: Blackwell, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Italian vowels
i
e

aou200
400
600
800500 700 900 1100 1300 1500 1700 1900 2100 2300 2500F2 (Hz)
ERB scales
6
8
1012141615 17 19 21 23 25F2 (E)
E(F1)</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Interactions between sequential sounds
 A preceding sound can affect the auditory nerve response 
to a following tone (Delgutte 1980).
0 200 200 200200400
0600
400
M
ATNO  AT
TT = 27 dB SPLDischar ge rate (SP/S)AT = 13 dB SPL AT = 37 dB SPL
TT0 400
M0 400
M
Figure by MIT O penCourse Ware . Adapted f rom Stevens, Kenneth N. Acoustic Phonetics. Cambridge, MA: MIT Press, 1999. ISBN: 9780262194044, 
after Delgu tte, B. "Representation of Speech-like S ounds in the Discharge P atterns of Auditory-nerve Fibers." 
Journal of the Acoustical Society of America 68, no. 3 (1980): 843-857.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Loudness
 At short durations, loudness also depends on duration.
 Temporal integration: loudness depends on energy in the 
signal, integrated over a time window.
 Duration of integration is often said to be about 200ms, i.e. 
relevant to the perceived loudness of vowels.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>rms amplitude
.01 .02 0
Time in seconds
Figure by MIT OpenC ourseWare. Adapted from Johnson, K eith. Acoustic and Auditory Phonetics. Malden, MA: Blackwell Publishers, 1997. ISBN: 9780631188483.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>VOT, closure voicing
 Hindi - three-way contrast
 recordings from Ladefoged
http://www.phonetics.ucla.edu/vowels/chapter12/hindi.html
bal
hairTime (s)0 0.2311270.49720.494
0
Time (s)0 0.23112705000 Time (s)0 0.1135880.49710.3851
0
Time (s)0 0.11358805000 Time (s)0 0.1382 80.4750.2819
0
Time (s)0 0.1382 805000
pal
take care ofpal
knife blade
Listen to all three sound files here.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Auditory spectrograms
The auditory system performs a running frequency analysis of 
acoustic signals  - cf. spectrogram.
 A regular spectrogram analyzes frequency of equal widths, 
but the peripheral auditory system analyzes frequency bands 
that are wider at higher frequencies.
 Further disparities are introduced by the non-linearities of the 
peripheral auditory system, e.g.
 loudness is non-linearly related to intensity
 masking(simultaneous and nonsimultaneous)</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Phonetics and phonology of accent variation</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec11_accents/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Accents of English
Accents can differ in all aspects of phonology/phonetics
 Phoneme inventory - differences in the number and 
arrangement of phonemes.
 Phonological rules/phonotactics
 Phonetic realization - differences in the detailed realization of 
phonemes.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Regional Accents in the USA
 Traditional dialectology divides the USA into four major 
dialect areas based primarily on vocabulary (soda vs. pop, etc)
 North, Midlands, South, West
 Labov and colleagues (2006) have divided the USA into 
similar areas based purely in pronunciation
 Areas are grouped by distinctive combinations of shared 
sound changes - often sound changes in progress.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>12LOT-PALM merger
 A US innovation &gt; 
R.P. most US
ltl t lot
pmp m palm
b b bother 
f b bother 
/, // /</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Speaker normalization
 Dealing with dialect variation is conceptually similar to 
dealing with (within dialect) speaker variation, e.g. due to 
vocal tract size.
Ladefoged/BroadbentFigure by MIT OpenCourse Ware. i
Ii
Iu200
300
400
500
600
700
800
900
1000
11003000 2500 2000 1500 1000
eSecond formantFirst formant
 euHz
LL
WW</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>2Differences in allophonic rules
 California English // []/ _ [+nasal]
had stand
Time (s)0.250422 0.56010305000
Time (s)92.6242 92.963105000[Audio clip removed due to 
copyright restrictions]Listen: 
http://www.stanford.edu/~eckert/sounds/stand.wav</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Cross-dialect Communication
 Rakerd and Plichta (2003) adapted Ladefoged and 
Broadbents experimental method to show that perception 
of vowels is influenced by dialect information in the 
preceding context.
 Synthetic [-]continuum (hat-hot, sack-sock)
 Speakers and subjects from Detroit and Michigan Upper 
Peninsula.
 Detroit accent is characterized by fronting of / / and 
diphthongization of / / (Northern Cities Shift).
 Synthetic words were placed at the end of carrier phrases 
from Detroit and UP speakers.# #</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>4Eastern Massachusetts
 The Boston accent.
 Non-rhotic
Listen:
11_car.wav
11_spa.wav
11_floor.wav</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Norris, McQueen &amp; Cutler (2003)
 After lexical decision task, subjects categorized stimuli from 
an [f-s] continuum (same speaker).
 Boundary differed depending on condition in part 1:
1. [?] = [s], more stimuli categorized as [s].
2. [?] = [f], more stimuli categorized as [f].3. Non-word group did not differ from (1) or (2).
100
90
[?f]+[s]  words80
70ses60no [?]  nonwordspse50r ]f[ 40%
30
20 [?s]+[f]  words
10
0
121314151617181920212223242526
[f]-[s] continuum
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Norris, McQueen &amp; Cutler (2003)
 Evidence for rapid adaptation to a new pattern of pronunciation.
 Lexical decision task in Dutch.
 Some words contain a final sound [? ] that is ambiguou s between [f] and 
[s], created by averagi ng [f] and [s] waveforms.
 pretest to ensure ambiguity.
 Three conditions:
1. Words are meaningful if [?] is interpreted as [s].
 E.g [witlo?] - witlof chicory, witlos is not a word.
2. Words are meaningful if [?] is interpreted as [f].
 E.g [na:ldbo?] - naaldbos pine forest, naaldbof is not a word.
3. Non-word if [?] is interp reted as either [f] or [s].
 Subjects in each condition hear 20 ta rget words + the other 10 targets 
unedited + fillers.
 Subjects in (1) and (2) accepted ed ited words as corresponding word.</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>5Eastern Massachusetts
 The Boston accent.
 Non-rhotic
 Non-rhotic and variably rhotic accents are primarily found in 
E. New England, NYC, coastal plain of the South.
 But these areas also contain pockets of continuous 
rhoticity.
 This patterns seems to have resulted because r-loss spread 
from Southern England along trade routes to major ports of 
the Eastern seaboard, and then to surrounding areas.
 Non-rhotic accents used to be locally prestigious, but have 
largely lost their prestige and are in retreat.
Listen:
11_car.wav
11_spa.wav
11_floor.wav</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Differences in distribution of contrasts
Distinct [N=296]
Close [N=69]
Contrast in speech production of /l/ and /e/ before nasals in PIN and PEN, HIM and HEM.The PIN/PEN mergerSame [N=1 16]
Figure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>The Midlands
 Not very uniform. Primarily characterized by Labov as not 
participating in Northern Cities or Souther shifts.
 Some Southern features, e.g. unrounding of GOAT nucleus 
[].
 This is obviously a very broad characterization
 many small areas have distinctive accents that do not fit 
this classification (New York City, Philadelphia, Eastern 
New England etc).
 Does not incorporate cultural variation within regions, 
e.g. African American Vernacular English.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Differences in phoneme inventory
 Contrast / /, e.g. Inland North, Atlantic States
 Only //, West, NE New England
 Homophones: cot-caught, Don-dawn, hock-hawk
Contrast in production of /o/ and /oh/ before /t/ in COT vs. CAUGHT.The Merger of /o/ and /oh/Same [N=174]Distinct [N=262]
Close [N=70]
Figure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Phonetics and phonology of 
accent variation</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>US innovation: j-deletion
R.P. most US RP/US RP/US RP/US
tjun t un tune tul tool pjuni puny kjut cute
dju du dew du do bjuti beauty hju hue
sjut sut suit sun soon fju few
zjus zus Zeus zu zoo vju view
njuz nuz news nus noose mjuz muse
 j &gt;  / [+coronal] _
vljm, njn
 synchronic process also.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>An irregular sound change
 Regular sound change applies to all words that contain the 
relevant sound in the relevant context.
 Some sound changes appear to apply to a subset of words 
giving rise to complicated differences in lexical distribution.
 US/UK Englishes both have /, / but in different words
staff, bath, pass, grasp
dance, answer, demand, grant, example
 UK:  &gt; / _ voiceless fricative, NC
 but: gas, asp, passage, chaff, (plastic),
 but: romance, hand, band, ant, ample,</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Labov et al (1997)
The North
The urban dialect areas of the United States based on the
acoustic analysis of the vowel systems of 240 Telsur informants	The SouthThe Midland and the WestThe
Inland
NorthNorth
Central
The
Midland The West
The South
The South: The Southern Shift
Monophthongization of /ay/
F2 of checked /ey/ &gt; 2050 HzThe Coastal Southeast: Charleston and Savannah
retention of tense high and mid long vowelsNorth Central: Conservative long high and mid vowels
F2 of checked /ow/ &lt; 1100 Hz
F2 of /e/ - F2 of /o/ &lt; 375 Hz
/o/ = /oh/, vocalization of postvocalic /r/The Inland North: The Northern Cities Shift
Boston and E.N.E.: /r/ vocalization and low back merger
New York City: /I/ vocalization and raising of /Qh/, /oh/ 
Charleston-SavannahPhiladelphia
St.
Louis
South MidlandNo. MidlandProvidence
NYC PittsbghEastern
New
England
The North Midland: Approximates the initial position
Absence of any marked features on Map 1
Pittsburgh: localized monophthongization of /aw/
St. Louis: Localized merger of /ahr/ and /ohr/
Distance of /ohr/ from /ahr/ &lt; 125
The South Midland: fronting of checked /ow/
F2 of /ow/ &gt; 1350 Hz
Philadelphia: Northernmost extension of Southern Shift
Distance of /o/ from /oh/ &gt; 400. F2 of checked /ow/ &gt; 1275 Hz
The West: The low back merger and fronting of /uw/
/o/ = /oh/ before /t/: F2 of /uw/ &gt; 1850 Hz/o/ = /oh/, /aw/[a:]
Figure by MIT OpenCourseWare. Adapted from the Linguistics Laboratory of the University of Pennsylvania.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Accents and Dialects
Dialects of English can differ in all aspects of grammar
L e x i c o n
 soda, coke, pop
 Syntax
 I might do vs. I might
 The house needs painted (W. PA, E. Ohio, Scots)
 The house needs painting
 Phonology
 Phonetics
 Accent refers to phonetics and phonology only.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Differences in distribution of contrasts
 All accents contrast / , /.
 In some accents (e.g. South) this contrast is neutralized before
nasals.
pn pin, pen
hm him, hem
mni many, mini
lklength</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>The South
 The South is characterized by another series of vowel shifts, 
 referred to as the Southern Shif t, but its not clear to me why all 
the changes should be regarded as part of a single chain.
Figure by MIT OpenCourse Ware. 1
23456The  Northern  Cities  Shift
///i  /
idea
/e/
ked
/o/
cod/oh/
cawed/ b /
cud/i/
kid
cad
e</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Cross-dialect speech perception
 For Detroit listeners identification of continuum shifted as a function 
of carrier phrase.
Detroit (LM) carrier UP carrier100
07 6 5 4 3 2 1L
UP90
80
70
60
50
40
30
20
10Detroit (LM) carrier 
M-
-
07 6 5 4 3 2 1LMP
LMP100
90
80
70
60
50
40
30
20
10UP carrier
Figures by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Norris, McQueen &amp; Cutler (2003)
 Interpretation: subjects have learned that speaker has an 
unusual /s/ or /f/ on the basis of hearing this rendition in 20 
words. 
 This knowledge affects perceptual boundary between /f/ and 
/s/ for that speaker.
 i.e. subjects made a generalization about pronunciation 
of that sound.
 A follow-up study (Cutler et al 2005) followed the training 
phase with a cross-modal priming task (visual lexical decision following an auditory prime).
 Priming effect of modified words depended upon the 
interpretation of [?] learned in the training phase.
 Crucial words had not been heard in the training phase.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>3 Californian speakers (M open, F 
closed) (Hagiwara 1997).
//
// 
 N. Midwest speakers (M open, F 
closed) (Hillenbrand et al 1995).
[&gt;][Listen: 
http://www.stanford.edu/~eckert/sounds/move.wav ]
[Audio clip removed due to 
copyright restrictions][Listen: 
http://www.stanford.edu/~eckert/sounds/fund.wav ]
Differences in realizations of phonemes
Figures by MIT OpenCourseWare.i
iu200
300
400
500
600
700
800
900
1000
11003000 2500 2000 1500 1000
eSecond formantFirst formant
 eu200
300
400
500
600
700
800
900
100011003000 2500 2000 1500 1000 Second formantFirst formant
 i
i
e
eu
uHz Hz
LL
LL
WW
W W

I
II
I</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Cross-dialect Communication
 Labov points out that advanced Northern Cities 
pronunciations could result in apparent word changes for 
speakers of other accents.
 on &gt; Ann &gt; Ian
 block &gt; black
 Accent differences can lead to confusion, but we regular 
communicate across accents, and adapt quickly to new 
accents. How?
 Two experiments:
 Evidence that we can take accent into account in 
interpreting vowels.
 Evidence of a mechanism for rapid adaptation to new 
patterns of pronunciation.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>The North
 Much of the North is also characterized by Canadian Raising
 Usually written: /a / /_ [-voice]
/a/ /_ [-voice]
Listen to sound files here
- knife, knives
- lout, loud</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Cross-dialect speech perception
 Evidence for accent normalization is interesting because 
it cannot be achieved on the basis of the signal.
 For speaker-normalization, it has often been suggested that 
signals can be mapped onto a speaker-independent 
representation by a low-level transformation of the signal (e.g. formant ratios in place of formants).
 On the other hand, it has also been argued that speaker 
normalization requires that the signal be interpreted in relation to a model of the speaker that is constructed based on a variety of sources of information.
 Accent normalization fits into the second approach to 
normalization.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>The South
 One of the oldest Southern developments is PRICE 
monophthongization
a&gt; a (/_[-voice])
 GOOSE /u/ fronting Texas 
 THOUGHT // raising/diphthongization       Texas
 GOAT /o / &gt; [ ]   Texas
 Variably rhotic Texas
Audio files removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>j-deletion
 Actually deletion of [j] started earlier , applying in some environments in 
both English and US accents:
&gt; ju
 j &gt;  / palato-alveolars, Cl, _( o r  &gt; u)
17thC Most modern
tuz t uz chews
tuz t uz choose
u u threw
u u through
fluf l u flew
flu flu through
 In many English accents j-deletio n has since applied after [l], e.g. lewd</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Northern Cities Shift
 Chain shift: a series of connected sound changes. Can result in 
wholesale rotations of portions of the vowel system.
 Earliest stages: 
 fronting of LOT/PALM &gt; a
Buffalo Chicago Kenosha
 tensing of TRAP  &gt; e /
Buffalo Chicago Detroit
 Less advanced:
 THOUGHT lowering/unrounding
&gt; 
Rochester 
S T R U T / / backing     Detroit
H E A D / / backing DetroitFigure by MIT OpenCourse Ware. 1
23456The  Northern  Cities  Shift
////   /
idea
/e/
ked
/o/
cod/oh/
cawed/  /
cud/i/
kid
cadi 
//^ 
Audio files removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Describing English Accents
 Northern / u/ and California / / are corresponding phonemes 
in the two accents because they generally occur in the same 
words.
 where Northern has / u/ Californian has / /.
 So a convenient way to refer to vowel phonemes in describing 
accents is in terms of the words in which they appear.
 Wells (1982) proposes a set of keywords for referring to 
classes of words that (gener ally) share a vowel phoneme, e.g.
 KIT, DRESS, TRAP, LOT, STRUT, etc.</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Describing English accents - an historical 
approach
 The keyword approach works because of the approximate 
correctness of two assumptions:
 All accents of English are descended from the same 
language via sound change.
 Sound change is regular (Neogrammarian) -
exceptionless and phonetically conditioned.
 Labov takes an explicitly historical approach to description of 
accents.
 accents are described in terms of changes from an initial 
position - our best estimation of th e common base for American 
English dialects which resulted from  the mixing of various English 
dialects in the 16th and 17th centuries</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Adaptation to a new accent
 The Norris et al experiment shows that listeners are capable of 
rapid adaptation to a novel accent (novel in one respect).
 Presumably involves:
 Ability to interpret ambiguous stimuli as words, given 
context.
 Ability to generalize based on segments.
 How broad is the generalization?
 All s/f? Word-final s/f? Coda s/f? Word-final s/f after certain 
vowels?
 Value of decomposing words into segments: facilitates rapid 
generalization to new speakers 
 Given that variation tends to affect segments in context, 
rather than e.g. individual words. Cf. Regularity of sound 
change.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>6Eastern Massachusetts
 Post-vocalic / / in many contexts is better thought of as 
vocalized (de-rhotacized) rather than simply deleted.
 Many historical vowel-r sequences are now diphthongs.
 floor[a] (=/a/?) horse
 hoarse [ ]
 This contrast has been lost in many UK and US accents.
N E A R  [ i ]
 SQUARE [e ]
Listen:
11_floor.wav
11_horse.wav
11_hoarse.wav</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>11Some differences between English and US 
accents
 To a first approximation, the differences between English and 
US accents are the result of i ndependent sound changes in one 
region or the other.
 E.g. a Southern English innovation: loss of post-vocalic / /
st&gt;  st star
f&gt;  f for
st&gt;  st star
&gt; /  _  { C, #}</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>The North
 Generally retains conservative long /i, e , u, o /
 Inland North characterized by a chain shift , primarily 
involving historically lax vowels - Northern Cities Shift.
 Change in progress, most advanced in major cities 
(Buffalo, Rochester, Cleveland, Detroit, Chicago, Madison etc).
Figure by MIT OpenC ourseWare. 1
23456The  Northern  Cities  Shift
////   /
idea
/e/
ked
/o/
cod/oh/
cawed/  /
cud/i/
kid
cad
Figure by MIT OpenC ourseWare. i 
//^ 200
300
400
500
600
700
800
900
1000
11003000 2500 2000 1500 1000Second formantFirst formant
 i
I i
I
e
eu
uHz
LL
W
W</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>7Eastern Massachusetts
Some unusual features
 Neutralization of LOT/ /-THOUGHT// to / a/
 Boston      cot      caught      hot
P A L M / / remains distinct, but fronted /a/
 car spa  
 Contrast between three front lax vowels before / /
Mary-merry-marry
Listen:
11_boston.wav
11_cot.wav
11_caught.wav
11_hot.wav
Listen:
11_car.wav
11_spa.wav
Listen:
11_mary.wav
11_merry.wav
11_marry.wavListen:
11_hairy.wav
11_barry.wavMary
merry
marrye
eee
eeeI (fairy, hairy vs. Carey)
(carry, Harold vs. marry, Barry)
Table by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>The West
The West is primarily characterized by a combination of two 
developments:
Cot-caught merger: / /, no / /
 Spreading East through the midlands.
 Fronting of GOOSE vowel to [ ] (similar change in the South 
and elsewhere. Not in North)
Figure by MIT OpenCourseWare. i
iu200
300
400
500
600
700800
900
1000
11003000 2500 2000 1500 1000
eSecond formantFirst formant
 euHz
LL
WW
I
I</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Licensing by cue (cont.)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec6_cues/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>37</slideno>
          <text>Categorical perception
 Discrimination has never been found to be precisely 
predictable from identification - Discrimination is always 
better than predicted.
 More loosely, categorical perception is sometimes said to 
be exhibited where there is a discrimination peak at the category boundary determined by identification, even if the 
relationship is not precisely as predicted.
 A sharp transition in the identification function for a 
stimulus continuum is notcategorical perception in any 
technical sense.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Steriade (1997) - obstruent voicing
 Implicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)
more cues to voicing available Totontepec Mixe
Lithuanian
French
Shilha
Khasi#_O, O_#
e.g. bsa vs. psaR_O
e.g. absa vs. apsaR_#
e.g. ab vs. ap_R
e.g. ba vs. paR_R
e.g. aba vs. apa
no
no
no
no
yesno
no
yes
yesyesno
yes
yes
yes
yes yesyesyesyesyes no
no
no
yes
n/a
O = obstruent, R = sonorant, inc. vowel
Figure by  MIT O penC ourseW are. Adapted from Steriade, Donca. Phonetics in Phonology: 
The Case of Laryngeal Neutralization. Manuscript, UCLA, 1997. (PDF)   ___</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Speech perception and phonology
 Steriades analysis of the typology of obstruent voicing 
depends on the ranking of *[voice] constraints.
 This is supposed to follow from the relative strength of 
cues to voicing available in each context.
 What are these cues? How do we know how strong they 
are?
Figure by MIT OpenCourse Ware . Adapted from Steriade, Donca. "Phonetic s in Phonology: The Case of Laryngeal N eutralization." 
Manuscript, UCLA, 1997. (PDF)    ___Environment
*aV oice/ V_ [-son]Cues
clo voi, clo dur
clo voi, clo dur, V1 dur, F0, F1 in V1
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp, F0, F1 in V2*aV oice/ [-son] _ [-son], [-son]_#, #_[-son]
*aV oice/ V_ #
*aV oice/ V_ [+son]&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Different contrasts have different characteristic patterns of 
distribution (Steriade 1999):
(i) Obstruent voicing contrasts are permitted only before 
sonorants
(e.g. German, Lithuanian, Russian, Sanskrit).
(ii) Major place contrasts (labial vs. coronal vs. dorsal) are 
permitted only before vowels 
(e.g. Japanese, Luganda, Selayarese).
(iii) Retroflexion contrasts (retroflex vs. apical alveolar) are 
permitted only after vowels 
(e.g. Gooniyandi, Miriwung, Walmatjari).</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
 Phonological contrasts generally have restricted 
distributions.
 E.g. Lithuanian voicing contrasts
a. obstruent voicing is distinc tive before vocoids and consonantal 
sonorants:
ukle nuk niati au glingas dregna
silpnas ry tmetss k o bnis bd metys
b. obstruent voicing is neutralized (to voiceless) word-finally:
[dau k][ k a t]
c.obstruent voicing is neutralized be fore any obstruent (assimilating in 
voicing to following obstruent):
a[d-g]al m[ z-d]avau dr[p -t]i d[ k-t]i</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Cues to consonant contrasts
 Obstruent voicing cues (Wright, Frisch and Pisoni 1999)
Release burst amplitude
F3
F2
F1Aspiration noise
Periodicity VOTVowel duration
Vowel duration
Stricture duration
Figure by MIT OpenCourseWare. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. "Speech Perception." In Wiley Encyclopedia 
of Electrical and Electronics Engineering. Vol. 20. New York, NY: John Wiley and Sons, 1999, pp. 175-195.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Whalen, Abramson, Lisker &amp; Mody (1993).100
90
80
70
60
50
40
30
20
10
10 15 20 25 30 35 40 45 500
0 598 Hz
108 Hz
114 Hz
120 Hz
130 Hz
VOT% "P" Response
Figure by  MIT O penC ourseW are.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Typology of Voicing Neutralization
 Basic requirement for rapid, robust communication: 
perceptually distinct contrasts.
 cf. analysis of vowel inventories.
 Steriade (1997, 1999) argues that perceptual considerations  
shape the typology of voicing neutralization:
 Less distinct contrasts are dispreferred.
 Specifically: contrasts are neutralized first in 
environments where they would be less distinct.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Neutralization with assimilation
 *TD: *[-voice][-son, +voice]
 It is more difficult to initiate vo icing during an obstruent than to 
maintain voicing during an obstruent.
 Universal ranking: *TD &gt;&gt; *[-son, +voice]
*[voi]/
_[-son]Ident(voi) *[voice]/
_#*TD *[+voi, -son]
krtbe-krdb *! * ***
krtbe * *! *
) krdb * **</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Optimality Theory
 Optimality Theory separates problems from solutions
 E.g. *[+voice, -son]# (provisional formulation)
* [ voice, son][- voice, -son]
 These are Markedness constraints - they ban dispreferred
sound sequences and configurations.
 The other basic kind of constraints are Correspondence 
constraints - they require output forms to be as similar to 
the input underlying form as possible.
 If a underlying form like /sled/ is realized as [sled], it 
would violate *[+voi, -son]#.
 If it is changed to [slet] to satisfy the markedness
constraint, that violates the correspondence constraint Ident(voice).</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Cues to vowel quality
 The main cues to vowel quality are related to the 
frequencies of the first two or three formants.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Steriade (1997) - obstruent voicing
 Implicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)
Totontepec Mixe
Lithuanian
French
Shilha
Khasi#_O, O_#
e.g. bsa vs. psaR_O
e.g. absa vs. apsaR_#
e.g. ab vs. ap_R
e.g. ba vs. paR_R
e.g. aba vs. apa
no
no
no
no
yesno
no
yes
yesyesno
yes
yes
yes
yes yesyesyesyesyes no
no
no
yes
n/a
O = obstruent, R = sonorant, inc. vowel
Figure by  MIT O penC ourseW are. Adapted from Steriade, Donca. Phonetics in Phonology: 
The Case of Laryngeal Neutralization. Manuscript, UCLA, 1997. (PDF)   ___</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Optimality Theory and Typology
 It is hypothesized that most constraints are universal - i.e. 
the same in every language.
 Languages differ in the ranking of those constraints.
 So cross-linguistic similarities in phonological systems 
result from shared constraints.
 Where do these universal constraints come from?
 Hypothesis: Most constraints arise from:
 basic requirements for rapid, robust communication
 limitations of speech production and perception
apparatus
 cognitive limitations
 Common to all languages.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>. The distribution of acoustic cues
 Cues to a contrast are temporally di stributed and cues to more than one 
contrast may be present in the sign al simultaneously (i.e. no strict 
segmentation).
 The availability and nature of the cues to a given contrast type vary 
systematically with context.
 This observation is central to licensing by cue analyses of the 
distribution of phonological contrasts.
 Broad distinction between internal and external cues to a contrast:
 Internal: cues realized during the segment itself, e.g. vowel formants, 
fricative noise.
 External: cues realized on adjacent segments, e.g. VOT, formant 
transitions.
 External cues typically depend on th e presence of particular segment 
types in the context, e.g. VOT re quires a following voiced sonorant.
 So the presence of external cues can be highly variable across 
contexts.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Hypothesized explanation: The likelihood that distinctive 
values of the feature F will occur in a given context is a 
function of the relative perceptibility of the F-contrast  in 
that context (Steriade 1999).
 Contrasts differ in their distribution of cues so they are 
subject to different patterns of neutralization.
 Obstruent voicing is best cued by Voice Onset Time - only 
realized with a following sonorant.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>The phonological analysis
 Russian voicing (like Lithuanian)
 Rule-based analysis:
 /vrag/, /brat/, /led/, /pros // - a /
 final devoicing
 voicing assimilationson[][-voice]/ __ #
-son[] [voice]/ __ -sonvoice</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Optimality Theory
 Constraint conflict: Any output viol ates one constraint or the other:
/sled/ *[+voi,-son]# Ident(voice)
) sled *
slet *
 Conflict is resolved by reference to  a ranking of the constraints: the 
higher ranked constraint prevails:
/sled/ *[+voi,-son]# Ident(voice)
sled *!
) slet *</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>The nature of acoustic cues
 There are multiple cues to every contrast  the speech signal is 
highly redundant.
 E.g. stop voicing in English
1. Low-frequency spectral energy, periodicity (Stevens and 
Blumstein 1981:29)
2. Voice onset time (Lisker 1975)
3. Amplitude of aspiration (Repp 1979)
4. Amplitude of release burst (Repp 1979)
5. Closure duration (Lisker 1957)
6. Duration of the preceding vowel (Massaro and Cohen 1983) 
7. F1 adjacent to closure (Lisker 1975, Raphael 1972)
8. f0 adjacent to the closure (H aggard, Ambler and Callow 1970)
9. Amplitude of F1 at release (Lisker 1986).</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Speech perception
 Production studies can reveal many differences between minimal 
contrasting words, e.g. contrasting vowels of English differ in formant 
frequencies and duration.
 Are listeners sensitive to these di fferences in speech perception?
 What is the nature of the per ceptual representations of speech?
 These questions are addressed thro ugh perceptual experiments (cf. 
Johnson p.70).
 Most direct test of perceptual significance of an acoustic property: 
manipulate the acoustic property synt hetically and see if perceptual 
response is affected. E.g. vary fo rmant frequencies in synthetic vowels, 
and have subjects categorize the vowels.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Categorical perception
 Identification: Subjects id entify stimuli as b, d, g
 Discrimination: Subjects are presente d with pairs of stimuli and asked 
to judge whether they are the same or different.
 Relatively abrupt 
transitions in 
identification functions.
 Peaks in discrimination 
function at the category boundary
Percent correct
Percent /b.d.g./
+8
DL0 -60100+8
PG0 -60100+8
HC0 -60100
HC+8 0 -6
PG+8 0 -6
DL+8 0 -6Discrimination Identification
/b/ /d/ /g/
Figure by MIT OpenC ourseWare. Adapted from Liberman, A. M. "Some Characteristics of Perception in the Speech Mode." Perception and its Disorders 48 (1970): 238-254. 
And Liberman, A. M. " Discrimination in Speech and Nonspeech Modes." Cognitive Psychology  2 (1970): 131-157.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Cues to consonant contrasts
 Manner cues (Wright, Frisch and Pisoni 1999)
Stop release b urstAbruptness and de gree
of attenuation
Nasal pole and zeroF3
F2
F1
a
a a a at a a a a s
n lSlope of formant
transitionsNasalization
of vowelPresence of formant
structure
Figure by  MIT O penC ourseW are. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. "Speech Perception." In Wiley Encyclopedia
 of Electrical and Electronics Engineer ing. Vol. 20. New York, NY: John Wiley  and Sons, 1999, pp. 175-195.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Hungarian
 Russian: 
*[voice]/_[-son] &gt;&gt; * [ voice]/_# &gt;&gt; Ident(voice) &gt;&gt; *[ voice]/_[+son]
 Hungarian:
*[voice]/_[-son] &gt;&gt; Ident(voice) &gt;&gt; * [ voice]/_# &gt;&gt; *[ voice]/_[+son]
Ident(voice) *[voice]/_# *[voi]/_[+son] *[+voi, -son]
ha:z-ha:s *
*!
*!ha:s*
ha:z *
Ident(voice) *[voice]/_# *[voi]/_[+son] *[+voi, -son]
)edme-etme
*!
*!etme* *
edme *)</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Steriade (1997) - obstruent voicing
 Markedness of obstruent voicing contrast in context C 
depends on strength of cues to voicing in C.
Assumptions:
 More cues are better.
 Release cues are stronger th an preceding/closure cues.Figure by MIT O penC ourseW are. Adapted from Steriade, Donca. "Phonetic s in Phonology: The Case of Laryngeal N eutralization." 
Manuscript, UCLA, 1997. (PDF)    ___Environment
*aV oice/ V_ [-son]Cues
clo voi, clo dur
clo voi, clo dur, V1 dur, F0, F1 in V1
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp, F0, F1 in V2*aV oice/ [-son] _ [-son], [-son]_#, #_[-son]
*aV oice/ V_ #
*aV oice/ V_ [+son]&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings: 
 ToBI tutorial, 1.0, 2.0, 2.1 
 Johnson pp.28-31 (on pitch tracking).
 Ladd chapter 3
Assignment:
 Make stimuli for voicing perception experiment.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
Licensing by Cue
Figure by MIT OpenCourseWare. release burst amplitude
aspiration noisevowel duration
VOT
stricture
durationvowel
durationperiodicityF1F2F3</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Steriade (1997) - obstruent voicing
 Implicational universals (cf. Lombardi 1991, Wetzels and Mascaro 2001)
more cues to voicing available #_O, O_# R_O R_# _R R_R
e.g. bsa vs. psa e.g. absa vs. apsa e.g. ab vs. ap e.g. ba vs. pa e.g. aba vs. apa
Totontepec Mixe no no no no yes
Lithuanian no no no yes yes
French no no yes yes yes
Shilha no yes yes yes yes
Khasi yes n/a yes yes yes
O = obstruent, R = sonorant, inc. vowel
Figure by  MIT O penC ourseW are. Adapted from Steriade, Donca. Phonetics in Phonology: 
The Case of Laryngeal Neutralization. Manuscript, UCLA, 1997. (PDF)   ___</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Optimality Theory
Components of Optimality Theory:
 A set of constraints
 Markedness constraints - define dispreferred
configurations.
 Correspondence constraints - penalize deviations 
from identity between input and output.
 Generation function: takes an input (e.g. /sled/) and 
generates all possible output candidates (sled, slet, sle, 
slen, led, let, etc).
 Evaluation function: given a ranked set of constraints, 
identifies the candidate that best satisfies the constraint ranking.
 Thats the actual output.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Speech perception
 The problem faced by the listener: To extract meaning 
from the acoustic signal.
 This involves the recognition of words, which in turn 
involves discriminating the segmental contrasts of a 
language.
 Much phonetic research in speech perception has been 
directed toward identifying the perceptual cues that 
listeners use.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Hungarian
 Contrast before sonorants
vedmeg buy it! a:t me to cross
 Contrast word-finally
rb prisoner kl p hat
ha:z house k rt garden
 Neutralization before obstruents
ha:s-to:l from the house krd-b in the garden
*-zt- *-tb-</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Optimality Theory
 Different rankings of the same constraints (different grammars) yield 
different outputs:
/sled/ Ident(voice) *[+voi,-son]#
) sled *
slet *!
/sled/ *[+voi,-son]# Ident(voice)
sled *!
) slet *</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Cues to consonant contrasts
 Place cues (Wright, Frisch and Pisoni 1999)
Stop release burstFricative noise
F2 Transitions
Nasal pole and zeroRelative spacing of F2 and F3F3
F2
F1
a
a a a at a a a a s
n l
Figure by  MIT O penC ourseW are. Adapted from Wright, R., S. Frisch, and D. B. Pisoni. "Speech Perception." In Wiley Encyclopedia
of Electrical and Electronics Engineering.  Vol. 20. New York, NY: John Wiley and Son s, 1999, pp. 175-195.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Categorical perception
 Strict categorical perception is said to occur where 
discrimination performance is limited by identification 
performance, i.e. listeners only have access to category 
labels, so stimuli can only be distinguished if they are 
identified as belonging to different categories.
 Tested in two stages:
 Identification of a synthetic continuum
 Discrimination of stimuli from the continuum</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Analyses
 Russian: 
*[voice]/_[-son] &gt;&gt; * [ voice]/_# &gt;&gt; Ident(voice) &gt;&gt; *[ voice]/_[+son]
 Hungarian:
*[voice]/_[-son] &gt;&gt; Ident(voice) &gt;&gt; * [ voice]/_# &gt;&gt; *[ voice]/_[+son]
*[voice]/_# Ident(voice) *[voi]/_[+son] *[+voi, -son]
sled-slet *!
) slet *
sled ** !
*[voice]/_# Ident(voice) *[voi]/_[+son] *[+voi, -son]
)sleda-sleta *
slet *!*
sled *! *</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Why is categorical perception significant?
 The (loose) categorical perception pattern contrasts with the pattern 
observed in psychophysical experi ments using non-speech stimuli: 
Typically, nonspeech stimuli that vary acoustically along a single 
continuum are perceived continuous ly, resulting in discrimination 
functions that are monotonic wi th the physical scale (Luce and 
Pisoni, p.31).
 This contrast was used by Liberman and others to argue that speech 
perception is special  i.e. it uses  special mechanisms, not the general 
mechanisms of non-speech auditory perception.</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Categorical perception
 E.g. Liberman (1970) place of articulation F2 
transition continuum, b-d-g.
Figure by MIT Open Course Ware . Adapted from Liberman, A. M. "Some Characteristics of Perception in the Speech Mode." Perception and its Disorders 48 (1970): 
238-254. And Liberman, A. M. " Discrimination in S peech and N onspeech M odes." Cognitive Psychology  2 (1970): 131-157. 200 100
Time (msec)(Hz)
0050010001500
-6+6+7+8+9
-5+5
-4+4
-3+3
-1+1
-2+2
020002500</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Assessing relative cue strength
 Compare discrimination/identifica tion of contrasting sounds in 
different contexts under the same conditions.
 E.g. Wright (2003) compares perception of [b][d][g] before vowels 
and after vowels in noise [ba/da/ga], [ab/ad/ag].
 We want to know about [ba] vs. [pa], [ab] vs. [ap].
 Conflicting cues: Create stimuli with conflicting cues to a contrast and 
see which prevails.
 E.g. Fujimura et al: cross-spliced [ab-] from [aba] and [-da] from 
[ada]
 closure transitions cue [b], release transitions+burst cue [d]
 What do listeners perceive? I.e. which cues dominate?
 Try this for voicing in stops and fricatives.
 create four stimuli each, 2 stops, 2 fricatives.
 send the sound files to me.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Raphael (1972) JASAb
d
bzgzzd
gdbddz
Vowel Duration in Millisecondsvz100
80
60
4040
20
0
100
80
60
4040
20
0
350 300 250 200 150 350 350 300 250 200 150 100 350
Figure by  MIT Open Course Ware .
Figure by  MIT O penC ourseW are.
gV oicedV oiced
V oiceless V oiced V oicelessV oiceless
/pIks//pIgz//dus//duz//b  t/
(d)(b)(a)/b  d/Frequency in Hz
Examples of patterns used to synthesize the voiced/voiceless 
stimuli series in the perceptual tests.</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Why is categorical perception significant?
 Vowels are not usually perceived categorically, even in the loose sense 
(Luce and Pisoni and refs there).
 The argument for specialness from categorical perception has been 
weakened by:
Evidence for categorical percepti on of non-speech sounds (noise-
buzz, Miller et al 1976).
Evidence that Chinchillas percei ve a VOT continuum categorically
(Kuhl and Miller 1975).
 Recent data from eye-tracking studie s suggest that perception may 
depend gradiently on the speech signal - categorical effects are 
introduced by decision processes (McMurray et al 2002 ).</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Steriade (1997) - obstruent voicing
 Implementation: constraints against obstruent 
voicing contrasts in different contexts.
 Ranked according to the strength of the cues 
available in that context (fixed ranking).
Figure by MIT OpenC ourseW are. Adapted from Steriade, Donca. "Phonetic s in Phonology: The Case of Laryngeal N eutralization." 
Manuscript, UCLA, 1997. (PDF)    ___Environment
*aV oice/ V_ [-son]Cues
clo voi, clo dur
clo voi, clo dur, V1 dur, F0, F1 in V1
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp
clo voi, clo dur, V1 dur, F0, F1 in V1,
burst dur &amp; amp, F0, F1 in V2*aV oice/ [-son] _ [-son], [-son]_#, #_[-son]
*aV oice/ V_ #
*aV oice/ V_ [+son]&gt;&gt;
&gt;&gt;&gt;&gt;
&gt;&gt;</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Intonation</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec7_intonation/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>31</slideno>
          <text>Interpreting f0 Tracks
 No f0 during voiceless sounds.
 Consonants can perturb f0.
 f0 is usually raised after obstruen ts, more after voiceless obstruent.
 f0 can fall before obstruents.
 Creaky voice - often involves very long pitch periods, and 
fluctuating period lengths.
 Pitch trackers often fail to detect periodicity.
 Creaky voice is common at the ends of phrases.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Inventory of pitch accents
P80
7 pitch accentsPB86
6 pitch accentsToBI
5 pitch accents + 
downstep (!)
H* H* H*
L* L* L*
H+L* H+L* H+!H*
H*+L H*+L (H* follo wed by downstep)
L*+H L*+H L*+H
L+H* L+H* L+H*
H*+H
Phrase accents: H-, L- Boundary tones: H%, L%</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>The relationship between phrasing and 
intonation
 Nuclear accent = last accent in an intermediate phrase.
 The phrase tone spreads over the interval between the nuclear accent 
and the end of the phrase.
L+H*                                            L-L%
[[   Marianna       made the   marmalade       ]]
Audio: 
7_006.wav</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 80</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 0</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 30</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Autocorrelation function
-0.6-0.4-0.200.20.40.60.811.2
0 50 100 150 200 250
lag (samples)</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Phrasing
This is a very impoverished form of constituent structure that can be fully 
specified by placing a symbol betw een each pair of words indicating 
the strongest boundary between those words.
 This is how the break index notatio n represents prosodic structure.
Only  1 one 4 remembered 3 the 0 lady 1 in 1 red 4Pwd    Pwd           Pwd              Pwd      Pwd Pwdip                      ip                              ipIP IP</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>In many languages, segments are lengt hened before constituent boundaries.
 Pauses typically occur at intonational phrase boundaries.
Wightman et al (1992)
 Studied a corpus of sentences r ead by professional news announcers.
 Prosodic boundaries marked usi ng a 7-point scale of break indices
0 No prosodic break (cliticization)
1 Prosodic word boundary
2 accentual phrase
3 Intermediate phrase
4 Intonation phrase
5 Superior major tone group
6 Sentence boundary
 Lengthening measured in terms of normalized segment durations (standard deviations 
from the mean for that segment, adjusted for estimated speech ra te of each sentence).Phonetic correlates of phrasing</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Lengthening before constituent bou ndaries, localized to the final 
syllable rhyme (VC).
 Degree of final lengthening increase s with size of prosodic constituent.Phonetic correlates of phrasing
-1 0 1 2 3 4 5 6-1.50-1.00-0.500.000.501.001.502.002.50Mean normalized duration
Perceptual labels (break index)
Mean normalized duration of the word-final vowel as a function of the break index following 
the word. The vertical bars correspond to confidence intervals (95% protection level).
Figure by MIT Open Course Ware . Adapted fr om Wightman, Colin W., Stefanie Shattuck-Hufnagel, Mari Ostendorf, and Patti J. Price.
"Segmental D urations in The V icinity of P rosodic Phrase Boundaries." Journal of the Acoustical Society of America 91 (1992): 1707-1717.</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>The relationship between phrasing and 
intonation
ToBI:
 Intermediate phrase must contain one or more pitch 
accents and a final phrase accent (associated to the ip
boundary)
 Intonation phrase may begin, and must end, with a 
boundary tone (associated to the IP boundary) (as in P80)
E.g. H*L- H*L- L%
[[ I]ip[means insert]ip]IP</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
Intonation</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Phrasing
 Utterances are divided into intonational phrases, which are 
subdivided in intermediate phrases.
 Hierarchical bracketing of the phonological string, usually 
assumed to be related to, but distinct from, syntax. 
Only  1 one 4 remembered 3 the 0 lady 1 in 1 red 4Pwd    Pwd           Pwd              Pwd      Pwd Pwdip                      ip                              ipIP IP</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Intonation
 Tones are realized primarily in terms of f0 (fundamental 
frequency).
 Pitch accented syllables are also generally louder and 
longer than unaccented syllables.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>The representation of intonation
L*          H*    L-L%
He knew the millionaire
 A string of tones, H(igh), L(ow).
 Three kinds of tones are differentiated, for purposes of tune-
text alignment and phonetic interpretation:
 Pitch accents  T* gravitate to stressed syllables
 Phrase accents T- or Tgravitate to (smaller) phrase 
boundaries
 Boundary tone T% gravitate to (larger) phrase 
boundaries</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 50</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Pitch Tracking in PRAAT
Parameters:
 Pitch halving: two periods treated as one.
 Pitch doubling: one period is treated as two.
 Failure to detect periodicity.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 92 samples</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 70</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Why phonological constituents?
Why is a phonological constituent structure needed in addition to syntactic 
structure?
 Syntax-prosody mismatches
This is [the cat [that caught [the  rat [that stole [the cheese]]]]]
[This is the cat][that caught the rat][that stole the cheese]
[Sesame St. is brought to you]IP[ by the childrens television workshop]IP
[Sesame St. is brought to you by]IP[ the childrens television workshop]IP</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Pitch Tracking
 ToBI transcription is performed with reference to an f0 
contour, so it is useful to have some idea of how f0 
contours are calculated and interpreted, and what can go wrong at both stages.
 Pitch tracking algorithms:
 Tentative identification of f0 at regular intervals, 
usually through picking peaks in an autocorrelation function.
 Additional processing to select best contour, e.g. 
dynamic programming, subject to smoothness 
constraints.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Autocorrelation function
-0.6-0.4-0.200.20.40.60.811.2
0 50 100 150 200 250
lag (samples)1 period 2 periods</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Stress
 Stress: relative prominence of syllables (Liberman &amp; Prince 1977).
 Lexical stress:
n.to.n .tion pr .ba.bly
Words have a stress pattern.
One syllable (primary stress) is more prominent than the rest.
 Precise phonetic correlates are complex.
 Usually: loudness (duration, intensity), voice quality.
 Most important here: If a word bears a single pitch accent, it is aligned to the 
primary stressed syllable.
 Completely unstressed syllables us ually cannot bear pitch accents.
 There are also prominence relations between words.
 Generally correlates with pitch accent pl acement: the syllables with the strongest 
stresses have pitch accents.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>The phonology of intonation
 Three components relevant to the theory of 
intonation:
 Intonation (pitch contour)
S t r e s s
P h r a s i n g
 Collectively can be referred to as prosody.
 But sometimes intonation is used to cover all of 
these components.
 These components are closely interrelated.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Pitch Tracking
Common errors:
 Pitch halving: two periods treated as one.
 Pitch doubling: one period is treated as two.
 Failure to detect periodicity.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Phrasing
Unlike syntactic structure, prosodic structure is argued to be:
 Exhaustive: a string is fully parsed  into constituents of each type.
 Strictly layered: There is a hierarch y of constituents such that each type 
of constituent only dominates the next type of constituent on the 
hierarchy
Only  1 one 4 remembered 3 the 0 lady 1 in 1 red 4Pwd    Pwd           Pwd              Pwd      Pwd Pwdip                      ip                              ipIP IP</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The Phonology of English Intonation
A very brief introduction to English intonation primarily 
based on Pierrehumbert (1980) (P80) and Beckman and 
Pierrehumbert (1986) (BP86).
Pierrehumbert, Janet (1980) The Phonology and Phonetics of English 
Intonation. PhD dissertation, MIT.
Beckman, Mary and Janet Pierrehumbert (1986) Intonational structure in 
Japanese and English. Phonology Yearbook 3.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>H* L- H*     L-L%        
[[Marianna][made the marmalade]]
H*                      H*     L-L%        
[[Marianna made the marmalade]]
L+H*L-H%               H*     L-L%     
[[Marianna ]][[made the marmalade]]
Audio: 
7_003.wav
Audio: 
7_004.wav
Audio: 
7_005.wav</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>The representation of intonation
Motivation for analyzing intonation contour in terms 
of a sequence of tones.
 When similar tunes are associated with texts of 
differing lengths, the consistent properties of the 
melody are alignments between landmarks in the 
f0 contour and stressed syllables and the edges of 
phrases.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>QuickTime and a
TIFF (LZW) decompressor
are needed to see this picture.
A: I hear Nells taking a course to be a driving instructor.
A: I hear Nells got a job as a marmalade maker.N          e            ll
a   m    ar m a l     a    de m   a    k      er L+H* associates to the 
main stress,  
 L-H% rise occurs at the end 
of the phrase, 
 any interval between is 
filled by L-.
Audio: 
7_001.wav
Audio: 
7_002.wav</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>-0.25-0.2-0.15-0.1-0.0500.050.10.150.2
0 50 100 150 200 250autocorrelation
Lag = 10 samples</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings: 
 ToBI tutorial, 2.2-2.5, 2.8 
Assignments:
 Write a short description of your project.
 ToBI transcription exercises (to be posted).</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Introduction
Laboratory phonology</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec1/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Articulatory description of speech sounds
Consonants:
V o i c i n g
 Place of articulation
M a n n e r
 Lateral/Central
N a s a l / O r a l
 [s] voiceless alveolar central oral fricative</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>010002000300040005000
010002000300040005000
 m      s       t       u    s       s      m             k          
1 second Hz 
Hz</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>III. Focus and intonation in English
Focus sensitive particles make the truth 
conditions of a sentence dependent on the 
location of focus:
1. Jan only gave BILL money.
2. Jan only gave Bill MONEY.
 Focus is marked by some kind of 
prominence. What is this exactly?</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Spectrums and spectrograms
 The spectrum of a sound plays a central role 
in determining its quality or timbre.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>The vocal tract
Alveolar ridge
Upper gums
(alveolus)
Teeth
Lips
Lower ja w
Hyoid
Thyroid cartilageNasal ca vity
Palate
Soft palate
Velum
Uvula
Tonsils
Tongue
Pharynx
Epiglottis
Vocal cords
(glottis)
Larynx
Esophagus
Vocal tract conf iguration with raised soft palate for articulating non-nasal sounds.
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02
-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Spectral representation
 Any complex wave can be analyzed as the combination of 
a number of sinusoidal waves of different frequencies and 
intensities (Fouriers theorem). 
 In the case of a periodic sound like a vowel these will be
 the fundamental frequency
 multiples of the fundamental frequency (harmonics)
 The quality of a periodic sound depends on the relative 
amplitude of its harmonics.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Articulatory description of speech sounds
Vowels:
 High-low
 Front-back
 Rounded-unrounded
 [e] mid front unrounded vowel</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>F2 (Hz)
F1 (Hz)200
300
400
500
600
700
8002500 2000 1500 1000i
u
I
e
Wc
Figure by MIT O penCourse Ware. Adapted from Peter Ladefoged. A Course in Phonetics. 5th ed. Berlin, Germany: Heinle, 2005. ISBN: 9781413006889. 
Availa ble at: http://www.phonetics.ucla .edu/course/contents.htmlQA</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings: Johnson (textbook)
 chapter 1, 
 chapter 2, pp.19-33, 
 chapter 3</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Spectrogram image removed due to copyright restrictions.
See: http://hctv.humnet.ucla.edu/departments/linguistics/VowelsandConsonants/course/chapter8/8.3.htm</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Distribution of retroflexion contrasts
Explanation (Steriade 1995, etc):
 The primary cues to the contrast between retroflex and 
apical alveolar are located in the VC transitions (unlike 
major place contrasts.
 Most retroflex consonants are retroflexed at closure, but 
the tongue tip moves forward during closure.
 At release tongue tip position is similar to an apical 
alveolar, consequently the release and CV transitions of the two consonant types are similar.
 Contrasts preferentially appear in environments where they 
are better cued.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>The phonetics and phonology of 
retroflex consonants
apical alveolar [t] retroflex []
Malayalam
Courtesy of Ashtu Killimangalam. Used with permission.</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>III. Focus and intonation in English
 Focus - the informative part of an utterance.
 the information in the sentence that is assumed by the 
speaker not to be shared by him and the hearer
(Jackendoff 1972).
 E.g. Question/answer pairs: 
a. (When did John paint the shed?)
John painted the shed YESTERDAY.
#JOHN painted the shed yesterday.
b. (Who painted the shed yesterday?)
JOHN painted the shed yesterday.
#John painted the shed YESTERDAY.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Linguistic
LevelPhysiological
LevelAcoustic
LevelPhysiological
LevelLinguistic
LevelSound Waves
EarEar
Sensory
NervesSensory NervesBrain
BrainFeedback
Link
Motor 
NervesVocal
MusclesListener SpeakerThe Speech Chain
Figure by M IT O penC ourseWare. Adapted from Denes, Peter B., and Elliot N. Pinson. 
The Speech Ch ain: The Physics and Biology of Spoken Speech. 2n d ed. New York, NY: W. H. Freeman, 1993. ISBN : 9780716723448.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Distribution of retroflexion contrasts
 Acoustic studies provide evidence concerning the 
differences between apical alveolar and retroflex 
consonants.
 Articulatory studies help to explain the observed 
acoustic patterns.
 Perceptual studies confirm that retroflexion
contrasts are more difficult to discriminate in the 
absence of a preceding vowel (Anderson 1997).
 Phonological theory to relate these properties to 
the observed distribution of retroflexion contrasts.</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Aperiodic sounds have waveforms that do not repeat.
 Fricative noise is aperiodic.Aperiodic sounds
Segment of [s]Time (s)0 0.01153230.095490.1253
0</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Idealized vowel spectrum
4000 3000 2000 1000 00204060Intensity level (decibels)
Frequency (Hertz)
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
 Phonological contrasts generally have restricted 
distributions.
 E.g. Lithuanian voicing contrasts
a. obstruent voicing is distinc tive before vocoids and consonantal 
sonorants:
ukle nuk niati au glingas dregna
silpnas ry tmetss k o bnis bd metys
b. obstruent voicing is neutralized (to voiceless) word-finally:
[dauk][ k a t]
c.obstruent voicing is neutralized be fore any obstruent (assimilating in 
voicing to following obstruent):
a[d-g]al m[ z-d]avau dr[p -t]i d[ k-t]i</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Vowel quality
 The quality of a vowel depends on the 
shape of its spectrum.
 The shape of the spectrum depends on the 
shape of the vocal tract.
Frequency (Hz)0 350002040
Frequency (Hz)0 350002040[] []</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02
Fundamental frequency</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>The phonetics and phonology of 
retroflex consonants
dental [ l] retroflex [ ]
MRI images of Tamil laterals (Narayanan et al 1999)
Figure by MIT OpenCourseWare, adapted from Narayanan, Shrikanth, Dani Byrd, and Abigail Kaun. 
"Geometry, Kinematics, and Acoustics of Tamil Liquid Consonants." The J ournal of the Acoustical Society of America 106, no. 4 (October 1999): 1993-2007. Lower LipTip
Lower LipTip</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>The peaks in the spectrum of a vowel are 
called formants .
 Perceived vowel quality depends primarily 
on the frequencies of the first three 
formants.Vowel quality
Frequency (Hz)0 350002040
Frequency (Hz)0 350002040[] []</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Warlpiri [t] from onset of closure to post-release
380 381 383 384 385 386 387 1388 389 390 391 392 393 MAX
MAX382CLO
394 395 397 398 399 400 401 402 403 404 405 406 407 396
2408 409 410REL411 412 413 414 415 416 417 418 419 420 421
Figure by MIT OpenCourseWare. Adapted from Butcher, Andrew. "The Phonetics of Australian Languages." Flinder University, South Australia, 1993. Unpublished manuscript.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02
2nd harmonic</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Broad focus: 
what happened
Time (s)2.00885 3.4781105000
Time (s)2.61383 4.0809605000
Time (s)2.79811 4.2325205000
verb focus
Time (s)2.55219 3.9466905000
Annabel married  Maloney Annabel   married  MaloneyObject focus: Who did Annabel marry?
Audio: 
1_broad.wavAudio:
1_subj.wav
Audio: 
1_obj.wavSubject focus:who married Maloney?

Verb focus:what did Annabel do to Maloney?Audio: 
1_verb.wav</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Spectrograms
8000
0 0.0040.952106
0
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Different contrasts have different characteristic patterns of 
distribution (Steriade 1999):
(i) Obstruent voicing contrasts are permitted only before 
sonorants
(e.g. German, Lithuanian, Russian, Sanskrit).
(ii) Major place contrasts (labial vs. coronal vs. dorsal) are 
permitted only before vowels 
(e.g. Japanese, Luganda, Selayarese).
(iii) Retroflexion contrasts (retroflex vs. apical alveolar) are 
permitted only after vowels 
(e.g. Gooniyandi, Miriwung, Walmatjari).</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Common vowel inventories:I. Vowel inventories
 i     u   i     u   i     u  
       e    o  e  
    o  
   a       a       a    
Arabic, 
Nyangumata, 
Aleut, etc. Spanish, 
Swahili, 
Cherokee, etc.  Italian, 
Yoruba, 
Tunica, etc. 
 
 Unattested vowel inventories:
 i       i        i     u  
 e      e        
      
 
   a       a          
 Why?</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Voiced sounds have complex (quasi-)periodic wave forms.
 The perceived pitch of a sound depends on its frequency.Periodic sounds
0 .01 .02 .03 s- Air pressure +
Time in seconds
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Distribution of retroflexion contrasts in 
Gooniyandi (Steriade 1995)</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Spectral representation
-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Frequency (Hz)0 400020020
Frequency (Hz)0 400002040
Frequency (Hz)0 400020020
Time (s)3.46955 3.773105000
3.55 3.59 3.69</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Representing sound waves
TimePressure0
C = Compression
R = RarefactionC C C C C R R R R RSound is a Pressure Wave
Image by MIT OpenCourseWare. Adapted from The Physics Classroom Tutorial.</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Frequency 
(Hz)Amplitude
100 1
200 0.6
300 0.45
400 0.3
500 0.100.20.40.60.811.2
0 100 200 300 400 500 600
frequency (Hz)Spectral representation
Power spectrum Phase differences are relatively un important to sound quality, so key 
properties of a complex wave can  be specified in terms of the 
frequencies and amplitudes of its sinusoidal components.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Waveform of a sentence
Please pass me my book16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1
Figure by MIT OpenCourse Ware.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Periodic sounds
 A waveform is periodic if it repeats at regular intervals.
 Frequency of a wave is the number of cycles occurring per 
unit of time.
 Units: 1 Hertz (Hz) is 1 cycle/second</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Frequency (Hz)0 400002040
0 1000 2000 3000 4000
Frequency (Hz)0 400020020
Frequency (Hz)0 400002040narrow band 
(long window)
broad band
(short window)</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Introduction to acoustics
 Sound consists of pressure fluctuations in a 
medium
which displace the ear drum in such a way as to result in 
stimulation of the auditory nerve.
animation</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Distribution of retroflexion contrasts in 
Gooniyandi
Summary:
 Contrast between retroflex and apical alveolar after vowels 
V_#,  V_V
 No contrast elsewhere    #_,  V_C
 This pattern of distribution is common in Australian and 
Dravidian languages.
 An unusual pattern of distribution - major place contrasts, 
voicing contrasts occur preferentially before vowels.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>II. Perceptual cues and the distribution of 
phonological contrasts
Hypothesized explanation: The likelihood that distinctive 
values of the feature F will occur in a given context is a 
function of the relative perceptibility of the F-contrast  in 
that context (Steriade 1999).
 Contrasts differ in their distribution of cues so they are 
subject to different patterns of neutralization.
 Obstruent voicing is best cued by Voice Onset Time - only 
realized with a following sonorant.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Movie removed due to copyright restrictions.
Please see  Tongue Video  in Peter Ladefogeds Vowels 
and Consonants.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Warlpiri [ ] from onset of closure to post-release: Butcher 1993
307
321 322 323 324 325 326 327 328 329 330 331 332 333 334
335 336 337 338 339 340 341 342 343 344 345 346 347 348
349 350 351 352 353 354 355 356 357 358 359 360 361 362308 310 311 312 313 314 315 316 317 318 319 320 309 CLO MAX
MAX Rel
Figure by MIT OpenCourseWare. Adapted from Butcher, Andrew. "The Phonetics of Australian Languages." Flinder University, South Australia, 1993. Unpublished manuscript.</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02-2-1.5-1-0.500.511.52
0 0.005 0.01 0.015 0.02</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Speech acoustics
 Movements at a source produce a sound 
wave in the medium which carries energy to 
the perceiver.
 Pressure fluctuations move through space, 
but each air particle moves only a small 
distance.
Animated image of longitudinal pressure wave removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Articulation-
The speech production system
Figure by MIT OpenC ourseWare. Hard P alate
Nasal Ca vity
Nostril
Lip
Tongue
Teeth
Oral (or Buccal) Ca vity
Jaw
Trachea
Lung
DiaphragmSoft P alate
(Velum)
Pharyngeal Ca vity
Larynx
Esophagus</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Frequency (Hz)0 350002040
[]vowel spectrum</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Distribution of retroflexion contrasts in 
Gooniyandi</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Effects of the lexicon and context on speech perception (cont.)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec10_listener/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>20</slideno>
          <text>Self-centered behavior in production of 
repetitions
 Each subject lead two partners through the same map task.
 Key words were excised from the recorded dialogues and 
presented in noise for identification in a follow-up experiment.
 Durations of words were also measured.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>References
 Aylett, M. and Turk, A. (2006). Language redundancy predicts syllabic 
duration and the spectral characteristics of vocalic syllable nuclei. JASA 
119:3048-3058.
 Bell, A., Jurafsky, D., Fosler-Lussier, E., Girand, C., Gregory, M., and Gildea, 
D. (2003). Effects of disfluencies, predictability, and utterance position on 
word form variation in English conversation. JASA 113, 1001-1023.
 Billerey-Mosier, R. (2000). Lexical effects on the phonetic realization of 
English segments. UCLA Working Papers in Phonetics 100.
 Boothroyd and Nittrouer (1988). Mathematical  treatment of context effects in 
phoneme and word recognition. JASA 84(1):101.
 Dambacher, M., Kliegl, R., Hofmann, M., and Jacobs, A.M. (2006). Frequency 
and predictability effects on event-related potentials during reading, Brain 
Research 1084, 89-103.
 Goldinger, S.D., Pisoni, D.B., &amp; Luce, P.A. (1996). Speech perception and 
spoken word recognition: Research and theory. In N.J. Lass (Ed.), Principles 
of Experimental Phonetics. St. Louis: Mosby. 277-327.
 Griffin, Z.M., and Bock, K. (1998). Constraint, word frequency, and the 
relationship between lexical processing levels in spoken word production. Journal of Memory and Language 38, 313-338.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Lexical access and neighborhood density
 Munson (2004) directly tested the lexical access account of 
neighborhood density effects:
 Four classes of words crossing:
 High vs. low frequency
 High vs. low neighborhood density
 Subjects read words in two conditions:
 Read word immediately on presentation.
 Wait 1000 ms after presentation before speaking word.
 Assumption: 1000 ms delay gives speakers plenty of time to 
complete lexical access, so difficulty with lexical access 
should not affect pronunciation at this time lag.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Effects of lexical statistics on production
 Wright (2004) found that neighborhood density/relative 
frequency affects pronunciation of isolated words.
 Hard words - low frequency, high neighborhood density
 Easy words - high frequency, low neighborhood density
 Vowels in hard words are more dispersed from each other in 
F1*F2 space than vowels in easy words.
Figure by  MIT Open Course Ware .F2  Bark15 14 13 12 11 10 9 82
3
4
5
6
7
8
9F1 Barki+
iu
I+e+eu+
I
a
a++c
+c +oo</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Frequency and Neighborhood Density:
Munson &amp; Solomon (2004)
 Point out that Wright (a) confounded frequency and 
neighborhood density, (b) didnt measure duration, so we cant 
be sure if neighborhood density affects vowel formants 
directly or via vowel duration.
Figure by MIT OpenCourseWare.Vowel Lexically easy Lexically difficult
a
a
a
a



I
I
I
i
i
o
o
ujob
shop
wash
watch
gas
path
give
ship
thing
peace
teeth
both
vote
foodjackcod
cot
knob
wad
hack
pat
hick
kin
kit
bead
weed
goat
moat
hoophash</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Self-centered behavior in production of 
repetitions
 Intelligibility and duration 
of first mentions of 
landmarks were reduced in 
second trial, even though they were new to the second listener.
 Speakers seem to act as if 
an item is old if it is old to them, even if it is new to the listener (egocentric).Form
Trial 2Face condition
Citation
Screened
Visible
Screened
Visible0.746 0.636 0.818
(0.072) (0.182)
0.578 0.693 0.808
(0.230) (0.115)
0.703 0.659 1.309
(0.606) (0.650)
0.646 0.588
(0.655) (0.713)1.301Intelligibility
DurationTrial 1
Figure by  MIT O penC ourseW are.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Van Son, R.J.J.H. and Pols, L.C.W. (2003b). An Acoustic Model of 
Communicative Efficiency in Consonants and Vowels taking into Account 
Context Distinctiveness" . Proceedings of ICPhS, Barcelona, Spain, 2141-2144.
 Vitevitch, M. (2002). The influence of phonological similarity neighborhoods 
on speech production. Journal of Experimental Psychology: Learning, 
Memory, and Cognition 28, 735-747.
 Wright, R. (2004) Factors of lexical comp etition in vowel articulation. In J. 
Local, R. Ogden, and R. Temple (eds.), Papers in Laboratory Phonology VI. 
Cambridge: CUP.References</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Repetition and speech production
 Fowler &amp; Housum (1997) examined the pronunciation and 
perception of first and second uses of words in a monologue 
(Garrison Keillor - 35 pairs of words) and some short news 
interviews (45 pairs).
 Second mentions should generally be more predictable.
 Very high frequency words excluded (function words).
M e a s u r e d :
 Word duration.
 Peak amplitude.
 f0 of stressed vowel.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Experiment 1
 Words read in isolation by 10 subjects.
 Vowel space is expanded in Hard words (mean Euclidian 
distance of vowels from the me an F1, F2 of all vowels).
 Vowels were shorter in hard words (222 ms vs. 232 ms).
Image removed due to copyright restrictions.
Please see Figures 1 and 2 in Munson, B., and N. P. Solomon. "The Effect of Phonological Neighborhood Density on Vowel Articulation."
Journal of Speech, Language, and Hearing Research 47 (2004): 1048-1058.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Effects of the lexicon and context 
on speech production</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Selfish speakers and neighborhood density
 The H&amp;H account: Words from dense neighborhoods/low 
frequency words are pronounced more clearly because the 
speaker knows they are likely to  be more difficult for listeners
to recognize.
 Speaker-oriented account (e.g. Pierrehumbert 2002): 
 Speakers have to perform lexical access in speech 
production.
 Hypothesize that high neighborhood density impedes 
lexical access in production.
 Slower lexical access results in clearer pronunciation.</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Self-centered behavior in production of 
repetitions
 Bard et al (2000) examined shortening of repeated words in 
spontaneous dialogues produced in the context of a map task.
 Two participants each have a map, one showing a route. Not 
all landmarks are shown on both maps.
 Subject with the route map instructs the second subject on how 
to reproduce the route on his/her map.
 Elicits spontaneous speech, but with multiple mentions of 
items marked on the maps.
 Basic set-up: look for effects of repetition on intelligibility of 
words.
 Twist: subjects repeat the task with two partners. Will they 
produce clear, longer pronunciations on first mentions to a 
new listener?</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Experiment 2
 4 classes of words, crossing:
 high vs. low frequency
 high vs. low neighborhood density
 20 words /class
 Duration:
 High frequency words had shorter vowels 
(205 ms vs. 211 ms)
 No effect of neighborhood density.
 Vowel space expansion:
 Less expanded in high frequency words.
 More expanded in high neighborhood density 
words.
 No significant interaction.Image removed due to copyright restrictions.
Please see Figures 3 in Munson, B., and N. P. Solomon.
"The Effect of Phonological Neighborhood Density on
Vowel Articulation." Journal of Speech, Language, and
Hearing Research 47 (2004): 1048-1058.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Selfish speakers and neighborhood density
 But high neighborhood density can actually speed lexical 
access in production: Pictures are named more quickly when 
their names are in dense neighborhoods (Vitevitch 2002).
 Lexical access in production starts from meaning, so there is 
no problem of competition based on phonetic similarity .
 Vitevitch offers two explanations for the facilitatory effect of 
dense neighborhoods:
 In an interactive activation model: activation spreads 
between phonetically similar words. In dense neighborhoods more activation reverberates back to the target word.
 OR: words in dense neighborhoods generally involve more 
common sound sequences - perhaps the motor plans for frequent sound sequences are easier to access/assemble.</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Munson (2004) results
 In both conditions vowels were more dispersed in words from dense 
neighborhoods.
 suggests effect of neighborhood density is not due to speaker 
difficulty with lexical access.
 The effect of frequency depends on condition:
 No delay: vowels more dispersed in low frequency words.
 Delay: effect of frequency not significant.
2022242628303234
High-Frequency Words
Low-Frequency wordsDelay ImmediateAverage Vowel-Space Dispersion (1brk)
Average Vowel-Space Dispersion (1brk)
Delay Condition2022242628303234
Delay Immediate
Delay Condition
Figure by  MIT Open Course Ware .</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Listener-Oriented Speakers
 It has been hypothesized that the production and 
perception effects are linked:
 Words that are more difficult to recognize are 
pronounced more clearly.</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Contextual predictability and speech 
production
 H&amp;H theory predicts that speaker clarity should generally 
mirror listener difficulty with lexical access.
 Given the evidence above concerning word recognition 
performance, we should expect to find:
1. Reduced clarity where a word is predictable from 
context.
2. Reduced effects of frequency/neighborhood density in 
contexts where a word is more predictable.
 Some evidence for (1), e.g Lieberman (1963), Hunnicutt
(1985), Fowler &amp; Housum (1987), Bell et al (2003).
 Only one inconclusive test of (2).</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Pierrehumbert, J.B. (2002). Word-specific phonetics. In C. Gussenhoven and 
N. Warner (eds.) Papers in Laboratory Phonology VII, Mouton de Gruyter, 
New York, 101-139.
 Scarborough, R. (2003). The word-level  specificity of lexical confusability 
effects . Poster presented  at the 146th Meeting of the Acoustical Society of 
America, Austin,  TX.
 Sommers, M.S., and Danielson, S.M. (1999). Inhibitory processes and spoken 
word recognition in young and older adults: the interaction of lexical 
competition and semantic context. Psychology and Aging 14, 458-472
 Sommers, M., Kirk, K., Pisoni, D. (1997). Some considerations in evaluating 
spoken word recognition by normal-hearing, noise-masked normal-hearing, and cochlear implant listeners. I. The effects of response format. Ear and 
Hearing 18, 8999.
 Van Petten, C. and Kutas, M. (1990). Interactions between sentence context 
and word frequency in event-related brain potentials. Memory and Cognition
18, 380-393.
 Van Son, R.J.J.H. and Pols, L.C.W. (2003a). Information Structure and 
Efficiency in Speech Production" , Proceedings of EUROSPEECH2003, 
Geneva, Switzerland, 769-772.References</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Contextual predictability and speech 
production
 Lieberman (1963):
 The word you will hear is nine .
 A stitch in time saves nine .
 Words excised from predictable contexts are less accurately 
identified in noise.
 Hunnicutt (1985) replicated and extended this result.
 Assessed predictability of words by Cloze procedure: 
Present sentences with word left out. Ask subjects to guess 
the missing word. Cloze probability = proportion of correct 
guesses.
 No investigation of the acoustic bases of these effects.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Word recognition
The speed an accuracy of word recognition depends on:
 Word frequency
 Neighborhood density
 and frequency of neighbors
 Contextual predictability
 Speech production is also affected by these factors.</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Are speakers altruistic or selfish?
 Implication of H&amp;H account of pronunciation vari ability: Speakers 
estimate listener difficulty moment to moment and adjust clarity of speech 
accordingly.
 Speaker has a model of the listener (altruistic speaker).
 Alternative line of analysis: Speakers are selfish.
 Speakers do not track listener difficulty.
 Pronunciation variation is related to  speaker difficulty with lexical 
access for production:
 Slower lexical access results in clearer speech.
 To the extent that ease of lexica l access is similar for speaker and 
hearer, similar results are predic ted (but there are differences).
 But why does slow lexical a ccess result in clearer speech?</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Repetition and speech production
 New words were longer, louder and had higher f0.
 Only the duration effect is statistically reliable across 
speakers. 
 A subsequent experiment established that old words were, on 
average, more predictable in a Cloze task:
 New 18.3% correct vs. Old 31.1% correctNew New New Old Old Old
Keillor
Others562
436119
135 134110 492
3951.12
1.921.03
1.77Duration (ms) f0 (Hz) Amplitude (volts)
Figure by  MIT O penC ourseW are.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Listener-oriented behavior in speech 
production
 While a variety of the effects predicted by the H&amp;H model 
have been observed (neighborhood density, frequency, 
predictability), there are alternative selfish explanations for 
most of them.
 H&amp;H theory offers a simple, unified account of these 
phenomena, the speaker-oriented analysis are more ad hoc (e.g. why should slower lexical access result in clearer speech?
Lexical access cannot explain neighborhood density effects, etc.)
 It is clear that H&amp;H theory needs to be supplemented by an 
account of how well speakers track listener needs.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Repetition and speech production
 Words from the first experiment were excised and presented for 
identification by subjects (+ confidence rating /5)
 Both versions of each word were heard in separate blocks.
 Order or pairs counter-balanced across subjects.
 Repeated words are less accurately identified.
 Accuracy correlated with duration difference (new-old).
 Words identified more accurately on second presentation.
 repetition facilitates recogniti on (i.e. could offset reduction).
Block
of testOccurrence in monolog
1st 2nd
Percentage err ors
Confidence judgments1
12
211.6
8.8
4.47
4.6916.2
12.4
4.26
4.49
Figure by  MIT Open Course Ware .</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>A Bayesian model of the listener - context 
effects
 The Bayesian analysis implies that word frequency 
affects word recognition because it is a good basis for 
estimating prior probability of a word in the absence 
of any other constraint.
 But in general the prior probability of a word depends 
on context, e.g. discourse topic, previous words, 
syntactic structure.
 Ideal listener should inco rporate these contextual 
effects into estimates of prior probabilities.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Self-centered behavior in production of 
repetitions
 Bard et als results might be attribut ed to effects of lexical access (priming).
 Bard et al actually argue that speake rs are incompetent altruists: tracking 
listener needs is simply too demand ing, so speakers usually make the 
simplifying assumption that liste ners know what they know.
 A further complication: Bard et al (2000) did not compare repeated 
interactions with the same vs. diff erent listeners. The second listener was 
always new.
 Gregory (2001) had speakers tell a story twice, either to the same listener or 
to two different listeners.
 Measured durations of repeated re ferring expressions (e.g. names).
 When hearer changes for second narra tion, first use in second narration 
is longer than first us e in narration 1 (n.s.)
 When hearer is the same for both narrations, first use in narration 2 is 
shorter than first use in narration 1.
 Indicates that speakers can take  listener knowledge into account.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Production and perception
Broad outlines of an explanatory model of the effects of 
frequency, neighborhood density etc on production (Wright 
2004, Scarborough 2004, 2006):
 Speaker wishes to be understood.
 Speaker wishes to minimize the effort involved in speech 
production.
 Reduced effort tends to lead to reduced identifiability of 
words.
 Optimal strategy: reduce effort more where clarity is less 
important, i.e. where top-down evidence makes it easier for a 
listener to identify a word - Hyper- &amp; Hypoarticulation theory (Lindblom 1990).</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Grosjean, F., &amp; Itzler, J. (1984). Can semantic constraint reduce the role of 
word frequency during spoken word recognition? Bulletin of the Psychonomic
Society 22, 180-182.
 Jurafsky, D. (1996). A probabilistic mode l of lexical and syntactic access and 
disambiguation. Cognitive Science 20, 137-194
 Lindblom, B. (1990). Explaining phonetic variation: A sketch of the H&amp;H 
theory. In W.J. Hardcastle and A. Marchal (eds.) Speech Production and 
Speech Modeling . Kluwer: Dordrecht
 Luce, P., and Pisoni, D. (1998). Recognizing spoken words: The neighborhood 
activation model. Ear and Hearing 19. 1-36.
 Munson, B. (2004) Lexical access, lexical representation, and vowel 
production. To appear in J. Cole and J. I. Hualde (eds.), Papers in Laboratory 
Phonology IX . Mouton de Gruyter.
 Munson, B., and Solomon, N.P. (2004). The effect of phonological
neighborhood density on vowel articulation. Journal of Speech, Language, and Hearing Research 47, 1048-1058.
 Norris, D. (in press)  The Bayesian Reader: Explaining word recognition as an 
optimal Bayesian decision process. Psychological Review.References</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>The meaning of intonation</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/24-910-topics-in-linguistic-theory-laboratory-phonology-spring-2007/resources/lec8_focus/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>Broad focus: 
what happened
Time (s)2.00885 3.4781105000
Time (s)2.61383 4.0809605000
Time (s)2.79811 4.2325205000
verb focus
Time (s)2.55219 3.9466905000
Annabel married  Maloney Annabel   married  MaloneyObject focus: Who did Annabel marry?
Audio: 
1_broad.wavAudio:
1_subj.wav
Audio: 
1_obj.wavSubject focus:who married Maloney?

Verb focus:what did Annabel do to Maloney?Audio: 
1_verb.wav</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Pitch range in intonation
 These phenomena suggest that  pitch range is central to 
intonation in many languages.
 Pitch range is treated as a continuous variable, independent 
of the tone sequence in Pierrehumbert 1980 and is not 
transcribed in AmE_ToBI.
 The preliminary M_ToBI (pan-Mandarin) transcription 
system proposed by Peng et al (2001) includes labels for 
pitch range effects:
 %q-raise - flat raised pitch range regularly seen in echo questions
 %e-prom - local expansion of pitch range due to emphatic 
prominence
 %compressed - reduction of pitch range (e.g. following focus).</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Cooper, Eady and Mueller (1985)
 Experiment 2 used a similar method, but with longer answers and a 
neutral condition: sentences read without context.
My sister took the trolley from school to the store to buy fishand 
chips on Tuesday .
(A) Did your sister or your brother take the trolley from 
school to the store to buy fish and chips on Tuesday?
(B) Did your sister take the trolley from school to the store 
or from school to the restaurant to buy fish and chips 
on Tuesday?
(C) Did your sister take the trolley from school to the store 
to buy fish and chips on Tuesday or on Friday?
Excerpted from Cooper, William E., Stephen J. Eady, and Pamela Mueller. "Acoustical 
Aspects of C ontrastive Stress in Question-answer Contexts." Journal of the Acoustic 
Society of America 77, no. 6 (June 1985).</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Focus intonation in Mandarin Chinese
 Focus is also marked by duration: Word is longer when 
narrowly focused, compared to neutral/non-focused 
realizations.
 Compression of pitch range following focus is comparable 
to post-nuclear deaccenting in English.
 It is also seems to be common cross-linguistically.
 Could English deaccenting actually be pitch range 
compression?</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Figure by  MIT Open Course Ware .Figure by  MIT Open Course Ware .
The Uruguayan bulldozer drivers' union.MYL
H*
H*L-
L%
L-L%b
6.0The Uruguayan bulldozer drivers' union.MYL
H*
H*L-
L%
L-L(H*)
(H*)b
6.0</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Declarative/Interrogative intonation
Higher pitch range and/or less dow ndrift differentiate questions from 
statements in a number of languages:
 in Hausa H is downstepped following L in statements, but not in 
questions (Inkelas and Leben 1990).
 Chichewa: questions have higher pitch range and less downdrift of H 
than statements (Myers 1996).
Loud.Q
Loud.S
Norm.Q Norms.S
Soft.Q
Soft.S275
250
225
200
175
150
125
100H1 H2 H3 H4F0
Mean f0 values by position Speaker DJPOSITION
Figure by  MIT O penC ourseW are.</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Focus in Korean
Given the absence of pitch accents, how is material marked as 
prominent? Jun (2002):
 AP boundary preceding focus.
 Focus word lengthened, realized with expanded pitch 
range.
 Dephrasing: all words up to the end of the IP included in 
the AP with focus, or
 Pitch range compression - if post-focus string is long, AP 
boundary may be retained, but post-focal AP has reduced 
pitch range.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Readings: 
 ToBI tutorial 2.6, 2.7, Welby (2003) 
Assignments:
 ToBI transcription/investigating focus 
marking.
 Run 2 subjects for perception experiment.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Time (s)0 1.3669804500
Time (s)0 1.3669875300Contrast
H*                L-L%
Time (s)0.326843 2.1206804500
Time (s)0.326843 2.1206850300L+H*                L-L%
Listen: 
8_marmelade2.wav
Listen: 
8_marmelade5.wav
These audio files are from MIT course 6.911 Transcribing Prosodic Structure of Spoken Utterances with ToBI , IAP 2006 
(published in MIT OpenCourseWare, http://ocw.mit.edu )  Lecture Notes, Chapter 2.5. (Audio files courtesy of the Ohio 
State University Research Foundat ion and the OSU ToBI Research Group. Used with permission.)</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Summary 
 Pierrehumbert and Hirschberg (1990): intonation marks the 
way that the propositional content of an utterance relates to 
an evolving discourse model or set of mutual beliefs
 This includes marking information status of parts of an 
utterance
 focus/background
 contrast
 Many of the specifics remain unclear. Need:
 Production studies - what intonation do people use in 
different contexts?
 Perception studies - what meaning do people extract 
from different intonation contours?</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Examples from Peng et al (2001):
Images removed due to copyright restrictions.
Please see: Peng, Shu-hui, Marjorie K. M. Chan, Chiu-yu Tseng, Tsan Huang, 
Ok Joo Lee, and Mary E. Beckman. "Towards a Pan-Mandarin System for 
Prosodic Transcription." In Prosodic Typology: The Phonology of Intonation and 
Phrasing. Edited  by Sun-Ah Jun. New York, NY: Oxford University Press, 2005, 
pp. 230-270.</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Post-nuclear pitch range compression?
 Occasionally a pitch accent is observable (and audible) on SOF:
Figure by  MIT Open Course Ware .today Even the doctor only gave Fate aHiF0 = 175 Hz450 Hz
18.09 Hz
01 Hz
Focus
sentenceHiF0 = 212 Hz
pil
Sample with a pitch accented second occurrence focus.</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Focus Marking
How is focus marked prosodically?
 It is clear that focused constituents contain pitch accents.
 But non-focused elements can bear pitch accents also.
Who did Annabel marry?
H*                         H*L-L%
Annabel married Maloney.
 Hypothesis: focus is marked by a nuclear pitch accent.
 The last pitch accent in a phrase, followed by a phrase 
accent.
Who married Maloney?
H*                             L-L% H* H*L-L%
Annabel married Maloney. ?Annabel married Maloney.
 But do allnuclear accents mark focus?</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Welby (2003)
Results (1 is best, 5 is worst):
 all accent patterns are equally  acceptable whether focus is on VP 
or NP
 H*L-H*, object H* &lt; two-peak &lt; verb-H*
1.02.03.04.05.0
Accent Patternverb-H* hat object-
NP-H*two-peakobject-NP-focus
VP-focusQuestion TypeRating
Figure by MIT OpenCourseWare. Adapted from Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Projection." Language and Speech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Mandarin Chinese Intonation
 How does intonation work in languages with lexical tone?
Mandarin Chinese
 4 contrasting tones distinguish words
Figure by  MIT Open Course Ware .ms 350 700 1050 1400 1750Hz100150200Waveform and analysis of four Chinese words
ma (high level)
motherma (high rising)
hempma (low falling rising)
horsema (high falling)
scold</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Focus
The focus/background distinction is clearest in answers to 
WH-questions:
 Question-answer congruence:
A. What newspaper do you read?
B. I read the DISPATCH.
C. *I READ the Dispatch.
1. Who did Annabel marry?
2. Annabel married MALONEY.
1. Who married Maloney?
2. ANNABEL married Maloney.</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Cross-linguistic variation in prosody
Prosodic resources used in English:
 Pitch accents
 Placement
 Type
 Phrasing
 Boundary tones
 Pitch range (accents, phrases).
Not all languages make use of all of these resources, and 
there appear differences in the roles of these resources in 
signaling meaning.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Welby (2003) - results
 Hypothesis: Two peak answer is not appropriate for VP focus, it 
is a double focus. 
 I write for the Times but I READ the DISPATCH.
 So it may be the case that only nuclear accents mark focus - it is 
the details of focus marking of constituents that need 
clarification.
 Production studies needed!
hat two peak
I READ the DISPATCH. I READ the DISPATCH.H* H*
L-H* H*H* H*
L- L-L-L-L%
L%H* H* L- L%
L%
Figure by MIT OpenCourse Ware. Adapted from  Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Proj ection." Language and Spe ech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Topic/Contrast
(L+)H*L- H*           H*L-L%
Mary married a man from Milan
 Mary is Background and Given, so why does it get a nuclear accent?
 It has been suggested that accentuation is also used to mark Topics - the 
topic under discussion (cf. Jackendoff 1972).
 Mary is plausibly the topic  here.
 What about Mary, who did she marry?
 But not all topics get (nuclear ) accents - see Obj &amp; V focus above.
 Alternative hypothesis: Accent  is used to mark contrast with particular 
alternatives (usually explicitly mentioned).
 E.g. were talking about Jans daughters, Mary, Alex &amp; Phyllis and 
who they got married to.
 Specifically claimed that  L+H* marks contrast (e.g. Pierrehumbert &amp; 
Hirshberg 1990)</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Cooper, Eady and Mueller (1985)
 No significant differences 
between broad focus (N) 
and words up to and 
including focus in A,B,C.
 Final focus (C) is not 
distinct from broad focus 
(N) by peak f0 (as claimed 
by Jackendoff, Selkirk)
 Final word is longer under 
narrow focus (C) than 
broad focus (N).
 Post-focus f0 is similar in 
A,B  and looks unaccented. 170
160
150
140
130
120
110
100
1
2
3
4
5
6
7Version A
Version BVersion CVersion N
Key-Word PositionFundamental Frequency (Hz)
Figure by MIT OpenCourseWare. Adapted from Excerpted form Cooper, William E., Stephen J. Eady, 
   and Pamela Mueller. "Acoustical Aspects of Contrastive Stress in Question-Answer Contexts." 
Journal of the Acoustic Society of America  77, no. 6 (June 1985).</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Post-nuclear pitch range compression?
Pierrehumbert (1980) observes sm all echo accents in post-nuclear 
stretches:
Figure by  MIT Open Course Ware .
220
(L*)(L*)
(L*)H%
60L*H
L * (L*) (L*) H (L*)HMYL
The Uruguayan bulldozer drivers' union</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Mandarin Chinese Intonation
 Every syllable has a lexical tone (with the possible 
exception of neutral tone syllables).
 Unsurprisingly, there are no pitch accents.
 It is not clear that there are boundary tones.</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Welby (2003)
 Sample f0 tracks of answers.
 Recorded by one speaker.
Dispatch the read I
Dispatch the read IDispatch
H*
H* H* L-the
H* L-L%
H* H* L-L%L-L%
L-L%I read
Dispatch the I readA. C.
B. D.
Figure by MIT OpenCourseWare. Adapted from Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Projection." Language and Speech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Focus intonation in Mandarin Chinese
 Focus is implemented as variations  in the local pitch range in which 
lexical tones are realized.
 Non-final focused words: expanded pitch range
 Post-focus words: lowered, compressed pitch range
 Pre-focus, final focu s: neutral pitch range
H H H H HFocus
Neutral
Word 1
Word 2
Word 3
Figure by  MIT Open Course Ware .</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Focus and Pitch Range
The most consistent correl ates of focus appear to be expansion of pitch 
range on the focus and reduction of pitch range following the focus.
 Applies in languages with  and without pitch accents.
 Could the appearance of post-focu s dephrasing actually result from 
pitch range compression?
 Japanese is supposed to have dephrasing after focus, but Sugahara
(2003) found major syntactic boundaries are marked by final 
lengthening even in post-focal position. Small f0 differences correlated 
with phrasal difference.
 Compression of post-focal tones, not dephrasing?
 Could the appearance of post-nuc lear deaccenting actually result from 
pitch range compression?
 Post-focal pitch range compression without complete elimination of 
accents/tones is observed in Mandarin, NK Korean, Swedish (Bruce
1982), Neapolitan Italian (DImperio 2000), Catalan (Estebas Vilaplana
2003), European Portuguese (Frota 1998).</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Korean Intonation
Accentual Phrase
 AP is also the domain of a number of phonogical processes. E.g. Lenis 
stops are voiceless in AP-initial po sition, but voiced between voiced 
sounds AP-medially (S. Jun 1996).
[ta] soy sauce [kotud
a] red pepper paste
red pepper-soy sauce
Intonational Phrase
 There is an astonishing variety of boundary tones, realized on the final 
syllable: L%, H%, LH%, HL%, LHL%, HLH%, HLHL%, LHLH%, 
LHLHL%
HL  H%
Listen to Ex.21 here, but 
only the last quarter 
(ANSoni
pakiNsINiMnida)</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Nuclear accents and focus
Who did Mary marry?
Mary       married   a  man        from    Milan.
Listen: 
8_mary.wav</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Welby (2003)
 Experiment 1 materials: Recorded question-answer pairs.
 Two questions:
Object-NP focus, e.g. What newspaper do you read?
VP focus, e.g. How do yo u keep up with the news?
 Four versions of each answer (below).
48 Q/A pairs, 2 versions of each question * 4 versions of each answer
 Subjects rated the appropriateness of answers on a 5 point scale.
80 subjects. Each heard each  Q/A pair in one condition.
verb-H* object-NP-H*
hat two peakI READ the Dispatch. I read the DISPATCH.
I READ the DISPATCH. I READ the DISPATCH.H*H*
H* H*L-L-
L-L%L%H*H*L-L-L%
L%
H* H*H* H*
L- L-L-L-L%
L%H* H* L-L%
L%
Figure by MIT OpenCourseWare. Adapted from Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Projection." Language and Speech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>The meaning of intonation 
 Pierrehumbert and Hirschberg (1990): intonation marks the 
way that the propositional content of an utterance relates to 
an evolving discourse model or set of mutual beliefs
 This includes marking information status of parts of an 
utterance
 given/new
 focus/background
 contrast</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>24.910
Laboratory Phonology
The Meaning of Intonation
Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.
Source: Xu, Yi. "Effects of Tone and Focus on the Formation and Alignment of f_sub_0 Contours." Journal of Phonetics 27 (1999): 55-105.</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Post-nuclear pitch range compression?
 Typical SOF utterance:
Figure by  MIT Open Course Ware .today Even the kids would only let the cats in theHiF0 = 93 Hz200 Hz
50 022
50 Hz
Focus
sentenceHiF0 = 96 Hz
tent
Sample with a second occurrence focus lacking any pitch accent.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Given/New 
 Given: previously mentioned (accessible).
 New: not previously mentioned (or less accessible).
 It is often suggested that New information tends to be 
accented, while Given information tends to be unaccented.
(1) a. I found an article for you in a German journal.
b. I dont READ German.
(2) I brought her a bottle of whisky, but it turns out she doesnt LIKE 
whisky.
 More accurate: Given information can be unaccented.</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Focus intonation in Mandarin Chinese
 Xu (1999) elicited sentences with focus on different words 
by providing questions as context and underlining narrow 
focus item. E.g.
(What is kitty doing?)
(Who is touching kitty?)
(What is kitty doing to kitty?)
(What is kitty touching?) Maomi ganma ne?
 Shei mo maomi?
 Maomi zenmo nong maomi? Questions preceding the target sentences
 Maomi mo shenmo?Subj focus
V focus
Obj focusVP focus
'
'
 '
'
Word  2 Word  3
'touches' mo H
Rna'takes'Word  1
HRmaomi'cat-fan'
Fmai'sells' maomi HL 'cat-rice'
HFmaomi'cat-honey'HH 'kitty' maomi 'kitty' maomi HH
LH 'sabre' madao
'
'Figure by  MIT Open Course Ware .
Figure by  MIT Open Course Ware .</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Korean Intonation
Two prosodic constituents:
 Accentual Phrase
 generally a lexical item plus  a case marker or postpositions
 marked by a melody: THLH (T=H if the AP initial segment is 
aspirated or tense, T=L otherwise)
 Intonation Phrase
 consists of one or more APs
 marked by final lengthen ing and a boundary tone.
T H H LW (W) (W)(AP) APIP
%s s s s
Figure by  MIT Open Course Ware .</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Focus - production studies
 Few production studies of focus realization that provide full analyses of 
the intonation contours.
 Cooper, Eady and colleagues have published several studies that report 
peak f0 and durations of key words under various focus conditions.
 Cooper, Eady and Mueller (1985) examined contrastive focus elicited by 
questions about explicit alternatives, e.g.
Chuck liked the present that Shirley sent to her sister.
(A) Did William or Chuck like the present that Shirley 
sent to her sister?
(B) Did Chuck like the letter or the present that Shirley 
sent to her sister?
(C) Did Chuck like the present that Melanie sent to her 
sister or the one that Shirley sent?
(D) Did Chuck like the present that Shirley sent to her 
sister or the one she sent to her brother?
Excerpted from Cooper, William E., Stephen J. Eady, and Pamela Mueller. "Acoustical 
Aspects of C ontrastive Stress in Question-answer Contexts." Journal of the Acoustic 
Society of America 77, no. 6 (June 1985).</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>verb-H* object-NP-H*
hat two peakI READ the Dispatch. I read the DISPATCH.
I READ the DISPATCH. I READ the DISPATCH.H*H*
H* H*L-L-
L-L%L%H*H*L-L- L%
L%
H* H*H* H*
L- L-L-L-L%
L%H* H* L- L%
L%Welby (2003) - predictions
Object-NP focus, e.g. What newspaper do you read?
VP focus, e.g. How do you keep up with the news?
Obj: Bad - no accent on Obj
VP: Bad - projection impossibleObj: Should be good
VP: Good if projection is OK
Obj: Bad if nuclear accents mark focus.
VP: OKObj: OK if only nuclear accents mark focus.
VP: OK
Figure by MIT OpenCourseWare. Adapted from Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Projection." Language and Speech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Korean Intonation
Accentual Phrase
 If the AP contains 4 or more syllables, TH associate to 1st and 2nd 
syllables (or sometimes 1st and 3rd in longer AP), LH associate to 
penultimate and last syllables.
 If the AP contains fewer than 4 sy llables various subsets of the full 
tonal pattern are realized (so far unpredictable, but not apparently 
contrastive).
[ L    H          
 L    H][ L   L   H][  L     H              L%]
To listen, 
please see 
Ex.9 here</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Focused constituents
Time (s)2.14785 3.5393405000
Time (s)2.55219 3.9466905000
Object focus: VP focus:
Who did Annabel marry? what did Annabel do?
Listen:
1_obj.wavListen:
8_vp.wav</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Focused constituents
 Semantically, focus can be a constituent, not just a word.
Q1: How do you keep up with the news?
Q2: What newspaper do you read?
A: I [read the [DISPATCH.]F2]F1
 It is often claimed that a cons tituent is marked as focused 
by placing a pitch accent on the strongest stress in the 
phrase - usually the last content word (cf. Jackendoff 1972, 
Selkirk 1995).
 focus projection
 in particular, a transitive VP can be marked as focused by placing a 
(nuclear) pitch accent on the object.
 So a sentence like A should be ambiguous with respect to 
the scope of focus (object vs. VP (vs. sentence?)).</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Information conveyed by prosody - Focus
 Focus - the informative part of an utterance.
 the information in the sentence that is assumed by 
the speaker not to be shared by him and the 
hearer (Jackendoff 1972).
 Presupposition (aka Ground, Background): the 
information in the sentence that is assumed by the speaker to be shared by him and the hearer
(Jackendoff 1972).
 NB substantial overlap with Given/New partition on 
this view.
 Focus is marked by accentuation.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Focus
 The same concept of focus is claimed to interact 
with the meaning of  focus sensitive particles
A. John only gave Bill MONEY.
B. John only gave BILL money.
 Other focus sensitive particles: just, too, even, 
always.
 What is the prosody denoted by 
CAPITALIZATION? I.e. how is focus marked?</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Korean Intonation
 The intonation of Seoul Korean has been studied in detail 
(e.g. S. Jun 1996, 1998), and there is also work on a 
number of other dialects.
 Seoul Korean lacks lexical tone, but also does not have 
intonational pitch accents - intonation is phrase-based.
 Description based on K_ToBI (S. Jun 2000).</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Cross-linguistic variation in prosody
Survey selected languages that illustrate some of these 
differences, concentrating on:
 Mandarin Chinese - no pitch accents, no boundary tones?
 Korean - no pitch accents.
 Revisit the role of pitch range in English in light of its 
importance in languages without pitch accents.</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Declarative/Interrogative intonation in Mandarin
 Yuan et al 2002 analyzed matched declarative and 
interrogative sentences (indicated by punctuation).
Figure by MIT OpenCourseWare. Adapted from Yuan, Jiahong, Chilin Shih, and Greg P. Kochanski. "Comparison of Declarative and Interrogative Intonation in Chinese." 
Proceedings of Speech Prosody 2002 (2002): 711-714.1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 -0.2100.0120.0140.0160.0180.0200.0220.0240.0260.0280.0300.0Declarative
1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 -0.2100.0120.0140.0160.0180.0200.0220.0240.0260.0280.0300.0
1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 -0.2100.0120.0140.0160.0180.0200.0220.0240.0260.0280.0300.0Question
1.8 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 -0.2100.0120.0140.0160.0180.0200.0220.0240.0260.0280.0300.0
luo2yan4li3bai4wu3 yao4 mai3 yang2
Luoyanwants to buy sheep on Friday
luo2yan4li3bai4wu3 mai4 ye3lu4Luoyansells wild deer on Friday</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Summary
 Not all languages have pitch accents or boundary tones.
 All languages seem to use pitch range and phrasing.
 In English, pitch range may play a bigger role in signaling 
basic functions like focus than is implied by ToBI.</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Declarative/Interrogative intonation in Mandarin
 Interrogatives can be disti nguished from declaratives by 
global pitch range effects (Garding 1985, 1987, Yuan et al 
2002, etc). (Examples from Peng et al 2001).
Images removed due to copyright restrictions.
Please see: Peng, Shu-hui, Marjorie K.M. Chan, Chiu-yu Tseng, Tsan Huang, 
Ok Joo Lee, and Mary E. Beckman. "Towards a Pan-Mandarin System for 
Prosodic Transcription." In Prosodic Typology: The Phonology of Intonation and 
Phrasing. Edited  by Sun-Ah Jun. New York, NY: Oxford University Press, 2005, 
pp. 230-270.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Welby (2003) - results
 Object-NP focus, e.g. What newspaper do you read?
 VP focus, e.g. How do you keep up with the news?
verb-H* object-NP-H*
hat two peakI READ the Dispatch. I read the DISPATCH.
I READ the DISPATCH. I READ the DISPATCH.H*
H* H*L-L-
L-L%L%H*H*L-
L-L%
L%
H* H* H* H* L- L-
L-L-L%
L%H*
H* H* L- L%
L%Obj: OK 
VP: OK - projection is OK
Obj: OK -  only nuclear accents mark focus?Least acceptable
Obj: Less good - Should be bad if 
nuclear accents mark focus.
VP: OK VP: Less good
Figure by MIT OpenCourseWare. Adapted from Welby, Pauline. "Effects of Pitch Accent Type and Status on Focus Projection." Language and Speech 46, no. 1 (2003): 53-81.</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Focus intonation in Mandarin Chinese
Examples from Peng et al (2001):
Images and audio files removed due to copyright restrictions.
Please see: Peng, Shu-hui, Marjorie K. M. Chan, Chiu-yu Tseng, Tsan Huang, 
Ok Joo Lee, and Mary E. Beckman. "Towards a Pan-Mandarin System for 
Prosodic Transcription." In Prosodic Typology: The Phonology of Intonation and 
Phrasing. Edited  by Sun-Ah Jun. New York, NY: Oxford University Press, 2005, 
pp. 230-270.</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Post-nuclear pitch range compression?
Beaver et al (2004) provide evidence that phrasal prominence distinctions 
are possible in post-nuclear pos ition, although marked more 
consistently by duration and intensity rather than f0.
 Studied Second Occurrence Focus sentences:
 SOF word is longer and more intense than matched non-focused word.
 Minimal f0 effects (minimum f0 is slightly lower in SOF in d.obj position)</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Focus intonation in Mandarin Chinese
 Focus is implemented as variations  in the local pitch range in which 
lexical tones are realized.
 Non-final focused words: expanded pitch range
 Post-focus words: lowered, compressed pitch range
 Pre-focus, final focu s: neutral pitch range
Courtesy Elsevier, Inc., http://www.sciencedirect.com. Used with permission.
Source: Xu, Yi. "Effects of Tone and Focus on the Formation and Alignment of f_sub_0 Contours." Journal of Phonetics 27 (1999): 55-105.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu
24.910 Topics in Linguistic Theory: Laboratory Phonology
Spring 2007
 
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms .</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Cooper, Eady and Mueller (1985)
 Subjects read sentences as answers to pre-recorded questions.
 Subjects were asked to repeat an item if experimenter judged that 
contrastive focus had not been placed appropriately (rarely).
 Measured duration and peak f0 in each key word.
 No broad focus condition
 Pre-focal words appear to 
be accented although 
given.
 Post-focus f0 is similar 
across conditions, and looks unaccented. 
 Cant determine accent 
type. 115120125130135140145150155160
1 2 3 4Version AVersion B
Version C
Version D
Key-Word PositionFundamental Frequency (Hz)
Figure by MIT OpenCourseWare. Adapted from Excerpted form Cooper, William E., Stephen J. Eady, and Pamela Mueller. "Acoustical
           Aspects of Contrastive Stress in Question-Answer Contexts." Journal of the Acoustic Society of America  77, no. 6 (June 1985).</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Focus and Phrasing
Focus has effects on phrasing in a number of languages:
 Korean - AP boundary preceding focu s, dephrasing following focus.
 Japanese - Intermediate phrase (a.k .a. Major Phrase) boundary before 
focus, dephrasing following focus (supression of 
MajorPhrase/Intermediate Phrase boundaries) ( Beckman and 
Pierrehumbert 1986, Nagahara 1994 )
 Supresses phrasal intonation, but not final lengthening (Sugahara
2003).
 Restricted to given material (Sugahara 2003).
 Hausa - emphasized words begin new phrase (Inkelas and Leben 1990)
 Bengali - focused constituen t is a phonological phrase (Hayes and Lahiri
1991).
 Chichewa - phonological phrase boundary after focus (Kanerva 1990).
 Focus is not marked by phrase boundaries in English.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Summary of hypotheses
 Focus is marked by pitch accents.
 Focus is marked by a nuclear accent.
 Prenuclear accents do not mark focus.
 All nuclear accents mark focus.
 Can all types of pitch accents mark focus?
 Focus projection: accenting an object can mark the 
whole VP as focussed.
 (Explicit) contrast is marked by L+H*
 Only Given material can be deaccented.
 Relatively little evidence.</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
