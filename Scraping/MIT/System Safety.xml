<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/</course_url>
    <course_title>System Safety</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Systems Engineering </list>
      <list>Business </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Introduction, Causality, Bhopal, Hindsight Bias (PDF - 1.5MB)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes1/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>62</slideno>
          <text>Hindsight Bias 
 Almost impossible to go back and understand how world 
looked to somebody not having knowledge of outcome 
 Oversimplify causality because start from outcome and reason 
backward  
 Overestimate likelihood of the outcome and peoples ability to 
foresee it because already know outcome 
 Overrate rule or procedure violations 
 Misjudge prominence or relevance of data presented to people 
at the time 
 Match outcomes with actions that went before it: if outcome bad, 
actions leading to it must have been bad too (missed 
opportunities, bad assessments, wrong decisions, and 
misperceptions) 
 
63</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Additional Information about  
Systemic Factors 
 Demand for MIC dropped sharply after 1981, leading to 
reductions in production and pressure on company to cut 
costs. 
 Plant operated at less than half capacity when accident occurred. 
 UC put pressure on Indian subsidiary to reduce losses, but gave 
no specific details about how this was to be done. 
 In response, maintenance and operating personnel cut in 
half. 
 Top management justified cuts as merely reducing avoidable and 
wasteful expenditures without affecting overall safety. 
 As plant lost money, many of skilled workers left for more 
secure jobs. They either were not replaced or replaced by 
unskilled workers. 
37</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Root Cause Seduction 
 Assuming there is a root cause gives us an illusion of 
control. 
 Usually focus on operator error or technical failures 
 Ignore systemic and management factors 
 Leads to a sophisticated whack a mole game 
 Fix symptoms but not process that led to those symptoms 
 In continual fire-fighting mode 
 Having the same accident over and over 
 
51</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Understanding Accident 
Causality 
15</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>What were the causes of this accident 
given what you know so far? 
What additional questions were raised 
by what you have seen so far? 
 
28</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Morale at the plant was low. Management and labor 
problems followed the financial losses. 
    There was widespread belief among employees that the  
management had taken drastic and imprudent measures to cut 
costs and that attention to the details that ensure safe operation 
were absent.        
 Five months before accident, local UC India management 
decided to shut down refrigeration system. 
 Most common reason given was cost cutting. 
 Local management claimed unit was too small and never 
worked satisfactorily. 
 Disagreement about whether UC in U.S. approved this 
measure. 
 High temperature alert reset and logging of tank 
temperatures discontinued. 
 
40</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>UC went into large-scale production of MIC without having 
performed adequate research on stability of the chemical. Did 
not know of an effective inhibitor for the type of reaction that 
occurred. 
 After the accident, both UC and OSHA announced same type 
of accident could not occur at Institute WV plant because of 
plants better equipment, better personnel, and Americas 
general higher level of technical culture. 
 Eight months later a similar accident occurred there. Led to brief 
hospital stays for 100 people. Consequences less serious only 
because of incidental factors such as direction of wind and tank 
contained a less toxic substance at the time. 
 Warning siren delayed and company slow in making information 
available to public. 
 
44</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Communication Links Theoretically in  
Place in Uberlingen Accident 
5
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Technology. Used with permission.</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>What about all the safety devices  
and procedures ? 
 How could the vent scrubber, flare tower, water spouts, 
refrigeration unit, alarms, and monitoring instruments all 
fail simultanously? 
 Not uncommon for a company to turn off passive safety 
devices to save money; gauges are frequently out of 
service. 
 At Bhopal, few alarms, interlocks, or automatic shutoff 
systems in critical locations that might have warned 
operators of abnormal conditions or stopped the gas leak 
before it spread. 
 Thresholds established for production of MIC routinely 
exceeded. e.g., workers said it was common to leave MIC 
in the spare tank. 
 
30</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Has your view of this accident changed with 
this additional information ? 
 
What additional causal factors would you 
now include? 
 
What additional questions would you want 
answered? 
 
 
35</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Events at Bhopal 
 Dec. 2, 1984, relatively new worker assigned to wash out 
some pipes and filters, which were clogged. 
 Pipes being cleaned were connected to the MIC tanks by 
a relief valve vent header, normally closed 
 Worker closed valve to isolate tanks but nobody inserted 
required safety disk (slip blind) to back up valves in case 
they leaked 
 Maintenance sheet contained no instruction to insert disk 
 Worker assigned task did not check to see whether pipe 
properly isolated because said it was not his job to do so. 
 He knew valves leaked, but safety disks were job of 
maintenance department. 
21</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Design for Safety 
 Eliminate or control scenarios (causal factors) identified 
by hazard analysis 
 Fault Tolerance 
 Failures will occur 
 Need to make sure they dont result in an accident 
 Design to prevent operator error 
 Human errors will occur 
 Need to make sure they dont result in an accident 
 Design so that dont induce human error 
14</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Jerome Lederer (1968) 
Systems safety covers the total spectrum of risk management.  
It goes beyond the hardware and associated procedures of 
systems safety engineering. It involves: 
 Attitudes and motivation of designers and production people, 
 Employee/management rapport, 
 The relation of industrial associations among  
     themselves and with government,  
 Human factors in supervision and quality control 
 The interest and attitudes of top management, 
48
 New Mexico Museum of Space
History. All rights reserved. This
content is excluded from our
Creative Commons license. For
more information, see http V://
ocw.mit.edu/help/faq-fair-use/ .</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Congressional approval of Alaska oil pipeline and tanker transport 
network included an agreement by oil corporations to build and use 
double-hulled tankers. Exxon Valdez did not have a double hull. 
 Crew fatigue was typical on tankers 
 In 1977, average oil tanker operating out of Valdez had a crew of 40 
people. By 1989, crew size had been cut in half. 
 Crews routinely worked 12-14 hour shifts, plus extensive overtime 
 Exxon Valdez had arrived in port at 11 pm the night before. The crew 
rushed to get the tanker loaded for departure the next evening 
 Coast Guard at Valdez assigned to conduct safety inspections of 
tankers. It did not perform these inspections. Its staff had been cut 
by one-third. 
54</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Safety Features 
 UC specified requirements to reduce hazards: 
 MIC was to be stored in underground tanks encased in 
concrete 
 Bhopal used three double-walled, stainless steel tanks, 
each with a capacity of 60 tons. 
 Operating manual specified that tanks were never to 
contain more than half their maximum volume or a standby 
tank was to be available to which some of chemical could 
be transferred in case of trouble. 
 Bhopal tanks were interconnected so that MIC in one tank 
could be bled into another tank. 
 As specified in operating manual, tanks embedded in 
concrete. 
17</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Operator Error: Systems View (2) 
 To do something about error, must look at system in which 
people work: 
 Design of equipment 
 Usefulness of procedures 
 Existence of goal conflicts and production pressures 
Human error is a symptom of a system that needs to 
be redesigned 
 
60</text>
        </slide>
        <slide>
          <slideno>64</slideno>
          <text>Overcoming Hindsight Bias 
 Need to consider why it made sense for people to do what 
they did   
 Some factors that affect behavior 
 Goals person pursuing at time and whether may have conflicted 
with each other (e.g., safety vs. efficiency, production vs. 
protection) 
 Unwritten rules or norms  
 Information availability vs. information observability 
 Attentional demands 
 Organizational context 
 
65</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Safety Features (cont) 
 MIC was to be stored in an inert atmosphere of nitrogen 
gas at 2 to 10 psi over atmospheric pressure. 
 Regularly scheduled inspection and cleaning of valves 
specified as imperative 
 Storage limited to 12 months maximum. 
 If staff were doing sampling, testing, or maintenance at a 
time when there was a possibility of a leak or spill, 
operating manual specified they were to use protective 
rubber suits and air-breathing equipment. 
 To limit its reactivity, MIC was to be maintained at a 
temperature near 0 C.  
 Refrigeration unit provided for this purpose 
 High temperature alarm if MIC reached 11 C. 
19</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Operating manual said refrigeration unit must be operating 
whenever MIC was in the system 
 Chemical has to be maintained at a temp no higher than 5 C. 
to avoid uncontrolled reactions. 
 High temperature alarm to sound if MIC reached 11 C. 
 Refrigeration unit turned off and MIC usually stored at nearly 
20 C. 
 Plant management adjusted threshold of alarm, accordingly, 
from 11 C to 20 C., thus eliminating possibility of an early 
warning of rising temperatures. 
 Flare tower was totally inadequate to deal with estimated 
40 tons of MIC that escaped during accident. 
 Could not be used anyway because pipe was corroded and 
had not been replaced.  
 
31</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Root Cause Seduction 
 Accidents always complex, but usually blamed on 
human operators 
 Cannot prevent them unless understand ALL the factors 
that contributed 
 Always additional factors (sometimes never identified) 
 Equipment failure and design 
 Procedures 
 Management decisions 
 Etc. 
 
50</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Fumbling for his recline button Ted  
unwittingly instigates a disaster The second important factor is 
system design vs. operator error 
 
All human behavior is affected by 
the context in which it occurs. 
 
58
 Gary Larson. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see httpV://ocw.mit.edu/help/faq-fair-use/ .</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Hierarchical models 
36</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Safety Features (cont) 
 Several backup protection systems and lines of defense 
 Vent gas scrubber designed to neutralize any escaping gas 
with caustic soda. Scrubber was capable of neutralizing 
about 8 tons of MIC per hour at full capacity 
 Flare tower to burn off any escaping gas missed by scrubber; 
toxic gases would be burned high in the air, making them 
harmless 
 Small amounts of gas missed by scrubber and flare tower 
were to be knocked down by a water curtain that reached 40 
to 50 feet above ground. Water jets could reach as high as 
115 feet, but only if operated individually. 
 In case of an uncontrolled leak, a siren was installed to warn 
workers and surrounding community. 
 
18</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Syllabus
Assignments and Grading
 Reading assignments and exercises 
 Two group assignments: accident analysis and project
 One take home exam (on accident analysis and hazard 
analysis)
Textbooks and Readings:
 Safeware
 Engineering a Safer World (published by MIT Press
            Download free from the MIT Press website 
 STPA Primer (draft) for reference only
 Optional readings just if you are interested
9</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Vent scrubber (had it worked) was designed to 
neutralize only small quantities of gas at fairly low 
pressures and temperatures.  
 Pressure of escaping gas during accident exceeded 
scrubbers design by nearly 2  times 
 Temperature of escaping gas at least 80 degrees more 
than scrubber could handle.  
 Shut down for maintenance 
 Water curtain designed to reach height of 40 to 50 feet. 
MIC vapor vented over 100 feet above ground. 
 Practice alerts did not seem to be effective in preparing 
for an emergency (ran from contaminated areas and 
ignored buses sitting idle and ready to evacuate them) 
 
 
32</text>
        </slide>
        <slide>
          <slideno>60</slideno>
          <text>(Sidney Dekker, 2009) Hindsight Bias 
should have, could have, would have 
61Courtesy of Sidney Dkker. Used with permission.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>MIC vented from stack 108 feet above ground. 50,000 
pounds of MIC gas would escape. 
 Operator turned off water-washing line when first heard 
loud noises at 12:45 am and turned on vent scrubber 
system, but flow meter showed no circulation of caustic 
soda.  
 He was unsure whether meter was working 
 To verify flow had started, he would have to check pump 
visually. 
 He refused to do so unless accompanied by supervisor 
 Supervisor declined to go with him. 
 Operator never opened valve connecting tank 610 to the 
spare tank 619 because level gauge showed it to be 
partially full. 
24</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Uncoordinated Control Agents 
Control Agent 
(ATC) Instructions Instructions SAFE STATE  
 ATC provides coordinated instructions to both planes SAFE STATE  
 TCAS provides coordinated instructions to both planes 
Control Agent 
(TCAS) 
Instructions Instructions UNSAFE STATE  
 BOTH TCAS and ATC provide uncoordinated &amp; independent instructions 
Control Agent 
(ATC) Instructions Instructions No Coordination 
4</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Other examples of unsafe conditions that were permitted 
to exist: 
 At time of accident, chloroform contamination of MIC was 4 
to 5 times higher than specified in operating manual, but no 
corrective action taken. 
 MIC tanks were not leak-tight to a required pressure test. 
 Workers regularly did not wear safety equipment, such as 
gloves or masks because of high temperatures in plant. 
There was no air conditioning. 
 Inspections and safety audits at the plant were few and 
superficial. 
 
41</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Maintenance procedures severely cut back and shift 
relieving system suspended (if no replacement showed 
up at end of shift, following shift went unmanned). 
 Indian government required plant to be operated 
completely by Indians 
 At first, UC flew plant personnel to West Virginia for 
intensive training and had teams of U.S. engineers make 
regular on-site safety inspections. 
 By 1982, financial pressures led UC to give up direct 
supervision of safety at the plant, even though it retained 
general financial and technical control. 
 No American advisors resident at Bhopal after 1982. 
 Minimal training of many of workers in how to handle 
non-routine emergencies. 
38</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>What were some of the causal factors 
in the Uberlingen accident? 
 
3</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Assistant plant manager called at home at 1 am and 
ordered vent flare turned on. He was told it was not 
operational (out of service for maintenance). A section of 
pipe connecting it to the tank was being repaired. 
 Plant manager learned of leak at 1:45 am when called by 
the city magistrate. 
 When MIC leak was serious enough to cause physical 
discomfort to workers, they panicked and fled, ignoring four 
buses intended for evacuating employees and nearby 
residents. 
 A system of walkie-talkies, kept for such emergencies, 
never used. 
25</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Operator Error: Systems View (1) 
 Human error is a symptom, not a cause 
 All behavior affected by context (system) in which occurs 
 Role of operators in our systems is changing 
 Supervising rather than directly controlling 
 Systems are stretching limits of comprehensibility 
 Designing systems in which operator error inevitable and then 
blame accidents on operators rather than designers 
 
59</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Acident Causes are Complex 
 The vessel Baltic Star, registered in Panama, ran aground at full 
speed on the shore of an island in the Stockholm waters on 
account of thick fog. One of the boilers had broken down, the 
steering system reacted only slowly, the compass was 
maladjusted, the captain had gone down into the ship to 
telephone, the lookout man on the bow took a coffee break, and 
the pilot had given an erroneous order in English to the sailor 
who was tending the rudder. The latter was hard of hearing and 
understood only Greek. 
                                                        Le Monde 
   
Were there also larger organizational and economic factors? 
 
10</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Hierarchical models 
29</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Alarms sounded so many times a week (20 to 30) that no 
way  to know what the siren signified 
 Emergency signal was identical to that used for other 
purposes, including practice drills. 
 Not turned on until 2 hours after MIC leak started and then 
turned off after 5 minutes (company policy) 
 Plant workers had only bare minimum of emergency 
equipment, e.g., shortage of oxygen masks discovered 
after accident started. 
 They had almost no knowledge or training about how to 
handle non-routine events. 
 Police were not notified when chemical release began 
 When called by police and reporters, plant spokesmen first 
denied accident and then claimed MIC was not dangerous. 
 Surrounding community not warned or prepared 
34</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Hazard Analysis 
 Investigating an accident before it occurs 
 Identify potential scenarios 
 Worst case analysis vs. average (expected) case 
analysis 
 Use results to prevent losses 
13</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Operator Error: Traditional View 
 Operator error is cause of most incidents and accidents 
 So do something about operator involved (suspend, 
retrain, admonish)  
 Or do something about operators in general 
 Marginalize them by putting in more automation 
 Rigidify their work by creating more rules and procedures 
 
57</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Hierarchical models 
20</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>System Safety Introduction 
1</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Blame is the Enemy of Safety 
 Goal of the courts is to establish blame 
 People stop reporting errors 
 Information is hidden 
 Goal of engineering is to understand why accidents 
occur in order to prevent them                   
52</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>To understand and prevent accidents,  
must consider system as a whole 
                                  
And so these men of Hindustan 
Disputed loud and long, 
Each in his own opinion 
Exceeding stiff and strong, 
Though each was partly in the right 
And all were in the wrong. 
           John Godfrey Saxe  (1816- 1887)   
47Image by MIT OpenCourseWare..
.Image by MIT
 OpenCourseWare Image by MIT OpenCourseWare.
Image by MIT OpenCourseWare Image by MIT OpenCourseWare.
Image by MIT OpenCourseWare. Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Bhopal 
 Worst industrial accident in history 
 Conservative estimate of 2000-3000 killed, 10,000 
permanent disabilities (including blindness), and 200,000 
injured. 
 Blamed by management on operator error 
 Union Carbide blamed on sabotage 
 MIC (methyl isocyanate) used in production of 
pesticides and polyurathanes (plastics, varnishes, and 
foams) 
 Highly volatile, vapor heavier than air 
 A major hazard is contact with water, which results in 
large amounts of heat. 
 Gas burns any moist part of body (throat, eyes, lungs) 
16</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Uberlingen continued 
 TCAS Pilots Guide was ambiguous about TCAS / ATC 
precedence 
Tu-154 Flight Operations Manual had contradictory 
sections 
 Chapter 8.18.3.2 forbids maneuvers contrary to TCAS 
 Chapter 8.18.3.4 says most important tool is executing 
ATC instructions. TCAS described as an additional 
instrument. 
8</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>MIC supervisor could not find his oxygen mask and ran to 
boundary fence, where he broke his leg attempting to 
climb over it. 
 Control room supervisor stayed in control room until the 
next afternoon, when he emerged unharmed. 
 Toxic gas warning siren not activated until 12:50 am when 
MIC seen escaping from vent stack.  
 Turned off after only 5 minutes, which was Union Carbide 
policy. 
 Remained off until turned on again at 2:30 am.  
 Police were not notified and when they called between 1 
and 2, were given no useful information. 
26</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Components of System Safety 
Engineering 
 Investigating accidents 
 Preventing Accidents 
 Hazard Analysis 
 Design for Safety 
 Operations 
 Management 
 
11</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Several Indian staff who were trained in U.S. resigned and 
were replaced by less experienced technicians. 
 When plant first built, operators and technicians had 
equivalent of two years of college education in chemistry or 
chemical engineering. 
 In addition, UC provided them with 6 months training. 
 When plant began to lose money, educational standards and 
staffing levels were reportedly reduced. 
 In 1983, chemical engineer managing MIC plant resigned 
because he disapproved of falling safety standards. He 
was replaced by an electrical engineer. 
39</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>A review and audit of Bhopal plant in 1982 noted many 
of deficiencies involved in accident 
 No follow-up to ensure deficiencies were corrected. 
 A number of hazardous conditions were known and allowed to 
persist for considerable amounts of time or inadequate 
precautions were taken against them. 
 Report noted such things as filter-cleaning operations without 
using slip blinds, leaking valves, possibility of contaminating 
the tank with material from the vent gas scrubber, bad 
pressure gauges. 
 Report recommended raising capacity of water curtain. 
Pointed out that alarm at flare tower was non-operational and 
thus any leakage could go unnoticed for a long time. 
 According to Bhopal manager, all improvements called for in 
the report had been taken care of, but obviously not true. 
42</text>
        </slide>
        <slide>
          <slideno>61</slideno>
          <text>Hindsight Bias 
 After an incident 
 Easy to see where people went wrong, what they should 
have done or avoided 
 Easy to judge about missing a piece of information that 
turned out to be critical 
 Easy to see what people should have seen or avoided 
 
 
62</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Workers looked for leak and saw a continuous drip on 
outside of MIC unit. 
 Reported it to the MIC supervisor 
 Shift supervisor did not consider it urgent and postponed an 
investigation until after the tea break. 
 12:40 am on Dec. 3: Control room operator noticed tank 
610 pressure gauge was approaching 40 psi and 
temperature was at top of scale (25 C) 
 12:45 am: Loud rumbling noises heard from tank. 
Concrete around tank cracked. 
 Temperature in tank rose to 400 C, causing an increase in 
pressure that ruptured relief valve. 
 Pressurized gas escaped in a fountain from top of vent 
stack and continued to escape until 2:30 am. 
23</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>A few months later, leak at another UC plant created a toxic cloud 
that traveled to a shopping center.  
 Several people had to be given emergency treatment, but for two 
days, doctors and health officials did not know what toxic 
chemical was or where it came from because UC denied leaks 
existence. 
 OSHA fined UC $1.4 million after Institute accident charging 
constant, willful, and overt violations at the plant and a general 
atmosphere and attitude that a few accidents here and there are 
the price of production. 
45</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Investigating/Understanding Accidents 
 What are ALL the factors involved? 
 Are there tools to help us find all the factors? 
 How do we minimize hindsight bias? 
 How do we learn from accidents in order to prevent them 
in the future? 
 
12</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Uberlingen continued 
 A year prior there was a near miss due to conflicting 
TCAS and ATC commands 
 Two Japanese airliners 
 One pilot made evasive maneuvers based on visual 
judgement. 
 Aircraft came within 300 ft 
 Evasive maneuvers caused ~100 injuries 
 Japan called for changes, but ICAO did not take action 
until after Uberlingen 
 Four other near misses in Europe before Uberlingen 
collision 
7</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Do Operators Really Cause Most 
Accidents? 
 When say human error, usually mean operator error 
 Operator error vs. design error  
 Hindsight bias 
 
56</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Tanker crews relied on the Coast Guard to plot their position 
continually. 
 Coast Guard operating manual required this. 
 Practice of tracking ships all the way out to Bligh reef had been 
discontinued. 
 Tanker crews were never informed of the change. 
 Spill response teams and equipment were not readily available. 
Seriously impaired attempts to contain and recover the spilled oil. 
Summary: 
 Safeguards designed to avoid and mitigate effects of an oil spill 
were not in place or were not operational 
 By focusing exclusively on blame, the opportunity to learn from 
mistakes is lost 
Postscript:   
    Captain Hazelwood was tried for being drunk the night the Exxon 
Valdez went aground. He was found not guilty 
 
55</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Night shift came on duty at 11 pm.  
 Pressure gauge indicated pressure was rising (10 psi 
instead of recommended 2 to 3 psi). But at upper end of 
normal range. 
 Temperature in tank about 20 C.  
 Both instruments were ignored because believed to be 
inaccurate. Operators told instead to use eye irritation as 
first sign of exposure. 
 11:30 pm: detected leak of liquid from an overhead line 
after some workers noticed slight eye irritation. 
 Leaky valves were common and were not considered 
significant 
22</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Prior warnings and events presaging the accident were 
ignored: 
 6 serious incidents between 1981 and 1984, several of which 
involved MIC 
 One worker killed in 1981, but official inquiries required by law 
were shelved or tended to minimize governments or companys 
role. 
 A leak similar to one involved in the big one had occurred the 
year before. 
 Journalists and others tried to warn of dangers 
 At least one person within government tried to bring up hazards 
of plant. He was forced to resign. 
 Local authorities and plant managers did nothing in response. 
 
43</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Exxon Valdez 
 Shortly after midnight, March 24, 1989, tanker Exxon Valdez ran 
aground on Bligh Reef (Alaska)  
 11 million gallons of crude oil released 
 Over 1500 miles of shoreline polluted 
 Exxon and government put responsibility on tanker Captain 
Hazelwood, who was disciplined and fired 
 Was he to blame? 
 State- of-the-art iceberg monitoring equipment promised by oil 
industry, but never installed. Exxon Valdez traveling outside normal 
sea lane in order to avoid icebergs thought to be in area 
 Radar station in city of Valdez, which was responsible for monitoring 
the location of tanker traffic in Prince William Sound, had replaced its 
radar with much less powerful equipment. Location of tankers near 
Bligh reef could not be monitored with this equipment. 
 
53</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>The effects of the legal system on accident investigations and 
exchange of information, 
 The certification of critical workers, 
 Political considerations 
 Resources 
 Public sentiment 
And many other non-technical but vital influences on the  
attainment of an acceptable level of risk control. These non- 
technical aspects of system safety cannot be ignored. 
49</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Communication Links Actually in Place 
6
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Technology. Used with permission.</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Do you see any additional factors you 
did not note before? 
 
Are these factors unique to the  
Bhopal accident? 
 
 
46</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Pipe-washing operation should have been supervised 
by second shift operator, but that position had been 
eliminated due to cost cutting. 
 Tank 610 contained 40 to 50 tons of MIC out of total 
capacity of 60 tons, which violated safety 
requirements. 
 Tanks were not to be more than half filled 
 Spare tank was to be available to take excess 
 Adjacent tank thought to contain 15 tons according to 
shipping records, but contained nearer to 21 tons 
 Spare tank (619) contained less than 1 ton, but level 
gauge showed it was 20 percent full 
 Many of gauges not working properly or were 
improperly set.  
33</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>No information given to public about protective measures 
in case of an emergency or other info on hazards. 
 If had known to stay home, close their eyes, and breathe 
through a wet cloth, deaths could have been prevented. 
 Army eventually came and tried to help by transporting 
people out of area and to medical facilities.  
 This help was delayed because nobody at plant notified 
authorities about the release 
 Weather and wind contributed to consequences. 
 Because happened in middle of night, most people asleep 
and it was difficult to see what was happening. 
27</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Uberlingen Mid-Air Collision 
 
    https://www.youtube.com/watch?v=CHjqun9c4Pc
 
 (khchung787) 
2</text>
        </slide>
        <slide>
          <slideno>63</slideno>
          <text>Overcoming Hindsight Bias 
 Assume nobody comes to work to do a bad job. 
 Assume were doing reasonable things given the complexities, 
dilemmas, tradeoffs, and uncertainty surrounding them. 
 Simply finding and highlighting peoples mistakes explains 
nothing.  
 Saying what did not do or what should have done does not 
explain why they did what they did. 
 
 
64</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Traditional HA, Lecture on Quantification (PDF - 5.6MB)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes4/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>48</slideno>
          <text>Fault tree examples
Example from original 1961 Bell Labs study
Part of an actual TCAS fault tree (MITRE, 1983)Gas valve stays open
Missing:
Conflict alert 
displayed, but 
never observed 
by controller
49
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>90</slideno>
          <text>Risk Matrix
Based on common idea:
Risk = Severity * LikelihoodLikelihoodVery LikelyLow Med Medium Med Hi High High
LikelyLow Low Med Medium Med Hi High
PossibleLow Low Med Medium Med Hi Med Hi
UnlikelyLow Low Med Low Med Medium Med Hi
RareLow Low Low Med Medium Medium
Negligible Minor Moderate Significant Severe
SeverityUses expected 
values (averages)
91</text>
        </slide>
        <slide>
          <slideno>67</slideno>
          <text>HAZOP
Hazard and Operability Analysis
68</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>FMEA: A Forward
Search Technique
This figure is in the public domain.
8</text>
        </slide>
        <slide>
          <slideno>69</slideno>
          <text>HAZOP
Guidewords applied to 
variables of interest
E.g. flow, temperature, pressure, tank 
levels, etc.
Team considers potential 
causes and effects
Questions generated from guidewords
Could there be no flow?
If so, how?
How will operators know there is no flow?
Are consequences hazardous or cause inefficiency?
HAZOP: Generate the right questions,
not just fill in a tree
70Image removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>117</slideno>
          <text>Human Error Example
118
This image is in the public domain.</text>
        </slide>
        <slide>
          <slideno>71</slideno>
          <text>HAZOP Strengths
Easy to apply
A simple method that can uncover complex 
accidents
Applicable to new designs and new design 
features
Performed by diverse study team , facilitator
Method defines team composition, roles
Encourages cross -fertilization of different 
disciplines
72</text>
        </slide>
        <slide>
          <slideno>104</slideno>
          <text>Fukushima Diesel Generators
105</text>
        </slide>
        <slide>
          <slideno>105</slideno>
          <text>Quantitative results are affected by the 
way barriers are chosen
Barrier 1a
Initial conditions keep aircraft &gt; 10NM apart
P(success) = 0.99
Barrier 1b
Initial conditions keep aircraft &gt; 5NM apart
P(success) = 0.99
Barrier 1c
Initial conditions keep aircraft &gt; 1NM apart
P(success) = 0.99
Barrier 2
Flight crew detects traffic by means other than visual, avoid NMAC
P(success) = 0.90
Barrier 3
Flight crew detects traffic by visual acquisition, avoid NMAC
P(success) = 0.80
RTCA DO -312
106 RTCA Inc. All rights reserved. This content is excluded
from our Creative Commons license. For more information,see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>94</slideno>
          <text>Hazard Level Assessment
Combination of Severity and Likelihood
Difficult for complex, human/computer controlled 
systems
Challenging to determine likelihood for these 
systems
Software behaves exactly the same way every time
Not random
Humans adapt, and can change behavior over time
Adaptation is not random
Different humans behave differently
Not I.I.D (independent and identically distributed)
Modern systems almost always involve new designs and 
new technology
Historical data may be irrelevant
Severity is usually adequate to determine effort to spend 
on eliminating or mitigating hazard.High
Med Hi
Medium
Low Med
LowHazard Level or
Risk Level:
95</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>FTA
Fault Tree Analysis
36</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>General FMEA Process
1.Identify individual components
2.Identify failure modes
3.Identify failure mechanisms (causes)
4.Identify failure effects
17</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>http://www.lean.ohio.gov/Portals/0/docs/trai
ning/GreenBelt/GB_Fishbone%20Diagram.pdf
Breaking the 
accident chain of 
events (see 
video) LeanOhio, Ohio Department of Administrative Services. All rights reserved. This content is excluded
from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.
11</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Safe  Reliable
Safety often means making sure X never happens
Reliability usually means making sure Y always 
happens
Safe Unsafe
Reliable Typical commercial flight
Unreliable Aircraft engine fails in flight
32</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Thrust reversers
1991 Accident
B767 in Thailand
Lauda Air Flight 004
Thrust reversers deployed in flight, caused 
in-flight breakup and killing all 223 people. 
Deadliest aviation accident involving B767
Simulator flights at Gatwick Airport which 
appeared to show that deployment of a 
thrust reverser was a survivable incident.
Boeing had insisted that a deployment was 
not possible in flight. In 1982 Boeing 
established a test where the aircraft was 
slowed to 250 knots, and the test pilots then 
used the thrust reverser. The control of the 
aircraft had not been jeopardized. The FAA 
accepted the results of the test.
Recovery from the loss of lift from the 
reverser deployment "was uncontrollable for 
an unexpecting flight crew. The incident led 
Boeing to modify the thrust reverser system 
to prevent similar occurrences by adding 
sync -locks, which prevent the thrust 
reversers from deploying when the main 
landing gear truck tilt angle is not at the 
ground position.
43 Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Toyota
2004: Push -button ignition
2004 -2009
102 incidents of uncontrolled acceleration
Speeds exceed 100 mph despite stomping on the 
brake 
30 crashes
20 injuries
2009 , Aug :
Car accelerates to 120 mph
Passenger calls 911, reports stuck accelerator
Car crashes killing 4 people
Driver was offensive driving instructor for police
Today
Software fixes for pushbutton ignition, pedals
http://www.reuters.com/article/2010/07/14/us -toyota -idUSTRE66D0FR20100714
http://www.statesman.com/business/u -s-toyota -cite-driver -error -in-many -803504.htmlAll component requirements were met
Yet system behavior was unexpected, unsafe !
55</text>
        </slide>
        <slide>
          <slideno>88</slideno>
          <text>Reversal Example
Using Ratio Scale:
Event A
Likelihood = 20%
Severity = 0
Event B
Likelihood = 10%
Severity = 1
Event C
Likelihood = 3%
Severity = 7Ordinal
4
3
2
1Ratio
4
3
2
1
056785
C
Risk = 0.00
B
Risk = 0.10
Risk =  0.21 
A
89</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>FMEA Exercise
Automotive brakes
System components
Brake pedal
Brake lines
Rubber seals
Master cylinder
Brake pads
Rubber seals
FMEA worksheet columns
Component
Failure mode
Failure mechanism
Failure effect (local)
Failure effect (system)
Rubber Seals
21
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Hazard (Causal) Analysis
Investigating an accident before it happens
Goal is to identify causes of accidents (before they occur) so 
can eliminate or control them in
Design
Operations
Requires
A system design model
An accident model(even if only in the mind 
of the analyst )
3</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>FTA Limitations
Independence between 
events is often assumed
Common -cause failures 
not always obvious
Difficult to capture non-
discrete events
E.g. rate -dependent events, 
continuous variable changes
Doesnt easily capture 
systemic factors
47</text>
        </slide>
        <slide>
          <slideno>127</slideno>
          <text>Ford Pinto
Ford noticed design flaw too late to eliminate
Fuel tank directly behind axle
Rear -end collision can cause disaster
Engineers developed a patch
$11 per car, reinforced structure
Cost -benefit analysis
Total cost to fix: $137.5 million
Human life is worth $200,000
180 expected burn deaths
Serious human injury is worth $67,000
180 expected serious burn injuries
Burned out vehicle is worth $700
2,100 expected burned out vehicles
Total cost if not fixed: $49 million
One lawsuit ruling (1972):
-Ford to pay $2.5 million compensatory damages
-Ford to pay $3.5 million because Ford was aware of design defects before production but did    
not fix the design
128</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Event Tree Analysis: Process
1.Identify initiating 
event
2.Identify barriers
3.Create tree
4.Identify outcomes
12
3
4
59</text>
        </slide>
        <slide>
          <slideno>120</slideno>
          <text>UH-60MU SAR Hazard Classification
UH-60MU SAR marginal hazards
Loss of altitude indication in DVE
Loss of heading indication in DVE
Loss of airspeed indication in DVE
Loss of aircraft health information
Loss of external communications
Loss of internal communicationsSTPA Unsafe Control Action
The Flight Crew does not provide collective 
control input necessary for level flight, resulting 
in controlled flight into terrain
Scenario 1: The Flight Crew has a flawed process 
model and believes they are providing sufficient 
control input to maintain level flight.  This flawed 
process model could result from:
a)The altitude indicator and attitude indicator are 
malfunctioning during IFR flight and the pilots are 
unable to maintain level flight
b)The Flight Crew believes the aircraft is trimmed 
in level flight when it is not
c)The Flight Crew has excessive workload due to 
other tasks and cannot control the aircraft
d)The Flight Crew has degraded visual conditions 
and cannot perceive slow rates of descent that 
result in a continuous descent
e)The Flight Crew does not perceive rising terrain 
and trims the aircraft for level flight that results in 
controlled flight into terrainUH-60MU SAR identifies various hazards as 
marginal that actually could lead to a 
catastrophic accident
121
This content is in the public domain.</text>
        </slide>
        <slide>
          <slideno>106</slideno>
          <text>Quantitative FTA
107</text>
        </slide>
        <slide>
          <slideno>83</slideno>
          <text>Aviation Severity Levels
Level 1: Catastrophic
Failure may cause crash.
Failure conditions prevent continued safe flight and landing
Level 2: Severe
Failure has negative impact on safety, may cause serious or fatal 
injuries
Large reduction in functional capabilities
Level 3: Major
Failure is significant, but less impact than severe
Significant reduction in functional capabilities
Level 4: Minor
Failure is noticeable, but less impact than Major
Slight reduction in safety margins; more workload or inconvenience
Level 5: No effect on safety
From ARP4671, DO -178B
84</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Toyota Unintended Acceleration
2004 -2009: 102 incidents
53</text>
        </slide>
        <slide>
          <slideno>101</slideno>
          <text>FMECA Exercise: Actual automotive brakes
Brake
PedalBrake fluid
FMEA worksheet columns
Component
Failure mode
Failure mechanism
Failure effect (local)
Failure effect (system)
Criticality (Severity)Severity Levels
1. No effect
2. Minor, not noticed by average 
customer
3. Major, loss of primary function
4. Catastrophic, injury/death
102
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>A real accident: Toyotas unintended 
acceleration
2004 -2009
102 incidents of stuck accelerators
Speeds exceed 100 mph despite stomping on the brake 
30 crashes
20 injuries
2009, Aug :
Car accelerates to 120 mph
Passenger calls 911, reports stuck accelerator
Some witnesses report red glow / fire behind wheels
Car crashes killing 4 people
2010, Jul:
Investigated over 2,000 cases of unintended 
acceleration
Captured by FMEA?
24</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Failure Mode and Effect Analysis
Program:_________                                     System:_________                                 Facility:________
Engineer:_________ Date:___________                                  Sheet:_________
Component Name Failure Modes Failure 
MechanismsFailure effects 
(local)Failure effects
(system)
Main Hoist Motor Inoperative, does 
not moveDefective bearings
Loss of power
BrokenspringsMainhoist cannot 
be raised. Brake 
will hold hoist
stationaryLoad held 
stationary, cannot 
be raised or 
lowered.FMEA uses an accident model
*FMEA example adapted from ( Vincoli , 2006)Defective 
bearingsCauses Inoperative 
hoist motorCauses Main hoist 
frozenCauses Main load held 
stationaryFMEA method:
Accident model:Accident model: Chain -of-events
20
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Safe  Reliable
Safety often means making sure X never happens
Reliability usually means making sure Y always 
happens
Safe Unsafe
Reliable Typical commercial flight Computer reliably executes unsafe 
commands
Increasing tank burst pressure
A nail gun without safety lockout
Unreliable Aircraft engine wont start 
on ground
Missile wont fireAircraft engine fails in flight
33</text>
        </slide>
        <slide>
          <slideno>74</slideno>
          <text>Quantitative Hazard Analysis
75</text>
        </slide>
        <slide>
          <slideno>60</slideno>
          <text>Event Trees
vs.
Fault Trees
Event Tree
-Shows what failed, but not how. 
-Shows order of events
Fault Tree
-Complex, but shows how failure occurred 
-Does not show order of events
61</text>
        </slide>
        <slide>
          <slideno>96</slideno>
          <text>FMECA
Same as FMEA, but with criticality 
information
Criticality
Can be ordinal severity values
Can be likelihood probabilities
An expression of concern over the effects of failure 
in the system*
*Vincoli , 2006, Basic Guide to System Safety
97</text>
        </slide>
        <slide>
          <slideno>99</slideno>
          <text>Severity Level Examples
Rating Severity of Effect
10 Safety issue and/or non -compliance with government regulation without warning.
9 Safety issue and/or non -compliance with government regulation with warning.
8 Loss of primary function.
7 Reduction of primary function.
6 Loss of comfort/convenience function.
5 Reduction of comfort/convenience function.
4 Returnable appearance and/or noise issue noticed by most customers.
3 Non -returnable appearance and/or noise issue noticed by customers.
2 Non -returnable appearance and/or noise issue rarely noticed by customers.
1 No discernable effect.
*http://www.harpcosystems.com/Design -FMEA -Ratings -PartI.htm
100 Harpco Systems. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>75</slideno>
          <text>Agenda
Traditional hazard analysis
Qualitative techniques
Failure Modes and Effects Analysis
Fault Tree Analysis
Event Tree Analysis
HAZOP
Quantitative techniques
FMECA
Quant. Fault Tree Analysis
Quant. ETA
Traditional hazard analysis

76</text>
        </slide>
        <slide>
          <slideno>111</slideno>
          <text>Quantitative Fault Tree Analysis
Where do the probabilities come from?
Historical data
Simulations
Expert judgment
Are there any issues 
using these sources?
*Actual qualitative -quantitative conversion from RTCA DO -312
112 RTCA Inc. All rights reserved. This content is excluded
from our Creative Commons license. For more information,see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>116</slideno>
          <text>Software Example
117
This image is in the public domain.</text>
        </slide>
        <slide>
          <slideno>79</slideno>
          <text>Risk
Common idea:
Some combination of severity and likelihood
How would you combine severity and 
likelihood mathematically?
Risk = f(Severity, Likelihood)
What is f ?
80</text>
        </slide>
        <slide>
          <slideno>97</slideno>
          <text>Failure Mode and Effect Analysis
Program:_________                                          System:_________                            Facility:________
Engineer:_________ Date:___________                            Sheet:_________
Component 
NameFailure Modes Failure 
MechanismsFailure effects 
(local)Failure effects
(system)Criticality 
Level
Main hoist 
motorInoperative, 
does not moveDefective 
bearings
Loss of power
BrokenspringsMainhoist 
cannot be 
raised. Brake 
will hold hoist
stationaryLoad held 
stationary, 
cannot be 
raised or 
lowered.(5) High, 
customers 
dissatisfiedFMEA worksheet
*FMEA example adapted from ( Vincoli , 2006)Bridge crane system
98
 Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>95</slideno>
          <text>FMECA
Failure Modes Effects and Criticality Analysis
96</text>
        </slide>
        <slide>
          <slideno>62</slideno>
          <text>Event Tree Analysis: Exercise
Elevator
1.Identify initiating event
Cable breaks
2.List Barriers
3.Create Tree
4.Identify outcomes
Image from official U.S. Dept of Labor, Mine Safety and Health Administration paper:
http://www.msha.gov/S&amp;HINFO/TECHRPT/HOIST/PAPER4.HTM
63This image is in the public domain.</text>
        </slide>
        <slide>
          <slideno>76</slideno>
          <text>Quantitative analysis
How do you include numbers and math?
What do you quantify?
Tends to focus on two parameters
Severity
Probability
77</text>
        </slide>
        <slide>
          <slideno>108</slideno>
          <text>Quantitative Fault Tree Analysis
If we can assign probabilities to lowest 
boxes
Can propagate up using probability theory
Can get overall total probability of hazard!
AND gate
P(A and B) = P(A) * P(B)
OR gate
P(A or B) = P(A) + P(B)Only if events A,B are 
independent!
109</text>
        </slide>
        <slide>
          <slideno>107</slideno>
          <text>Quantitative Fault Tree Analysis
If we can assign probabilities to lowest 
boxes
Can propagate up using probability theory
Can get overall total probability of hazard!
AND gate
P(A and B) = P(A) * P(B)
OR gate
P(A or B) = P(A) + P(B)
Any assumptions being made?
108</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Safety vs. Reliability
Unsafe Unreliable 
scenarios scenarios
31</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Safety vs. reliability
Reliability  Failures
Safety  IncidentsComponent 
property
System 
property
28
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>66</slideno>
          <text>Event Tree Analysis: Limitations (cont)
Can be difficult to define functions across top of 
event tree and their order
Requires ability to define set of initiating events that 
will produce all important accident sequences
Most applicable to systems where:
All risk is associated with one hazard
(e.g. overheating of fuel)
Designs are fairly standard, very little change over time
Large reliance on protection and shutdown systems
67</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Forward vs. Backward Search
 Copyright Nancy Leveson, Aug. 2006
9</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Failure Mode and Effect Analysis
Program:_________                                     System:_________                                Facility:________
Engineer:_________ Date:___________                                 Sheet:_________
Component Name Failure Modes Failure Mechanisms Failure effects 
(local)Failure effects
(system)
Main hoist motor Inoperative, 
does not moveDefective bearings
Motor brushes worn
BrokenspringsMainhoist cannot 
be raised. Brake 
will hold hoist
stationaryLoad held 
stationary, cannot 
be raised or 
lowered.FMEA worksheet
*FMEA example adapted from ( Vincoli , 2006)Example: Bridge crane system
Courtesy of John Thomas. Used with permission.
18 Wiley.  All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>118</slideno>
          <text>No. Task Hazard Risk Risk Reduction Final Risk
1 Position 3 Tasks, Install ECS Falling object crushing person or body part Yellow Investigate process improvements Green
2 Site Acceptance
Test/Qualification Testing
Passerby unauthorized entryPerson entering cell exposed to significant risks from robot,
etc,yellow IML workstand gates to stop process when entered; Interlocked gates at Brand
Scaffolding; access control, signageGreen
3 Robots Crossing Ped aisle
in &amp; out of replenishment
cell (K)AGV/moblie equipment
impacts personYellow AGV control system, signage, crossing markings on
pedestrian aisle,Green
4 Light Curtain Alternatives
AnalysisExposure to impact,
crushing, etc. when safety scanners are deactivated when OML's "leapfrog"Yellow Establish safe procedure, use
of spotters, hand guidingGreen
5 All Sub -processes
All Users
normal operationExposure to movement of
robots, motors and cylinders.Red Safety perimeter, category 4, that stops automation when
violated; investigate use of Kuka.safesolutions, e -stop control, access control, procedures, 
trainingGreen
6 normal operation mechanical: Drill penetration of fuselage 
Operator exposes body part
to drill penetrationYellow Only one operator in
workspace, proper trainingGreen
7 AFB movement systems AGV trapping person against immovable object or running
someone overYellow AGV safety system with scanners Green
8 Traffic management mechanical : Impact,
pinching, crushing
Exposure to impact,
pinching, crushing by AGV, OML's, etcyellow AGV's equipped with safety Laser scanners with 360 degrees coverage, hand guiding, use of  
spotters, proceduresGreen
9 Maintenance activities ingress / egress : Exposure
to being hit by robot
performing maintenance
Maintenance person exposed
while working on machineryYellow Lock out auto to enter, lock out other sources as required
10 AGVs &amp; Movement Systems mechanical : Collision -impact two robots same side of
barrier
AGV impacts personYellow AGV safeguarding using SICK area scanners 360 degree coverage to stop AGV
if violated; people will be clear of cell(another line); walls (Anacortes) or ?light curtains? to 
stop motion if violated; Training and Amin
proceduresGreen
119</text>
        </slide>
        <slide>
          <slideno>109</slideno>
          <text>Quantitative Fault Tree Analysis
If we can assign probabilities to lowest 
boxes
Can propagate up using probability theory
Can get overall total probability of hazard!
AND gate
P(A and B) = P(A) * P(B)
OR gate
P(A or B) = P(A) + P(B)
Is independence a good assumption?
Hardware?
Software?
Humans?
110</text>
        </slide>
        <slide>
          <slideno>81</slideno>
          <text>Risk Matrix
Based on common quantification:
Risk = Severity * LikelihoodLikelihoodVery LikelyLow Med Medium Med Hi High High
LikelyLow Low Med Medium Med Hi High
PossibleLow Low Med Medium Med Hi Med Hi
UnlikelyLow Low Med Low Med Medium Med Hi
RareLow Low Low Med Medium Medium
Negligible Minor Moderate Significant Severe
Severity
82</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Safety vs. Reliability
FMEA is a reliability technique
Explains the inefficiency
FMEA sometimes used to identify unsafe outcomesUnsafe Unreliable 
scenarios scenarios
FMEA can 
only 
identify FMEA identifies these 
these safe scenarios too
unsafe 
scenarios
34
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Vesely FTA Handbook
Considered by many to be the textbook 
definition of fault trees
50</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Fault tree examples
Example from original 1961 Bell Labs study
Part of an actual TCAS fault tree (MITRE, 1983)
39
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Warsaw
Warsaw
Crosswind landing (one 
wheel first)
Wheels hydroplaned
Thrust reverser would not 
deploy
Pilots could not override and 
manually deploy
Thrust reverser logic
Must be 6.3 tons on each 
main landing gear strut
Wheel must be spinning at 
least 72 knots
45
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>115</slideno>
          <text>Hardware Example
116
This image is in the public domain.</text>
        </slide>
        <slide>
          <slideno>68</slideno>
          <text>HAZOP: Hazards and Operability Analysis
Developed by Imperial 
Chemical Industries in early 
1960s
Not only for safety, but 
efficient operations
Accident model:
Chain of failure events (that 
involve deviations from 
design/operating intentions)
69An image of a chemical plant is removed
due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>A simpler example
Safe or unsafe?
*Image: bluecashewkitchen.com
29</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Event Tree Analysis
57</text>
        </slide>
        <slide>
          <slideno>73</slideno>
          <text>Summary
Well -established methods
Time -tested, work well for the problems they were 
designed to solve
Strengths include
Ease of use
Graphical representation
Ability to analyze many failures and failure combinations
Application to well -understood mechanical or physical systems
Limitations include
Inability to consider accidents without failures
Difficulty incorporating systemic factors like managerial 
pressures, complex human behavior, and design/requirements 
flaws
Other methods may be better suited to deal with the 
challenges introduced with complex systems
74</text>
        </slide>
        <slide>
          <slideno>128</slideno>
          <text>Ford Pinto
Cost of human life was based on National Highway Traffic 
Safety Administration regulations
$200,725 per life
Fuel tank location was commonplace at that time in 
American cars
California supreme court had tolerated and encouraged 
manufacturers to trade off safety for cost
NHTSA recorded 27 Pinto rear -impact fires
Lower than average for compact cars at the time
129</text>
        </slide>
        <slide>
          <slideno>125</slideno>
          <text>Cost Benefit Analysis
126</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Top-Down Example
Image from VeselyThis image is in the public domain.
14</text>
        </slide>
        <slide>
          <slideno>61</slideno>
          <text>ETA uses an accident model
Pressure 
too highRelief valve 
1 failsRelief valve 
2 failsExplosionEvent Tree:
Accident model:Accident model: Chain -of-events
62</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>FTA Process
1.Definitions
Define top event
Define initial 
state/conditions
2.Fault tree construction
3.Identify cut-sets and 
minimal cut -sets
Vesely
38</text>
        </slide>
        <slide>
          <slideno>123</slideno>
          <text>Controlling Heuristic Biases
Cannot eliminate completely but can reduce
Use structured method for assessing and managing risk
Following a structured process and rules to follow can diminish power 
of biases and encourage more thorough search
Concentrate on causal mechanisms vs. likelihood
Require action or procedures (to avoid defensive avoidance)
Use worst case analysis (vs. design basis 
accident)
Prove unsafe rather than safe 
Hazard analysis vs. safety case
124</text>
        </slide>
        <slide>
          <slideno>132</slideno>
          <text>Boeing 787 Lithium Battery Fires
A module monitors for 
smoke in the battery bay, 
controls fans and ducts to 
exhaust smoke overboard.
Power unit experienced 
low battery voltage, shut 
down various electronics 
including ventilation.
Smoke could not be 
redirected outside cabin
All software requirements were satisfied!
The requirements were inadequate
133
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Failure discussion
Component Failure
Vs.
Design problem
Vs.
Requirements problem
25</text>
        </slide>
        <slide>
          <slideno>86</slideno>
          <text>Reversal Example
Event A
Likelihood = 20%
Event B
Likelihood = 10%
Event C
Likelihood = 3%
Calculate riskOrdinal
4
3
2
1Ratio
4
3
2
1
056785
A
B
C
87</text>
        </slide>
        <slide>
          <slideno>89</slideno>
          <text>Reversal ExampleOrdinal
4
3
2
1Ratio
4
3
2
1
056785
C
B
ARisk (using
ordinal scale)Risk (using
ratio scale)
Event A 0.20 0.00
Event B 0.20 0.10
Event C 0.12 0.21
90</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Failure -based methods
Tend to treat safety as a component property
Use divide -and-conquer strategies
Reductionism
Reasonable?
51</text>
        </slide>
        <slide>
          <slideno>98</slideno>
          <text>Severity Level Examples
Rating Meaning
1No effect
2Very minor (only noticed by discriminating customers)
3Minor (affects very little of the system, noticed by average 
customer)
4Moderate (most customers are annoyed)
5High (causes a loss of primary function; customers are dissatisfied)
6Very high and hazardous (product becomes inoperative; customers 
angered; the failure may result unsafe operation and possible 
injury)
*Otto et al., 2001, Product Design
99 Pearson. All rights reserved. This content is excluded from our Creative Commons
license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>FMEA Exercise
Automotive brakes
System components
Brake pedal
Brake lines
Rubber seals
Master cylinder
Brake pads
Rubber seals
FMEA worksheet columns
Component
Failure mode
Failure mechanism
Failure effect (local)
Failure effect (system)
Rubber Seals
How would you make this system safe?
22
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>114</slideno>
          <text>Risk Assessment Matrix
115
This table is in the public domain.</text>
        </slide>
        <slide>
          <slideno>63</slideno>
          <text>Event Tree Analysis: Exercise
What are the 
barriers?
64
 HowStuffWorks. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>131</slideno>
          <text>Boeing
Boeing 787 LiCo Batteries
Prediction/Certification:
No fires within 107flight hours
Followed 4761 certification 
paradigm 
Actual experience:
Within 52,000 flight hours 2 such 
events
2.6 x 104flight hours [NTSB 2013]
[http://upload.wikimedia.org/wikipedia/commons/9/95/Boeing_Dreamliner_battery_original_and_damaged.jpg]
Cody Fleming, 2014
132These images are in the public domain.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Top-Down Search
13</text>
        </slide>
        <slide>
          <slideno>65</slideno>
          <text>Event Tree Analysis: Limitations
Not practical when chronology of events is not 
stable (e.g. when order of columns may change )
Difficult to analyze non-protection systems
Can become exceedingly complex and require 
simplification
Separate trees required for each initiating event
Difficult to represent interactions among events
Difficult to consider effects of multiple initiating 
events
66</text>
        </slide>
        <slide>
          <slideno>78</slideno>
          <text>Quantitative methods
Good assumptions?
-Hardware?
-Humans?
-Software?The quantification is 
usually based on 
probability theory and 
statistics
Common assumptions
Behavior is random
Each behavior independent
Identical distributions / EV
79An image of a pinball table removed due to copyright restrictions.</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>FTA Strengths
Captures combinations of failures
More efficient than FMEA
Analyzes only failures relevant to top -level event
Provides graphical format to help in 
understanding the system and the analysis
Analyst has to think about the system in great 
detail during tree construction
Finding minimum cut sets provides insight 
into weak points of complex systems
46
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>134</slideno>
          <text>A response
"In truth, a good case could be made that if 
your knowledge is meagre and unsatisfactory, 
the last thing in the world you should do is 
make measurements; the chance is negligible 
that you will measure the right things 
accidentally.
George Miller (a psychologist)
135</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Systems -Theoretic Approaches
Focus of next class
Need to identify and prevent failures , but also:
Go beyond the failures
Why werent the failures detected and mitigated ?
By operators
By engineers
Prevent issues that dont involve failures
Human -computer interaction issues
Software -induced operator error
Etc.
56
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>FMECA : A Forward
Search Technique
19</text>
        </slide>
        <slide>
          <slideno>92</slideno>
          <text>Another Example Hazard Level Matrix
93
 Addison-Wesley Professional. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>121</slideno>
          <text>Current State of the Art: PRA
Risk and Risk Assessment
Little data validating PRA or methods for calculating 
it
Other problems
May be significant divergence between modeled system 
and as -built and as -operated system
Interactions between social and technical part of system 
may invalidate technical assumptions underlying analysis
Effectiveness of mitigation measures may change over time
Why are likelihood estimates inaccurate in practice?
Important factors left out (operator error, flawed decision 
making, software) because dont have probability 
estimates
Non -stochastic factors involved in events
Heuristic biases
122</text>
        </slide>
        <slide>
          <slideno>110</slideno>
          <text>Quantitative Fault Tree Analysis
Actual fault trees from RTCA DO -312
111 RTCA Inc. All rights reserved. This content is excluded
from our Creative Commons license. For more information,see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>FTA Limitations (cont)
Difficult to capture delays and 
other temporal factors
Transitions between states or 
operational phases not 
represented
Can be labor intensive
In some cases, over 2,500 pages of 
fault trees
Can become very complex very 
quickly, can be difficult to review
48</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Event Tree Example
Small 
releaseNo accident
No release
Moderate 
release
No release
Major
release
60</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Failure Modes, Mechanisms, Effects
Examples and definitions of "Failure modes,
mechanisms, effects"
35</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Physical System Design Model 
(simplified)
Pressurized 
Metal TankValve control input Valve control inputWater 
SupplyDrain
4</text>
        </slide>
        <slide>
          <slideno>119</slideno>
          <text>Example Risk Assessment: 
Manufacturing Robot
N
o
.Task Hazard Risk Risk Reduction Final Risk
2Site Acceptance
Test/Qualification Testing
Passerby unauthorized entryPerson entering cell exposed to 
significant risks from robot,
etc,Yellow Access control, signage Green
3Robots Crossing Ped aisle
in &amp; out of replenishment cellMobile equipment
impacts personYellow AGV control system, signage, 
crossing markings on pedestrian 
aisle,Green
4Light Curtain Alternatives
AnalysisExposure to impact,
crushing, etc. when safety 
scanners are deactivated when 
OML's "leapfrog"Yellow Establish safe procedure, use
of spotters, hand guidingGreen
Position 3 Tasks, Install ECS Falling object crushing person 
or body partRed Investigate process 
improvementsGreen
120</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>FMEA: Failure Modes and Effects Analysis
1949: MIL -P-1629
Forward search 
technique
Initiating event : 
component failure
Goal : identify effect of 
each failure
Courtesy of John Thomas. Used with permission.
16</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Toyota Unintended Acceleration
2004: Push -button ignition
2004 -2009
102 incidents of uncontrolled acceleration
Speeds exceed 100 mph despite stomping on 
the brake 
30 crashes
20 injuries
Today
Software fixes for pushbutton ignition, pedals
http://www.reuters.com/article/2010/07/14/us -toyota -idUSTRE66D0FR20100714
http://www.statesman.com/business/u -s-toyota -cite-driver -error -in-many -803504.htmlPushbutton was reliable!
Software was reliable!
54</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Bottom -Up Search
12</text>
        </slide>
        <slide>
          <slideno>126</slideno>
          <text>Cost-benefit analysis
Goes beyond identifying risk
Is it worth fixing?
$
How much does it 
$ cost NOT to fix?
How much does it 
cost to fix?
127</text>
        </slide>
        <slide>
          <slideno>80</slideno>
          <text>Risk Matrix
Based on common quantification:
Risk = Severity * LikelihoodLikelihoodVery Likely
Likely
Possible
Unlikely
Rare
Negligible Minor Moderate Significant Severe
Severity
81</text>
        </slide>
        <slide>
          <slideno>124</slideno>
          <text>Misinterpreting Risk
Risk assessments can easily be misinterpreted:
125</text>
        </slide>
        <slide>
          <slideno>84</slideno>
          <text>Risk Matrix
Based on common quantification:
Risk = Severity * Likelihood
Aviation Severity Levels
Level 1: Catastrophic
Level 2: Severe
Level 3: Major
Level 4: Minor
Level 5: No effect on safetyHow to quantify?
85</text>
        </slide>
        <slide>
          <slideno>77</slideno>
          <text>Quantitative methods
The quantification is 
usually based on 
probability theory and 
statistics
Common assumptions
Behavior is random
Each behavior independent
Good assumptions?
78
 source unknown. All rights reserved. This content is
excluded from our Creative Commons license. For moreinformation, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Chain-of-events example
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Technology. Used with permission.
How do you find the chain of events before an accident?
5</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Input OutputForward search?
7</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Traditional Qualitative Methods
FMEA (Failure Modes and Effects 
Analysis)
15</text>
        </slide>
        <slide>
          <slideno>100</slideno>
          <text>Failure Mode and Effect Analysis
Program:_________                                          System:_________                            Facility:________
Engineer:_________ Date:___________                            Sheet:_________
Component 
NameFailure Modes Failure 
MechanismsFailure effects 
(local)Failure effects
(system)Probability of 
occurrence
Main hoist 
motorInoperative, 
does not moveDefective 
bearings
Loss of power
BrokenspringsMainhoist 
cannot be 
raised. Brake 
will hold hoist
stationaryLoad held 
stationary, 
cannot be 
raised or 
lowered.0.001per 
operational 
hourFMECA worksheet
*FMEA example adapted from ( Vincoli , 2006)Bridge crane system
Could also 
specify 
likelihood
101
 Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse/.</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Forward vs. Backward Search
 Copyright Nancy Leveson, Aug. 2006
6</text>
        </slide>
        <slide>
          <slideno>87</slideno>
          <text>Reversal Example
Using Ordinal Scale:
Event A
Likelihood = 20%
Severity = 1
Event B
Likelihood = 10%
Severity = 2
Event C
Likelihood = 3%
Severity = 4Ordinal
4
3
2
1Ratio
4
3
2
1
056785
C
Risk = 0.20
Risk = 0.20
Risk = 0.12 
B
A
88</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Fault tree symbols
From NUREG -0492 ( Vesely , 1981)
40This image is in the public domain.</text>
        </slide>
        <slide>
          <slideno>93</slideno>
          <text>Hazard Level :  A combination of severity (worst potential damage in 
case of an accident) and likelihood of occurrence of the hazard.
Risk: The hazard level combined with the likelihood of the hazard 
leading to an accident plus exposure (or duration) of the hazard.
Safeware p179.  Copyright Nancy LevesonRISK
HAZARD LEVEL
Hazard
severityLikelihood of
hazard occurringHazard
ExposureLikelihood of hazard
Leading to an accident
Safety : Freedom from accidents or losses.
94 Addison-Wesley Professional. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Safety vs. Reliability
Common assumption:
Safety = reliability
How to improve safety?
Make everything more 
reliable!
*Image from midas.comMaking car brakes safe
Make every component reliable
Include redundant components
Is this a good assumption?
27 source unknown. All rights reserved. This content is
excluded from our Creative Commons license. For moreinformation, see https://ocw.mit.edu/help/faq-fair-use/.
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>64</slideno>
          <text>Event Tree Analysis: Strengths
Handles ordering of events better than fault trees
Most practical when events can be ordered in 
time (chronology of events is stable) 
Most practical when events are independent of 
each other.
Designed for use with protection systems
(barriers)
65</text>
        </slide>
        <slide>
          <slideno>130</slideno>
          <text>General Motors
Systemic factors
Wrote service bulletin to fix key slot, but kept it 
private
Knew in 2001 that ignition switches did not meet 
specification
4-10 vs. 15 -25
Updated part in 2006
Kept old part number, confusion
Still didnt meet specification (10 -15 vs. 15 -25)
131</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>FTA uses an accident model
Relay spring 
failsCauses Relay contacts 
fail closedCauses Excessive 
current providedFault Tree:
Accident model:Accident model: Chain -of-failure -events
42
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>FTA: Fault Tree Analysis
Top-down search 
method
Top event: 
undesirable event
Goal is to identify 
causes of hazardous 
event
1961: Bell labs analysis of Minuteman missile 
system
Today one of the most popular hazard 
analysis techniques
37
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>102</slideno>
          <text>Quantitative ETA
103</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>52 Associated Press. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>FMEA Limitations
Component failure incidents only
Unsafe interactions? Design issues? Requirements issues?
Single component failures only
Multiple failure combinations not considered
Requires detailed system design
Limits how early analysis can be applied
Works best on hardware/mechanical components
Human operators? (Driver? Pilot?)
Software failure?
Organizational factors (management pressure? culture?)
Inefficient, analyzes unimportant + important failures
Can result in 1,000s of pages of worksheets
Tends to encourage redundancy
Often leads to inefficient solutions
Failure modes must already be known
Best for standard parts with few and well -known failure modes
26</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Event Tree Analysis
1967: Nuclear power 
stations
Forward search technique
Initiating event : component 
failure (e.g. pipe rupture)
Goal : Identify all possible 
outcomes
58</text>
        </slide>
        <slide>
          <slideno>122</slideno>
          <text>Heuristic Biases
Confirmation bias (tend to deny uncertainty and vulnerability)
People look for evidence that supports their hypothesis
Reject evidence that does not
Construct simple causal scenarios
If none comes to mind, assume impossible
Tend to identify simple, dramatic events rather than events that are 
chronic or cumulative
Incomplete search for causes
Once one cause identified and not compelling, then stop search
Defensive avoidance
Downgrade accuracy or dont take seriously
Avoid topic that is stressful or conflicts with other goals
123</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Safety is not a component property
Safety is an emergent property of the system
Depends on context and environment!
Individual components are not inherently safe or unsafe
30 source unknown. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.</text>
        </slide>
        <slide>
          <slideno>113</slideno>
          <text>Preliminary Hazard Analysis
114
 Wiley. All rights reserved. This content is excluded from our Creative Commons license. For more information, see https://ocw.mit.edu/fairuse/.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Agenda
Today
Intro to Hazard Analysis
Traditional Qualitative Methods
FMEA
FTA
ETA
HAZOP
Strengths / Limitations
Next: Traditional Quantitative Methods
FMECA
FTA
PRA
Strengths / Limitations
2</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Actual automotive brakes
FMEA heavily used in mechanical engineering
Tends to promote redundancy
Useful for physical/mechanical systems to identify 
single points of failure
Brake
PedalBrake fluid
23
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>112</slideno>
          <text>Risk Assessment and Preliminary Hazard 
Analysis (PHA)
113</text>
        </slide>
        <slide>
          <slideno>129</slideno>
          <text>General Motors
13 deaths, 130 reported incidents
Design flaws
Ignition switches easily switch to off position
Bumps, vehicle collision, heavy keychain, etc.
Keys have wide slot, increased torque
Airbags and other safety systems immediately disabled when key is off
Cost -benefit analysis
GM aware of problem for over a decade
Developed a fix, costs $0.57 per car
Recommended no further action because there was no acceptable 
business case
Tooling cost and piece price was too high
CEO response
That is very disturbing if true
This is not how GM does business
If there is a safety issue we take action. We do not look at the cost 
associated with it.
130</text>
        </slide>
        <slide>
          <slideno>85</slideno>
          <text>Numerical Scales
Severity is usually ordinal
Only guarantees ordering along increasing 
severity
Distance between levels not comparable
Ordinal multiplication can result in 
reversals
Multiplication assumes equal distance
and fixed 0
Assumes severity 4 is 2x worse than severity 2
A Med Hi result may actually be worse 
than High
Another challenge1234Ordinal
123456Interval
01234Ratio
86</text>
        </slide>
        <slide>
          <slideno>91</slideno>
          <text>Expected Value Fallacy
P-value Fallacy
Flaw of Averages
Jensens Law
Simpsons paradox
Beware when averages are used to simplify 
the problem!
Can make adverse decisions appear correct
92</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Traditional Hazard Analysis
1</text>
        </slide>
        <slide>
          <slideno>103</slideno>
          <text>Quantitative Event Tree Analysis
Quantify p(success) for each barrier
Limitations
P(success) may not be random
May not be independent
May depend on order of events and context
Ex: Fukushima
104</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Fault Tree cut -sets
Cut-set: combination of 
basic events (leaf nodes) 
sufficient to cause the top -
level event
Ex: (A and B and C)
Minimum cut -set: a cut -set 
that does not contain 
another cut -set
Ex: (A and B)
Ex: (A and C)
41
Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>133</slideno>
          <text>Lord Kelvin quote
I often say that when you can measure what 
you are speaking about, and express it in 
numbers, you know something about it; but 
when you cannot measure it, when you 
cannot express it in numbers, your knowledge 
is of a meagre and unsatisfactory kind; it may 
be the beginning of knowledge, but you have 
scarcely in your thoughts advanced to the 
state of Science , whatever the matter may be.
[PLA, vol. 1, "Electrical Units of Measurement", 
1883 -05-03]
134</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>5 Whys Example (A Backwards Analysis)
Problem: The Washington 
Monument is disintegrating .
Why is it disintegrating?   
Because we use harsh chemicals
Why do we use harsh chemicals?
To clean pigeon droppings off the monument
Why are there so many pigeons?
They eat spiders and there are a lot of spiders at 
monument
Why are there so many spiders?
They eat gnats and lots of gnats at monument
Why so many gnats? 
They are attracted to the lights at dusk 
Solution : 
Turn on the lights at a later time.
 Diliff. License: CC-BY-SA. This content is excluded from
our Creative Commons license. For more information,see https://ocw.mit.edu/help/faq-fair-use/.
 source unknown. All
rights reserved. This contentis excluded from our CreativeCommons license. For moreinformation, see https://ocw.mit.edu/help/faq-fair-use/.
10</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>FTA example
Aircraft reverse thrust
Engines
Engine reverse thrust panels
Computer
Open reverse thrust panels after 
touchdown
Fault handling: use 2/3 voting. (Open 
reverse thrust panels if 2/3 wheel weight 
sensors AND 2/3 wheel speed sensors 
indicate landing)
Wheel weight sensors (x3)
Wheel speed sensors (x3)
Create a fault tree for the top -level event:
Reverse thrusters dont operate on landing.
Image from: http://en.wikipedia.org/wiki/File:Klm_f100_ph -kle_arp.jpg
44Courtesy of John Thomas. Used with permission.</text>
        </slide>
        <slide>
          <slideno>82</slideno>
          <text>Automotive Severity Levels
Level 0: No injuries
Level 1: Light to moderate injuries
Level 2: Severe to life -threatening injuries 
(survival probable)
Level 3: Life -threatening to fatal injuries 
(survival uncertain)
From ISO26262
83</text>
        </slide>
        <slide>
          <slideno>72</slideno>
          <text>HAZOP Limitations
Requires detailed plant information
Flowsheets , piping and instrumentation diagrams, plant layout, 
etc.
Tends to result in protective devices rather than real design 
changes
Developed/intended for chemical industry
Labor -intensive
Significant time and effort due to search pattern
Relies very heavily on judgment of engineers
May leave out hazards caused by stable factors
Unusual to consider deviations for systemic factors
E.g. organizational, managerial factors, management systems, 
etc.
Difficult to apply to software
Human behavior reduces to compliance/deviation from 
procedures
Ignores why it made sense to do the wrong thing
73</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Accident Models, STAMP, Systems Theory (PDF - 1.8MB)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes2/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>16</slideno>
          <text>Accident with No Component Failure 
 Navy aircraft were ferrying missiles from one location to 
another. 
 One pilot executed a planned test by aiming at aircraft in 
front and firing a dummy missile.  
 Nobody involved knew that the software was designed to 
substitute a different missile if the one that was 
commanded to be fired was not in a good position.  
 In this case, there was an antenna between the dummy 
missile and the target so the software decided to fire a 
live missile located in a different (better) position instead. 
17</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Controls/Controllers Enforce Safety Constraints  
 Power must never be on when access door open 
 Two aircraft must not violate minimum separation 
 Aircraft must maintain sufficient lift to remain airborne 
 Public health system must prevent exposure of public to 
contaminated water and food products 
 Pressure in a deep water well must be controlled 
 Truck drivers must not drive when sleep deprived 
 
26</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Accident Causality 
Using STAMP 
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Technology. Used with permission.
38</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Controller 
Controlling emergent properties 
(e.g., enforcing safety constraints) 
Process Control Actions Feedback Individual component behavior 
Component interactions 
Process components interact in  
direct and indirect ways Air Traffic Control: 
      Safety 
      Throughput 
25</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Accident Causality Models 
 Underlie all our efforts to engineer for safety 
 Explain why accidents occur 
 Determine the way we prevent and investigate accidents 
 May not be aware you are using one, but you are 
 Imposes patterns on accidents 
    All models are wrong, some models are useful 
                                                  George Box 
3</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Summary 
 New levels of complexity, software, human factors do not 
fit into a reductionist, reliability-oriented world. 
 Trying to shoehorn new technology and new levels of 
complexity into old methods will not work 
 
Image s removed due to copyright restrictions.
19</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Controller 
Controlling emergent properties 
(e.g., enforcing safety constraints) 
Process Control Actions Feedback Individual component behavior 
Component interactions 
Process components interact in  
direct and indirect ways 
24</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Applying Systems Theory to Safety 
 Accidents involve a complex, dynamic process 
 Not simply chains of failure events 
 Arise in interactions among humans, machines and the 
environment 
 Treat safety as a dynamic control problem 
 Safety requires enforcing a set of constraints on system 
behavior  
 Accidents occur when interactions among system 
components violate those constraints 
 Safety becomes a control problem rather than just a 
reliability problem 
 
35</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Example 
Safety 
Control 
Structure 
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Techno logy. Used with permission.
27</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>The Domino Model in action 
Image removed due to copyright restrictions.
8</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Safety as a Dynamic Control Problem (2)
Events are the result of the inadequate control
Result from lack of enforcement of safety constraints 
in system design and operations
A change in emphasis:
prevent failures 
   
enforce safety constraints on system behavior 
37</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Types of Accidents 
 Component Failure Accidents 
 Single or multiple component failures 
 Usually assume random failure 
 Component Interaction Accidents 
 Arise in interactions among components 
 Related to interactive and dynamic complexity 
 Behavior can no longer be  
 Planned 
 Understood 
 Anticipated 
 Guarded against 
 Exacerbated by introduction of computers and software 
16</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Plan for Today 
 Accident Models  
 Introduction to Systems Thinking 
 STAMP: A new loss causality model 
2</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>STAMP: 
System-Theoretic Accident 
Model and Processes 
Based on Systems Theory  
(vs. Reliability Theory)  
34</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Cambridge University Press. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/ .
13</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Week 2 Class Notes 
1</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Domino Chain of events Model 
Event-based Cargo 
door fails Causes Floor 
collapses Causes Hydraulics 
fail Causes Airplane 
crashes DC-10: Image by MIT OpenCourseWare.
7</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Controlled Process    
 Process 
Model 
Control 
Actions Feedback The Role of Process Models in Control 
Accidents often occur when process 
model inconsistent with state of 
controlled process (SA) 
A better model for role of software and 
humans in accidents than random 
failure model 
Four types of unsafe control actions: 
Control commands required for safety 
are not given 
Unsafe ones are given 
Potentially safe commands given too 
early, too late 
Control stops too soon or applied too 
long Controller 
33 
(Leveson, 2003); (Leveson, 2011) Control 
Algorithm 
33</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Analytic Reduction 
 Divide system into distinct parts for analysis 
        Physical aspects  Separate physical components or functions  
              Behavior          Events over time 
 Examine parts separately and later combine analysis 
results 
 Assumes such separation does not distort phenomenon 
 Each component or subsystem operates independently 
 Analysis results not distorted when consider components 
separately 
 Components act the same when examined singly as when 
playing their part in the whole 
 Events not subject to feedback loops and non-linear interactions 
 
5</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Safety as a Dynamic Control Problem   
 Examples 
 O-ring did not control propellant gas release by sealing gap in field 
joint of Challenger Space Shuttle 
 Software did not adequately control descent speed of Mars Polar 
Lander  
 At Texas City, did not control the level of liquids in the ISOM tower;  
 In DWH, did not control the pressure in the well;  
 Financial system did not adequately control the use of financial 
instruments 
 
36</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Accident with No Component Failures 
 Mars Polar Lander 
 Have to slow down spacecraft to land safely 
 Use Martian gravity, parachute, descent engines 
(controlled by software) 
 Software knows landed because of sensitive sensors on 
landing legs. Cut off engines when determine have landed. 
 But noise (false signals) by sensors generated when 
parachute opens 
 Software not supposed to be operating at that time but 
software engineers decided to start early to even out load 
on processor 
 Software thought spacecraft had landed and shut down 
descent engines 
 
15</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Reason Swiss Cheese 
 Cambridge University Press. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/ .
12</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Variants of Domino Model 
 Bird and Loftus (1976)   
 Lack of control by management, permitting 
 Basic causes (personal and job factors) that lead to 
 Immediate causes (substandard practices/conditions/errors), which are 
the proximate cause of 
 An accident or incident, which results in 
 A loss.  
 Adams (1976)  
Management structure (objectives, organization, and operations)  
 Operational errors (management or supervisor behavior)  
 Tactical errors (caused by employee behavior and work conditions) 
 Accident or incident 
 Injury or damage to persons or property.  
11</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Chain-of-events example 
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Techno logy. Used with permission.
9</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Traditional Ways to Cope with Complexity 
1.Analytic Reduction 
2.Statistics 
 
 
4</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our
Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/ .
29</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Systems Theory (2) 
 Focuses on systems taken as a whole, not on parts 
taken separately 
 Emergent properties 
 Some properties can only be treated adequately in their 
entirety, taking into account all social and technical aspects 
     The whole is greater than the sum of the parts 
 These properties arise from relationships among the parts of 
the system  
       How they interact and fit together 
 
22</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Event Chain 
  E1: Worker washes pipes without inserting a slip blind. 
  E2: Water leaks into MIC tank 
  E3: Gauges do not work 
  E4: Operator does not open valve to relief tank 
  E3: Explosion occurs 
  E4: Relief valve opens 
  E5: Flare tower, vent scrubber, water curtain do not work 
  E5: MIC vented into air 
  E6: Wind carries MIC into populated area around plant. 
What was the root cause?  
10</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>evel 
re for 
Example High-L
Control StructuITP 
32</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Control Structure Diagram  Level 0 
 Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our
Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/ .
30</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Emergent properties 
(arise from complex interactions) 
Process 
Process components interact in  
direct and indirect ways 
Safety is an emergent property 
23</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Standard Approach to Safety
 Reductionist
 Divide system into componen
ts
 Assume accidents are caused by component failure
 Identify chains of directly related physical or logical component 
failures that can lead to a loss 
 Assume randomness in the failure events so can derive 
probabilities for a loss 
 Forms the basis for most safety engineering and reliability 
engineering analysis and design
Redundancy and barriers (to prevent failure propagation), 
  high component integrity and overdesign, fail-safe design,  .
6</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Systems Theory 
 Developed for systems that are 
 Too complex for complete analysis 
 Separation into (interacting) subsystems distorts the results 
 The most important properties are emergent 
 Too organized for statistics 
 Too much underlying structure that distorts the statistics 
 New technology and designs have no historical information 
 Developed for biology and engineering  
 First used on ICBM systems of 1950s/1960s  
21</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>But the world is too complex to look at the 
whole, we need analytic reduction 
 Right? 
20</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Control Structure Diagram  ISS Level 1 
 Japan Aerospace Exploration Agency. All rights reserved. This content is excluded from our
Creative Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/ .
31</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Courtesy of Qi D. Van Eikema Hommes. Used with permission.
28</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Analytic Reduction does not Handle 
 Component interaction accidents 
 Systemic factors (affecting all components and barriers) 
 Software and software requirements errors 
 Human behavior (in a non-superficial way) 
 System design errors 
 Indirect or non-linear interactions and complexity 
 Migration of systems toward greater risk over time (e.g., 
in search for greater efficiency and productivity) 
 
18</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Swiss Cheese Model Limitations 
Ignores common cause failures of defenses (systemic 
accident factors) 
Does not include migration to states of high risk 
Assumes accidents are random events coming together 
accidentally 
Assumes some (linear) causality or precedence in the 
cheese slices (and holes)   
Just a chain of events, no explanation of whyevents 
occurred
14</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Operations, Regulation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>21</slideno>
          <text>Safety Culture 
Safety culture is a subset of culture that reflects general 
attitude and approaches to safety and risk management 
Trying to change culture without changing environment 
in which it is embedded is doomed to failure 
Simply changing organizational structures may lower risk 
over short term, but superficial fixes that do not address 
the set of shared values and social norms are likely to be 
undone over time. 
 
22</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Major Ingredients of Effective  
Safety Management 
Commitment and leadership 
Corporate safety policy 
Risk awareness and communication channels 
Controls on system migration toward higher risk 
Strong corporate safety culture 
Safety control structure with appropriate assignment of 
responsibility, authority, and accountability 
Safety information system 
Continual improvement and training 
Education, training, and capability development 
19</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Communication and Risk Awareness 
In general, risk is unknowable (severity X likelihood) 
In absence of hard evidence, tends to be evaluated 
downward over time 
Delays between relaxation of controls and accidents 
Complacency results from inadequate feedback and 
process models 
Using STAMP, risk is defined as a function of 
effectiveness of controls to enforce safe behavior 
   Note this is potentially knowable  
Key is communication and feedback (reporting systems) 
 
37</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.
MIT Press,  Massachusetts Institute of Technology. Used with permission.
18</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Controls for Planned Changes 
Safety control structure must continue to be effective 
despite changes (including changes in environment, 
human behavior, organization) 
Most companies have management of change (MOC) 
procedures to evaluate impact on safety 
Cost will depend on quality of documentation and how 
original hazard analysis done 
MOC procedures are often skipped. Need to establish 
responsibility for enforcement 
4</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Other Operations Topics 
Education and training 
Operations Safety Management Plan 
Implications of STAMP for occupational (workplace) safety 
17</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Blame is the Enemy of Safety 
 My UK safety customers are incredibly spooked by [the 
Nimrod accident report] because of the way it singled out 
individuals in the safety assessment chain for criticism. It 
has made a very difficult process of assessing safety risk 
even more difficult.  
 
People stop reporting errors and problems 
Just Culture movement 
13</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Effective Safety Management Systems 
Process safety is integrated into the dominant culture, not a 
separate sub-culture 
Safety is integrated into line operations: a mixture of top-down 
re-engineering and bottom-up process improvement 
Individuals have required knowledge, skills, and ability 
Organization has clearly articulated safety vision, values and 
procedures, shared among stakeholders 
Tensions between safety priorities and other system priorities 
are addressed through a constructive, negotiated process. 
Key stakeholders (e.g., unions) have full partnership roles and 
responsibilities regarding system safety 
Passionate, effective leadership at all levels committed to 
safety as a high priority for the organization 
43</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Design Principles for Safety  
Control Structures 
Need clear definition of expectations, responsibilities, 
authority, and accountability at all levels of safety control 
structure 
See new book for list of responsibilities that need to be 
assigned. 
 
39</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Leadership is Key to Changing Culture (2) 
 Minimize blame (Just Culture)  
Blame is the enemy of safety 
Peer pressure can be effective 
Moratorium after DWH 
Customers have more power than government 
Engineer the incentive structure to encourage the 
behavior you want 
30</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Feedback Channels 
Information flow is key to maintaining safety 
 Probably not general leading indicators but can identify 
system specific ones using safety constraints. 
Also need to ensure feedback channels are operating 
effectively. Cultural problems can interfere with feedback 
Three general types: 
Audits and performance assessments 
Reporting systems 
Accident/incident causal analysis 
 
8</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Paul ONeill and Alcoa (2) 
Within a year of O'Neill's speech, Alcoa's profits hit a 
record high and continued that way until he retired in 
2000.  
All that growth occurred while Alcoa became one of the 
safest companies in the world. 
Understood that safety and productivity are not 
conflicting 
 
 
 
29</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Audits and Performance Assessments 
Starts from safety constraints and assumptions in safety 
design 
Need to audit entire safety control structure, not just 
lower levels 
Audit teams must be free of conflicts of interest 
Participatory and non-punitive audits 
 
9</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Safety Control Structure Design (2)  
Where should safety activities be put? 
Safety permeates every part of development and 
operations 
Need not be located in one place, but common methods and 
approach will strengthen the separate disciplines 
 If distributed, need a clear focus and coordinating body. Dont 
want fragmented, uncoordinated efforts. 
Basic Principles: 
1.System safety needs a direct link to decision makers and influence 
on decision-making (influence and prestige ) 
2.System safety needs to have independence  from project 
management (but not engineering)  
3.Direct communication channels are needed to most of the 
organization ( oversight and communication ) 
40</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Food Safety Example (2) 
Unexpected benefit is worker retention 
Sure, the money is important, but I also feel good 
because I am helping to improve quality and safety, Mr. 
Esteban said. Those things are important to my family, 
too. 
Products carry certification to inform consumers  
 
 
 
32</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.
MIT Press,  Massachusetts Institute of Technology. Used with permission.
1</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Controls for Unplanned Changes 
How might deal with unplanned and unsafe changes? 
1.Need to identify potential unsafe changes  
2.Need to respond (reduce risk)  
What is a leading indicator? 
 
 
6</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Culture of Denial Examples 
 Our accident rates are going down 
Look at worker injury rates: personal or occupational 
safety vs. system or process safety 
Choose statistics that give best result 
 Accidents are the price of productivity. A dangerous 
domain  
 Mines: Everyone has lots of safety violations  
26</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Corporate Safety Policy 
 Provides a clear shared vision of organizations safety 
goals and values and a way to achieve them. 
Two parts: 
Short and concise statement of  
Safety values of organization 
What is expected of employees with respect to safety 
Details about how policy will be implemented 
Needs to be followed 
Establish feedback channels  
Monitor improvements 
Identify, prioritize, and implement improvements 
34</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Example Operational Safety Philosophy (2) 
(Colonial Pipeline) 
Exposure to workplace hazards shall be minimized and/or 
safeguarded. 
We will empower and encourage all employees and contractors to 
stop, correct and report any unsafe condition. 
Each employee will be evaluated on his/her performance and 
contribution to our safety efforts. 
We will design, construct, operate and maintain facilities and 
pipelines with safety in mind. 
We believe preventing accidents is good business. 
36</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Controls for Unplanned Changes (2) 
Need to interrupt risk re-evaluation process before safety 
margins seriously eroded 
Requires an alerting function to person with responsibility 
Want to allow change as long as does not violate safety 
constraints 
Key is to allow flexibility in how safety goals achieved and 
provide information that allows accurate risk assessment 
by decision makers. 
 Dont allow waiving requirements. Re -evaluate them. 
Establish appropriate feedback loops 
 
7</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Using the Feedback 
Information must be presented in form that people can 
learn from, apply to daily jobs, and use through system 
life cycle. 
Precursors almost always exist before major accidents 
Use to update process models, change control 
algorithms, modify safety control structure, update 
training and education 
(Your operations plan for your project should include how 
feedback will be obtained and how it will be used) 
14</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Commitment and Leadership 
Management open and sincere concern for safety in 
everyday dealings 
Studies show management support for and participation 
in safety activities is most effective way to control and 
reduce accidents. 
Support shown by: 
Personal involvement 
Assigning capable people 
Providing resources 
Creating appropriate control structure 
Responding to initiatives by others 
20</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Safety Information System 
Second in importance only to management commitment 
Creating and maintaining a successful one requires a 
culture that values the sharing of knowledge learned from 
experience (learning culture) 
Important source for identifying leading indicators of 
potential safety problems and as feedback on hazard 
analysis process. 
Need communication channels for getting info to those who 
can understand it and to those making decisions.  
15</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Safety in Operations  
Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to Safety.
MIT Press,  Massachusetts Institute of Technology. Used with permission.
2</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Safety Information System: Contents 
Updated safety plan 
Status of activities 
Hazard analysis (HAZOP) results and hazard logs 
Tracking and status information on all known hazards 
Incident and accident tracking 
Reports  
Corrective Actions (status) 
Trend analysis 
Lessons learned 
16</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Controls on System Migration  
to Higher Risk 
Adaptation is predictable and potentially controllable 
Identify potential causes and institute controls 
Perform audits and performance assessments based on 
safety constraints identified during system development 
Anchor safety efforts beyond short-term program 
management pressures 
38</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Example Operational Safety Philosophy (1) 
(Colonial Pipeline) 
All injuries and accidents are preventable. 
We will not compromise safety to achieve any business objective. 
Leaders are accountable for the safety of all employees, contractors, 
and the public. 
Each employee has primary responsibility for his/her safety and the 
safety of others. 
Effective communication and the sharing of information is essential 
to achieving an accident-free workplace. 
Employees and contractor personnel will be properly trained to 
perform their work safely. 
35</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Safety Management System (2) 
Early warning systems for migration toward states of high risk 
are established and effective 
Effective communication channels exist for disseminating safety 
information 
Visibility of state of safety at all levels through appropriate 
feedback 
Results of operating experience, process hazard analyses, 
audits, near misses, or accident investigations are used to 
improve process operations and process safety management 
system. 
Deficiencies found during assessments, audits, inspections and 
incident investigation are addressed promptly and tracked to 
completion 
 
44</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Safety Control Structure Design (3) 
Use of working groups for communication 
Very effective in DoD 
Different groups at different levels 
Responsible for coordinating safety efforts at each level, 
reporting status of outstanding safety issues, providing 
information to other levels and to external review boards 
Provides important information sharing: Changes in one 
subsystem may affect other subsystems and system as a 
whole 
 
41</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Paul ONeill and Alcoa 
"I intend to make Alcoa the safest company in America. I 
intend to go for zero injuries." 
 The board put a crazy hippie in charge and he's going 
to kill the company  
I ordered my clients to sell their stock immediately, before 
everyone else in the room started calling their clients and 
telling them the same thing.  It was literally the worst piece of 
advice I gave in my entire career.  
 
28</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Managing/Controlling Change 
Adaptation or change is an inherent part of any system 
A common factor in accidents 
Was a change involved in any of the accidents you 
studied? 
Controls needed to: 
Prevent unsafe changes 
Detect them if they occur 
3</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Reporting Systems 
If not being used, then find out why.  
Common reasons why not used: 
Difficult or awkward to use 
Information appears to go into a black hole. No point in 
reporting because organization wont do anything anyway  
Fear information will be used against them 
Examples of successful systems:  
Nuclear Power 
Commercial Aviation 
 
 
11</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Examples of Positive Cultural Values and 
Assumptions 
Incidents and accidents are valued as an important 
window into systems that are not functioning as they 
should  triggering causal analysis and improvement 
actions. 
Safety information is surfaced without fear  
Safety analysis is conducted without blame 
Safety commitment is valued 
23</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Food Safety Example 
New alliance of retailers, food growers, and farm workers 
Workers had little incentive to report safety problems. Paid 
at a piece rate and taking even 10 minutes to report a 
safety problem would reduce their pay. One manager said 
that if workers spotted animal feces in an area where ripe 
strawberries were ready to be plucked, they might have 
still simply picked those berries.  
Teach workers how to spot signs of food contamination 
and train in good practices in exchange for better pay and 
working conditions 
 This program means that instead of one auditor coming 
around once in a while to check on things, we have 400 
auditors on the job all the time.  
31</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Types of Flawed Safety Cultures  
Culture of Denial 
Risk assessment is unrealistic  
Credible risks and warnings are dismissed without 
appropriate investigation (only want to hear good news)  
Believe accidents are inevitable, the price of productivity 
Compliance Culture 
Focus on complying with government regulations 
 Produce extensive safety case arguments  
Paperwork Culture 
Produce lots of paper analyses with little impact on design 
and operations 
25</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Leadership is Key to Changing Culture 
Safety requires passionate and effective leadership 
Tone is set at the top of the organization 
Not just sloganeering but real commitment 
Setting priorities 
Adequate resources assigned 
A designated, high-ranking leader 
 Minimize blame (Just Culture)  
Understand that safety and productivity are not 
conflicting if take a long-term view 
27</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Accident/Incident Investigation 
CAST must be embedded in organizational structure that 
allows exploitation of results 
Training: Analysts must be managerially and financially 
independent. 
Could use trained teams with independent budgets 
Need to get away from blame  
Follow-up:  
Ensure recommendations implemented and are effective 
Findings should be input to future audits and performance 
assessments 
If reoccurrence of same factors, investigate why 
10</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Controls for Unplanned Changes 
How might deal with unplanned and unsafe changes? 
 
5</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>What is Safety Culture? 
Shein: The Three Levels of Organizational Culture 
Safety culture is set by the leaders who establish 
the values under which decisions will be made. 
21</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Safety Policy  
Reflects how the company or group values safety 
Should be easy to understand, easily operationalized 
Based on the way the company views safety: guiding 
principles (safety philosophy) 
33</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Example Cultural Values and 
Assumptions (2) 
There is a feeling of openness and honesty, where 
everyones voice is valued. Employees feel managers 
are listening. 
Trust among all parties (hard to establish, easy to break). 
Employees feel psychologically safe about reporting 
concerns  
Employees believe that managers can be trusted to hear 
their concerns and will take appropriate action   
Managers believe employees are worth listening to and are 
worthy of respect. 
 
24</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Encouraging Reporting 
Maximize accessibility 
Reporting forms easily and ubiquitously available 
Not cumbersome to fill in or send up 
Minimize anxiety 
Written policy that explains 
What reporting process looks like 
Consequences of reporting 
Rights, privileges, protections, and obligations 
Without written policy, ambiguity exists and people will disclose less 
Act on reports and send information back  (provide feedback) 
 
 
12</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Summary: Safety Management System 
Key components 
Management commitment 
Management involvement 
Employee empowerment 
Incentive structures 
Reporting systems 
Organizational learning and improvement process 
 
42</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Systems Theoretic Process Analysis (STPA) Introduction, Basic Components (hazard, constraints, HCS) (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes5/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>26</slideno>
          <text>Defining Safety Constraints
Unsafe Control Action Safety Constraint
Pilot performs ITP when ITP 
criteria are not met or request 
has been refusedPilot must not perform ITP 
when criteria are not met or 
request has been refused
Pilot starts maneuver late 
after having re -verified ITP 
criteriaPilot must start maneuver 
within X minutes of re -verifying 
ITP criteria
Etc. Etc.
27</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>North Atlantic Tracks
 source unknown. All rights reserved. This content is excluded from our Creative
Commons license. For more information, see https://ocw.mit.edu/help/faq-fair-use/.
15</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>STPA
(System -Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control 
structure
Step 1: Identify 
unsafe control 
actions
Step 2: Identify 
causal factors and 
create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
2</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Example System: Aviation
System -level Accident (Loss): ?Image removed due to copyright restrictions.
7</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>STPA Analysis
High -level (simple) Control Structure
Main components and controllers?
? ? ?
17</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>STPA
(System -Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control 
structure
Step 1: Identify 
unsafe control 
actions
Step 2: Identify 
causal factors and 
create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
28</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>FAACongress
ATC
AircraftExample High -level control structure
PilotsDirectives, funding
Regulations, procedures
Instructions
Execute maneuversReports
Reports
Aircraft status, position, etcAcknowledgement, requests
22</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Systems Theoretic Process Analysis 
(STPA)
1</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Hazard
Definition: A system state or set of conditions 
that, together with a particular set of worst-case 
environmental conditions, will lead to an accident (loss).
Something we can control
Examples:
Accident Hazard
Satellite becomes lost or Satellite maneuvers out of orbit
unrecoverable
People die from exposure to toxic Toxic chemicals are released into 
chemicals the atmosphere
People die from radiation Nuclear power plant releases 
sickness radioactive materials
People die from food poisoning Food products containing 
pathogens are sold
10</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>STPA Analysis
More complex control 
structure
21</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>STPA Step 2: Causal scenarios
UCA: Pilot executes 
maneuver when 
criteria are not met 
[H-1]
From Leveson, Nancy (2012). Engineering a Safer World: Systems Thinking Applied to
Safety. MIT Press,  Massachusetts Institute of Technology. Used with permission.
29</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>System -level Accident (Loss): Two aircraft collide
System -level Hazard: ?Image removed due to copyright restrictions.
9</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Identify Unsafe Control Actions
Flight Crew 
Action (Role)Not providing 
causes hazardProviding 
Causes hazardIncorrect 
Timing/
OrderStopped Too 
Soon
Execute 
Passing 
ManeuverPilots perform 
ITP when ITP 
criteria are not 
met or request 
has been refused 
[H-1]ATC
PilotsInstructions
Execute maneuversAircraft status, position, etcAcknowledgement, requests
Aircraft
25</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Structure of a Hazardous Control 
Action
Four parts of a hazardous control action
Source Controller: the controller that can provide the control action
Type: whether the control action was provided or not provided
Control Action: the controllers command that was provided / 
missing
Context: conditions for the hazard to occur
(system or environmental state in which command is provided)Source ControllerExample:
Pilots provide ITP maneuver when ITP criteria not met 
Type
Control ActionContext
26</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>ITP Exercise
a new in -trail procedure 
for trans -oceanic flights
5</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>STPA Analysis
High -level (simple) 
Control Structure
What commands are 
sent?
AircraftFlight CrewAir Traffic 
Control
??
??
19</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>STPA Analysis
High -level (simple) Control Structure
Who controls who?
Flight Crew? Aircraft?Air Traffic 
Controller?
18</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Aviation Examples
System -level Accident (loss)
A-1: Two aircraft collide
A-2: Aircraft crashes into terrain / ocean
System -level Hazards
H-1: Two aircraft violate minimum separation
H-2: Aircraft enters unsafe atmospheric region
H-3: Aircraft enters uncontrolled state
H-4: Aircraft enters unsafe attitude
H-5: Aircraft enters prohibited area
13</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>STPA
(System -Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control 
structure
Step 1: Identify 
unsafe control 
actions
Step 2: Identify 
causal factors and 
create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
14</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>STPA
(System -Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control 
structure
Step 1: Identify 
unsafe control 
actions
Step 2: Identify 
causal factors and 
create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
6</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>ATC Ground 
Controller
Updates and 
acknowledgements
AircraftInstructions
AircraftOther Ground 
ControllersATC Front Line Manager (FLM)
Company 
Dispatch
ATC Radio
ACARS Text MessagesInstructions Status 
UpdatesInstructions Status 
Updates
Instructions Status 
UpdatesStatusQueryInstructions Status 
Updates
Aircraft AircraftPilots Pilots Pilots Pilots
Execute 
maneuversExecute 
maneuversExecute 
maneuversExecute 
maneuversAir Traffic Control (ATC)
23</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Example System: Aviation
System -level Accident (Loss): Two aircraft collideImage removed due to copyright restrictions.
8</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>STPA Analysis
High -level (simple) 
Control Structure
AircraftFlight CrewAir Traffic 
Control
Issue 
clearance 
to pass
Execute 
maneuverFeedback?Feedback?
20</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>System -level Accident (Loss): Two aircraft collide
System -level Hazard: Two aircraft violate minimum 
separationImage removed due to copyright restrictions.
11</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>STPA
(System -Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control 
structure
Step 1: Identify 
unsafe control 
actions
Step 2: Identify 
causal factors and 
create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
24</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>STPA
(System-Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control structure
Step 1: Identify 
unsafe control actions
Step 2: Identify causal factors and create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
4</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>STPA application:
NextGen In-Trail Procedure (ITP)
Current State
Proposed Change
Pilots will have separation 
information
Pilots decide when to 
request a passing maneuver
Air Traffic Control 
approves/denies request
16</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>STPA
(System-Theoretic Process Analysis)
Identify accidents 
and hazards
Draw the control structure
Step 1: Identify 
unsafe control actions
Step 2: Identify causal factors and create scenariosControlled 
processControl
ActionsFeedbackController
(Leveson, 2012)
3</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Aviation Examples
System -level Accident (loss)
Two aircraft collide
Aircraft crashes into terrain / ocean
System -level Hazards
Two aircraft violate minimum separation
Aircraft enters unsafe atmospheric region
Aircraft enters uncontrolled state
Aircraft enters unsafe attitude
Aircraft enters prohibited area
12</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>Design for Safety (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes7/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>Redundancy (3) 
 Limitations  
 Common-cause and common-mode failures 
 May add so much complexity that causes failures 
 More likely to operate spuriously 
 May lead to false confidence (Challenger) 
 Extra costs including maintenance and extra weight 
 Useful to reduce hardware failures. But what about software? 
 Ariane 5 loss 
 Design redundancy vs. design diversity 
 Bottom line: Claims that multiple version software will achieve 
ultra-high reliability levels are not supported by empirical data or 
theoretical models 
24</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Copyright Nancy Leveson, Aug. 2006 
3</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Redundancy (2) 
 Identical designs or intentionally different ones (diversity) 
 Diversity must be carefully planned to reduce 
dependencies 
 Problem is potential lack of independence 
 Common mode failures: fail in same way, causes may be 
different 
 Common cause failure: Fail due to same cause 
 Can also introduce dependencies in maintenance, 
testing, repair 
 
23</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Basic Design for Safety 
Principles 
1</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Example: Nuclear Detonation (4) 
Intended
humanactionStimuli Source Communication 
System
Unique
signal
no. 1
Unique
signal
no. 2
Arming
signalSafing and Firing
System
Intendedhumanaction
Intended
humanaction
Human-machine interfaceAABABBB
Arming and
fusing system
Image by MIT
 OpenCourseWare.
30</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Redundancy  
 Goal is to increase component reliability and reduce 
component failures 
 Standby spares vs. concurrent use of multiple devices (with 
voting) 
 Identical designs or intentionally different ones (diversity) 
 Diversity must be carefully planned to reduce dependencies 
 Can also introduce dependencies in maintenance, testing, 
repair 
 
21</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>A cartoon from Rube Goldberg vs. the Machine Age by Reuben L. Goldberg is removed due to copyright restrictions.
5</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Protection Systems and  
Fail-Safe Design (5) 
 May need multiple types of shutdown procedures 
 Normal emergency stop (cut power from all circuits) 
 Production stop (stop after current task completed) 
 Protection stop (shut down immediately but not necessarily by cutting 
power from circuits, which could result in damage).   
 If cannot design to fail into safe state or passively change to safe 
state, the hazard detectors must be of ultra-high reliability. 
 May add equipment to test detection system periodically by simulating 
condition sensor is supposed to detect (e.g., challenge system) 
 Challenge system must not obscure a real hazard and must be 
independent from monitor system 
   
40</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Damage Reduction 
 In emergency, may not be time to assess situation, diagnose what is 
wrong, determine correct action, and then carry out action. 
 Need to prepare emergency procedures and practice them 
 May need to determine a point of no return where recovery no longer 
possible or likely and should just try to minimize damage . 
 Distinguish between warnings used for drills and those for real 
emergencies 
 Damage minimization includes 
 Escape routes 
 Safe abandonment of products and materials (e.g., hazardous waste 
disposal) 
 Devices for limiting damage to people or equipment (e.g., blowout panels 
and frangible walls, collapsible steering columns on cars, sheer pins in 
motor-driven equipment 
 
 
42</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>More Examples 
 Retractible landing gear: wheels drop and lock into position if 
system that raises and lowers them fails (e.g., pneumatic pressure 
system) 
 Elevator: if hoist cables fail, safety mechanism wedges into guide 
rails 
 Bathyscope: ballast held in place by magnets. If electrical power 
lost, ballast released and ascends to surface 
 Railway signalling systems: signals not in use kept in danger 
position. Positive action required (setting signal to clear) is required 
before train can pass. 
 Design cars so drivable with one flat tire. Also run-flat tires with 
solid rubber core 
10</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Barriers (2) 
Lockin 
 Make it difficult or impossible to leave a safe state, maintain a 
safe condition 
 Possible uses: 
 Keep humans within an enclosure, e.g., seatbelts and 
harnesses, doors on elevators 
 Contain harmful products or byproducts, e.g., electromagnetic 
radiation, pressure, noise, toxins, ionizing radiation 
 Contain potentially harmful objects, e.g., cages around an 
industrial robot in case it throws something 
 Maintain a controlled environment (e.g., buildings, spacecraft, 
space suits, diving suits) 
 Maintain a safe state (e.g. speed governors, relief valves to 
maintain pressure below dangerous levels) 
 
16</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Barriers 
Lockout 
 Make access to dangerous state difficult or impossible 
 Fences and physical barriers to block access to a dangerous 
condition (sharp blades, heated surfaces, high-voltage 
equipment) 
 Logical barriers (authority limiting, software firewalls) 
 
15</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Software Recovery 
 Backward 
 Assume can detect error before does any damage 
 Assume alternative will be more effective 
 Forward 
 Robust data structures 
 Dynamically altering flow of control 
 Ignoring single cycle errors 
 But real problem is detecting erroneous states 
26</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Monitoring (2) 
 Two ways to detect equipment malfunction: 
 Monitor equipment performance (requires redundant info) 
 Monitor equipment condition 
In general, monitors should  
 Detect problems as soon as possible 
 Be independent from devices they are monitoring 
 Add as little complexity to system as possible 
 Be easy to maintain, check, and calibrate 
 Be easily interpreted by operators (e.g., mark limits on dials) 
13</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Fault or Failure Tolerance  
 Goal is to tolerate faults so they have no or little negative 
impact 
 Isolation or independence: so that misbehavior of one 
component does not negatively affect behavior of another 
 Failure warnings and indicators: to provide early detection of 
failures so preventive actions can be taken 
 Carefully designed and practiced flight crew procedures to 
enable safe flight and landing when problems occur  
 Design to tolerate human error 
 Physical damage tolerance: ability to sustain damage without 
hazard resulting. 
 Eliminate impact of common hardware failures on software 
 E.g., do not use 1 or 0 to denote safe vs. armed 
  
20</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Protection Systems and Fail-Safe Design 
 May have multiple safe states, depending on process 
conditions 
 General rule is hazardous states should be hard to get into 
and safe states should be easy 
 Typical protective equipment: 
 Gas detectors 
 Emergency isolation valves 
 Trips and alarms 
 Relief valves and flare stacks 
 Water curtains 
 Firefighting equipment 
 Nitrogen blanketing 
 
36</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Designing to Prevent Accidents 
 Standards and codes of practice contain lessons learned 
from the past 
 Standard precedence 
 Try to eliminate hazards from the design 
 Identify causes of hazards and try to reduce their likelihood 
of occurring through design 
 Control hazards once they occur 
 Design to reduce damage 
2</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Elimination (2) 
 Decoupling:  
 Tightly coupled system is one that is highly interdependent 
 Coupling increases number of interfaces and leads to unplanned 
interactions 
 Computers tend to increase system coupling unless very careful. 
 Reduce opportunities for human error 
 Make impossible or possible to detect immediately 
 Examples: wiring errors (color code, female/male connectors), 
typos, making displays readable, showing status of plant 
 Reduce hazardous materials or conditions 
 Example: keep fewer hazardous chemicals on hand 
 
 
6</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Design for Controllability 
 Make system easier to control, both for humans and 
computers 
 Use incremental control 
 Perform critical steps incrementally rather than in one step 
 Provide feedback 
 To test validity of assumptions and models upon which 
decisions are made 
 To allow taking corrective action before significant damage is 
done 
 Provide various types of fallback or intermediate states 
 Lower time pressures 
 Provide decision aids 
 
11</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Hazard Control 
 Detect hazard and control it before damage occurs 
 May be able to reverse it before necessary environmental 
conditions occur 
 Resources (physical and informational, such as diagnostics 
and status information) may be needed to control hazard 
 First need to detect hazard 
 Warning signals should be not present for too long or too 
frequently (people become insensitive to constant stimuli) 
 Do not assume hazard will never occur because of other 
protection devices or because software never fails 
 
 
31</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Protection Systems and  
Fail-Safe Design (6) 
 Hazard detection system may have three subsystems: 
 Sensor to detect hazardous condition 
 Challenge subsystem to exercise and test sensor 
 Monitor subsystem to watch for any interruption of challenge-
and-response sequence. 
Note that complexity creeping up, decreasing probability 
these protection facilities will work when needed.   
41</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Fail-Safe (Passive) Safeguards Examples 
 Design so system fails into a safe state 
      Examples: 
 Deadman switch 
 Magnetic latch on refrigerators 
 Railroad semaphores: if cable breaks, fails into STOP position 
 Cover over a high-energy source with circuit run through it 
 Relays or valves designed to fail open or fail safe 
 Air brakes: held in off position by air pressure. If line breaks, lose 
air pressure and brakes applied 
 Early Apollo program: use free return trajectory. If engines failed 
at lunar orbit insertion, spacecraft safely coasts back to earth  
9</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Hazard Control 
LIMITING EXPOSURE (level or duration of hazard) 
 Stay in safe state as long and as much as possible 
e.g., nitroglycerine used to be manufactured in a large batch 
reactor. Now made in small continuous reactor and residence 
time reduced from 2 hours to 2 minutes. 
 Start out in safe state and require deliberate change to 
unsafe state 
e.g., arm missile only when near target 
    NPP shutdown software keeps variables in trip state and  
requires change to non-trip. 
 Critical conditions should not be complementary, e.g., 
absence of an arm condition should not be used to indicate 
system is unarmed 
32</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Monitoring 
 To detect a problem need to  
 Check conditions that are assumed to indicate a potential 
problem 
 Validate or refute assumptions made during design and analysis 
Can be used to indicate  
 Whether a specific condition exists 
 Whether a device ready for operation or operating satisfactorily 
 Whether required input is being provided 
 Whether a desired or undesired output is being generated 
 Whether a specific limit being exceeded or whether a measured 
parameter is abnormal 
Need to design for checkability and inspectability  
 
12</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Example: Nuclear Detonation (3) 
Image by MIT OpenCourseWare.BarrierRemovable
barrier
Human
intent
Isolated componentUnique Signal Source  
UQS ReaderCommunications channel
incompatible - Unique signalDiscriminator/Driver
Arming
and firing
voltagesInclusion
Region
Stored
UQSInoperable in abnormal
environments
Isolated element
 Exclusion Region 
29</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Protection Systems and  
Fail-Safe Design (2) 
 Panic Button: stops a device quickly, perhaps by cutting off power 
 Must be within reach when needed 
 Operators must be trained to react quickly to unexpected events 
 Passive devices better than active again 
 Watchdog timer: Timer that system must keep restarting. If not then 
takes protective action 
 Sanity checks (Im alive signals): detects failure of computers 
 Protection system should provide information about its control 
actions and status to operators or bystanders. 
 Failure containment: limit effects of failure or hazard to local area 
 
37</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Fail-Safe Design in Aviation (2) 
 Design for checkability and inspectability 
 Failure containment 
 Damage tolerance  
 Systems surrounding failures should be able to tolerate 
them in case failure cannot be contained 
 Designed failure paths 
 Direct high energy failure that cannot be tolerated or 
contained to a safe path 
 E.g. use of structure fuses in pylons so engine will fall off 
before it damages the structure 
 
35</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Hazard Elimination 
 Substitution  
 Use safe or safer materials 
 Nontoxins, non-explosives 
 Chlorine blower example 
 Simplification 
 Minimize parts, modes, interfaces 
 Reduce unknowns  
 Computers make it easy to build dangerously complex 
systems 
 
4</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Barriers (3) 
Interlock 
 Used to enforce a sequence of actions or events 
1.Event A does not occur inadvertently  
2.Event A does not occur while condition C exists 
3.Event A occurs before event D 
 (1) and (2) are called inhibits, (3) is a sequencer 
 Examples: 
 Pressure sensitive mat or light curtain that shuts off a robot if someone 
comes near 
 Deadman switch 
 Guard gates and signals at railway crossings 
 
17</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Hazard Reduction 
 Try to minimize likelihood of hazard occurring 
7</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Protection Systems and  
Fail-Safe Design (4) 
 May design various types of fallback states 
 e.g., traffic lights that fail to blinking red or yellow states, unstable 
aircraft have analog backup devices because cannot be flown manually 
(but less functionality) 
 Types of fallback states: 
 Partial shutdown (partial or degraded functionality) 
 Hold (no functionality provided, but steps taken to maintain safety or 
limit amount of damage) 
 Emergency shutdown (system shut down completely) 
 Manually or externally controlled 
 Restart (system in transitional state from non-normal to normal) 
 Conditions under which each of fallback states should be invoked 
must be determined, along with how transitions between states will 
be implemented and controlled. 
 
39</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Fail-Safe Design in Aviation 
 Design integrity and quality 
Redundancy  
 Isolation (so failure in one component does not affect another) 
 Component reliability enhancement 
 Failure indications (telling pilot a failure has occurred, may 
need to fly plane differently) 
 Specified flight crew procedures 
34</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Software Redundancy 
 Software errors are design errors 
 Data redundancy: extra data for detecting errors: 
e.g., parity bit and other codes 
         checksums 
         message sequence numbers 
         duplicate pointers and other structural information 
 Algorithmic redundancy: 
1.Acceptance tests (hard to write) 
2.Multiple versions with voting on results 
3.Found to have lots of common faults 
25</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Hazard Control 
ISOLATION AND CONTAINMENT 
 Provide barriers between system and environment  
       e.g., containment vessels and shields 
 Isolate away from people: Very hard to maintain over time 
 
PROTECTION SYSTEMS AND FAIL-SAFE DESIGN 
 Move system to a safe or safer state 
 Requires existence of a safe state (shutdown in NPP, sleep state in 
spacecraft cruise mode) 
 Also requires an early warning with enough time between detection 
of hazard and actual loss event 
33</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Barriers (4) 
 Examples (cont): 
 Device on machinery to ensure all prestart conditions met, correct 
startup sequence followed, conditions for transitions between phases 
are met 
 Device to ensure correct sequencing of valve turn-off or turn-on or both 
not on or off at same time. 
 Devices to preventing disarming a trip (protection) system unless 
certain conditions occur first or to prevent system from being left in 
disabled state after testing or maintenance 
 Disabling car ignition unless automatic shift in PARK 
 Freeze plug in a cars engine cooling system (expansion will force plug 
out rather than crack cylinder if water in block freezes) 
 Fusible plug in boiler becomes exposed if excessive heat and water 
level drops below predetermined level. Plug melts, opening permits 
steam to escape, reduces pressure in boiler, and prevents explosion.  
18</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Barriers (5) 
 Design Considerations for interlocks 
 Design so hazardous functions stop if interlock fails 
 If interlock brings something to a halt, provide status and alarm 
information to indicate which interlock failed. 
 If use interlock during maintenance or testing, must preclude 
inadvertent interlock overrides or being left inoperative once system 
becomes operational again.  
 When computers introduced, physical interlocks may be defeated or 
omitted.  
 Software programmers may not understand physical devices they are 
replacing.  
 May still need physical interlocks to protect against software errors. 
 Make sure in safe state when resume operation, dont just start from where 
left off. 
Remember, the more complex the design, the more likely errors 
or hazards will be introduced by the protection facilities  
themselves. 
19</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Redundancy 
22</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Example: Nuclear Detonation  
 Safety depends on NOT working 
 Three basic techniques (callled positive measures) 
1.Isolation 
 Separate critical elements 
2.Inoperability 
 Keep in inoperable state, e.g., remove ignition device or 
arming pin 
3.Incompatibility 
 Detonation requires an unambiguous indication of human 
intent be communicated to weapon 
 Protecting entire communication system against all credible 
abnormal environments (including sabotage) not practical. 
 Instead, use unique signal of sufficient information 
complexity that unlikely to be generated by an abnormal 
environment 
27</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Designed failure path: direct failure along a less critical path 
 Example: jet engine mounted on wing by a pylon structure. Severe 
engine unbalance caused by loss of a number of fan blades from 
foreign object ingestion could destroy wing. But pylon and engine 
mount system designed to fail under these loads before main wing 
structure, allowing engine to fall harmlessly from airplane. 
 The easier and faster is return of system to operational state, the 
less likely protection system will be purposely bypassed or turned off 
 Try to control hazard while causing least damage in process 
 May need to do more than simply shut down, e.g., blowing up an 
errant rocket. 
 Such facilities may do harm themselves, e.g., French weather balloon 
emergency destruct facility, if inadvertently initiated 
 Protection Systems and  
Fail-Safe Design (3) 
38</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Limitations of Monitoring 
 Difficult to make monitors independent 
 Checks usually require access to information being 
monitored, but usually involves possibility of corrupting that 
information 
 Depends on assumptions about behavior of system and 
about errors that may or may not occur 
 May be incorrect under certain conditions 
 Common incorrect assumptions may be reflected both in 
design of monitor and devices being monitored. 
14</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Design Modification and Maintenance 
Need to re -analyze safety for every proposed/implemented 
change 
 Recording design rationale from beginning and traceability will 
help. 
 
43</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Passive vs. Active Protection 
 Passive safeguards : 
 Maintain safety by their presence 
 Fail into safe states 
 Active safeguards : 
 Require hazard or condition to be detected and corrected 
Tradeoffs 
 Passive rely on physical principles 
 Active depend on less reliable detection and recovery 
mechanisms 
BUT 
 Passive tend to be more restrictive in terms of design freedom 
and not always feasible to implement 
8</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Example: Nuclear Detonation (2) 
 Unique signal discriminators must 
1.Accept proper unique signal while rejecting spurious inputs 
2.Have rejection logic that is highly immune to abnormal 
environments 
3.Provide predictable safe response to abnormal 
environment 
4.Be analyzable and testable 
 Protect unique signal sources by barriers 
 Removable barrier between these sources and 
communication channels 
28</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
    <lecture>
      <lecture_title>(Part 1 CAST Class Notes, PDF - 1.3MB)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/16-863j-system-safety-spring-2016/resources/mit16_863js16_lecnotes3-1/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>15</slideno>
          <text>Identifying Components to Include  
 Start with physical process 
 What inadequate controls allowed the physical events?
	
 Physical 
 Direct controller 
 Indirect controllers 
 Add controls and control components as required to  
explain the inadequate controls already identified.
	
16</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>NTSB Findings 
Probable Cause: 
 FCs failure to use available cues and aids to identify the  
airplanes location on the airport surface during taxi
	
		FCs failure to cross-check and verify that the airplane was on the correct runway before takeoff. 
		Contributing to the accident were the flight crews nonpertinent conversation during taxi, which resulted in a 
loss of positional awareness, 
		Federal Aviation Administrations (FAA) failure to require that all runway crossings be authorized only by specific air traffic control (ATC) clearances. 
53</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Context in Which Decisions Made:
	
		No communication that the taxi route to the departure runway was 
different than indicated on the airport diagram 
		No known reason for high-threat taxi procedures 
		Dark out 
		Comair had no specified procedures to confirm compass heading 
with runway 
		Sleep loss fatigue 
		Runways 22 and 26 looked very similar from that position 
		Comair in bankruptcy, tried to maximize efficiency 
		Demanded large wage concessions from pilots 
		Economic pressures a stressor and frequent topic of conversation for 
pilots (reason for cockpit discussion) 
22</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>FAA Airport Safety &amp; Standards Office 
Safety Requirements and Constraints: 
		Establish airport design, construction, maintenance, operational 
and safety standards and issue operational certificates accordingly. 
		Ensure airport improvement project grant compliance and release of grant money accordingly. 
		Perform airport inspections and surveillance. Enforce compliance if problems found. 
		Review and approve Safety Plans Construction Documents in a 
timely manner, consistent with safety. 
 Assure all stake holders participate in developing methods to  maintain operational safety during construction periods .  
38</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Airport Authority
	
Unsafe Control Actions: 
		Relied solely on FAA guidelines for determining adequate signage 
during construction. 
		Did not seek FAA acceptable options other than NOTAMs to inform airport users of the known airport chart inaccuracies. 
		Changed taxiway A5 to Alpha without communicating the change by other than minimum signage. 
		Did not establish feedback pathways to obtain operational safety information from airport users. 
35</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Standard and Enhanced Hold Short  
Markings
	
Courtesy of Lund University. Used with permission. 
41</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Communication Links Actually in Place
	
57</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Blue Grass Airport Authority (LEX)
	
Safety Requirements and Constraints: 
		Establish and maintain a facility for the safe arrival and departure of 
aircraft to service the community. 
 Operate the airport according to FAA certification standards, FAA  regulations (FARs) and airport safety bulletin guidelines (ACs).  
		Ensure taxiway changes are marked in a manner to be clearly understood by aircraft operators. 
34</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>hospital reports , input from medical community 
reports 	 reports 
IACEsl I t 
v10E int ~cl In BGOS Medical ! 	 Ministry of rei 'lrts 	 Advisories, warnings
budgets , laws Health 	 Dept. of Health Public HealthreOI lations ,regulatory polic Guidelines 
rr port" ... .. ... .. ... ...... ..... 
status "............ : . L water samples : 
requestsFederal 	 : ... .......: GovernmL'nt : 	 and
Provincialguidelines 	 : T ,t' L b : reports report. e ling a 	.................. 
Government 	 :rep.01.s.: .. .. .. .. .. .. : 	 contaminantswater samples 
................. ............ .... .... : : 
: ,, In' pection and other reports 	 I 
budgets , laws 	 : 
chlorine residual measurement 
regulatory polic : : 	 Water systemMinistry of , ODWO ,Chlorination Bulletin 
the Envi ronment Certificates of Approval 
reports Operator certification 
water 
Walkerton PUC 
chlorinationMinistry of 	 operations Well 7 Well 5Poli( ~s 
Agriculture , WPUC Commissioners 	 Well Design flaw: Design flaw: 
Budget 	 selection No chlorinator Shallow location budgets , laws 	 Food , and 
Rural Affairs 
Oversight I 
Porous bedrockFinancial Info. 
Minimal overburden 
Heav rains 
Walkerton Private 
Residents Testing Lab Farm 
55</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Air Traffic Organization: Terminal Services
	
Safety Requirements and Constraints 
		Ensure appropriate ATC Facilities are established to safely and 
efficiently guide aircraft in and out of airports. 
		Establish budgets for operation and staffing levels which maintain safety guidelines. 
		Ensure compliance with minimum facility staffing guidelines. 
		Provide duty/rest period policies which ensure safe controller performance functioning ability. 
Unsafe Control Actions 
		Issued verbal guidance that Tower and Radar functions were to be separately manned, instead of specifying in official staffing policies. 
		Did not confirm the minimum 2 controller guidance was being followed. 
		Did not monitor the safety effects of limiting overtime
. 
48</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Goals for an Accident Analysis Technique  
		Minimize hindsight bias 
		Provide a framework or process to assist in 
understanding entire accident process and identifying systemic factors 
		Get away from blame (who) and shift focus to why and how to prevent in the future 
		Goal is to determine 
		Why people behaved the way they did 
		Weaknesses in the safety control structure that allowed 
the loss to occur 
12</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Airport Authority 
Process Model Flaws: 
		Believed compliance with FAA guidelines and inspections would 
equal adequate safety. 
		Believed the NOTAM system would provide understandable information about inconsistencies of published documents. 
		Believed airport users would provide feedback if they were confused. 
Context in Which Decisions Made: 
		The last three FAA inspections demonstrated complete compliance with FAA regulations and guidelines. 
		Last minute change from Safety Plans Construction Document phase III implementation plan. 
36</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Federal Aviation 
Administration 
Comair: Delta 
Connection 
Airport Safety &amp; 
Standards District 
Office LEX ATC 
Facility 
National 
Flight Data 
Center Jeppesen 5191 
Flight 
Crew Certification, Regulation, 
Monitoring &amp; Inspection 
Procedures, Staffing, Budget Aircraft Clearance and 
Monitoring 
Charts, NOTAM Data 
(except L) to Customer Read backs, Requests 
Local 
NOTAMs 
Reports, Project Plans 
NOTAM Data 
Chart Discrepancies ATIS &amp; L NOTAMs 
Operational Reports 
ALPA 
Safety 
ALR 
Airport 
Diagram 
Airport 
Diagram 
Verification Optional construction 
signage 
Certification, Inspection, 
Federal Grants 
Composite Flight Data, except L NOTAM Graphical Airport Data ATO: 
Terminal 
Services 
Pilot perspective 
information 
Construction information Blue Grass Airport 
Authority Procedures &amp; 
Standards Flight release, Charts etc. 
NOTAMs except L 
IOR, ASAP 
Reports 
Certification &amp; Regulation 
= missing feedback lines 
Courtesy of Lund University. Used with permission. 
42</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Summary (2)
	
		Consider dynamics (changes in control structure) and 
migration to higher risk 
		Determine the changes that could eliminate the inadequate 
control (lack of enforcement of system safety constraints) in the future. 
		Generate recommendations 
		Continuous Improvement 
		Assigning responsibility for implementing recommendations 
		Follow-up to ensure implemented 
		Feedback channels to determine whether changes effective 
 If not, why not? 
59</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Conclusions
	
		The model used in accident or incident analysis determines 
what we what look for, how we go about looking for facts, and what facts we see as relevant. 
		A linear chain of events promotes looking for something that broke or went wrong in the proximal sequence of events prior to the accident. 
		A stopping point, often, is arbitrarily determined at the point 
when something physically broke or an operator error (in 
hindsight) occurred . 
 Unless we look further, we limit our learning and almost  
guarantee future accidents related to the same factors.
	
		Goal should be to learn how to improve the safety control structure 
60</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Federal Aviation 
Administration 
Comair: Delta 
Connection 
Airport Safety &amp; 
Standards District 
Office LEX ATC 
Facility 
National 
Flight Data 
Center Jeppesen 5191 
Flight 
Crew Certification, Regulation, 
Monitoring &amp; Inspection 
Procedures, Staffing, Budget Aircraft Clearance and 
Monitoring 
Charts, NOTAM Data 
(except L) to Customer Read backs, Requests 
Local 
NOTAMs 
Reports, Project Plans 
NOTAM Data 
Chart Discrepancies ATIS &amp; L NOTAMs 
Operational Reports 
ALPA 
Safety 
ALR 
Airport 
Diagram 
Airport 
Diagram 
Verification Optional construction 
signage 
= missing feedback lines Certification, Inspection, 
Federal Grants 
Composite Flight Data, except L NOTAM Graphical Airport Data ATO: 
Terminal 
Services 
Pilot perspective 
information 
Construction information Blue Grass Airport 
Authority Procedures &amp; 
Standards Flight release, Charts etc. 
NOTAMs except L 
IOR, ASAP 
Reports 
Certification &amp; Regulation 
Courtesy of Lund University. Used with permission. 
37</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Aircraft Flight Crew Comair/Delta 
Connection 
25</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>National Flight Data Center  
Safety Requirements and Constraints 
		Collect, collate, validate, store, and disseminateaeronautical 
information detailing the physical description and operational status of all components of the National Airspace System (NAS). 
		Operate the US NOTAM system to create, validate, publish and disseminate NOTAMS. 
		Provide safety critical NAS information in a format which is understandable to pilots. 
		NOTAM dissemination methods will ensure pilot operators receive 
all necessary information.  
32</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Communication Links Theoretically in  
Place in Uberlingen Accident
	
56</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Oversimplification of Causes
	
 Almost always there is : 
 Operator error 
 Flawed management decision making
	
 Flaws in the physical design of equipment 
 Safety culture problems 
 Regulatory deficiencies 
 Etc. 
4</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Mental Model Flaws 
		Hazard of pilot confusion during North end taxi operations was 
unrecognized. 
		Believed flight 5191 had taxied to runway 22. 
		Did not recognize personal state of fatigue. 
Context in Which Decisions Made 
		Single controller for the operation of Tower and Radar functions. 
		The controller was functioning at a questionable performance level due to sleep loss fatigue 
		From control tower, thresholds of runways 22 and 26 appear to 
overlap 
45</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Identify Hazard and Safety  
Constraint Violated
	
		Accident: death or injury, hull loss 
		System hazard: Runway incursions and operations on 
wrong runways or taxiways. 
		System safety constraint: The safety control structure 
must prevent runway incursions and operations on 
wrong runways or taxiways 
Goal : Figure out why the safety control structure did 
not do this 
15</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Federal Aviation 
Administration 
Comair: Delta 
Connection 
Airport Safety &amp; 
Standards District 
Office LEX ATC 
Facility 
National 
Flight Data 
Center Jeppesen 5191 
Flight 
Crew Certification, Regulation, 
Monitoring &amp; Inspection 
Procedures, Staffing, Budget Aircraft Clearance 
and Monitoring 
Charts, NOTAM Data 
(except L) to Customer Read backs, Requests 
Local 
NOTAMs 
Reports, Project Plans 
NOTAM Data 
Chart Discrepancies ATIS &amp; L NOTAMs 
Operational Reports 
ALPA 
Safety 
ALR 
Airport 
Diagram 
Airport 
Diagram 
Verification Optional construction 
signage 
= missing feedback lines Certification, Inspection, 
Federal Grants 
Composite Flight Data, except L NOTAM Graphical Airport Data ATO: 
Terminal 
Services 
Pilot perspective 
information 
Construction information Blue Grass Airport 
Authority Procedures &amp; 
Standards Flight release, Charts etc. 
NOTAMs except L 
IOR, ASAP 
Reports 
Certification &amp; Regulation 
Courtesy of Lund University. Used with permission. 
50</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Operator Error: Traditional View
	
		Human error is cause of incidents and accidents 
		So do something about human involved (suspend, 
retrain, admonish) 
		Or do something about humans in general 
		Marginalize them by putting in more automation 
 Rigidify their work by creating more rules and 
procedures 
6</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>ComAir 5191 (Lexington) Sept. 2006  
Analysis using CAST by Paul Nelson, 
ComAir pilot and human factors expert 
(for report: http://sunnyday.mit.edu/papers/nelson-thesis.pdf 
14</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>19</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Jeppesen (2) 
Process Model Flaws 
 Believed Document Control System software always generated  
notice of received NFDC data requiring analyst evaluation.  
		Any extended airport construction included phase and time data as a normal part of FAA submitted paper work. 
Context in Which Decisions Made 
		The Document Control System software generated notices of received NFDC data. 
		Preferred Chart provider to airlines
. 
Feedback 
		Customer feedback channels are inadequate for providing information about charting inaccuracies. 
31</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Overcoming Hindsight Bias
	
		Assume nobody comes to work to do a bad job. 
		Assume were doing reasonable things given the complexities, 
dilemmas, tradeoffs, and uncertainty surrounding them. 
		Simply finding and highlighting peoples mistakes explains nothing. 
		Saying what did not do or what should have done does not 
explain why they did what they did. 
10</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Comair (Delta Connection) Airlines (2)
	
Unsafe Control Actions: 
		Internal processes did not provide LEX local NOTAM on the flight 
release, even though it was faxed to Comair from LEX 
		In order to advance corporate strategies, tactics were used that fostered work environment stress precluding crew focus ability during critical phases of flight. 
		Did not develop or train procedures for take off runway confirmation.
	
27</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>LEX Controller Operations
	
Safety Requirements and Constraints 
		Continuously monitor all aircraft in the jurisdictional airspace and 
insure clearance compliance. 
		Continuously monitor all aircraft and vehicle movement on the airport surface and insure clearance compliance. 
		Provide clearances that clearly direct aircraft for safe arrivals and departures. 
		Provide clearances that clearly direct safe aircraft and vehicle 
surface movement. 
		Include all Local NOTAMs on the ATIS broadcast. 
43</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>LEX Air Traffic Control Facility (2)
	
Mental Model Flaws 
		Believed verbal guidance requiring 2 controllers was merely a 
preferred condition. 
		Controllers would manage fatigue resulting from use of the 2-2-1 rotating shift. 
Context in Which Decisions Made 
		Requests for increased staffing were ignored. 
		Overtime budget was insufficient to make up for the reduced staffing. 
47</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Operator Error: Systems View (1)
	
		Human error is a symptom, not a cause 
		All behavior affected by context (system) in which occurs 
		Role of operators in our systems is changing 
		Supervising rather than directly controlling 
		Systems are stretching limits of comprehensibility 
		Designing systems in which operator error inevitable and then 
blame accidents on operators rather than designers 
7</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Comair (3)
	
Process Model Flaws: 
		Trusted the ATIS broadcast would provide local NOTAMs to crews.
	
		Believed tactics promoting corporate strategy had no connection to 
safety. 
		Believed formal procedures and training emphasis of runway confirmation methods were unnecessary. 
Context in Which Decisions Made: 
		In bankruptcy. 
28</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Comair (Delta Connection) Airlines
	
Safety Requirements and Constraints 
		Responsible for safe, timely transport of passengers within their 
established route system 
		Ensure crews have available all necessary information for each flight 
		Facilitate a flight deck environment that enables crew to focus on flight safety actions during critical phases of flight 
		Develop procedures to ensure proper taxi route progression and 
runway confirmation 
26</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Some Questions to Answer
	
		Why was the crew not told about the construction? 
		Why didnt ATC detect the aircraft was in the wrong 
place and warn the pilots? 
		Why didnt the pilots confirm they were in the right place? 
		Why didnt they detect they were in the wrong place?
	
24</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Jeppesen
	
Safety Requirements and Constraints 
		Creation of accurate aviation navigation charts and information 
data for safe operation of aircraft in the NAS. 
		Assure Airport Charts reflect the most recent NFDC data 
Unsafe Control Actions 
		Insufficient analysis of the software which processed incoming 
NFDC data to assure the original design assumptions matched 
those of the application. 
		Not making available to the NAS Airport structure the type of information necessary to generate the 10-8 Yellow Sheet airport 
construction chart. 
30</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Blame is the Enemy of Safety
	
		To prevent accidents in the future, need to focus on why 
it happened, not who to blame 
		Blame is for the courts, prevents understanding what occurred and how to fix it. 
5</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Root Cause Seduction
	
		Assuming there is a root cause gives us an illusion of 
control. 
		Usually focus on operator error or technical failures
	
		Ignore systemic and management factors 
		Leads to a sophisticated whack a mole game 
 Fix symptoms but not process that led to those symptoms 
 In continual fire-fighting mode 
 Having the same accident over and over 
3</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Analysis Results Format
	
		For each component, will identify: 
		Safety responsibilities 
		Unsafe control actions that occurred 
		Contextual reasons for the behavior 
		Mental (process) model flaws that contributed to it 
		Two examples will be done in tutorial. Lots of examples 
in the ESW book (chapters 6 and 11 as well as the ESW 
appendices). 
		Comair Lexington crash 
		Train Derailment (Niels Smit) 
13</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Mental Model Flaws: 
		Believed they were on runway 22 when the takeoff was initiated.
	
		Thought the taxi route to runway 22 was the same as previously 
experienced. 
		Believed their airport chart accurately depicted the taxi route to runway 22. 
		Believed high-threat taxi procedures were unnecessary. 
		Believed lights were out all over the place so the lack of runway lights was expected. 
21</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>The Airport Diagram 
What The Crew Had What the Crew Needed  
23</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Analyzing Accidents and Incidents
	
with CAST  
1</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Unsafe Control Actions:  
		Controller and Crew duty/rest regulations were not updated to be 
consistent with modern scientific knowledge about fatigue and its 
causes. 
		Required enhanced taxiway markings at only 15% of air carrier 
airports: those with greater than 1.5 million passenger  
enplanements per year .  
Mental Model Flaws 
		Enhanced taxiway markings unnecessary except for the largest US 
airports. 
		Crew/controller duty/rest regulations are safe. 
Context in Which Decisions Made 
		FAA funding battles with the US congress.  
		Industry pressure to leave duty/rest regulations alone. 
52</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Airport Safety &amp; Standards Office  
Process Model Flaws 
		Did not believe pilot input was necessary for development of safe 
surface movement operations. 
		No recognition of negative effects of changes on safety. 
		Belief that the accepted practice of using NOTAMs to advise crews of charting differences was sufficient for safety. 
Context in Which Decisions Made: 
		Priority was to keep Airport Facility Directory accurate. 
40</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Copyright Nancy Leveson, Aug. 2006
	
54</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Operator Error: Systems View (2)
	
		To do something about error, must look at system in which 
people work: 
		Design of equipment 
		Usefulness of procedures 
		Existence of goal conflicts and production pressures 
		Human error is a symptom of a system that needs to 
be redesigned 
8</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>5191 Flight Crew  
Safety Requirements and Constraints : 
		Operate the aircraft in accordance with company procedures, ATC 
clearances and FAA regulations. 
		Safely taxi the aircraft to the intended departure runway. 
		Take off safely from the planned runway 
Unsafe Control Actions: 
		Taxied to runway 26 instead of continuing to runway 22. 
		Did not use the airport signage to confirm their position short of the runway. 
		Did not confirm runway heading and compass heading matched (high threat taxi procedures0 
		40 second conversation violation of sterile cockpit 
20</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Process Model Flaws
	
		Believed verbal guidance (minimum staffing of 2 controllers) was 
clear. 
		Believed staffing with one controller was rare and if it was unavoidable due to sick calls etc., that the facility would coordinate the with Air Route Traffic Control Center (ARTCC) to control traffic. 
		Believed limiting overtime budget was unrelated to safety. 
		Believed controller fatigue was rare and a personal matter, up to the individual to evaluate and mitigate. 
Context in Which Decisions Made 
		Budget constraints. 
		Air Traffic controller contract negotiations. 
Feedback 
		Verbal communication during quarterly meetings. 
		No feedback pathways for monitoring controller fatigue. 
49</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Overcoming Hindsight Bias
	
		Need to consider why it made sense for people to do what 
they did 
		Some factors that affect behavior 
		Goals person pursuing at time and whether may have conflicted 
with each other (e.g., safety vs. efficiency, production vs. protection) 
		Unwritten rules or norms 
		Information availability vs. information observability 
		Attentional demands 
		Organizational context 
11</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>LEX Controller Operations (2)
	
Unsafe Control Actions 
		Issued non-specific taxi instructions; i.e. Taxi to runway 22 instead 
of Taxi to runway 22 via Alpha, cross runway 26. 
		Did not monitor and confirm 5191 had taxied to runway 22. 
		Issued takeoff clearance while 5191 was holding short of the wrong runway. 
		Did not include all local NOTAMs on the ATIS 
44</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Hindsight Bias 
Courtesy of Sidney Dkker. Used with permission. 
(Sidney Dekker, 2009) 
should have, could have, would have
	
9</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Common Traps in Understanding
	
Accident Causes
	
 Root cause seduction 
 Hindsight bias 
 Narrow views of human error 
 Focus on blame 
2</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Unsafe Control Actions  
		Did not use the FAA Human Factors Design Guide principles to 
update the NOTAM text format. 
		Limited dissemination of local NOTAMs (NOTAM-L). 
 Used multiple and various publications to disseminate NOTAMs,  none of which individually contained all NOTAM information.
	
Process Model Flaws: 
		Believed NOTAM system successfully communicated NAS changes. 
Context in Which Decisions Made 
		The NOTAM systems over 70 year history of operation. Format 
based on teletypes  
Coordination: 
		No coordination between FAA human factors branch and the NFDC for use of HF design principle for NOTAM format revision. 
33</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>LEX Air Traffic Control Facility
	
Safety Requirements and Constraints 
		Responsible for the operation of Class C airspace at LEX airport.
	
		Schedule sufficient controllers to monitor all aircraft with in 
jurisdictional responsibility; i.e. in the air and on the ground. 
Unsafe Control Actions 
		Did not staff Tower and Radar functions separately. 
		Used the fatigue inducing 2-2-1 schedule rotation for controllers.
	
46</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Summary  
		A why analysis, not a blame analysis 
		Construct the safety control structure as it was designed to 
work 
		Component responsibilities (requirements) 
		Control actions and feedback loops 
		For each component, determine if it fulfilled its responsibilities or provided inadequate control. 
 If inadequate control, why? (including changes over time) 
 Context 
 Process Model Flaws 
		For humans, why did it make sense for them to do what they did (to reduce hindsight bias) 
		Examine coordination and communication 
58</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Airport Safety &amp; Standards Office
	
Unsafe Control Actions: 
		The FAA review/acceptance process was inconsistent, accepting 
the original phase IIIA (Paving and Lighting) Safety Plans Construction Documents and then rejecting them during the transition between phases II and IIIA. 
		Did not require all stake holders (i.e. a Pilot representative was not present) be part of the meetings where methods of maintaining operational safety during construction were decided. 
		Focused on inaccurate runway length depiction without 
consideration of taxiway discrepancies. 
		Did not require methods in addition to NOTAMs to assure safety during periods of construction when difference between LEX Airport physical environment and LEX Airport charts. 
39</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Federal Aviation 
Administration 
Comair: Delta 
Connection 
Airport Safety &amp; 
Standards District 
Office LEX ATC 
Facility 
National 
Flight Data 
Center Jeppesen 5191 
Flight 
Crew Certification, Regulation, 
Monitoring &amp; Inspection 
Procedures, Staffing, Budget Aircraft Clearance and 
Monitoring 
Charts, NOTAM Data 
(except L) to Customer Read backs, Requests 
Local 
NOTAMs 
Reports, Project Plans 
NOTAM Data 
Chart Discrepancies ATIS &amp; L NOTAMs 
Operational Reports 
ALPA 
Safety 
ALR 
Airport 
Diagram 
Airport Diagram 
Verification Optional construction signage 
= missing feedback lines Certification, Inspection, 
Federal Grants 
Composite Flight Data, except L NOTAM Graphical Airport Data ATO: 
Terminal 
Services 
Pilot perspective 
information 
Construction information Blue Grass Airport 
Authority Procedures &amp; 
Standards Flight release, Charts etc. 
NOTAMs except L 
IOR, ASAP 
Reports 
Certification &amp; Regulation 
Courtesy of Lund University. Used with permission. 
29</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Federal Aviation Administration
	
Safety Requirements and Constraints 
		Establish and administer the National Aviation Transportation 
System. 
		Coordinate the internal branches of the FAA, to monitor and enforce compliance with safety guidelines and regulations. 
		Provide budgets which assure the ability of each branch to operate according to safe policies and procedures. 
		Provide regulations to ensure safety critical operators can function 
unimpaired. 
		Provide and require components to prevent runway incursions. 
51</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Physical System (Aircraft) 
		Failures: None 
		Unsafe Interactions 
		Took off on wrong runway 
		Runway too short for that aircraft to become safely 
airborne 
Then add direct controller of aircraft to determine why 
they were on that runway 
17</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Aircraft Flight Crew 
18</text>
        </slide>
      </slides>
      <videos/>
    </lecture>
  </lectures>
</doc>
