<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/</course_url>
    <course_title>Database Systems</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Business </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Distributed transactions (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text> so must write a PREPARE log record to disk before sending VOTE YES
then during recovery:
if PREPARE record in log, ask TC whether to commit
if no PREPARE record, roll back
PREPARE log record must list the locks held
since must re-acquire while waiting to hear from TC 
implication:
TC must remember whether transaction aborted or committed
 when can TC forget about a transaction?
after it hears all ACKs
 it then knows all subordinates know the outcome
what if subordinate never gets a PREPARE?
TC crashed, or network failure
subordinate can abort w/o asking TC 
what if subordinate sent VOTE YES, got no COMMIT/ABORT?
cannot unilaterally abort
must wait for TC, ask it what happened 
what should TC do during recovery if it crashes?
and what should it tell inquiring subordinates?
if TC could not have sent a commit
 it can just abort, reply "abort" if anyone inquires
if TC might have sent a commit
it cannot change its mind, since one subordinate may have released locks
how can the TC tell on recovery if it might have sent a commit msg?
it must log a commit record to disk after VOTEs collected, before sending COMMIT
on recovery, look in log:
if no commit record, can abort
if commit record, must answer "commit" to any subordinate queries
when can the TC forget about a transaction?
when no subordinate could possibly inquire
so TC keeps track of who has ACKed a COMMIT/ABORT msg
ACK implies subordinate has logged a commit/abort record
when all have ACKed, can forget
delete from memory
can GC that part of log
paper's "end" log record tells recovery not to bother
concerns
 2pc can block: subordinate may have to wait forever if TC down
while holding locks!
when can we resolve?
 subordinate can always abort if hasn't replied to PREPARE
when do we have to wait?
 TC crashes after last PREPARE sent, before first COMMIT sent
did it time out and abort locally and say NO to client??
did it commit locally and say YES to client?
want to limit window of vulnerability to TC crashes
as little time as possible when subordinates can't unilaterally abort
this one reason for separate PREPARE at very end
rather than yes/no replies to each action RPC
performance: 2pc adds burdens to TC, subordinates
log forces -- super painful
messages -- somewhat painful
TC must keep state -- somewhat painful
what are the forced log records? </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>           so TC must detect this situation, tell the other subordinate to abort+UNDO 
we need an "atomic commitment" protocol
all subordinates complete their tasks
or none 
Two-Phase Commit is the standard atomic commitment protocol 
2PC message flow for ordinary operation
Client TC Subordinate
 ---------&gt;
 -- SQL cmds --&gt;
acquire locks
if update,
append to log
update blocks
check deadlock, integrity, &amp;c
-- PREPARE --&gt;
 [log prepare or abort]
&lt;-- VOTE YES/NO -
 -
wait for all VOTEs
 [log com/ab]
&lt;-- OK/NOT OK --- -
-- COMMIT/ABORT --&gt;
[log commit or abort]
release locks
 &lt;-- ACK ----
 -
wait for all ACKs
 [log end]
notes for ordinary operation:
if subordinate voted YES
 doesn't know outcome until COMMIT/ABORT msg from TC
since some other subordinate might vote NO
thus must be prepared to do either
must also hold locks
 if subordinate voted NO
 for sure whole xaction will be aborted
 so subordinate can immediately undo and release locks
what if TC gets no response to a PREPARE?
net failure, or subordinate crashed
TC keeps sending PREPARE for a while
what should TC do if still no response?
abort, and send ABORT msgs
why is this safe?
at this point, TC has sent no COMMITs
all subordinates waiting for PREPARE or COMMIT
so no partial updates have been made visible
if subordinate crashed and restarted
 can't resume a process after crash
can roll back, using log
respond VOTE NO to TC's PREPARE
what if TC gets no response to a COMMIT?
re-send for a while
 what should TC do if still no response?
can TC decide to abort instead?
 no: other subordinates may have received COMMIT, released locks
so the TC must wait
 what should subordinate do when it restarts and runs recovery?
no commit record: should it therefor roll back?
 subordinate must distinguish crash before PREPARE from crash after
</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830 2010 Lecture 16: Two-Phase Commit 
last time we were talking about parallel DBs
partitioned data across multiple servers
we mostly discussed read-only queries
what about read/write queries? 
high-level model
a bunch of servers
 rows are partitioned over servers
each server runs a complete DB on its partition
SQL, locks, logging
external client connects to one server: Transaction Coordinator (TC)
sends it commands for whole system
TC farms them out to correct "subordinate" server 
TC collects results, returns them to client
TC and servers exchange messages over a LAN 
example transaction:
begin
SELECT A ...
 SELECT B ...
 UPDATE A ...
 UPDATE B ...
 commit 
diagram
A on S1, B on S2
client connects to S3
 S3 sends SELECT A to S1, gets result
S3 sends SELECT B to S2, gets result
93 sends UPDATE A to S1
 S3 sends UPDATE B to S2
 S3 sends "transaction completed" reply to client
but wait, this is not enough! 
what about locking?
each r/w acquires lock on server w/ the data
so: S1/A/S, S2/B/S, S1/A/S, S1/A/X 
when should the system release the locks?
remember we want strict two-phase locking
for serializability and no cascading aborts
so can't release until after commit
 so there must be at least one more message:
TC tells S1,S2 that the transaction is over 
can we get deadlock?
yes, for example if we run two of this transaction
in general a subordinate could block+deadlock at any read or write
let's assume a global deadlock detector
which notifies a transaction that it must abort
 so a subordinate might be aborted at any step
more generally, a subordinate can fail for a number of reasons
deadlock
 integrity check (e.g. insert but primary key not unique)
crash
 network failure 
what if one subordinate fails before completing its update?
and the other subordinate didn't fail?
 we want atomic transactions! </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text> (required to recover state after crash)
subordinate prepare / abort
TC commit / abort
subordinate commit / abort
can we get rid of any of these log forces?
TC recovery never really looks at abort records
so there is no point in writing them at all
if didn't force, would be like crash before sending PREPARE, -&gt; abort
subordinates need not force abort log records
if crash, lack of commit and abort and prepare -&gt; can abort unilaterally 
Presumed Abort protocol exploits these ideas
YES votes and TC commit work as before
 TC abort:
 don't log anything!
forget about xaction
send ABORT msgs (so subordinates can release locks)
don't bother collecting ACK msgs
TC recovery:
nothing in log for aborted xaction
TC response to queries:
if no record of xaction, reply "aborted" (hence the "presumed abort" name)
subordinate NO vote:
 don't force abort to log
release locks
why might we care about Presumed Abort?
after all, abort is much less common than commit 
we can speed up read-only xaction *commit* if we use PA
don't in general know in advance that an xaction will be r/o
TC sends out PREPAREs
 subordinates send READ-ONLY VOTE if could commit but read-only
if TC gets all READ-ONLYs
send COMMITs so subordinates can release locks
 TC forgets about xaction w/o logging anything
subordinates need not log anything either
that is, convert read-only commit to abort
if sub missed COMMIT msg, asks what happened, can it release locks
TC sees no record, says "abort", which is fine
if we weren't using PA, TC would have to force an explicit commit or
abort log record, and would see no performance win for r/o xactions 
how many forced log writes does TC make for committed transaction?
2PL r/w: 1
2PL r/o: 1
PA r/w: 1
PA r/o: 0 
can we have Presumed Commit?
 to speed up common case of r/w committed transactions?
if TC commits: don't force the commit log record to disk
if TC aborts: force the abort log record to disk
if TC crash+recover, subordinate asks whether an xaction committed,
TC replies "commit" if no record of xaction 
problem:
TC sends PREPAREs, gets some YES votes, crashes before seeing all, recovers
TC recovery sees no commit record, no abort record
PREPAREd subordinate asks TC if xaction committed
 TC answers "commit", since no record of this xaction
</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>                   
                    
                     oops, since some other subordinate might have voted NO but
TC crashed before seeing its vote 
fix to make Presumed Commit work
 TC, before sending PREPARE msgs, logs PREPARE w/ list of subordinates
crash recovery at TC restarts PREPARE processing 
summary of costs, counted as # forced writes at TC:
 TC r/w TC r/o SUB r/w SUB r/o
2PC 1 1 2 2 
PA 1 0 2 0 
PC 2 1 1 0 
XXX why does paper say PC requires forced commit at TC?
maybe TC got all yes votes, said "yes" to client, then crashed
after restart, can't contact one of subordinates
changes its mind to "abort"
XXX why does sub do a forced log write for PC?
so it doesn't change its mind about its PREPARE vote? 
when do systems use 2pc?
in a single machine room, for parallel DBs
could use to get atomicity across heterogeneous DBs
probably not done very much
many DBs have 2pc interfaces (separate prepare and commit),
but not very standard
could in principle use between different organizations
expedia, one log on united, on one AA
but not done in practice
WAN msgs too slow
don't want expedia's flaky TC to cause locks to be held at United! </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Query optimization (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>      
      number of subsets of set of size n = power set of n = 2^n
(string of length n, 0 if element is in, 1 if it is out; clearly, 2^n such strings)
(reduced an n! problem to a 2^n problem)
what's W? (n)
so actual cost is: 2^n * n
So what's the deal with sort orders? Why do we kee p interestin g sort orders? 
Selinger says: although there may be a 'best' way to compute ABC, there may also be ways that produce
interesting orderings -- e.g., that make later joins cheaper or that avoid &#64257; nal sorts.
So we need to keep best way to compute ABC for different possible join orders. 
so we multiply by "k" -- the number of interesting orders 
how are thin gs different in the real world? 
-real optimizers consider bushy plans (why?)
A 
D 	 B 
C E
-selectivity estimation is much more complicated than selinger says
and is very important. 
how does selin ger estimate the size of a join? 
-selinger just uses rough heuristics for equality and range predicates. 
-what can go wrong?
consider ABCD 
suppose sel (A join B) = 1
everything else is .1 
If I don't leave A join B until last, I'm off by a factor of 10 
-how can we do a better job?
(multi-d) histograms, sampling, etc. 
example: 1d hist 
Image by MIT OpenCourseWare.0 10k 20k 30k 40kSalary &gt; 25k
.2 + .1 = .3.4 .1 .4 .1</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> CPU Costs! 
I/O costs are comparable
690 / 1000 seconds in sort merge are due to the costs of sorting
17.4 in the case of CPU for grace/hybrid! 
Will this still be true today?
(Yes) 
Selinger 
Famous paper. Pat Selinger was one of the early System R researchers; still active today. 
Lays the foundation for modern query optimization. Some things are weak but have since been improved 
upon. 
Idea behind query optimization:
(Find query plan of minimum cost ) 
How to do this? 
(Need a way to measure cost of a plan (a cost model) ) 
single table operations 
how do i compute the cost of a particular predicate?
compute it's "selectivity" - fraction F of tuples it passes 
how does selinger de&#64257; ne these? -- based on type of predicate and available statistics 
what statistics does system R keep? 
-relation cardinalities NCARD 
-# pages relation occupies TCARD 
-keys in index ICARD 
-pages occupied by index NINDX 
Estimatin g selectivit y F: 
col = val 
F = 1/ICARD()F = 1/10 (where does this come from?) 
col &gt; val 
high key - value / high key - low key1/3 o.w. 
col1 = col2 (key-foreign key)
1/MAX(ICARD(col1, col2))1/10 o.w. 
ex: suppose emp has 10000 records, dept as 1000 records
total records is 10000 * 1000, selectivity is 1/10000, so 1000 tuples expected to pass joinnote that selectivity is de&#64257; ned relative to size of cross product for joins! 
p1 and p2 
F1 * F2 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>example: 2d hist 
40k 80k
Salary60
30AgeSalary &gt; 1000*age
area below 
line.05 .05 .1
.2 .1 .1
.1 .1 .1
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text> p1 or p2 
1 - (1-F1) * (1-F2) 
then, compute access cost for scanning the relation.
how is this de&#64257; ned? 
(in terms of number of pages read) 
equal predicate with unique index: 1 [btree lookup] + 1 [heap&#64257; le lookup] + W 
(W is CPU cost per predicate eval in terms of fraction of a time to read a page ) 
range scan: 
clustered index, boolean factors: F(preds) * (NINDX + TCARD) + W*(tuples read) 
unclustered index, boolean factors: F(preds) * (NINDX + NCARD) + W* (tuples read)
unless all pages &#64257; t in buffer -- why? 
... 
seq (segment) scan: TCARD + W*(NCARD) 
Is an index always better than a segment scan? (no) 
multi-table operations 
how do i compute the cost of a particular join? 
algorithms:
NL(A,B,pred)
C-outer(A) + NCARD(outer) * C-inner(B) 
Note that inner is always a relation; cost to access depends on access methods for B; e.g.,
w/ index -- 1 + 1 + W
w/out index -- TCARD(B) + W*NCARD(B)
C-outer is cost of subtree under outer
How to estimate # NCARD(outer)? product of F factors of children, cardinalities of children
example: 
Merge_Join_x(P,A,B), equality pred 
C-outer + C-inner + sort cost 
(Saw cost models for these last time) 
At time of paper, didn't believe hashing was a good idea 
Overall plan cost is just sum of costs of all access methods and join operators
Then, need a way to enumerate plans s
A C1F1F1F2 NCARD A x NCARD BF2
B C2
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Now, join R1 with S1, R2 with S2, R3 with S3
Note -- need 1 page of memory per partition. Do we have enough memory?
We have |R| / M partitions
M &#8805; sqrt(|R|) 
worst case
|R| / sqrt(|R|) = sqrt(|R|) partitions 
Need sqrt(|R|) pages of memory b/c we need at least one page per partition as we write out (note that simple
hash doesn't have this requirement)
I/O:
read R+S (seq)
write R+S (semi-random)
read R+S (seq) 
also 3(|R|+|S|) I/OS
What's hard about this? 
When does grace outperform simple? 
(When there are many partitions, since we avoid the cost of re-reading tuples from disk in building partitions )
When does simple outperform grace?
(When there are few partitions, since grace re-reads hash tables from disk ) 
So what does Hybrid do?
M = sqrt(|R|) + E
Make &#64257;rst partition of size E, do it on the &#64258;y (as in simple)
Do remaining partitions as in grace. 
70 
123456789 
|R|/M 
Why does grace/hybrid outperform sort-merge? 0 7 14 21 28 35 42 49 56 63 I/O (r elative to simple with |R| = M) Grace 
Simple 
Hybrid </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Lecture 9 
10/8/09 
Query Optimization
Lab 2 due next Thursday. 
M pages memory
S and R, with |S| |R| pages respectively; |S| &gt; |R|
M &gt; sqrt(|S|)
External Sort Merge
split |S| and |R| into memory sized runs
sort each
merge all runs simultaneously 
total I/O 3 |R| + |S|
(read, write, read) 
"Simple" hash 
given hash function h(x), split h(x) values in N rangesN = ceiling(|R|/M) 
for (i = 1&#8230;N)
for r in R 
if h&#174; in range i, put in hash table Hr
o.w. write out 
for s in S 
if h(s) in range i, lookup in Hr
o.w. write out 
total I/O 
N (|R| + |S|) 
Grace hash: 
for each of N partitions, allocate one page per partitionhash r into partitions, &#64258; ushing pages as they &#64257; ll 
hash s into partitions, &#64258; ushing pages as they &#64257; ll 
for each partition p
build a hash table Hr on r tuples in phash s, lookup on Hr 
example: 
R = 1, 4, 3, 6, 9, 14, 1, 7, 11S = 2, 3, 7, 12, 9, 8, 4, 15, 6
h(x) = x mod 3 
R1 = 3 6 9 
R2 = 1 4 1 7 
R3 = 14 11 
S1 = 3 12 9 15 6 
S2 = 7 4 
S3 = 2 8 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>                  Iterate over plans, pick one of minimum cost 
Problem: 
Huge number of plans. Example: 
suppose I am joining three relations, A, B, C
Can order them as: 
(AB)CA(BC) 
(AC)B 
A(CB) (BA)CB(AC) 
(BC)A 
B(AC) (CA)B 
C(AB) 
(CB)A C(BA) 
Is C(AB) different from (CA)B?Is (AB)C different from C(AB)?
yes, inner vs. outer 
n! strings * # of parenthetizations 
how many parenthetizations are there? 
ABCD --&gt; (AB)CD A(BC)D AB(CD) 3
 XCD AXD ABX * 2 
=== 
6 --&gt; (n-1)!
==&gt; n! * (n-1)!
6 * 2 == 12 for 3 relations
Ok, so what does Selinger do? 
Push down selections and projections to leaves
Now left with a bunch of joins to order. 
Selinger simpli&#64257; es using 2 heuristics?  What are they? 
- only left deep; e.g., ABCD =&gt; (((AB)C)D) show 
-ignore cross products 
e.g., if A and B don't have a join predicate, doing consider joining them 
still n! orderings. can we just enumerate all of them? 
10! -- 3million 
20! -- 2.4 * 10 ^ 18
so how do we get around this? 
</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text> 
 
                       
                       
                       
 
     Estimate cost by dynamic programming: 
idea: if I compute join (ABC)DE -- I can &#64257; nd the best way to combine ABC  and then consider all the ways to
combine that with DE. 
i can remember the best way to compute (ABC), and then I don't have to re-evaluate it. best way to do ABC
may be ACB, BCA, etc -- doesn't matter for purposes of this decision. 
algorithm : compute optimal way to generate every sub-join of size 1, size 2, ... n (in that order). 
R &lt;--- set of relations to join
for &#8706; in {1...|R|}:
for S in {all length &#8706; subsets of R}:
optjoin(S) = a join (S-a), where a is the single relation that minimizes:
cost(optjoin(S-a)) +
min cost to join (S-a) to a +
min. access cost for a
example: ABCD 
only look at NL join for this example 
A = best way to access A (e.g., sequential scan, or predicate pushdown into index...)
B = " " " " B 
C = " " " " C
D = " " " " D 
{A,B} = AB or BA
{A,C} = AC or CA
{B,C} = BC or CB
{A,D}
{B,D}
{C,D}
{A,B,C} = remove A -  compare A({B,C}) to ({B,C})A
remove B - compare ({A,C})B to B({A,C})
remove C - compare C({A,B}) to ({A,B})C 
{A,C,D}
{A,B,D}
{B,C,D} 
{A,B,C,D} = remove A -compare A({B,C,D}) to ({B,C,D})A
 .... 	 remove B 
remove C 
remove D 
Complexity:
number of subsets of size 1 * work per subset = W+
number of subsets of size 2 * W +
... 
number of subsets of size n * W+ 
n + n + n ... n 
1 2 3 n 
</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Parallel databases (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>    no l ock server requi red 
--------- CLOSURE ---------- -
let's step back for a minute
 parallel programming is usually viewed as very difficult
 hard to reason about parallel algorithms
 hard to get good speedup
    hard to avoi d races, maintain correctness
 has turned out to be relatively easy to use parallel DBs -- why?
 1. relational model abstracts from physical layout
       can parti tion &amp;c w/o changi ng apps
 2. transactions and locking take care of parallel correctness
       on ce you're willing to program in  transaction model
 3. applications often have lo ts of inherent parallelism
       OLTP queri es on di fferent data -- due to many independent users
       scan &amp;c on di fferent parts of same tabl e </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>----------- OLTP ------------- -
how to partition data for OLTP
 suppose you are ebay
 item(item_id, title, &amp;c)
 bid(bid_id, amt, time, user_id, item_id)
  SELECT * FRO M bid, item WHERE item_id = 3; to display an item page w/ bids
  how abou t this part ition plan?
    items on server 1, bi ds on server 2
  how abou t this part ition?
    even i tem_ids on server 1, odd on server 2
    even bi d_ids on server 1, odd on server 2
 a good partition plan:
    assi gn item rows to  servers with hash(item_id)
    assi gn bid rows to servers wi th hash(i tem_i d) (same hash functi on)
    then di splaying item page onl y hits one server
      can di splay many di ff item pages in  parallel w / many servers
    and adding a bid hits on ly one server / one disk arm
  suppose you want to disp lay some user i nfo for each bi d too? e.g. user name
    can't parti tion users by i tem_id!
    hash by user_i d, hit on server per user
    or dupl icate some user i nfo in bid tabl e
  usual ly can't fi nd one parti tion plan that makes every query go to one server!
    so dupl icate data, or cache, or pre- compu te periodically , or bu y lots of servers 
suppose we spend money to increase from 10 to 100 servers
 are we going to see speedup of one query?
  are we going to be able to proce ss a larger # of queries in one second?
when mi ght we get l inear 10x  increase in performance? 
what might prevent us from getting 10x?
 bad load balance! or " skew "
  1 server wi th lots of work, 99 servers i dle, no speedup at al l
 some items much more popular than others
 accidentally put too many popular items on same server
 or maybe partition plan forces us to hit more servers  as # servers increases
 e.g. if bids for an i tem were spread over many servers 
what are the options for partitioning?
  usual ly "horizontal ": partition the row s (not the colu mns)
    each server respon sible for some set of rows
 1. hash on some column to get server #
 2. assign ranges of keys in some column to servers
 3. round-robin inserts (essentially random)
 you can also throw in " replication " for read-mostl y data, that
 are needed everywhere
when to use each kind of partitioning? 
load balance ? (data vs accesses)
    round-robi n is perfect
    hash mi ght work wel l, might not
</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>the 
results to the other tabl es, and fi nish the joi n&#8230; this might move l ess data. 
big tree of joins?
  re-parti tion after each joi n, send over network 
There is a non trivial query optimization problem here:
 - multi-site executi on (computati on mi ght need to change)
 - cost of network vs cpu vs di sk 
it's one more di mensi on than l ocal query opti mization... 
how fast does the network have to be?
 each host needs about as fast as its disk
  entire LAN needs n*di sk total  throughput
    but onl y 1*disk to any one host
 can we buy such LANs?
    you can get 10 gbi t host i nterfaces (i .e. 10x faster than one di sk)
    you can get 10 gbi t switches wi th modest #s of ports
  how to bui ld a swi tch that can handl e n*host total  traffi c?
 [4x4 crossbar]
 but you can't build very big+fast crossbars , maybe 16 ports
  if more? mu ltiple lev els, needs t o be fat in the middle
    can't just have one switch in the middle unless it has fast er links than hosts
      u sually a low  limit to how fast you can make the l inks
      so i nstead use more l inks
 C1 C2 C3 C4
 E1 E2 E3 E4
 each Ex has four hosts
 each Ex has link to each of Cx
 need to spread load from each Ex over all Cxs
suppose we spend money to in crease from 10 to 100 servers
 are we going to see speedup of one big query?
  are we goi ng to be abl e to join larger t ables in  same t ime? 
are we going to get a 10x increase in performance?
  when mi ght we get 10x?
  what mi ght prevent us from getti ng 10x? 
what about indices?
  might be abl e to get away wi th local indices
 each server just indexes its own partition
    an d only uses local in dices
    mi ght hel p with e.g. fi ltering scans
  what if you are look ing for one item?
    1. ask all serv ers to look up in local in dex
 2. arrange to onl y look for keys i n parti tion columns, then onl y ask one server
    3. painfully main tain global in dex 
how to cope with locking in a shared-nothing DB?
  do we need a gl obal lock server?
  if a row /page is read/ written, initially done on home serv er
 so each server can keep lock table for its own data
    don't need to l ock another server's data </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830  2009 Lecture 16: Parallel Databases 
today's topi c: parallel databases
  how to get more performance than is possible w/ one computer 
--------- MOTIVATION ----------- -
why might one CPU/one disk not yield enough performance?
 can only scan big tables at 50 MB/sec
  can only read random rows, or probe indices, 100 times/second
  CPU needed to push data around, hash, l ock, &amp;c
who mi ght have workl oads too big for one CPU/disk?
  data-warehouse gian t scans/joi ns/aggregati ons
 ebay-style OLTP, maybe 100s of updates/sec, 1000s of queries/sec 
--------- 3 STRATEGIES ----------- -
solution 1: SMP hardware
 [diagram: CPUs, sys bus, RAM, many disks]
  each cl ient gets a thread
  shared memory for l ock tabl e, buffer cache 
SMP i s very conveni ent for the software
  if you already used threads and latches, you're practi cally there
 mysql, postgresql, simpledb
 any transaction can use any disk
 see each others' updates and locks in shared mem
what kinds of workloads w ill ben efit from SM P hardware?
  OLTP i f many concurrent cl ients, not many l ocking confl icts
    keep di fferent di sks and di fferent CPUs busy
 single big join?
 if tables striped over disks
    if we write specializ ed parallel join  algorit hms
performance goals:
 what might we expect from our money?
 e.g. if we spend 10x as much money on a fancy SMP server
 1. speedup : make exi sting workl oad take 1/10th as much ti me
     usual ly no speedup for si ngle OLTP transacti on
     can get speedup for bi g joins by spl itting work over servers
 2. scaleup : run 10x larger job in same time
     coul d expect to handl e more OLTP  xacti ons concurrentl y on more servers
     can joi n bigger tabl es in same ti me
 we really want *linear*  speedup and/or scaleup
    don't want to pay 10x mo ney, get only 2x improv emen t
why isn't SMP hardware the final answer?
  shared memory and I/O very ha rd for more than a few dozen CPUs
    want any CPU to r/w any di sk buffer or l ock
    want any CPU to r/w any di sk at 50 MB/sec
    hard to bui ld the interconnect for thi s!
 16 CPU SMP currently cheap (i.e. about 16x one CPU)
  256 CPU SMP (really NUMA) possible but MUCH more expensive per CPU
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>solution 2:  shared disk
 get rid of shared memory!
  buy network-attached di sks -- a " SAN"
  and l ots of i ndependent DB servers attached to the SAN
  network al lows any DB server to r/w any di sk 
you only have to send disk I/O over interconnect
    not CPU memory traffi c as wel l, as in SMP 
shared disk pros:
 cheaper per CPU than SMP for many CPUs
    no expensi ve interconnect for shared memory
 can use commodity server boxes
  s/w parti ally the same as for si ngle server
    any CPU can r/w any di sk
 can act like solitary srvr w/ many disks
shared disk cons:
 the network may be expensive  -- 50 MB/sec per di sk?
 the disks may be expensive -- need special SAN interface 
** how to deal with locks?
    server 1 and server 2 want to update the same row
    central  lock manager?
 stripe locks over servers?
** how to deal with dirty buffers?
    server 1 wri tes a page
 server 2 wants to read that page
 can't read its own cached copy
      can't read from the di sk!
 server 1 must invalidate or update other cached copies
    server 2 must know to ask server 1 for l atest data
 lots of inter-server lock and buffer chatter!
  in practi ce cannot scale shared disk beyond dozens of CPUs
    onl y somewhat better than SMP 
 (Oracle promises in finite scalabilit y, but anecdot al evidence 
  seems to say typical 2 nodes no  more than 6-8 nodes seen in 
  practi ce, and often used mostl y for redundancy)
how to do better than SMP or shared disk?
 the real problem is not the hardware, but the DB software's
  desi re to r/w any di sk from any CPU 
solution 3: shared nothing 
DON'T let s/w r/w any disk from any CPU!
 each CPU has a disk (or a few disks)
  each  CPU only allow ed to direct ly use data on its own disks
  so: commodi ty server/di sk boxes,  LAN connecting them, clever s/w
 all the cool kids are playing this game
    vertica, DB2 parallel,  netezza, teradata, ad-hoc setups at facebook &amp;c 
** shared nothing questions:
  how to parti tion data over servers
 how to run big join and aggregate queries
  how to run OLTP queri es
  how to ensure l oad bal ance
  how to deal  with locks and recovery
</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>    range mi ght or mi ght not 
associative searches  want al l of same key toge ther, i.e. bids by item_id
    hash and range work wel l
    round-robi n does not
sequential access , e.g. all sales between May 10 and May 15
    hash and round-robi n: must tal k to all servers
 range: maybe talk to just one server
programmer convenience?
    range requi res thought, maybe re-parti tioning as data grows
    hash and round-robi n are automati c
----------- OLAP ------------- -
what about big scan/filter/join/aggregate, for data warehousing?
  on shared-nothi ng paral lel DB 
Sam al ready anti cipate a bi t of thi s&#8230; let's see more&#8230; 
SELECT name WHERE eyes = 'blue' and hair = 'red';
  if partitioned on eyes or h air?
  otherwi se?
  (advantage i n touchi ng many mach ines? depen ds on intra-query parallelism)
select type, avg(price) ... group by type;
 if partitioned by type?
  otherwi se?
    local part ial part ition
    report type, sum, n to queryi ng machi ne
      or i f many types, machi ne chosen by hash(type) 
select ... from big, small where big.x = small.y;
  equi-join big tabl e against small table
 send copy of small to each server
  do a hash joi n on each server  against its partition of big table
select ... from big1, big2 where big1.x = big2.y;
 equi-join two big tables
  if big1 partitioned on x, and big2 part itioned on y, join locally
  Otherwise? I mak e sure I w ill get  that propert y by re-partitioning tables!!
  let's re-parti tion bo th, on big1.x and big2.y
  scan bi g1, send each row to server[hash(x)]
    to temporary storage: memory or di sk
  scan bi g2, send each row to server[hash(y)]
 same hash!
 we sent each entire table across the LAN
    but now every pai r of rows that mi ght joi n are on same server
  each server now separately uses a standard join algorithm
 Even better: 
- I coul d appl y filters first.. 
- I coul d use semi -join: send onl y the joi n column from one tabl e, perf orm the semi-join, send </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Introduction (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>&#8226;	the soldiers (5000) need to report daily misdeeds in a daily-log, and report 
money expenses and collections 
&#8226;	the semi-public interface accessible by other bosses I collab orate with 
(searc h for cops on our books, check areas we already cover, etc..) 
person
organizationlogoperation
accountsname nickname phonelog_idauthortitlesummaryname desc$$coverup-nameinvolvecollaboration_withnamebossrankaccount-numberfalse-identitybalance
Figure 3: What data to store in my Ma&#64257;a database. 
3.1 An o&#64256;er you cannot refuse 
I make you an o&#64256;er you cannot refuse: &#8220;you are hired to create my Ma&#64257;a 
Information System, if you get it right you will have money , sexy cars, and a 
great life. If you get it wrong... well you don&#8217;t want to get it wrong&#8221;. 
As a &#64257;rst attempt, you think about just using a &#64257;le system: 
1.	What to represen t:, what are the key entities in the real world I need 
to represen t? how many details? 
2.	How to store data: maybe we can use just &#64257;les: people.txt, organiza&#173;
tions.txt, operations.txt, money .txt, daily-log.txt. Each &#64257;les contains a 
textual represen tation of the information with one item per line. 
3.	Control access creden tials at low granularit y: accoun tants should 
know about money movemen t, but not the name s and addresses of our 
soldiers. Soldiers should know about operations, but not access money 
information 
4.	How to access data: we could write a separate procedural program 
opening one or more &#64257;les, scanning through them and reading/writing 
information in them. 
5.	Access patterns and performance: how to &#64257;nd shop we didn&#8217;t col&#173;
lected money from for the longest time (and at least 1 month)? scan the 
huge operation &#64257;le, sort by time, pick the oldest, measure time? (need to 
be timely or they will stop paying, and this get the boss mad... you surely 
5 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>5 External schema A set of views over the logical schema, that predicates how 
users see/access data. (e.g., a set of views for the accoun tants). It is often not 
physically materialized, but main tain as a view/query on top of the data. 
Let try to show only coverup names of operations worth less or equal to $5M 
and the nicknames of all people involved using a view (see Figure 6): 
CREATE VIEW nick-cover AS 
SELECT nickname, coverup_name 
FROM operation o, involved i, person p 
WHERE p.name = i.person AND 
i.oper_name = o.name AND
o.econ_val &lt;= 5M;
schifezzalaundromatlungoirish pubbaffolaundromatnicknamecoverupnick-cover
Figure 6: Simple External Schema for a portion of our Ma&#64257;a database. 
What&#8217;s next? 
Next week lessons introduce more formally the relational model (and some of 
its history) and how to design the schema of a database. After that we will dive 
into the DBMS internals and study &#8220;how&#8221; DBMS are internally architected 
to achieve all the functionalities we discuss ed. Later on we will study how to 
guaran tee transactional behaviors, and how to scale a DBMS beyond a single 
node. The last portion of the course is devoted to more esoteric topics from 
recen t advances in database researc h. 
11 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Figure 2: Ma&#64257;a hierarc hy. 
large, so there is quite a bit of things going on at any momen t (i.e., many people 
accessing the database to record or read information). 
I need to store information about: 
&#8226;	people that work for me (soldiers, caporegime, etc..) 
&#8226;	organizations I do business with (police, &#8217;Ndrangheta, politicians) 
&#8226;	completed and open operations: 
&#8211;	protection rackets 
&#8211;	arms tra&#64259;c king 
&#8211;	drug tra&#64259;c king 
&#8211;	loan sharking 
&#8211;	control of contracting/p olitics 
&#8211;	I need to avoid that any of may man is involved in burglary , mugging, 
kidnapping (too much police atten tion) 
&#8211;	cover-up operations/business es 
&#8211;	money laundry and funds tracking 
&#8226;	assignmen t of soldiers to operations 
etc...&#8226; 
I will need to share some of this information with external organization s I 
work with, protecting some of the information. 
Therefore I need: 
&#8226;	the boss, underb oss and consigliere should be able to access all the data 
and do any kind of operations (assign soldiers to operations, create or 
shutdown operations, pay cops, check the total state of money movemen ts, 
etc...) 
&#8226;	the accoun tants (20 of them) access to perform money book-keeping (trac k 
money laundering operations, move money from bank to bank, report 
bribing expenses) 
4 
Consigliere
Caporegime
Soldiers
AssociatesBoss
Underboss
Caporegime
SoldiersCaporegime
Soldiers
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>shifezzatony789lungo456mikebaffocarlo123namenicknamephonepersonoperation..laundromatirish pubirish pubchocolatesnowflakecoverupcaffe$10Mecon_valtitledescr....$2M...$5Mchocolatemikechiefsnowflakesoldtonycarlosnowflakechiefpers_nameoper_namerolsinvolvedFigure 4: Simple Logical Schema for a portion of our Ma&#64257;a database. 
What about the physical organization of the data? As a database user you 
can ignor e the problem , thanks to the physical indep endence! As a studen t of 
this class you will devote a lot of e&#64256;ort in learning how to best organize data 
physically to provide great performance to access data. 
3.4 Accessing the data (transactionally) 
As we introduced before databases provide high-lev el declarativ e query lan&#173;
guages. The key idea is that you describ e &#8220;what&#8221; you want to access, rather 
than &#8220;how&#8221; to acces s it. 
Let&#8217;s consider the following operations you want to do on data, and how we 
can represen t them using the standard relational query language SQL: 
&#8226;	Whic h operations involve &#8220;Tony Schifezza&#8221;? 
SELECT oper_name 
FROM involved 
WHERE person = "tony"; 
&#8226;	Given the &#8220;laundromat&#8221; operation, get the phone numbers of all the people 
involved in operations using it as a cover up. 
SELECT p.phone 
FROM person p, operation o, involve i 
WHERE p.name = i.person AND 
i.oper_name = o.name AND 
o.coverup_name = "laundromat"; 
&#8226;	Reassign Tony&#8217;s operations to Sam and remo ve Tony from the database 
(he was the mole). 
BEGIN 
UPDATE involved i SET pers_name="sam" WHERE pers_name="tony"; 
DELETE FROM person WHERE name = "tony"; 
COMMIT 
9 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>3 
DB"a collection of structure data"DBMS"a system to create, manipulate, access databases(mediate access to the data)"APP1APP2Figure 1: What is a database managemen t system? 
&#8226;	Ubiquit y (anywhere from your smartphone to Wikip edia) 
&#8226;	Real world impact: software mark et (roughly same size as OS mark et 
roughly $20B/y). Web sites, big companies, scien ti&#64257;c projects, all manage 
both day to day operations as well as business intelligence + data mining. 
&#8226;	You need to know about datab ases if you want to be happy! 
The goal of a DBMS is to simplify the storing and accessing of data. To 
this purp ose DBMSs provide facilities that serve the most common operations 
performed on data. The database comm unity has devoted signi&#64257;can t e&#64256;ort in 
formalizing few key concepts that most application s exploit to manipulate data. 
This provides a formal ground for us to discuss the application requirements 
on data storage and access, and compare ways for the DBMS to meet such 
requiremen ts. This will provide you with powerful conceptual tools that go 
beyond the speci&#64257;c topics we tackle in this class, and are of general use for any 
application that needs to deal with data. 
Now we proceed in showing an example, and show how hard is doing things 
without a DB, later we will introduce formal DB concepts and show how much 
easier things are using a DB. 
Ma&#64257;a Example 
Today we cover the user perspective, trying to detail the many reason we want 
to use a DBMS rather than organi zing and accessing data directly , for example 
as &#64257;les. 
Let us assume I am a Ma&#64257;a Boss (Note: despite the accen t this is not the 
case, but only hypothetical!) and I want to organize my group of &#8220;picciotti&#8221; 
(sicilian for the criminals/bad guys working for the boss, a.k.a the soldiers, see 
Figure 2) to achieve more e&#64259;ciency in all our operations. I will also need a 
lot of book-keeping, securit y/priv acy etc.. Note that my organization is very 
3 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>We could represen t this data accordin g to many di&#64256;eren t data models: 
hierarc hies &#8226;
&#8226; objects
&#8226; graph 
&#8226; triples 
etc..&#8226; 
Let&#8217;s try using an XML hierarc hical &#64257;le: 
&lt;person&gt; 
&lt;name&gt; &lt;/name&gt; 
&lt;nickname&gt; &lt;/nickname&gt; 
&lt;phone&gt; &lt;/phone&gt; 
&lt;operation&gt; 
&lt;op_name&gt; &lt;/op_name&gt; 
&lt;description&gt; &lt;/description&gt; 
&lt;econ_value&gt; &lt;/econ_value&gt; 
&lt;coverup_name&gt; &lt;/coverup_name&gt; 
&lt;/operation&gt; 
&lt;/person&gt; 
Operations are duplicated in each person, this migh t make the update very 
tricky (inconsistencies) and the represen tation very verbose and redundan t. 
Otherwise we can organize the other way around with people inside operations, 
well we would have people replicated. 
Another possibilit y is using a graph structure with people, names, nick&#173;
names,phones, operation names etc.. as nodes, and edges to represen t relation&#173;
ships between them. Or we could have objects and metho ds on them, or triples 
like &lt;carlo,is a,person&gt;, &lt;carlo,phone,5554348882&gt; etc.. 
Di&#64256;er ent data models are more suite d for di&#64256;er ent problems. 
They di&#64256;eren t expressiv e power and di&#64256;eren t strengths depending on what 
data you want to represen t and how you need to access them. 
Let&#8217;s choose the relation al data model and represen t this problem using &#8220;ta&#173;
bles&#8221;. Again there are many ways to structure the represen tation, i.e., di&#64256;eren t 
&#8220;conceptual/logical schemas&#8221; that could captu re the realit y are modeling. For 
example we can have a single big table with all info together... again, is redun&#173;
dant and migh t slow down all the access to data. 
The &#8220;database design&#8221; is the art of capturing a set of real world concepts 
and their relations in the best possible organization in a database. A good 
represen tation is shown in Figure 4. It is not redundan t and contains all the 
information we care about. 
8 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>don&#8217;t want that, and make sure no one is accessing it right now). &#8220;Tony 
Schifezza&#8221; is a mole, we need to &#64257;nd all the operations and people he was 
involved or knew about and shut them down... quick... like REAL quick!!! 
6.	Atomicit y: when an accoun tant moves money from one place to another 
you need to guaran tee that either money are remo ved from accoun t A and 
added to accoun t B, or nothing at all happ ens... (You do not want to have 
money vanishing, unless you plan to vanish too!). 
7.	Consistency: guaran tee that the data are always in a valid state (e.g., 
there are no two operation s with the same name) 
8.	Isolation: multipl e soldiers need to add to daily-log.txt at the same time 
(risk is that they override each other work, and someone get &#8220;&#64257;red&#8221; be&#173;
cause not productiv e!!) 
9.	Durabilit y: in case of a computer crash we need to make sure we don&#8217;t 
lose any data, nor that data get scram bled (e.g., If the system says the 
paymen t of a cop went through, we must guaran tee that after reboot the 
operation will be presen t in the system and completed. The risk is police 
taking down our operation!) 
Using the &#64257;le system, you realize that most probably you will fail, and that 
can be very dangerous... Luckily you are enrolled in 6.830/6.814 and you just 
learned that: Datab ases address all of these issues!! you migh t have a chance! 
In fact, you migh t notice that the issues listed above are already related to the 
three concepts we mentioned before: 1-3 are problems related to Data Model, 
4-5 are problems related to the Query language and 6-9 are problems related to 
Transactions. 
So let&#8217;s try to do the same with a &#8220;database&#8221; and get the boss what he needs. 
3.2 More on fundamen tal concepts 
Database are a micro cosm of computer science, their study covers: languages, 
theory , operating systems, concurren t programming, user interfaces, optimiza&#173;
tion, algorithms, arti&#64257;cial intelligence, system design, parallel and distributed 
systems, statistical techniques, dynamic progr amming. Some of the key con&#173;
cepts we will investigate are: 
Represen ting Data We need a consisten t structur ed way to represen t data, 
this is importan t for consistency , sharing, e&#64259;ciency of access. From database 
theory we have the right concepts. 
&#8226;	Data Model: a set of constructs (or a paradigm) to describ e the organiza&#173;
tion of data. For example tables (or more precisely relations ), but we could 
also choose graph, hierarc hies, objects, triples &lt;sub ject,pr edicate,ob ject&gt;, 
etc.. 
6 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#8226;	Conc eptual/L ogical Schema: is a description of a particular collection 
of data, using the a given data model (e.g., the schema of our Ma&#64257;a 
database). 
&#8226;	Physic al Schema: is the physical organization of the data (e.g., data and 
index &#64257;les on disk for our Ma&#64257;a database). 
Declarativ e Querying and Query Processing a high-lev el (typically declar&#173;
ative) language to describ e operations on data (e.g., queries, updates). The goal 
is to guaran tee Data indep endenc e (logical and physic al), by separating &#8220;what&#8221; 
you want to do with data from &#8220;how&#8221; to achieve that (more later). 
&#8226;	High level language for accessing data 
&#8226;	&#8220;Data Indep endence&#8221; (logical and physical) 
&#8226;	Optimization Techniques for e&#64259;cien tly accessing data 
Transactions 
&#8226;	a way to group actions that must happ en atomically (all or nothing) 
&#8226;	guaran tees to move the DB content from a consisten t state to another 
&#8226;	isolate from parallel execution of other actions/transactions 
&#8226;	recoverable in case of failure (e.g., power goes out) 
This provide the application with guaran tees about a group of actions even 
in presence of concurrency and failures. It is a unit of access and manipulati on 
of data. And signi&#64257;can tly simplify the work of application developers. 
This course covers these concepts, and goes deep into the investigation of 
how modern DBMS are designed to achieve all that. We will not cover the more 
arti&#64257;cial-in teligence / statistical / mining related areas that are also part of 
database researc h. Instead, we will explore some of the recen t advanc ed topics 
in datab ase research&#8212;see class schedule to get an idea of the topics . 
3.3 Back to our Ma&#64257;a database 
What features of our organization shall we store? How do we want to capture 
them? Choose a level of abstraction and describ e only the relevant details (e.g., 
I don&#8217;t care about favorite movies for my soldiers, but I need to store their 
phone numbers). Let&#8217;s focus on a subset: 
&#8226;	each person has real name, nickname, phone number 
&#8226;	each operation has a name, descripti on, economical value, cover-up name 
&#8226;	info about the persons involved in an operation and their role, 
7 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>4 &#8226; Create a new operation with &#8220;Sam Astuto&#8221; in charge of it. 
BEGIN 
INSERT INTO operation VALUES (&#8217;newop1&#8217;,&#8217;&#8217;,0,&#8217;Sam&#8217;s bakery&#8217;); 
INSERT INTO involve VALUES (&#8217;newop1&#8217;,&#8217;sam&#8217;,&#8217;chief&#8217;); 
COMMIT 
Let us reconsider the procedural approac h. You migh t organize data into 
&#64257;les: one record of each table in a &#64257;le, and maybe sort the data by one of the 
&#64257;elds. Now every di&#64256;eren t access to the data, i.e., every &#8220;query&#8221; should become 
a di&#64256;eren t program opening the &#64257;les, scanning them, reading or writing certain 
&#64257;elds, saving the &#64257;les. 
Extras 
The two following concepts have been broadly mentioned but not discussed in 
details in class. 
Optimization The goal of a DBMS is to provide a library of sophisticated 
techniques and strategy to store, access, update data that also guaran tees per&#173;
formance, atomicit y, consistency , isolation, durabilit y. DBMS automatically 
compile the user declarativ e queries into an execution plan (i.e., a strategy that 
applies various steps to achieve the compute the user queries), looks for equiv&#173;
alent but more e&#64259;cien t ways to obtain the same result query optimization, and 
execute it, see example in Figure 5. 
scan(person)scan(involved)scan(operations)productproduct&#64257;lter(p.name=i.person)  &#64257;lter(i.oper_name=o.name)  &#64257;lter(o.coverup="laundromat")  project(p.phone)
scan(person)scan(involved)lookup(operations, coverup="laundromat")productproduct&#64257;lter(p.name=i.person)  project(p.phone)
&#64257;lter(i.oper_name=o.name)  project(p.name,p.phone)project(i.oper_name, i.person)project(o.name)BASIC PLANOPTIMIZED PLAN
Figure 5: Two equiv alent execution plan, a basic and an optimized one. 
10 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830/6.814 &#8212; Notes&#8727; for Lecture 1: 
Introduction to Database Systems 
Carlo A. Curino
September 10, 2010
2 Introduction
READING MATERIAL: Ramakrishnan and Gehrke Chapter 1 
What is a database? A database is a collection of structured data. A 
database captures an abstract representation of the domain of an application. 
Typically organized as &#8220;records&#8221; (traditionally, large numbers, on disk) &#8226; 
and relationships between records &#8226; 
This class is about database management systems (DBMS) : systems for cre&#173;
ating, manipulating, accessing a database. 
A DBMS is a (usually complex) piece of software that sits in front of a 
collection of data, and mediates applications accesses to the data, guaranteeing 
many properties about the data and the accesses. 
Why should you care? There are lots of applications that we don&#8217;t o&#64256;er 
classes on at MIT. Why are databases any di&#64256;erent? 
2 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.830 / 6.814 Database Systems
Fall 2010
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>NOSQL (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>  what works badl y in GFS?
    faul t-tolerance of master
    small f iles (master a bot tleneck)
    concurrent updates to same f ile from man y clien ts (except appen ds)
so GFS maybe already solves some problems for BigTable
 giant storage
  data faul t-tolerance
  high sequenti al throughput 
BigTable acts as a set of clients to GFS
 BigTable servers r/w GFS across the net, no local storage
  data not real ly tied permanentl y to particular BigTable servers
  if one (or all) BigTable serv ers have perman ent failures
    you don't l ose data -- data i s in GFS
    just fi re up repl acement BigTable servers, they read GFS
    this simplif ies the BigT able design
It splits each tabl e into lots of tabl ets
  part ition by row name
  each tabl et is stored i n a set of GFS fi les 
given a table name and row name, how to fi nd tabl et?
 1. tablet server needs to know what GFS files hold the tablet data
     METADATA of Fi gure 4
     Chubby i s a mini file server that says what
       GFS fi les hold the METADATA tabl e
     so Bi gTabl e knows where to start
 2. client needs to know what  tablet server serves the tablet
     (not the same as questi on #1, can be soft state)
     my guess: METADATA hol ds this too
     client doesn't ask the master (4th para of Secti on 5)
     but paper's only mention of tabl et -&gt; server mappi ng is in master mem
 e.g. booti ng master doesn't read thi s info from METADATA
       bu t by talking to all liv e tablet servers
so what does a METADATA entry contain?
  &lt;tabl e ID, starti ng row name&gt; -&gt;
 names of GFS files that store the tablet (sec 5.3)
    what tabl et server serves  it (guessing, paper doesn't say)
what properties of Chubby are important?
 why a master AND chubby?
  most systems integrate them; sepa ration means chubby can be reused
  chubby i s a generi c faul t-tolerant fi le and l ock server
 chubby does three things for BigTable
    stores root of METADATA tabl e in a file
    main tains mast er lock , so there's at most one master
 tracks which tablet servers are alive (via locks)
  key properti es:
    Chubby repl icates METADATA and l ocks
    Chubby keeps going even if  one (two?) Chubby servers down
 Chubby won't disagree with itself
      exampl e: network parti tion
      you update Chubby repl ica in one parti tion
      C hubby replica in  other part ition will *not* show stale dat a
</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830  2009 Lecture 19: BigTable 
big picture
 parallel db (one data center)
 mix of OLTP and batch analysis
  lots of data, high r/w rates, 1000s  of cheap boxes thus many failures 
what does paper say Google uses BigTable for?
 analyzing big web crawls
 analyzing click records to optimize ads
 some on-line uses: orkut, personalized search 
data model
  figure 1 shows a tabl e has three (four?) di mensi ons
    row, col umn fami ly, column, ti me 
query model
  single-row fetch by row/col umn key
  single-row atomi c update and read-modi fy-write
  scans i n key order
  no joi ns
 no aggregates (but they have MapReduce for this) 
example use: Figure 1, we b crawl for various analyses
 one row per URL (== page)
  one col umn for each l ink *to* a page! that's a l ot of col umns.
  how to store row /col/time in  a file?
    the model may be 3d, but under lying storage h as only one dimen sion
  guess fl attened l ayout for fi gure 1?
 com.cnet.www
 com.cnn.www
      anchor:cnnsi .com
 t9: CNN
      anchor:my.l ook.ca
 t8: CNN.com
 ...
 content
 t6: ...
 t5: ...
 t3: ...
 com.cnx.www
  very di fferent ki nd of "col umn" than an SQL db
    different rows have di fferent col umns!
 like a mini-b-tree table in every row
 a hierarchical data model
 or like one big btree
    keys are rowname+fami ly+colname+ti me
 so it's cheap to scan all the links to a certain page
 but *not* cheap to scan all content inserted at t5
    i.e. BigTable is n ot a three-dimen sional DB
  how woul d we store Fi gure 1 i n a relational DB?
    anchor(si te, from, ti me, text)
    content(si te, time, html )
    does i t make any di fference?
      usual  argument agai nst hi erarchy i s repeated data </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>single-tablet-server random read
  first row, fi rst col umn of Fi gure 6
  single client reads random rows
 how can one server do 1212 random reads/second?
 you can't seek 1212 times per second!
 answer: only 1 GB of data, split up over maybe 16 GFS servers
    so al l the data i s in the GFS Li nux kernel  file cache
 so why only 1212, if in memory?
    that's onl y 1 megabyte/second!
 each row read reads 64KB from GFS
    78 MB / second, about al l gig-e or TCP can do
single-tablet-server ran dom w rite
  single client reads random rows
  traditionally a hard workload
 how could it write 8850 per second?
    each  write must go to disk (the log, on GFS) for durabilit y
    log is probabl y in one GFS chunk (one tri ple of servers)
    you cannot seek or rotate 8850 times per second!
    presumabl y batchi ng many  log file writes, group commit
      does that means Bi gTabl e says "yes" to client before data is durable? 
what about scaling
 read across a row in Figure 6
 the per-server numbers go down
  so performance goes up w/ # tablet servers, but not l inearly
  why not linear?
    paper says l oad imbalance:
      some Bi gTabl e servers have other stuff runni ng on them
 master doesn't hand out tablets 100% balanced
    also network bottl eneck, at l east for random read
 remember 64K xfer over LAN per 1000-byte row read
 root of net only has about 100 gbit/second total
 enough to keep only about 100 tablet servers busy
i like this paper' s evaluation section
 shows good and bad aspects
 explains reasons for results
 connects performance back to design </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>        doesn't real ly apply here
      we'd want to cl uster tables for good scan performance
      rel ational model  lets us ask for al l anchors by from col umn
        whi le BigTabl e really only lets you scan by si te
      but i mplementi ng scan-by-fro m requires indices, would be slowish
  "Localit y group" mech anism pu ts some colu mn families in  separat e file
    so you could scan all pages' anch ors w/o havi ng to scan+i gnore content
 much like c-store 
example use: section 8.1 Google Analytics
  record and analyze us er actions on web sites
    are peopl e clicking on your ads?
    whi ch ads are the most effecti ve?
  row per user session, key is &lt;site,startti me&gt;
  colu mn per click ???
  periodic batch anal ysis of ea ch site's recent sessions/clicks
  data must arri ve at a huge rate!
 worse, it arrives in the wrong order
    arri ves sorted by ti me
    but we want to store and scan by si te
 this is a pretty classic problem
    bad solu tion: insert each  new click into a b-tree
    we'l l see l ater how they deal  with thi s
how do they implement BigTable? 
*not* an ordin ary parallel D B
 partition data over the servers and their disks
  each server does reads/wri tes for data on i ts disk
  this is n ot how BigTable w orks! 
starting poi nt: GFS
 GFS a cluster file system
  FS model:  direct ories,  files, names, open /read/write
  100s of Li nux chunk servers wi th disks
    store 64MB chunks (an ordi nary Li nux fi le for each chunk)
    each chunk repl icated on three servers
 GFS master server knows directory hierarchy
    for di r, what fi les are i n it
    for fi le, knows chunk servers for each 64 MB
    master has pri vate recoverabl e DB for metadata
 primary/backup to a slave
  clien t read:
    send fi le name and offset to master
    master repl ies with set of servers that have that chunk
 ask nearest chunk server
  clien t write:
 ask master where to store
    maybe master chooses a new set of chunk servers i f crossi ng 64 MB
    one chunk server i s primary
    it chooses order of updates and forwards to two backups
  what works wel l in GFS?
 huge sequential reads and writes
 appends
    huge throughput (3 copi es, stri ping)
    faul t tolerance of data (3 copi es)
</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>what i s the poi nt of the master?
  after all, the METADATA is all in  Chubby and GFS
  answer: there had better be only on e enti ty assi gning tabl ets to servers
    onl y the master wri tes METADATA
  chubby locking ensures there's at most one master
    even duri ng network parti tions
why isn't Chubby a bottleneck?
 clients cache METADATA
 METADATA doesn't change often
  tablet server will tell clien t if it is talking to wrong serv er 
read/write processing inside a tablet server
 similar to c-store
  log for fast wri tes, SSTabl es for fast l ookups
  [diagram:  log in  GFS, memt able, SSTables in  GFS]
  SStables in  GFS
    compact ordered row/fami ly/col/time data
 compressed
    index at the end
    immutabl e -- why not mutabl e b+tree?
      fast search, compact, comp ression, GFS not good at rand write
  log in  GFS
  compacti on
recovery from tablet server crashes
  key probl em:
    what i f it was i n the mi ddle of some update when i t crashed?
    do we need to wai t for it to reboot and recover from its log?
  chubby notices server is de ad (stops refreshi ng its lock)
    and/or master noti ces it is dead?
  even if tablet server is liv e but partitioned,
    it won't be able t o refresh its lock  if Chubby thinks it is dead
    so t able serv er will know to stop serv ing
 if master sees tablet server no longer has its lock:
    picks another tabl et server (preferabl y lightly loaded one)
    tel ls is "load that tabl et from GFS"
  new tablet server reads the crashed server's log from GFS! 
recovery from BigTable master crashes
 Chubby takes away its lock
  some other machine(s)  decide to be master
    onl y one gets the Chubby l ock
 recreate old master's state:
    read set of tabl ets from METADATA
    ask Chubby for l ist of live tabl et servers
    ask tabl et servers what they serve
Evaluation 
setup
 1700 GFS servers, N tablet servers, N clients
  all u sing same set  of mach ines
  two-level LAN with gig-e
 each row has 1000 bytes </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Buffer pool design and memory management (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>5 What about MySQL? 
(From MySQL documen tation online: http://dev.mysql.com/doc/refman/5. 
5/en/innodb- buffer- pool.html) 
A variation of the LRU algorithm operates as follows by default: 
&#8226;	3/8 of the bu&#64256;er pool is devoted to the old sublist. 
&#8226;	The midpoint of the list is the boundary where the tail of the new sublist 
meets the head of the old sublist. 
&#8226;	When InnoDB reads a block into the bu&#64256;er pool, it initially inserts it at 
the midp oint (the head of the old sublist). A block can be read in because 
it is required for a user-sp eci&#64257;ed operation such as a SQL query , or as part 
of a read-ahead operation performed automatically by InnoDB. 
&#8226;	Accessing to a block in the old sublist makes it ?young?, moving it to the 
head of the bu&#64256;er pool (the head of the new sublist). If the block was 
read in because it was required, the &#64257;rst access occurs immediately and 
the block is made young. If the block was read in due to read-ahead, the 
&#64257;rst access does not occur immediately (and migh t not occur at all before 
the block is evicted). 
&#8226;	As the database operates, blocks in the bu&#64256;er pool that are not accessed 
?age? by moving toward the tail of the list. Blocks in both the new and 
old sublists age as other blocks are made new. Blocks in the old sublist 
also age as blocks are inserted at the midp oint. Eventually , a block that 
remains unused for long enough reaches the tail of the old sublist and is 
evicted. 
You can control: 
&#8226;	innodb old blocks pct for the portion of new-old 
&#8226;	innodb old blocks time Speci&#64257;es how long in milliseconds (ms) a block 
inserted into the old sublist must stay there after its &#64257;rst access before it 
can be moved to the new sublist. 
6 LRU Cache misses in typical scenarios 
From the paper: I/O Reference Behavior of Production Datab ase Worklo ads 
and the TPC Benchmarks An Analysis at the Logical Level by Windsor W. 
Hsu, Alan Jay Smith, and Honest y C. Young. We report the LRU miss ratios 
for increasingly large bu&#64256;erp ool sizes, and for typical production databases and 
popular benchmarks. This should give you an idea of the fact that some portion 
of the DB are very &#8220;hot&#8221; while other are rather &#8220;cold&#8221;, thus throwing more and 
more RAM at the problem will provide less and less returns. On the other side 
choosing the &#8220;righ t&#8221; things to keep in RAM is clearly vital! 
3 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830/6.814 &#8212; Notes&#8727; for Lecture 7: 
Bu&#64256;er Managemen t 
Carlo A. Curino 
Septem ber 30, 2010 
1 Announcemen ts 
&#8226;	Lab 2 is going to go out today... Same as before... do not copy! (I mean 
it!) 
&#8226;	Project teams were due last time. Anyone still without a team? 
2 Readings 
For this class the suggested readings are: 
&#8226;	Hong-T ai Chou and David DeWitt. An Evaluation of Bu&#64256;er Managemen t 
Strategies for Relational Database Systems. VLDB, 1985. 
&#8226;	If you are interested in simple curren t implemen tation of bu&#64256;erp ool man&#173;
agemen t: http://dev.mysql.com/doc/refman/5.5/en/innodb- buffer- pool. 
html and 
3 Recap 
We are given many access metho ds. None of this is uniformly better than others 
across the map, but each has some particular case in which is best. B+T rees 
(both clustered and unclustered) and Heap&#64257;le are the most commonly used. 
We made a big case about the fact that Disk accesses are very expensiv e and 
that the DBMS has 2 ways to reduce their impact on performance: i) reduce 
the number of disk accesses by having smart access metho ds, ii) do a good job 
at caching in RAM data from disk. 
&#8727;These notes are only mean t to be a guide of the topics we touch in class. Future notes 
are likely to be more terse and schematic, and you are required to read/study the papers and 
book chapters we mention in class, do homew orks and Labs, etc.. etc.. 
1 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>4 Last lecture we assumed every access was o&#64256; of disk, and we tried our best 
to minimize the number of pages accessed, and to maximize the sequentiality of 
the accesses (due to the large cost of seeks) by designing smart access metho ds. 
Today we get into the investigation of &#8220;what&#8221; to try to keep in RAM to further 
avoid Disk I/O. The assumption is that we can&#8217;t keep everything, since the DB 
is in general bigger than the available RAM. 
Today&#8217;s topic 
Why don&#8217;t we just trust the OS? 
(DBMS knows more about the data access es, and thus can make a better 
job about what to keep in RAM and what to pre-fetc h, given an execution plan 
is rather clear what we are going to need next). 
DBMS manages its own memory: Bu&#64256;er management / Bu&#64256;er pool. 
Bu&#64256;er pool: 
&#8226;	cache of recen tly used pages (and more importan tly plans ahead of which 
one are likely to be accessed again, and what could be prefetc hed) 
&#8226;	convenien t &#8221;bottlenec k&#8221; through which references to underlying pages go 
useful when checking to see if locks can be acquired or not 
&#8226;	shared between all queries running on the system (imp ortan t! the goal is 
to globally optimize the query workload) 
Final goal is to achieve better overall system performance... often correlated 
to minimize physical disk accesses (e.g., executing 1 query at a time guaran tees 
minim um number of acces ses, but lead to poor throughpu t). 
Good place to keep locks: 
Cache &#8211; so what is the best eviction policy? 
USE SAM NOTES FROM HERE ON... 
2 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Join algorithms (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec07b/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>-Double buffering -- both OS and database may have a page in memory, wasting RAM. 
-Failure to write back -- the OS may not write back a page the database has evicted, which can cause problems if, for
example, the database tries to write a log page and then crashes. 
-Performance issues -- the OS may perform prefetching, for example, when the database knows it may not need it.
Disk controllers have similar issues (cache, performance tricks.)
(What are some possible solutions?)
-Add hooks to the OS to allow the database to tell it what to do.
-Modify the database to try to avoid caching things the OS is going to cache anyway.
In general, a tension in layered systems that can lead to performance anomalies.
</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>a) If someone else owns, nothing to be done
b) If no owner, requester becomes owner
3) Not in buffer pool - requester becomes owner, evict something from requester's memory 
How do you avoid running out memory? 
Don't admit queries into the system that will make the total sum of all of the l_ij variables &gt; total system memory. 
Metacomments about performance study. 
(It's good.) Interesting approach. What did they do?
Collect real access patterns and costs, use them to drive a simulation of the buffer pool.
(Why?) Real system would take a very long time to run, would be hard to control.
How much difference did they conclude this makes?
As much as a factor of 3 for workload with lots of concurrent queries and not much sharing. Seems to be 
mostly due to admission control. With admission control, simple &#64257; fo is about 60% as good as DBMIN.
DBMIN is not used in practice. What is? (Love hate hints). 
 What's that? (When an operator &#64257; nishes with a page, it declares its love or hate for it.  Buffer pool preferen &#8208;
tially evicts hated pages.) Not clear why (this would make a nice class project.) Perhaps love hate hints perform almost as well as DBMIN and are a lot simpler. They don't capture the need
for different buffer management policies for different types of &#64257; les. 
(What else might you want the buffer manager to do?)
Prefetch.
(Why does that matter.)
Sequential I/O is a lot faster. If you are doing a scan, you should keep scanning for awhile before servicing some other
request, even if the database hasn't yet requested the next page.
Depending on the access method, you may want to selectively enable prefetching.
Interaction with the operating system 
(What's the relationship between the database buffer manager and the operating system?)
Long history of tension between database designers and OS writers. These days databases are an important enough
application that some OSes have support for them.
(What can go wrong?)
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>SR - Straight Random (index scan through secondary index)
CR - Clustered Random (index NL join with with secondary index on inner, with repeat foreign keys on outer)
(skiP
SH - Straight Hierarchical (index lookup)
LH - Looping Hierarchical (repeated btree lookups)
So what's the right policy:
SH - 1 page, any access method
CS - size of cluster pages, LRU
LS - size of &#64257; le pages, any policy, or MRU plus however many pages you can spare
SR - 1 page, any access method
CR - size of cluster pages, LRU
SH - 1 page, any access method
LH - top few pages, priority levels, any access method for bottom level
How do you know which policy to use?
(Not said, presumably the query parser/optimizer has a table and can &#64257; gure this out.)
Multipage interactions. 
Diagram: 
Buffer pool per &#64257;le instance, with locality set for that instance,  plus "global table" that contains all pages.
Each page is "owned" by a at most one query. Each query has a "locality set" of pages for each &#64257; le instance it 
is accessing as a part of its operation, and each locality set is managed according to one of the above
policies.
Also store current number of pages associated with a &#64257;le instance (r) and the maximum number of pages 
associated with it (l).
How do you determine the maximum number of pages?
Using numbers above.
What happens when the same page is accessed by multiple different queries?
1) Already in buffer pool and owned locally
2) Already in buffer pool, but not owned </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>  LRU?  What if a query just does one sequential scan of a &#64257; le -- then putting it in the cache at all would be 
pointless. So you should only do LRU if you are going to access a page again, e.g., if it is in the inner loop of a
NL join. 
For the inner loop of a nested loops join, is LRU always the best policy? No, if the inner doesn't &#64257; t into 
memory, then LRU is going to evict the record over and over. 
E.g., 3 pages of memory, scanning a 4 page &#64257; le: 
pages A B C read hit/miss
1 1 m 
1 2 2 m 
1 2 3 3 m 
142 3 4 m 
142 13 1 m 
2 m 
Always misses?. What would have been a better eviction policy? MRU! 
pages A B C read hit/miss?
1 1 m 
1 2 2 m 
1 2 3 3 m 
1 2 3 4 4 m 
1 2 4 1 h 
1 2 4 2 h 
1 2 34 3 m 
1 3 4 4 h 
1 3 4 1 h 
1 23 4 2 m 
Here, MRU hits 2/3 times. 
DBMIN tries to do a better job of managing buffer pool by
1) allocating buffer pools on a per-&#64257; le-instance basis, rather than a single pool for all &#64257; les
2) using different eviction policies per &#64257; le 
What is a "&#64257; le instance"? 
(Open instance of a &#64257; le by some access method.) 
Each time a &#64257;le is opened, assign it one of several access patterns, and use that pattern to derive a buffer management policy. 
(What does a policy consist of?) 
Policy for a &#64257;le consists of a number of pages to allocate as well as a page replacement policy. 
(What are the different types of policies?) 
Policies vary according to access patterns for pages. What are the different access patterns for
pages in a database system? 
SS - Straight Sequential (sequential scan)
CS - Clustered Sequential (merge join) (skip) 
LS - Looping sequential (nested loops) </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Schema design (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text>Two ways to data base design 
a) Normalization b) E-R models Normalization:  start with an initial collection of tables, (for this example) Animals (name, species, age, cid, cage_size) Functional dependency:  for any two collections of columns, second set is determined by 
the first set; i.e. it is a function. 
Written A -&gt; B 
In our example: 
(name) is a key.  Everything is FD on this key 
Plus 
cid -&gt; size 
Problems: 
1) redundancy:  cage_size repeated for each animal in the case 
2) cannot have an empty cage 
Issue:
Cage_size is functionally dependent on cid which in turn is functionally dependent on 
name.  So called transitive dependency.
Solution normaliziation;
Cage (id, cage_size)
Animals (name, species, age, c_id)
1NF (Codd)  -- all tables &#8220;flat&#8221; 2NF (Codd)  -- no transitive dependencies 
3NF (Codd)  -- fully functional on key 
BCNF 
4NF </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Data model:  C++ data structures.  Declare them to be persistent: 
e.g.  persistent keeper_data DB [500]; 
to query data base: 
temp = DB[1].keeper_data.name 
to do an update: 
DB [3].keeper_data.name = &#8220;Joseph&#8221; 
Query language:  none &#8211; use C++  appropriate for CAD &#8211;style apps
Assumes programming language Esperanto (C++ only)
Later adopted a QL (OQL)
Transaction systems very weak &#8211; big downside
Never found a sweet spot.
***********
Semi -structured data problem
Hobbies of employees
Sam:  bicycle (brand, derailer, maximum slope hill, miles/week)
Mike:  hike (number of 4000 footers, boot brand, speed) 
Bike (builder, frame-size, kind-of-seat) 
&#8220;semi-structured data&#8221;
Not well suited to RM (or any other model we have talked about so far)
XML ( hierarchical, self -describing tagged data)
&lt;employee&gt; 
&lt;name&gt; Sam &lt;/name&gt; &lt;hobbies&gt; 
&lt;bike&gt; 
&lt;brand&gt;  XXX &lt;/brand&gt; 
. . 
&lt;next hobby&gt; 
XML is anything you want. 
Document guys proposed this stuff as simplified SGML.  </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Legacy &#8211; SQL is a terrible language -- Date paper in 1985
Animals (name, species, age, feeding_time, cid, kid)
Cages (id, size)   
Keepers (id, name, address)
Original idea (1974)
Block 
Block 
Block 
e.g. find the name of Freddies&#8217;s keeper 
select name 
from keepers 
where id = 
select kid 
from Animals 
where name = &#8220;Freddie&#8221; 
Evaluation from inside&#8211;out 
First:  this may not be the best plan 
Second:  not powerful enough 
Find animals older than Freddie who share the same cage 
Select name 
From animals A 
Where age &gt; 
Select age 
From animals 
Where name = &#8220;Freddie&#8221; 
And cid = A.cid 
Find all pairs of animals cared for by the same keeper 
Select A.name, B.name 
From Animals A 
Where a.zid = 
Select B. zid </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>5NF 
P-J NF 
&#8230;. 
Theoreticians had a field day&#8230;. 
Totally worthless 
1) mere mortals can&#8217;t understand FDs
2) have to have an initial set of tables &#8211; how to come up with these?
Users are clueless&#8230;
Plus, if you start with:
Animals (name, species, age, feeding_time)
Cages (id, size)   
Keepers (id, name, address)
Lives_in (aname, cid)
Cared_for_by (aname, kid)
No way to get
Animals (name, species, age, feeding_time, cid, kid)
Cages (id, size)   
Keepers (id, name, address)
******
Universal solution:
E-R model:
Entities (things with independent existence)
Keepers
Cages
Animals Entities have attributes 
Entities have a key 
Animals have name (key), species, age, &#8230; 
Entities participate in relationships </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>There are queries that cannot be flattened. 
e.g. ones with an &#8220;=&#8221; between the inner and out block. 
There are ones that cannot be expressed in nested  fashion, e.g. &#8220;pairs of 
Animals query&#8221; above 
Two collections of queries:  nested and flat:  venn diagram almost identical but not quite.  Awful language design. Lessons Never too late to toss everything and start over  -- glueing warts is never a good idea 
Simple is always good 
Language design should be done by PL folks not DB folks &#8211; we are no good at it 
************** 
OODB (1980s):  persistent C++   
Motivation:
Programming language world (C++)
struct animals {   
string name;
string feed_time ;
string species;int age; 
}; 
struct keeper_data  {   
string name;string address;
animals charges[20] ;   
}; DBobject; 
data base world:
Keepers (name, address)
Animals (name, feed_time, species, age, k_id)
There is an obvious impedance mismatch.  
Query returns a table &#8211; not a struct.  You have to convert DB return to data structures of
client program.
OODBs were focused on removing this impedance mismatch.
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>From Animals B 
Where B.name != A.name 
Requires refs from inside out and outside in 
No obvious query processing strategy &#61664; disallowed Okay for an inner to outer reference, but not the other way around. 
SQL solution (1976)  -- multi-table blocks Select A.name, B.name 
From Aminals A, Animals B 
Where A.zid = B.zid and A.name != B.name 
Net result:  horrible 
2 ways to express most queries, e.g. Freddie&#8217;s keeper: 
(nested) 
select name 
from keepers 
where id in 
select kid
from Animals
where name = &#8220;Freddie&#8221;
(flat)
Select k.name
From Animals A, Keepers K
Where A.kid = K.id and A.name = &#8216;Freddie&#8217;
Which one to choose?
1980&#8217;s:
Hierarchical got you inside-out evaluation
Flat got you a complete optimization &#61664; obviously better!
Don&#8217;t use hierarchical representation!!!!
More recently, SQL engines rewrite hierarchical queries to flat ones, if they can.  Big 
pain for the implementation!!!
</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Adapted by DBMS guys.  Always a bad idea to morph something to another purpose. 
If you want a structured collection, they  proposed XML-schema 
&lt;employee: schema&gt; 
&lt;employee:element name = &#8220;empname&#8221; type = &#8220;string&#8221;/&gt; 
&lt;employee:complextype name = &#8220;hobbies&#8221;, type = &#8220;hobbiestype&#8221;/&gt; 
XML representation for structure of the XML document 
Most complex thing on the planet&#8230; 
Tables (RM) Hierarchies (IMS style) 
Refs (Codasyl style) 
Set valued attributes (color = {red, green brown}) 
Union types (value can be an X or a Y) 
XQuery is one of the query languages (with XPath)
For $X in Employee
Where $x/name = Sam
Return $X/hobbies/bike
Huge language
Relational elephants have added (well behaved subsets) to their engines
getting some traction.
******8
Data base design
***************
Animals (name, species, age, feeding_time, cid, kid)
Cages (id, size)   
Keepers (id, name, address)
Or
Animals (name, species, age, feeding_time)
Cages (id, size)   
Keepers (id, name, address)
Lives_in (aname, cid)
Cared_for_by (aname, kid)
Data base design problem &#8211; which one to choose
</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Lives_in 
Cared_for_by 
Relationships are 1:1, 1::N or M::N  (use crows feet on the arcs to represent visually) 
Relationships can have attributes 
Draw an E-R diagram 
Keepers Cages 
(id (key), name, address) (id (key), size) 
| | 
| | 
| | 
\/ \/ 
Animals
(name (key), species, age, feeding_time)
Automatic algorithm generates 3NF (Wong and Katz 1979)
Each entity is a table with the key
M::N relationships are a table with their attributes
1::N relationships &#8211; add the key on the N side to the one side with all of the relationship 
attributes
Generates:
Animals (name, species, age, feeding_time, cid, kid)
Cages (id, size)   
Keepers (id, name, address)
Over the years has been extended with
Weak entities (no key &#8211; inherits the key of some other entity)  (learn to drive)
Inheritance hierarchies (generalization)  (student is a specialization of person)
Aggregation  (attribute of all of the participants in a relationship) (e.g count)
More than binary relationships (e.g. marriage ceremony)
Details in Ramakrishnan&#8230;..
</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Database operators and query processing (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>8 &#8226;	selection &#8211; 10,000 predicate ops 
&#8226;	1st Nested loops join &#8211; 100,000 predicate ops 
&#8226;	2nd nested loops join &#8211; 3,000,000 predicate ops 
Let&#8217;s look at number of disk I/Os assuming LRU and no indices 
if d is outer: 
1 scan of DEPT &#8226; 
&#8226;	100 consecutiv e scans of EMP (100 x 100 pg. reads) &#8211; cache doesn&#8217;t bene&#64257;t 
since e doesn&#8217;t &#64257;t 
&#8211;	1 scan of EMP: 1 seek + read in 1MB =10 ms + 1 MB / 100 MB/sec 
= 20 msec 
&#8211;	20 ms x 100 depts = 2 sec 
TOT AL: 10 msec seek to start of d and read into memory 2.1 secs 
if d is inner: 
&#8226;	read page of e &#8211; 10 msec 
read all of d into RAM &#8211; 10 msec &#8226; 
seek back to e &#8211; 10 msec &#8226; 
&#8226;	scan rest of e &#8211; 10 msec, 
joining with d in memory ... Because d &#64257;ts into memory 
TOT AL: total cost is just 40 msec 
No options if plan is pipelined, k must be inner: 
&#8226;	1000 scans of 300 pages 3 / 100 = 30 msec + 10 msec seek = 40 x 1000 = 
40 sec 
So how do we know what will be cached? That&#8217;s the job of the bu&#64256;er pool. 
What about indexes? The DBMS uses indexes when it is possible (i.e., when 
an index exists and it supp ort the required operation, e.g., hash-indexes do not 
supp ort range searc h) 
Bu&#64256;er Managemen t and Storage Subsystem 
Bu&#64256;er Manage r or Bu&#64256;er pool caches memory accesses... Why is it better if the 
DBMS does this instead of relying on OS-lev el caching? DBMS knows more 
about the query workload, and can predict which pages will be accessed next. 
Moreo ver, it can avoid cases in which LRU fails. 
Also explicit managemen t of pages is helpful to the locking / logging neces&#173;
sary to guaran tee ACID properties. 
8 ssal&gt;10kd
ek 1000
100 1000(NL join)
(NL join)</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>operator). 
&#8226;	Projection: &#928;a,b(T ) 
&#8226;	Selection: &#963;a=7(T ) 
&#8226;	Cross-Pro duct: R &#215; S 
&#8226;	Join: R &#65533;a1=a2 S 
&#8226;	Rename: &#961;a/b(R)
...
&#8226; 
Let&#8217;s use the example query from the introduction lecture (it&#8217;s in the notes 
but we didn&#8217;t discuss it last time): 
SELECT p.phone 
FROM person p, involve i, operation o 
WHERE p.name = i.person AND 
i.oper_name = o.name AND
o.coverup_name = "laundromat";
Corresp onding Algebra expression: 
&#928;phone (&#963;o.coverupname=&#8221;l aundr omat&#8221;(&#963;p.name=i.per son(&#963;i.oper n ame=o.name (person&#215;
(involved &#215; operation)) 
Let&#8217;s show this in a more visual way (the query plan): 
scan(person)scan(involved)scan(operations)productproduct&#64257;lter(p.name=i.person)  &#64257;lter(i.oper_name=o.name)  &#64257;lter(o.coverup="laundromat")  project(p.phone)
scan(person)scan(involved)lookup(operations, coverup="laundromat")productproduct&#64257;lter(p.name=i.person)  project(p.phone)
&#64257;lter(i.oper_name=o.name)  project(p.name,p.phone)project(i.oper_name, i.person)project(o.name)BASIC PLANOPTIMIZED PLAN
Figure 1: Two equiv alent query plans for the same query . 
The goal of the optimizer is two &#64257;nd equiv alent query plans that lead to 
higher performance. This is done in two ways: 
&#8226;	logical: re-ordering operators in a semantic-preserving way
2
</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>For example, fastest TPC-H benchmark result (data warehousing bench&#173;
mark), on 10 TB of data, uses 1296 &#215; 74 GB disks, which is 100 TB of storage. 
Additional storage is partly for indices, but partly just because they needed 
additional disk arms. 72 processors, 144 cores &#8211; 10 disks / processor! 
But, if we do a bad job, random I/O can kill us! 
7.2 An example 
Schema: 
DEPT(dno,name); 
EMPL(eid,dno,sal); KIDS (kidname,eid); 
Some characteristics of this hypothetical scenario: 
&#8226; 100 tuples/page 
&#8226; 10 pages in RAM 10 KB/page 
10 ms seek time &#8226; 
&#8226; 100 MB/sec I/O 
&#8226; cardinalit y DEPT = 100 tuples = 1 page 
&#8226; cardinalit y EMPL = 10K tuples = 100 pages 
&#8226; cardinalit y KIDS = 30K tuples = 300 pages 
SELECT dept.name,kidname 
FROM emp, dept, kids 
WHERE e.sal &gt; 10k AND 
emp.dno = dept.dno AND 
e.eid = kids.eid; 
&#928;name,k idname (dept &#65533;&#65533;dno=dno (&#963;sal&gt;10k(emp)) &#65533;&#65533;eno=eno kids)
More graphically:
CPU operations: 
7 ssal&gt;10kd
ek 1000
100 1000(NL join)
(NL join)
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#8226;	physical: choosing among multiple implem entation of the operators the 
one that for the speci&#64257;c scenario supp osedly will give best performance 
There are two main approac hes to do this: 
&#8226;	heuristic: uses a set of rules or strategies that typically pay back, e.g., 
push all selections before joins. 
&#8226;	cost-based: de&#64257;ne a cost model for the execution (e.g., number of CPU 
operations + amoun t of disk I/O) and compare various plans according 
to the expected cost. Pick lowest cost. 
Restricting to certain plans (left deep) allows to avoid temp orary bu&#64256;ering 
of results or re-computation. 
Optimization is often based on statistical information about the data... e.g., 
data distributions... often main tained as histograms. This can allows us to 
predict selectivit y of operators, and thus have an idea of size of intermediate 
results and the cost of the following operators. 
4 Query (Pre)Compilation 
Most DBMS supp ort query pre-compilation. I.e., the application can declare 
that is going to use (frequen tly?) a certain type of query . The structure of 
the query is provided in advance and later on the application only passes to 
the DBMS the &#8220;parameters&#8221; to specialize this template into a speci&#64257;c query . 
This allows to capitalize query parsing, some authorization work, and optimiz a&#173;
tion. However, predicate selectivit y becomes a static choice. This can lead to 
signi&#64257;can t performance impro vemen t for very regular workloads (e.g., Web / 
OLTP). 
5 Physical Storage 
All records are stored in a region on disk; probably easiest to just think of each 
table being in a &#64257;le in the &#64257;le system. 
Tuples are arranged in pages in some order. Simple append to the &#64257;le or 
&#8220;heap &#64257;le&#8221; access path is a way to access these tuples on disk. 
Now how to access the tuple (the access path)? 
&#8226;	heap scan 
&#8226;	index scan provide an e&#64259;cient way to &#64257;nd particular tuples 
What is an index? What does it do? 
Index is an auxiliary data structure that aims at impro ving a speci&#64257;c type 
of access to the data. In particular an index is de&#64257;ned on one of the attributes, 
and for each value points at the corresp onding positions on disk. Operations: 
&#8226;	insert (key, recordid) points from a key to a record on disk records 
3 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>for t1 in outer 
for t2 in inner 
if p(t1,t2) emit join(t1,t2) 
Bushy requires temporarily memorizing results or recompute. 
Left Deep: 
Results can be pipelined. No materialization of intermediate steps necessary . 
Many database systems restrict themselv es to left or right deep plans for this 
reason 
7.1 Cost of an execution plan 
In class we went over another example (the one above of basic and advanced 
plan). That example was mainly about CPU costs, in the example below, we 
explore more the importance of Disk I/O. This is also good to prepare our next 
class, which will be all about access metho ds. 
What&#8217;s the &#8221;cost&#8221; of a particular plan? 
CPU cost (# of instructions) 1Ghz == 1 billions instrs/sec 1 nsec / instr 
I/Ocost(#ofpagesread,#ofseeks) 100MB/sec 10nse c/byte 
(RandomI/O=pageread+seek) 10msec/seek 100seeks/sec 
Random I/O can be a real killer (10 million instrs/seek). (You probably 
experience it yourself on your laptop sometimes). 
When does a disk need to seek? 
Whic h do you think dominates in most database systems? 
Depends. Not always disk I/O. Typically vendors try to con&#64257;gure machines 
so they are &#8217;balanced&#8217;. Can vary from workload to workload. 
6 Inner Outer
A B C D
A BCDImage by MIT OpenCourseWare.
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830/6.814 &#8212; Notes&#8727; for Lecture 5: 
Database Internals Overview (2/2) 
Carlo A. Curino 
Septem ber 27, 2010 
1 Announcemen ts 
&#8226;	LABS: Do not copy... we perform automatic checking of plagiar ism... it 
is not a good gamble! 
&#8226;	Projects ideas and rules are posted online. 
2 Readings 
For this class the suggested readings are: 
&#8226;	Joseph Hellerstein, Michael Stonebrak er and James Hamilton. Architec&#173;
ture of a Database System. Online at: http://db.cs.berkeley.edu/ 
papers/fntdb07- architecture.pdf 
It is a rather long paper (don&#8217;t be too scared by the 119 pages, the page 
format makes it look much longer than it is) that is in general worth reading, 
however we only require you too read sections: 1, 2 (skim through it), 3, 4 (up 
to subsection 4.5 included), 5. You can also skim through section 6 that we will 
discuss later on. Probably doesn&#8217;t all make sense right now &#8211; you should look 
at this paper again to this paper through the semester for context. 
3 Query Optimization 
3.1 Plan Formation (SQL relational algebra) &#8594; 
First step in which we start thinking of &#8220;how&#8221; this query will be solved. It still 
abstracts many implemen tation details (e.g., various implemen tations of each 
&#8727;These notes are only mean t to be a guide of the topics we touch in class. Future notes 
are likely to be more terse and schematic, and you are required to read/study the papers and 
book chapters we mention in class, do homew orks and Labs, etc.. etc.. 
1 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#8226; (hasNext) 
next&#8226; 
close&#8226; 
This allows chaining of operators without any special logic to be imple&#173;
mented. Also it is a natural way to propagate data and control through the 
stack of calls. There are many variations around this principal to allow for 
more batch processing of data. 
Iterator code for a simple Select: 
class Select extends Iterator { 
Iterator child;
Predicate pred;
Select (Iterator child, Predicate pred) {
this.child = child;
this.pred = pred;
} 
Tuple next() {
Tuple t;
while ((t = child.next()) != null ) {
if (pred.matches(t)) {
return t;
}
}
return null;
} 
void open() { 
child.open();
}
void close() {
child.close(); 
} 
} 
Plan Types (only mentioned in class) 
Di&#64256;eren t type of plans. 
Bush y: 
5 7 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#8226;	lookup (key) given a single key return the location of records matc hing 
the key. 
&#8226;	lookup(lo wkey,highk ey) returns the location of records in the range of 
keys given as input. 
Hierarc hical indices are the most common type used&#8212;e.g., B+-T rees indices 
typically point from key values to records in the heap &#64257;le 
Shall we always have indexes? What are the pros and cons? (keep the index 
up-to-date, extra space required) 
Figure 2: B+T ree graphical represen tation. 
Special case, is a &#8220;cluster ed&#8221; index, i.e., when the order of the tuples on disk 
corresp ond to the order in which they are stored in the index. What is it good 
for? (range selections, scans). 
So now we have 2 option s: 
&#8226;	a heap scan (sequen tial, but load entire table), 
&#8226;	index scan (load less data, but migh t lead to a lot of Random I/O). 
In query plan we can mix multiple of this choosing the best for each basic 
table we need to read. Why this migh t be useful? Consider a case in which 
almost all tuples are needed from one table, but only very few from another 
table? 
Later on we will get more into optimization details, for now let&#8217;s keep in mind 
that for many basic relational algebra operators we migh t have multiple imple&#173;
mentations, and that depending on data distribution and query characteristics 
(selectivit y), di&#64256;eren t plans migh t be better than others. 
Query Execution 
It is very common for DBMS to implemen t the Iterator model. The key idea is 
that each operator implemen ts the same interface: 
&#8226;	open 
4 6 Courtesy of Grundprinzip on Wikipedia.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Introduction to database internals (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>1970&#8217;s : Several camps of proponents argue about merits of these comp eting 
systems while the theory of databases leads to mainstream researc h projects. 
Two main protot ypes for relational systems were developed during 1974-77. 
&#8226;	Ingres: Developed at UCB by (including guess who? Stonebrak er and 
Wong). This ultimately led to Ingres Corp., Sybase, MS SQL Server, 
Britton-Lee, Wang&#8217;s PACE. This system used QUEL as query language. 
&#8226;	System R: Developed at IBM San Jose (now Almaden) and led to IBM&#8217;s 
SQL/DS &amp; DB2, Oracle, HP&#8217;s Allbase, Tandem&#8217;s Non-Stop SQL. This 
system used SEQUEL as query language (later SQL). Lots of Berkeley 
folks on the System R team, including Gray (1st CS PhD @ Berkeley), 
Bruce Lindsa y, Irv Traiger, Paul McJones, Mike Blasgen, Mario Schkol&#173;
nick, Bob Selinger , Bob Yost. 
Early 80&#8217;s : comm ercialization of relational syste ms 
&#8226;	Ellison&#8217;s Oracle beats IBM to market by readin g white papers. 
&#8226;	IBM releases multiple RDBMSs, settles down to DB2. Gray (System 
R), Jerry Held (Ingres) and others join Tandem (Non-Stop SQL), Kapali 
Eswaran starts EsVal, which begets HP Allbase and Cullinet 
&#8226;	Relational Technology Inc (Ingres Corp), Britton-Lee/Sybase, Wang PACE 
grow out of Ingres group 
&#8226;	CA releases CA-Univ erse, a commercialization of Ingres 
&#8226;	Informix started by Cal alum Roger Sippl (no pedigree to researc h). 
&#8226;	Teradata started by some Cal Tech alums, based on proprietary network&#173;
ing technology (no pedigree to software researc h) 
Mid 80&#8217;s : 
&#8226;	SQL becomes &#8221;intergalactic standard&#8221;. 
&#8226;	DB2 becomes IBM&#8217;s &#64258;agship product. 
1990&#8217;s: 
&#8226;	Postgres project at UC Berkeley turned into successful open source project 
by a large comm unity, mostly driven by a group in russia 
Illustra (from Postgres) Informix IBM &#8226;	 &#8594;&#8594; 
&#8226;	MySQL 
2 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>5 Process Models 
Parallelism is a key to performance, in parti cular when I/O waits migh t stall 
computation. To maximize throughput you need to have enough stu&#64256; going on 
in parallel to avoid waiting/stalling. 
Process models: 
&#8226;	Back in the days there was no good OS thread supp ort, DB pioneered this 
ground (also due to the need of supp orting many OSs) 
&#8226;	Process per DBMS worker (need for shared memory [ASK: is it clear why 
we need to share across multiple workers?], context switc h is expensiv e, 
easy to port, limited scalabilit y) 
&#8226;	Thread per DBMS worker (great if good OS thread supp ort, or using 
DBMS separate implemen tation of threads... pro: portabilit y, cons: du&#173;
plicate functionalities) 
&#8226;	Process/Thread pool, and scheduling/allo cation of DBMS workers to pro&#173;
cesses or threads. 
6 Parallel Architecture 
&#8226;	Shared Memory: typically inside one machine, for large installati on high 
costs. All process models are applicable. Great for OLTP, many imple&#173;
mentation form almost every vendor. 
&#8226;	Shared Nothing: typically as a cluster of nodes. Require good partitioning, 
which is easier for OLAP workloads (Teradata, Greenplum, DB2 Parallel 
Edition,...). 
&#8226;	Shared Disk: cluster of nodes with a SAN. Simple model, because ev&#173;
ery node can access all the data, but requires cache-coherence protocols. 
Oracle RAC, DB2 SYSPLEX. 
&#8226;	NUMA: not that common, we will not discuss. (Often DBMS treat it as 
either shared nothing or shared memory , depending how non-uniform it 
is). 
Di&#64256;eren t failure modes... Partial failure is good to have when possible. 
We will go back to paral lel architectures later on, and dis 
7 Query Processing 
Query parsing (correctness check)
Query admission control / authorization
4 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>7.1 Query Rewrite: 
View Rewrite 
Remem ber the other day schema: 
shifezzatony789lungo456mikebaffocarlo123namenicknamephonepersonoperation..laundromatirish pubirish pubchocolatesnowflakecoverupcaffe$10Mecon_valtitledescr....$2M...$5Mchocolatemikechiefsnowflakesoldtonycarlosnowflakechiefpers_nameoper_namerolsinvolved
Figure 2: Simple Schema for a portion of our Ma&#64257;a database. 
What are views? A &#8220;named-query&#8221;, or a &#8220;virtual-table&#8221; (sometime s mate&#173;
rialized). 
CREATE VIEW nick-cover AS 
SELECT nickname, coverup_name 
FROM operation o, involved i, person p 
WHERE p.name = i.person AND 
i.oper_name = o.name AND
o.econ_val &lt;= 5M;
schifezzalaundromatlungoirish pubbaffolaundromatnicknamecoverupnick-cover
Figure 3: Simple External Schema for a portion of our Ma&#64257;a database. 
SELECT nickname 
FROM nick-cover nc 
WHERE nc.coverup_name="laundromat"; 
After view rewriting: 
SELECT nickname 
FROM ( 
SELECT nickname, coverup_name 
FROM operation o, involved i, person p 
WHERE p.name = i.person AND 
i.oper_name = o.name AND 
5 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>o.econ_val &lt;= 5M 
) as n 
WHERE n.coverup_name="laundromat" 
7.1.1 Contrain t Elimination / Logical Predicates manipulation 
Another importan t step of query rewriting consists of Constant Elimination and 
Logical Predicates manipulation 
WHERE (a &gt; 50+57 OR a =107) AND b &gt; 105 and a=b AND b &lt; 108 
becomes (after constan t elimination, and logical predicate manipulations): 
WHERE a = 107 and a = b and b = 107 
7.1.2 Subquery Flattening 
As Mike mentioned the last class another key step is Subquery &#64258;attening (Not 
every optimizer will successfully do this, so you should always try to think of a 
non nested query if you can &#64257;nd one): 
SELECT nickname 
FROM operation o, involved i, person p 
WHERE p.name = i.person AND 
i.oper_name = o.name AND
o.econ_val &lt;= 5M AND
o.coverup_name="laundromat";
7.1.3 Seman tic Optimization (Integrit y Constrain ts) 
Sometimes, knowledge of integrit y constrain ts, in particular, foreign keys, can 
be leveraged to avoid performing joins. As an example consider the following 
query: 
SELECT nickname 
FROM operation o, involved i, person p 
WHERE p.name = i.person AND 
i.oper_name = o.name AND 
If we know from the foreign keys that every person that appear in the involved 
tuple is involved in some operation, we can skip the join with operations. 
6 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830/6.814 &#8212; Notes&#8727; for Lecture 4: 
Database Internals Overview 
Carlo A. Curino 
Septem ber 22, 2010 
1 Announcemen ts 
&#8226;	Problem Set 1 is due today! (For next time please submit some open 
non-esoteric format .txt .rtf .pdf) 
&#8226;	Lab 1 is out today... start it right away, is due in 8 days! Do not copy... 
we perform automatic checking of plagiarism... it is not a good gamble! 
Projects ideas and rules are posted online.&#8226; 
2 Readings 
For this class the suggested readings are: 
&#8226;	Joseph Hellerstein, Michael Stonebrak er and James Hamilton. Architec&#173;
ture of a Database System. Online at: http://db.cs.berkeley.edu/ 
papers/fntdb07- architecture.pdf 
It is a rather long paper (don&#8217;t be too scared by the 119 pages, the page 
format makes it look much longer than it is) that is in general worth reading, 
however we only require you too read sections: 1, 2 (skim through it), 3, 4 (up 
to subsection 4.5 included), 5. You can also skim through section 6 that we will 
discuss later on. Probably doesn&#8217;t all make sense right now &#8211; you should look 
at this paper again to this paper through the semester for context. 
3 A bit of history 
Complemen ting Mike&#8217;s historical overview... Projects ideas and rules are posted online. 
&#8727;These notes are only mean t to be a guide of the topics we touch in class. Future notes 
are likely to be more terse and schematic, and you are required to read/study the papers and 
book chapters we mention in class, do homew orks and Labs, etc.. etc.. 
1 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>4 2000&#8217;s: 
Postgres Netezza, Vertica, Greenplu m, EnterpriseDB... &#8226;&#8594; 
MySQL Infobrigh t &#8226;&#8594; 
Ingres DATAllegro &#8226;&#8594; 
System R is generally considered the more in&#64258;uen tial of the two &#8211; you can 
see how many of the things they proposed are still in a database system today. 
However, Ingres probably had more &#8221;impact&#8221; by virtue of training a bunc h of 
grad studen ts who went on to fund companies + build products (e.g., Berke&#173;
leyDB, Postgres, etc.) 
Introduction 
Figure 1 shows the general architecture of a database. 
Figure 1: Architecture of a DBMS 
Today we will mainly look at the big picture, and go through the relational 
query rewriting and execution, the following lessons will focus on each of the 
pieces in more details. 
Show &#64258;ow of a query 
3 Local Client
ProtocolsRemote Client
ProtocolsCatalog
Manager
Memory
Manager
Administration,
Monitoring &amp;
Utilities
Replication and
Loading
Services
Batch UtilitiesQuery Parsing and Authorization
Query Rewrite
Query Optimizer
Plan ExecutorDDL and Utility
ProcessingClient Communications Manager
Relational Query Processor
Access Methods
Lock ManagerBuffer Manager
Log Manager
Transactional Storage ManagerAdmission
Control
Dispatch
and 
Scheduling
Process
Manager
Shared Components 
and Utilities
Image by MIT OpenCourseWare.</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Degrees of consistency (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text> Aries issues:
Crash during recovery &#8211; do it again.
CLRs are used for escrow xacts &#8211; cannot be undone multiple times.  Do an example
Group commit &#8211; why required
Application errors:  roll forward to a specific point in time, then undo backward &#8211; just not
to the present.
HA:  standard wisdom; active-passive.  Roll log forward at passive site.  Failover,  by 
recovering.  In flight transactions get aborted; not exactly HA.  I.e. failover in seconds.
Active-active:  2 active sites, each does all xacts .  No log.  One is primary &#8211; other is
secondary.  If primary crashes, then keep going from secondary.  Best with a stored -
procedure interface.
To go fast:  H-store data pie.
Buffer pool
Locking 
Threading 
WAL 
See times-10
See NoSQL.
Solution:  main memory, one-xact at a time, single thread, no log &#8211; failover to a backup &#8211;
active-active.
Draw H-store picture.
Yabut;  multicore
Yabut:  multi-shard xacts &#8211; spec X
What about network partitions:
Primary can&#8217;t talk to secondary.  Both up.  Either:
</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text> Primary continues, secondary blocks (less availability)
Or
Both continue &#8211; no consisitency
Give up one or the other.  Brewer has a CAP theorem &#8211; says you can&#8217;t have all 3.
Application errors, human errors, resource issues, [run out of mem, run out of disk, run 
out of &#8230;]  install new software, reprovision &#8211; these dwarf network partitions.
Byzantine failures.   
Have to have 3 replicas and voting.  Nobody worries about this &#8211; except theoreticians
</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Optimistic concurrency control (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text> 
 Reads set no locks.  However, get a historical (consistent) answer.  After a while can 
garbage collect old values &#8211; when they are no longer needed &#8211; i.e. there is no running 
xact older than the next guy in line. 
Can be turned into a full cc system 
MVCC.  Give every Xact a timestamp
Have a read TS and a write TS for every &#8220;granule&#8221;
Read-only xact:  get a TS..  Read whatever you want.  If multiple versions, then read the
one written just earlier than your time stamp. If reading the current version, install your 
TS if greater than the one that is there.
Update xact:  given a TS.  Read whatever you want, installing a read TS as above. 
Write a new value with your timestamp, keeping the old value, as above.  Do this only if
your timestamp greater than both ones there.  Otherwise, commit suicide.
******
Locking is pessimistic &#8211; i.e. ensure no conflict by assuming the worst case.  Other 
approaches to concurrency control are more aggressive: 
Optimistic concurrency control (Kung and Robinson &#8211; late &#8216;70s) 
Run transaction to completion.  Check at end if there was a proble m.  
3 phases:  read/write, validate, commit Read/write:  do logic normally.  Any write goes to a private copy (think of it as an 
update list).  Keep track of read-set and write-set.  R(Ti)  W (Ti) 
At end, enter the validate phase:
For all xacts, Tj, which commited after I started:
W(Tj) intersect R (Ti) empty,  if fail then abort (and restart)  if succeed, then enter 
commit phase. Commit:  install updates 
Issues:  one validator at a time!  One commiter at a time! Bottleneck </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Issues:  lose all work on a abort 
Issues:  starvation (cyclic restart) 
Issues:  a bit pessimistic &#8211; possible to restart when there is not a conflict. 
*********************************************************** 
So which one wins?
Several simulation studies in the 80&#8217;s.  Most have Mike Carey as an author or co-author.
Variables:
Prob (contention)
# concurrent xacts
Resources available (disks, CPU)
Locking wins, except in corner cases.  
If no conflict, then nobody waits and it is a wash 
If lots of conflicts, then locking wastes less work 
*****
Modern day OLTP:
Main memory problem
No-disk stalls in a Xact
Do not allow user-stalls in a Xact (Aunt Millie will go out for lunch)
--- hence no stalls.
A heavy Xact is 200 record touches &#8211; less than 1 Msec.  
Why not run Xact to completion &#8211; single threaded!  No latches, no issues, no nothing.  
Basically TS order !!!
Problem:  multiprocessor support &#8211; we will come back to this.  
</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Consider 
Begin Xact
Select avg (sal)  from emp
End xact
Begin xact
Update emp set &#8230;
End xact
Lock escalation will occur;
Either reader will starve (without scheduling help)
Or 
Writers will starve (while reader is doing a long query)
Both undesirable&#8230;&#8230;  So what to do:
1) nearly always done &#8211; run analytics on a companion data warehouse
2) take the stall (rarely done)
3) run less than serializability
Hence: 
Degree 0: (read uncommitted)  no locks (guarantees nothing).  Can read uncommitted 
data 
Degree 1: (read committed) X locks held till EOT &#8211; R locks obtained only temporarily. 
(can only read committed data &#8211; however subsequent reads of the same thing can produce different answers 
Degree 2:  (repeatable read) X, S  locks held till XOT &#8211; serializable unless  you squint. 
Degree 3:  (serializable) S and X locks held till EOT plus solve the phantom problem. Really do the right thing 
Choose what you want to pay for.  Degree 0 solves big read problem &#8211;at the expense of 
getting the wrong answer. 
4) use multi-version system 
read is given a time stamp (TS).  A write installs new timestamp and keeps old data 
for a while.  Read is given the biggest TS less than his.  I.e. read is &#8220;as of a time&#8221;.  </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Scientific databases (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>38</slideno>
          <text>Other FeaturesOther Features
&#139;Provenance (lineage)
&#139;What calibration generated the data
&#139;What was the &#8220;cooking&#8221; algorithm
&#139;In general &#8211; repeatability of data derivation</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.830 / 6.814 Database Systems
Fall 2010
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Courtesy of LSST. Used with permission.</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>O(100) petabytesCourtesy of LSST. Used with permission.</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Traditional WisdomTraditional Wisdom
&#139;Cooking pipeline outside DBMS
&#139;Derived data loaded into DBMS for subsequent 
querying</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>My Proposal My Proposal ----SciDBSciDB
&#139;Build a commercial-quality array DBMS from 
the ground up.</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Other Features Other Features 
Which Science Guys WantWhich Science Guys Want
(These could be in RDBMS, but Aren(These could be in RDBMS, but Aren &#8217;&#8217;t)t)
&#139;Uncertainty
&#139;Data has error bars
&#139;Which must be carried along in the computation 
(interval arithmetic)</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>What to do with Scientific Data?
by
Michael Stonebraker</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>What DBMS to Use?What DBMS to Use?
&#139;RDBMS (e.g. Oracle)
&#139;Pretty hopeless on raw data
&#139;Simulating arrays on top of tables likely to 
cost a factor of 10-100
&#139;Not pretty on time series data
&#139;Find me a sensor reading whose average 
value over the last 3 days is within 1% of 
the average value over the adjoining 5 
sensors </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>SciDB DDL
CREATE ARRAY Test_Array  
&lt; A: integer NULLS,
B: double, 
C: USER_DEFINED_TYPE &gt; 
[I=0:99999,1000, 10, J=0:99999, 1000, 10] 
PARTITION OVER (  Node1, Node2, Node3 )  
USING block_cycl ic();
chunk 
size
1000overlap
10attribute 
names
A, B, Cindex names
I, J</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Other FeaturesOther Features
&#139;Time travel
&#139;Don&#8217;t fix errors by overwrite
&#139;I.e. keep all of the data
&#139;Named versions
&#139;Recalibration usually handled this way</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Wrapper on Top of RDBMS Wrapper on Top of RDBMS ----MonetDBMonetDB
&#139;Arrays simulated on top of tables
&#139;Layer above RDBMS will replace SQL with 
something friendlier to science
&#139;But will not fix performance problems!!
Bandaid solution&#8230;&#8230;</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>RasDaMan SolutionRasDaMan Solution
&#139;An array is a blob 
&#139;or array is cut into chunks and stored as a 
collection of blobs
&#139;Array DBMS is in user-code outside DBMS
&#139;Uses RDBMS as a reliable (but slow) file 
system
&#139;Grid support looks especially slow</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Array Query Language (AQL)
SELECT Geo-Mean (  T.B )
FROM Tes t_Ar ray  T  
WHERE  
T.I  BETWEEN :C1 AND :C2 
AND T. J BETWEEN :C3 AND :C4
AND T.A  =  10
GROUP BY T . I ;User-defined aggregate on an
attribute B in array T
Subsample
Filter
Group-by
So far as SELECT / FROM / WHERE / GROUP BY queries are 
concerned, there is little logica l difference between AQL and SQL</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Data Storage
&#8226; Optimized for both dense and sparse array data
&#137;Different data storage, compression, and access 
&#8226; Arrays are &#8220;chunked&#8221; (in multiple dimensions)
&#8226; Chunks are partitioned across  a collection of nodes
&#8226; Chunks have &#8216;overlap&#8217; to support neighborhood operations 
&#8226; Replication provides efficiency and back-up
&#8226; Fast access to data sliced along any dimension
&#137;Without materialized views
</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Array Query Language (AQL)Array Query Language (AQL)
&#139;Array data management (e.g. filter, aggregate, 
join, etc.)
&#139;Stat operations (multiply, QR factor, etc.)
&#139;Parallel, disk-oriented
&#139;User-defined operators (Postgres-style)
&#139;Interface to external stat packages (e.g. R)</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Deployment OptionsDeployment Options
&#139;External cloud (e.g. EC2)
&#139;Amazon can &#8220;stand up&#8221; a node wildly 
cheaper than Exxon &#8211; economies of scale 
from 10K nodes to 500K nodes
&#139;Security/company policy issues will be an 
issue
&#139;Amazon pricing will be an issue
&#139;Likely to be the cheapest in the long run</text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Chemical Plant DataChemical Plant Data
&#139;Record all data  
{(time, sensor-1, &#8230; sensor-5000)}
&#139;Look for &#8220;interesting events &#8211; i.e. sensor 
values out of whack&#8221;
&#139;Cluster events near each other in 5000 
dimension space
&#139;Idea is to identify &#8220;near-failure modes&#8221;</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Architecture
&#8226; Shared nothing cluster
&#137;10&#8217;s&#8211;1000&#8217;s of nodes
&#137;Commodity hardware
&#137;TCP/IP between nodes
&#137;Linear scale-up
&#8226; Each node has a processor and storage
&#8226; Queries refer to arrays as if not distributed
&#8226; Query planner optimizes queries for efficient 
data access &amp; processing
&#8226; Query plan runs on a node&#8217;s local 
executor&amp;storage manager
&#8226; Runtime supervisor coordinates executionApplication
Language &#160;Specific&#160;UI
Query&#160;Interface&#160;
and&#160;ParserRuntime&#160;Supervisor
Plan&#160;Generator
Node&#160;3Node&#160;2
Local&#160;Executor Local&#160;Executor
Storage&#160;Manager Storage&#160;ManagerNode&#160;1Application &#160;Layer
Server&#160;Layer
Storage&#160;LayerApplication
Language &#160;Specific&#160;UI
Query&#160;Interface&#160;
and&#160;ParserRuntime&#160;Supervisor
Plan&#160;Generator
Node&#160;3Node&#160;2
Local&#160;Executor Local&#160;Executor
Storage&#160;Manager Storage&#160;ManagerNode&#160;1Node&#160;3Node&#160;2
Local&#160;Executor Local&#160;Executor
Storage&#160;Manager Storage&#160;ManagerNode&#160;1Application &#160;Layer
Server&#160;Layer
Storage&#160;LayerDoesn&#8217;t require JDBC, ODBC
AQL an extension of SQL
Also supports UDFsJava, C++, whatever&#8230;</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>RDBMS SummaryRDBMS Summary
&#139;Issues not likely to get fixed any time soon
&#139;Science is small compared to business data 
processing</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>SciDB Data ModelSciDB Data Model
&#139;Nested multidimensional arrays
&#139;Augmented with co-ordinate systems 
(floating point dimensions)
&#139;Ragged arrays
&#139;Array values are a tuple of values and arrays</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Chemical Plant DataChemical Plant Data
&#139;Plant is a directed graph of plumbing
&#139;Sensors at various places (1/sec observations)
&#139;Directed graph of time series
&#139;To optimize output plant runs &#8220;near the edge&#8221;
&#139;And fails every once in a while &#8211; down for a 
week</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>General ModelGeneral Model
sensors
Cooking
Algorithm(s)(pipeline)Derived data</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>RDBMS SummaryRDBMS Summary
&#139;Wrong data model
&#139;Arrays not tables
&#139;Wrong operations
&#139;Regrid not join
&#139;Missing features
&#139;Versions, no-overwrite, provenance, support 
for uncertain data, &#8230;</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>What DBMS to Use?What DBMS to Use?
&#139;RDBMS (e.g. Oracle)
&#139;Spatial data may (or may not) be ok
&#139;Cylinder queries will probably not work 
well
&#139;2-D rectangular regions will probably be 
ok
&#139;Look carefully at spatial indexing support 
(usually R-trees)</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>LSST DataLSST Data
&#139;Raw imagery
&#139;2-D arrays of telescope readings
&#139;&#8220;Cooked&#8221; into observations
&#139;Image intensity algorithm (data clustering)
&#139;Spatial data 
&#139;Further cooked into &#8220;trajectories&#8221;
&#139;Similarity query
&#139;Constrained by maximum distance</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Deployment OptionsDeployment Options
&#139;Internal grid (cloud behind the firewall)
&#139;Mimic what Google/Amazon/Yahoo/et.al do
&#139;Other report huge savings in DBA/SE costs
&#139;Does not require you buy VMware
&#139;Requires a software stack that can enforce 
service guarantees </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Matrix Multiply
&#8226; Smaller of the two arrays is replicated at all nodes
&#137;Scatter-gather
&#8226; Each node does its &#8220;core&#8221; of the bigger array with the replicated smaller 
one
&#8226; Produces a distributed answerCREATE ARRAY TS_Data &lt; A1:int32, B1:double &gt;
[ I=0:99999,1000,0, J=0:3999,100,0 ]
Select  multiply (TS_data.A1, test_array.B) </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>But your mileage may varyBut your mileage may vary &#8230;&#8230;&#8230;&#8230;
&#139;SQLServer working well for Sloan Skyserver 
data base
&#139;See paper in CIDR 2009 by Jose Blakeley</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>How to Do Analytics (e.g.clustering)How to Do Analytics (e.g.clustering)
&#139;Suck out the data
&#139;Convert to array format
&#139;Pass to MatLab, R, SAS, &#8230;
&#139;Compute
&#139;Return answer to DBMS</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Satellite ImagerySatellite Imagery
&#139;Raw data
&#139;Array of pixels precessing around the earth
&#139;Spherical co-ordinates
&#139;Cooked into images
&#139;Typically &#8220;best&#8221; pixel over a time window
&#139;i.e. image is a composite of several passes
&#139;Further cooked into various other things
&#139;E.g. polygons of constant snow cover</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Deployment OptionsDeployment Options
&#139;Supercomputer/mainframe
&#139;Individual project &#8220;silos&#8221;
&#139;Internal grid (cloud behind the firewall)
&#139;External cloud (e.g. Amazon EC20</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Example LSST QueriesExample LSST Queries
&#139;Recook raw imagery with my algorithm
&#139;Find all observations in a spatial region
&#139;Find all trajectories that intersect a cylinder in 
time</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>OutlineOutline
&#139;Science data &#8211; what it looks like
&#139;Hardware options for deployment
&#139;Software options
&#139;RDBMS
&#139;Wrappers on RDBMS
&#139;SciDB</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>My preferenceMy preference
&#139;Load the raw data into a DBMS
&#139;Cooking pipeline is a collection of user-defined 
functions (DBMS extensions) 
&#139;Activated by triggers or a workflow 
management system
&#139;ALL data captured in a common system!!!</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Snow Cover in the SierrasSnow Cover in the Sierras
&#169; Source unknown.  All rights reserved. This content is excluded from our Creative 
Commons license. For more information, see http://ocw.mit.edu/fairuse.</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Example QueriesExample Queries
&#139;Recook raw data
&#139;Using a different composition algorithm
&#139;Retrieve cooked imagery in a time cylinder
&#139;Retrieve imagery which is changing at a large 
rate</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Deployment OptionsDeployment Options
&#139;Supercomputer/main frame 
&#139;($$$$)
&#139;Individual project &#8220;silos&#8221;
&#139;Probably what you do now&#8230;.
&#139;Every silo has a system administrator and a 
DBA (expensive)
&#139;Generally results in poor sharing of data</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Problems with This ApproachProblems with This Approach
&#139;Easy to lose track of the raw data
&#139;Cannot query the raw data
&#139;Recooking is painful in application logic &#8211;
might be easier in a DBMS (stay tuned)
&#139;Provenance (meta data about the data) is often 
not captured
&#139;E.g. cooking parameters
&#139;E.g. sensor calibration</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Bad NewsBad News
&#139;Painful
&#139;Slow
&#139;Many analysis platforms are main memory only</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>ORM, DryadLINQ (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>ORM: 
Problem: 
Impedance Mismatch (i.e., dif ferent languages for data and programming, need casting 
between types, makes analysis dif&#64257;cult) 
Solution: 
Object-Relation-Mapping middleware (provide an persistence abstraction for objects, and 
takes care of transformation from/to the DB world)
"Everyone who is somebody has one! Either standar d (e.g., hibernate) or ad-hoc." 
The idea is to provide:
- pre-canned mapping between OO classes/&#64257; elds and table/columns
- manually de&#64257;ned mappings
- provides object persistency without looking at the DB 
Good: 
- abstraction 
- ease of debug 
Bad: 
- performance 
Example: Hibernate 
Example of Hibernate Mapping 
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE hibernate-mapping PUBLIC "-//Hibernate/Hibernate Mapping DTD 3.0//EN" 
Application dialogs
- Swing
- SWT- Web application
Application logic
Class Order Class Customer
Instance of Customer class
Hibernate
Row of Customer table
Database
Table Order
-id
-number
-customer_idTable Customer
-id
-firstname-lastname -date
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>---------------------------------------------------------------------------------------
---"http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd" &gt;&lt;hibernate-mapping&gt;
&lt;class name="de.laliluna.example.Honey" table="honey"&gt;
&lt;id name="id" column="id" &gt;
 &lt;generator class="increment"/&gt;
&lt;/id&gt;
&lt;property name="name" column="fooname" /&gt;
&lt;property name="taste" column="bartaste" /&gt;
&lt;/class&gt;
&lt;/hibernate-mapping&gt; 
Example of Hibernate Usage (many details ar e hidden)
 Honey honey = new Honey();
honey.setName("forest honey");
honey.setTaste("very sweet");
&#8230;
 tx = session.beginTransaction();
session.save(honey);
tx.commit();
&#8230;
 tx = session.beginTransaction();
session.update(honey);
tx.commit();
&#8230;
 tx = session.beginTransaction();
List honeys = session.createQuery("select h from Honey as h").list();
tx.commit();
Next we talk about DriadLINQ&#8230; it provides similar features but adds much more in 
particular:
- LINQ language integration
- Batch-oriented 
- Cluster -oriented 
- Mor e than SQL </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Transactions and locking (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>T1: update Hugh(!!!!!) 
T1:  update Fred 
T1 commits 
Both xacts obey locking protocol &#8211; but result is not serializiable. 
Issue is lock things you touch &#8211; but must guarantee non-existence of any new ones!!! 
How:  predicate locking (tried &#8211; doesn&#8217;t work well &#8211; need a theorem prover &#8211; also, 
conflicts are data dependent) 
How range locks in a B-tree index (assumes an index on dept).  Otherwise, table lock on 
the data. 
Escrow  transactions 
Begin Xact 
Update flights (set seats = seats -1) where flight = 234 
. 
. 
. 
End xact 
Locks 234 for the duration of the transaction.  Nobody else can get a seat.  Two transactions can go on in parallel as long as they perform only increment and decrement 
operations.  Xacts commute, so everything is ok 
However, if a Xact aborts, have to be careful with recovery logic.  Forward pointer to 
Aries discussion. </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>Transactions 
Model: 
Begin xact 
Sql-1 
Sql-2 
. 
. 
. 
Sql-n 
commit or abort 
Concurrency control  (Isolation) 
Crash recovery (Atomic, Durable) 
Example:  move $100 from acct-A to acct-B 
Atomic:  all or nothing 
Durable:  once done, it stays done 
Isolation: produces the &#8220;right&#8221; answer in a concurrent world 
Consistent:  acct cannot go below zero 
Consistent &#8211; deals with integrity constraints, which we are not going to talk about. 
Concurrency control first: 
Consider: 
Bill shoe 10K 
Sam shoe 15K 
George toy 12K 
Hugh toy 8K 
Fred shoe 14K 
T1: Give a 10% raise to everybody in the shoe dept. 
T2: Move George and Hugh into the shoe dept 
Easy to create a parallel schedule where one receives the raise and the other does not.  
Definition:  Want an outcome w hich is the same as doing T1 first and then T2 or vica -
versa.  Serializiability.  
Gold standard is to ensure serializiability for any commands, and any amount of 
parallelism. </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Gold standard mechanism:
Divide the data base into granules (bits, bytes, records, &#8230;)
Lock everything you touch.
Ensure that locking is 2-phase, i.e. a growing phase and then a shrinking phase.  
(In practice grow phase is whole xact, shrink phase is at commit.)
Deep theorem:  2 phase locking &#61664; serializiability
Easy generalization to Share (read) locks and exclusive (write) locks
Therefore, lock everything you touch, hold all locks to EOT.  
Generally called two phase locking, or dynamic locking.  Used by ALL major DBMSs.
Devil is in the details:
How  big a granule to lock?   Records (page level locking gets hammered)
However, what to do with 
select avg (salary)
from emp
don&#8217;t want to set 10**7 locks.  
Answer lock escalation to table level.  If you set too many record locks in a table, then 
trade it in for a table lock.
What to do if you can&#8217;t get a requested lock? Wait
What about deadlock?   Can happen.  Periodically look for a cycle in the &#8220;waits for&#8221;
graph.  Pick a victim (one who has done less work) and kill him.  I.e. run crash recovery 
on his transaction (to be discussed next time).
Alternative:  time-out and kill yourself.
Possible to starve.  Repeated execution, retry cycle&#8230;.
Doesn&#8217;t happen in practice.  In a well designed OLTP system, the average Xact does not
wait.  If not true, then redesign app to make it true.  Rule of  thumb:  probability of
waiting is .01 or less.
</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>To avoid deadlock possibility:
All at once lock request (silly in practice)
Order all locks and request in order (silly)
Pre-emption (murder &#8211; not really done in practice)
What about aux iliary structures? 
Lock table: must use latches, semaphores, etc. to serialize 
Buffer pool:  ditto 
System catalogs (table table &#8211; drop table does a delete in the table table &#8211; other 
Xacts read a cached version -- generally finessed.  Often the #blocks is in the 
table table.  If updated, then do it with latches, not locks, &#8230;) 
B-trees:  too expensive to latch the whole tree 
Hence, latch single blocks on access.  Go to descendent block, latch it, 
release one up  (called latch crabbing).  Get to page to upda te, and latch it 
for the update.  
Lehman&amp; Yao have a scheme to avoid the latches (red book paper) &#8211; at 
the expense of noticeable complexity. 
Hallow een problem (urban myth that System R guys discovered this on Halloween).  
Aka phantom problem 
T1:  begin xact 
update emp (set salary = 1.1 * salary) 
Where dept = &#8216;shoe&#8217;
        End xact 
T2:  begin xact 
Insert into emp values (&#8216;George&#8217;, 20,000, &#8216;shoe&#8217;) 
Insert into emp values (&#8216;Hugh&#8217;, 30,000, &#8216;shoe&#8217;)
        End xact 
At beginning of xacts, there are 3 shoe dept employees:  Bill, Sam, and Fred. 
Suppose the emp table is sorted in storage on name, and holes are left for inserts.  Suppose query plan for T1 does a scan of emp.  Consider the following sequence of 
operations: 
T1:  update Bill 
T1:  update Sam 
T2:  insert George 
T2: insert Hugh 
T2 commits and releases all locks </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Indexing and access methods (CC)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>1</slideno>
          <text>3 Recap 
We presen ted a broad overview of the DBMS internals.
We point out how importan t coming up with a good plan is.
I claimed disk I/O can be very importan t.
Remem ber I mentioned this last time: 
CPU cost (# of instructions) 1Ghz == 1 billions instrs/sec 1 nsec / instr 
RAM access 50ns 
I/Ocost(#ofpagesread,#ofseeks) 100MB/sec 10nsec/byte 
(RandomI/O=pageread+seek) 10msec/seek 100seeks/sec 
1 seek = 10M instructions!!! 
Moreo ver there is a big di&#64256;erence from sequen tial and random IO. Example: 
&#8226;	Read 10KB from a random location on disk: 10ms + 100&#181;s = 10.1ms; 
&#8226;	Read 10KB+10KB from a random location on disk (two blocks are next 
to each other): 10ms + 200&#181;s = 10.2ms; 
&#8226;	Read 10KB+10KB from two random locations on disk: 10ms + 100&#181;s+ 
10ms +100&#181;s= 20.2ms; 
WOW! So saving disk I/O, and in particular random ones is VERY impor&#173;
tant!! DB are usually well designed to: 1) avoid excessiv e disk I/O and try to be 
sequen tial, and 2) have a LOT of drives available (TPC-H comp eting machines: 
144 cores AND 1296 disks !!) 
Today we study how to do a good job at reducing Disk I/O by organize stu&#64256; 
intelligen tly on disk, next lecture we study what we should keep in RAM. 
4 Access Metho ds 
Today we get into more details on Access Metho ds, i.e., on the portion of the 
DBMS in charge of managin g the data on disk. We will show a bunch of 
organization of data, and their performance in supp orting typical accesses we 
need to supp ort queries. Next lecture we will study the Bu&#64256;er Manager, which 
tries to reduce the access to disk. 
What are the functionalities we need to support: 
scan&#8226; 
&#8226;	searc h (equalit y) 
2 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>6.830/6.814 &#8212; Notes&#8727; for Lecture 6: 
Access Metho ds 
Carlo A. Curino 
Septem ber 29, 2010 
1 Announcemen ts 
&#8226;	Problem Set 2 is out today... due in two weeks... be aware that is going 
to overlap to LAB 2. 
Projects ideas and rules are posted online.&#8226; 
2 Readings 
For this class the suggested readings are: 
&#8226;	In Database Managemen t Systems, read: 
&#8211;	Pages 273-289. If you are using anoth er book, this is the introduc&#173;
tion to the Section on Storage and Indexing which discusses di&#64256;eren t 
access metho ds and their relativ e performance. 
&#8211;	Pages 344-358. This is an in-depth discussion of the B+T ree data 
structure and its implemen tation. Most database books, as well as 
any algorithms text (such as CLR or Knuth) will provide an equiv a&#173;
lent discussion of B+T rees. 
&#8211;	(was not assigned, but is an importan t reading) Pages 370-378 on 
Static Hashing and Extensible Hashing 
&#8221;The R*-T ree: An E&#64259;cien t and Robust Access Metho d for Points and &#8226; 
Rectangles.&#8221; Beckmann et al, in The Red Book. 
&#8727;These notes are only mean t to be a guide of the topics we touch in class. Future notes 
are likely to be more terse and schematic, and you are required to read/study the papers and 
book chapters we mention in class, do homew orks and Labs, etc.. etc.. 
1 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>7 B+trees 
Hierarc hical indices are the most common type used&#8212;e.g., B+-T rees indices 
typically point from key values to records in the heap &#64257;le. Shall we always have 
indexes? What are the pros and cons? (keep the index up-to-date, extra space 
required) 
Figure 1: B+T ree graphical represen tation. 
Special case, is a &#8220;clustered&#8221; index, i.e., when the order of the tuples on disk 
correspond to the order in which they are stored in the index. What is it good 
for? (range selections, scans). 
5 Courtesy of Grundprinzip on Wikipedia.32* 16*
1* 5* 21* 13*
10*
15* 7* 19*
4* 12* 20*33
2
2
2
3000
001010
011
100
101
110
111Bucket A
Bucket B
Bucket C
Bucket D
Bucket A2
('split image'of Bucket A)DirectoryLocal Depth
Global DepthLocal Depth
Global Depth32* 16*
1* 5* 21* 13*
10*
15* 7* 19*
4* 12* 20*22222
2
00
0110
11Bucket A
Bucket B
Bucket C
Bucket D
Bucket A2
('split image'of Bucket A)Directory
Image by MIT OpenCourseWare.</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>5 &#8226;	variable length, &#64257;eld slots, two options: delimiters or directory with point&#173;
ers/o&#64256;sets 
What happ en when the &#64257;eld size changes? Need to move stu&#64256;... so if we 
have a mix of &#64257;xed/v ariable, the &#64257;xed &#64257;elds are best to be stored &#64257;rst. Null 
values? Good trick is using the pointers/o&#64256;set.. if two have same value, it 
means the &#64257;eld in between is null... this makes storing nulls 0 space overhead. 
Cost model (enhanced one, from the book) 
&#8226;	Heap Files: Equalit y selection on key; exactly one matc h. 
&#8226;	Sorted Files: Files compacted after deletions.
Indexes:
&#8226; 
&#8211;	Alt (2), (3): data entry size = 10% size of record 
&#8211;	Hash: No over&#64258;ow buckets. 80%pageo ccupancy Filesize=1.25datasize&#8594;
&#8211;	Tree: 67% occupancy (this is typical). File size = 1.5 data size &#8594; 
We use: 
&#8226;	B: number of data pages 
&#8226;	R: number of record per page 
&#8226;	D: average time to read/write from disk 
&#8226;	C: average time to process a record (e.g., equalit y check) 
Extensible hashing 
Good read on the book: pages... 370-378 
4 6 Image by MIT OpenCourseWare.BD
BD1.5BD
BD (R+0.15)
BD (R+0.125)0.5BD
Dlog
2B
Dlog F1.5B
D (1 + 
logF0.15B)
2DBD
Dlog 2B +
# matches
Dlog F1.5B +
# matches
Dlog F0.15B +
# matches
BDSearch + D
Search + BD
Search + D
Search + 2D
Serach + 2D2D
Search + BD
Search + D
D (3 + 
logF0.15B)
4DHeap
Sorted
Clustered
Unclustered
tree index
Unclustered
hash indexScan Equality Range Insert Delete</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#8226;	searc h (range)
insert
&#8226; 
delete&#8226; 
Various access metho ds: 
&#8226;	heap &#64257;le: unordered, typically implemen ted as a linked list of pages 
&#8226;	sorted &#64257;le: ordered records, expensiv e to main tain 
&#8226;	index &#64257;le: data + extra structures around to quickly access data 
&#8211;	migh t contain data (primary index) 
&#8211;	or point at the data, often stored in a heap&#64257;le or other index (sec&#173;
ondary index) 
&#8211;	if the data are sorted in the same order of the &#64257;eld is index, we say is 
a cluster ed index (we will see this is good for scans since disk accesses 
are sequen tial) 
Type of indexes: 
hash&#8226; 
B+trees&#8226; 
R*trees&#8226; 
4.1 Data organization within &#64257;le 
&#64257;le organization: 
&#8226;	pages 
&#8226; records (record ids: page id, slot id)
page layout:
&#8226;	&#64257;xed length records page of slots, 
&#8226;	free bit map &#8221;slotted page&#8221; structure for var length records or slot direc&#173;
tory (slot o&#64256;set, len) 
What about big records? Hard to place, and migh t over&#64258;ow on another 
page. 
tuple layout (similar story): 
&#8226;	&#64257;xed length (structure know by the syste m catalog, can predict where 
&#64257;elds start) 
3 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Recovery (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text>Crash during recovery &#8211; do it again.
CLRs are used for escrow xacts &#8211; cannot be undone multiple times.
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Turn around and go forward, redoing the effects of all committed xacts.
If you crash during recovery, do it all again.
Example done in class
Problems:	 have to force buffer pool at a checkpoint -- expensive 
All operations logical.  Recovery may be slow -- have to do B-tree inserts 
and deletes Won&#8217;t work for escrow xacts &#8211; do example 
Aries:  more sophisticated, faster and way more complex.
We will simplify it somewhat &#8211; It is very complex.
Aries in a nutshell:
Cheap checkpoints
Physical redo
Logical undo
Redo-done first
Deals with escrow xacts (but we won&#8217;t)
Now the details
Every log record has a LSN (sequence#)
Dirty page table &#8211; block#, current-LSN  (dirtiers log record)
Xact table (1st-log record, last log record)
When you write a log record, you set the current in th e xact table to it.  You also store in
Each log record the previous current one &#8211; i.e. one way linked list of log records.
On a crash:
Find the dirty page table and the xact table.
Start at the min (LSN in dirty page table)
Physical redo to the end of the log  -- data plus B-trees &#8211; all xacts &#8211; whether committed or 
not &#8211; brings data base to the state at the time of the crash
Look at xact table.  Find max LSN.  Get that record and undo it.  Replace current in xact
table by it.  Go backwards to the end &#8211; doing logical undo.
</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>WAL recovery 
Options for the log: 
Can use a physical log.  Whenever the bits change, write a log record. 
Insert could cause logging of 4K worth of data 
Can use a logical log &#8211; record the SQL command (nobody does this &#8211; too slow -- and 
won&#8217;t work for undo)
Can use something in between &#8211; e.g. insert (record) on page-X.  We will assume for now
(insert, page#, slot#, bytes)
(delete, P#, slot#, bytes)
update, p#, slot#, old bytes, new bytes)
Can log B-tree updates
Physical:  means 8K bytes for a page splitter
Logical:  do nothing &#8211; side effect of SQL
In between: insert (key, block#)
We will assume for now &#8211; no logging of B-trees
One simple scheme.
Periodically take a checkpoint.  
Force all dirty blocks to disk 
Write a log record containing a list of active (uncommitted xacts) 
Do not log any information on B-trees
Logical (in between) data log  -- per above
On a crash.
Start at the end of the log.
Look for commit and abort records; keep a list of finished xacts.  Any log record that
corresponds to an uncommitted or aborted xact, perform undo, by logically undoing the
operation, but only if the after image matches the bytes on the page. modifying any 
affected B-trees, by searching the B-tree for the correct index key and fixing it, if
necessary.
When you reach a checkpoint, compare checkpoint list of active xacts, with commit/abort
of finishers list.  If checkpoint list in commit/abort list, then &#8220;far enough&#8221;.  Otherwise, 
keep going until you find such a checkpoint.
</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>C-store
Guest lecture: Sam Madden, MIT CSAIL</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>3</slideno>
          <text>Q4 is 400x faster on c-store -- why?
print o_orderdate, l_shipdate
group by o_orderdate
filter on l_orderkey, o_orderkey, o_orderdate
must be using D2: o_orderdate, l_shipdate, l_suppkey | o_orderdate, l_suppkey
D2 is missing o_orderkey and l_orderkey -- do we need them?
D2 already in good order to aggregate by o_orderdate
how much data is c-store scanning?
two columns with 60 M rows
 o_orderdate probably compressed down to a bit or byte
l_shipdate might be 4 bytes
so 300 MB?
 read from disk in 6 seconds
 read from RAM in 0.3 seconds
 actual performance is in between: 2 seconds
maybe skipping due to o_orderdate &gt; D? maybe some in mem, some in disk?
what is row DB probably doing? for 723 seconds
would have to scan 2 GB LINEITEM table
 if doesn't fit in RAM, 40 seconds at 50 MB/sec from disk
must join to ORDERS table, fits in memory, should be fast hash
then sort (or something) by o_orderdate
hard to guess why row DB takes 723 rather than 40+ seconds
Q2 is only 3x faster w/ c-store
needs l_suppkey, l_shipdate
filter by l_shipdate
group by l_suppkey
probably uses D1: l* | l_shipdate, l_suppkey
D1 lets c-store only look at l_shipdate = D, needn't scan most of LINEITEM
D1 sorted well for aggregation
what would row DB do?
 maybe has a b+tree also keyed by l_shipdate, l_suppkey?
does not need to scan or seek into LINEITEM
They win by keeping multiple copies, tailored to different queries
How much storage penalty for queries in Eval?
Actually LESS storage! 2 GB vs 4.5 GB
Uncompressed data was also about 2 GB
Would be more for more queries </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>How long would queries take on traditional DB?
probably have to look at every page of fact table
even if only 1% of records pass filter
means every block might have one relevant record
so index into fact table may not be very useful
joins to dimension tables pretty cheap
they fit in memory, fast hash lookups
how long to read the whole fact table?
35 TB, say 100 disks, 50 MB/sec/disk =&gt; 2 hours
outch!
You can imagine building special setups
e.g. maintain aggregates in real time -- pre-compute
know all the queries in advance
update aggregate answers as new data arrives
table of daily sales of Nikon cameras, &amp;c
but then hard to run "ad-hoc" queries
C-Store 
Why columns?
Why store each column separately?
avoid reading bytes from fact table you don't need 
Why "projections" of columns?
you usually want more than one column
e.g. sid and price for example 1 
Why is a projection sorted on one of the columns?
to help aggregation: bring all data for a given store together
or to help filtering by bringing all data w/ given col value together
so you only have to read an interval of the column 
What projection would help example 1?
columns: sid, price, store.state, time.day
note we are including columns from multiple logical tables
note we are duplicating a lot of data e.g. store.state
note projection must have every column you need -- can't consult "original" row
thus you don't need a notion of tupleID
note i'th row in each column comes from same xact row
 order?
 sid
 state, sid
Why multiple overlapping projections?
why store the same column multiple times? 
What projection would help example 2?
columns: price, time.year, time.mon, time.day, product.supplier
note we are not including join columns! e.g. pid
order?
 supplier, year, mon, day
year, mon, day, supplier
What if there isn't an appropriate projection for your query?
You lose -&gt; wait 2 hours
 Ask DB administrator to add a projection 
Could we get the same effect in conventional DB?
Keep heap files sorted ("clustered")?
can only do it one way
B+Trees for order and filtering? </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare
http://ocw.mit.edu 
6.830 / 6.814 Database Systems
Fall 2010
 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text> have to avoid seeks into main heap file, so multi-key B+trees
copy data into many tables, one per projection
So yes, we could
But very manual
choose right table for each query
updating?
"materialized views" partially automates this for conventional DB
and Eval in Section 9 shows they make row store perform 10x better
but c-store still faster
Won't all this burn up huge quantities of disk space? 
How do they compress? 
Why does self-order vs foreign-order matter in Section 3.1? 
How to compress for our example projections?
sid ordered by sid?
price ordered by sid?
store.state ordered by sid?
time.day ordered by sid? 
Won't it be slow to update if there are lots of copies? 
How does C-Store update efficiently? 
How does C-Store run consistent r/o queries despite updates? 
Why segment across a cluster of servers?
Parallel speedup
many disks, more memory, many CPUs 
How do they ensure good parallel speedup on a cluster?
What is a "horizontal partition"?
Why will that lead to good parallel speedup?
Sorting allows filtering and aggregating to proceed in parallel
will talk about parallel DBs more later 
Evaluation? Section 9
 what are the main claims that need to be substantiated?
 faster on data warehouse queries than a traditional row store
uses a reasonable amount of space
Experimental setup
standard data-warehouse benchmark "TPC-H"
 single machine
one disk
 2 GB RAM
 this is a little odd -- original data also 2 GB
small reduction in memory requirement could give a huge boost in this setup
but make no difference for larger data sets 
TPC-H scale_10
 standard data warehouse benchmark
 comes in different sizes ("scale")
defines how many rows in each table
customer: 1.5 M rows, abt 15 MB
orders: 15 M rows, abt 150 MB
lineitem: 60 M rows, abt 2.4 GB
results are spectacular!
mostly &gt; 100x faster than row store </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text> 6.830 2010 Lecture 15: C-Store (Sam Madden) 
Why are we reading this paper?
C-store has standard interface, but very different design
Help us understand what choices standard DBs made
Think about different set of apps than OLTP 
Paper status
Most individual techniques already existed
C-Store pulls them together
Not just a limited special-purpose DB
transparent -- sql interface
read/write, not just r/o
consistency
transactional update
Paper doesn't describe complete system
design + partial implementation
Commercialized as Vertica
What's a data warehouse?
 big historical collection of data
companies analyze to spot trends &amp;c
what products are in? what's out? where is cherry coke popular?
spot early, order more
mostly r/o but must be updated, maybe continuously
Example: big chain of stores, e.g. Walmart
each store records every sale
upload to central DB at headquarters
keep the last few years 
Typical schema (logical):
time(tid,year,mon,day,hour) product(pid,type,color,supplier)
 xact(tid,pid,sid,cid,price,discount,tax,coupon,&amp;c)
 store(sid,state,mgr,size) customer(cid,city,zip) 
called a "star schema"
 "fact table" in the middle
 gigantic # of rows
might have 100s of columns
"dimension tables" typically much smaller 
How big is the data?
50K products (5 MB)
3K stores (1 MB)
5M customers (5 GB)
150K times (5 MB) (10 minute granularity)
350B xact rows (35 TB) (100 bytes/sale)
3000 stores * 10 registers * 20 items/min * 2 years 
example 1:
total sales by store in Texas on Mondays
join xact to time and store
filter by day and state
group by sid, aggregate 
example 2:
average daily sales for Nikon cameras
join xact to product, time
filter by supplier
group by day, aggregate </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Recovery (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec12/</lecture_pdf_url>
      <lectureno>12</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text>Modern day OLTP: 
Main memory problem
No-disk stalls in a Xact
Do not allow user-stalls in a Xact (Aunt Millie will go out for lunch)
--- hence no stalls.
A heavy Xact is 200 record touches &#8211; less than 1 Msec.  
Why not run Xact to completion &#8211; single threaded!  No latches, no issues, no nothing.  
Basically TS order !!!
Problem:  multiprocessor support &#8211; we will come back to this.  
Ok to ask for Xact classes in advance (no ad-hoc updates in an OLTP system).
Look at the xacts classes&#8230;..
They might commute:  if so run with no locking
They might never conflict &#8211; if so run with no locking.
Might be only two classes that conflict (Ti and Tj).  Run everybody else with no controls.  
Serialize Ti and Tj (with timestamp techniques or something else)
If a transaction is alive for nanoseconds (processor transactional memory) or 
microseconds (modern OLTP), then interesting to rerun Carey simulations (which 
assumed disk not main memory). 
Contracts/Saga 
Vacation in San Diego 
T1:  get a plane ticket 
T2: get a hotel 
T3: get a rental car 
T4: tickets to San Diego zoo 
Oops &#8211; get sick &#8211; can&#8217;t go.  Want to &#8220;unwind&#8221; whole &#8220;workflow&#8221;.  Want something 
bigger than a Xact which can be reversed.  Notion of Sagas and Contracts.  Need 
compensation actions, which will reverse a xact.  Can&#8217;t abort after a commit.  </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Insert could cause logging of 4K worth of data
Can use a logical log &#8211; record the SQL command (nobody does this &#8211; too slow -- and 
won&#8217;t work for undo)
Can use something in between &#8211; e.g. insert (record) on page-X
Can log B-tree updates
Physical:  means 8K bytes for a page splitter
Logical:  do nothing &#8211; side effect of SQL
In between: insert (key, block &#8211; X)
Most of these have been used at one time or another.
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Crash recovery
Never lose my data ever.  Surest recipe to get fired on the spot.
Scenarios:
1) transaction aborts  (back him out)
2) transaction deadlocks, and is picked as a victim (ditto)
3) transaction violates  an integrity constraint (ditto)
OS fails (rule of thumb &#8211; MVS crashes once a year, Linux once a month, Windows once 
a week or more)  -- reload OS, reload DBMS, undo losers, redo winners 
DBMS fails 
bohrbugs (repeatable).  These are knocked out quickly b y a good QA process.  If 
you are buying a DBMS, get clear on how serious the vendor is about QA (typically not very)  -- don&#8217;t run a DBMS in production until it is &#8220;mature&#8221; -- like 
1-2 years after release 
heisen bugs (not repeatable)  timing problems, race conditions, &#8230;  Unbelievably hard to find.  Usually put engineers on airplanes. 
Disk crash:  modern disks fail every 5 years or so.  Usually start to see disk errors (redo 
reads or writes in advance).  In any case, take a dump periodically, weekly ful l with daily 
partials.  Must roll forward from the last partial, redoing history using a log. 
Bad, bad, bad, bad:  unrecoverable failures (corrupted the log)  -- &#8220;up the creek&#8221; 
App fails: 
Not an issue in this class 
Comm. Failures: 
We will come back to these when we deal with multi-processor issues 
Disaster  -- Machine room fails (fire, flood, earthquake, 9/11, &#8230;) 
1970&#8217;s solution to disasters: 
Write a log (journal) of changes
Spool the log to tape
</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Hire iron mountain to put the tapes under the mou ntain 
Buy IBM hardware &#8211; they were heroic in getting you back up in small numbers of 
days 
1980&#8217;s solution 
Hire Comdisco to put the tapes at their machine room in Atlanta 
Send your system programmers to Atlanta, restore the log tapes, divert comm to 
Atlan ta 
Back up in small numbers of hours 
(average CIO conducts a disaster drill more than once a year) 
2000&#8217;s solution (for some) 
Run a dedicated &#8220;hot standby&#8221; in Atlanta 
Fail over in seconds to a few minutes 
Driven by plummeting cost of hardware and the increasing cost of  downtime (thousands 
of dollars per minute) 
In most cases, you tend to lose a few transactions.  Too costly to lower the probability to zero.  Write disgruntled users a check!!! 
Disk -intact recovery 
1) undo uncommitted transactions 
2) redo committed transactions 
write this stuff in a log 
depends on buffer pool tactics. 
Buffer pool implements &#8220;steal&#8221; &#8211; can write to disk a dirty page of an uncommitted xact &#8211; 
when it needs the slot for something else.  All systems do this.  Requires the before image 
in a log to perform undo 
Buffer pool implements &#8220;no force&#8221; &#8211; do not require dirty blocks to be forced at commit time &#8211; takes too long.  Everybody does this.  Requires the after image to perform redo 
Hence write (before image, after image) in a  log.  Must write the log before writing the 
data.  Otherwise screwed.  Hence WAL. 
Options for the log: 
Can use a physical log.  Whenever the bits change, write a log record. </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>The relational model (MS)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-830-database-systems-fall-2010/resources/mit6_830f10_lec02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>6</slideno>
          <text>Data base design problem &#8211; which one to choose
High level language for access (physical data independence)
What you want not how to get it
Codd&#8217;s proposals (he did cellular automata previously)
Data language alpha (basically 1st order predicate calculus)
Relational algebra (a collection of operations chained together &#8211; APL style)
No mere mortals could understand either language
SQL (and Quel) were much more accessible
Find the cages that Sam touches
Select cid
From Animals
Where zid in
Select id 
From Keepers 
Where name = &#8216;Sam&#8217; 
Complete physical data independence 
Eliminates redundancy 
Better change at logical data independence  (views) 
Define view (Sam_Cages) as 
Select cid 
From Animals 
Where zid in 
Select id 
From Keepers 
Where name = &#8216;Sam&#8217; 
Select &#8230;
From Sam_Cages
Where &#8230;
All queries and many updates can be supported on views.  Interested reader is referred to 
SIGMOD 1976 and a litany of papers that have followed.
Debate raged throughout the 1970&#8217;s over:
</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text> cannot do inserts into sequential root structure 
maintenance required if you change the tuning knobs!  Hard to tell how bad it will 
be. 
One of Codd&#8217;s key points!!!  This is terrible system design 
d) limited logical data independence 
Zoo acquires a pair of Pandas &#8211; they have 2 feeding times! 
Keepers
 |
Animals
| |
     Feedings Cages
Must change the schema! 
As a cost cutting measure, Zoo management decides a keeper will be responsible for a cage &#8211; and all the animals in that cage.
       Keepers
|
    Cages
|
      Animals
Should change the schema to match the business problem 
Management decides to have &#8220;patrons&#8221; who buy cages
             Keepers
 |
Animals
 |
Cages
 |
Patrons
Schemas change for all these reasons &#8211; plus 
Feds change the rules (OSHA)
Tax rules change (IRS)
Merge with another zoo
</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Keepers Keepers 
1)  | 2)  | 
Animals Cages
 |  | 
Cages Animals 
Instance of 1) 
Sam 
Freddie 
1 
Jimmy 
2 
Sally 
1 
All have redundancy! 
1) repeat cage info for animals which share a cage 
2) repeat cage info for animals in a shared cage with different keepers 
Bad:  possibility of inconsistency 
Fundamental problem: 
Keepers Cages 
| | 
Animals 
Cannot be represented as a hierarchy! IMS Storage 
Root dependents 
***** ********* 
Sequential Sequential 
Index Sequential 
Hash Pointer spaghetti 
Index Pointer spaghetti 
Note:  no indexes on dependent segments! DL/1 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Current of app
Current of every record type
Current of every set type
(6 pins) 
Programming is 
Find an entry point Navigate in N-D space 
For a defense of this programming style see 1973 Turing award lecture by Charlie
Bachmann.
Codasyl issues
Horrible complexity.
No physical data independence &#8211; change most anything &#61664; recode
No logical data independence &#8211; change most anything -&gt; recode
If you screw up the data structure inadvertently, then must reload everything. (No 
isolation)
Initial load must be done all at once.  &#8211; many hours.
Codd:  relations
Unordered collections of tuples
Animals (name, species, age, feeding_time)
Cages (id, size)   
Keepers (id, name, address)
**************************************
Animals (name, species, age, feeding_time, cid, kid)
Cages (id, size)   
Keepers (id, name, address)
Or
Animals (name, species, age, feeding_time)
Cages (id, size)   
Keepers (id, name, address)
Lives_in (aname, cid)
Cared_for_by (aname, kid)
</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>MIT OpenCourseWare 
http://ocw.mit.edu 
6.830  / 6.814 Database Systems 
Fall 2010 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Every segment has a hierarchical sequential key (HSK) 
Key of the segment, prepended by keys of path back to the root 
All segments are logically in HSK order (for purposes of DL/1) 
Commands 
GU [segment type] [predicate] 
GN 
GNP 
D 
I 
Find all cages that Sam enters: 
GU Keepers (name = &#8216;Sam&#8217;) 
Until no more 
GNP Cages 
Find the keepers that enter cage 6
GU Keepers
GNP Cages (id = 6)
Until no more 
GN Keepers GNP Cages (id = 6) 
Notes:  GU is really get first 
Some commands are fast; some are slow; depends on the storage chosen and the schema 
chosen.  IMS wizards make gobs of money; even today. 
IMS problems: 
a) duplication of data (we talked abou t this above) 
b) painful low level programming interface &#8211; have to program the search algorithm 
c) limited physical data independence 
change root from indexed to hash --- programs that do GN on the root segment 
will fail </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>3 data models from the 1970s
Hierarchical (IMS + DL/1)
Network (Codasyl + Codasyl DML)
Relations (Codd proposal + DL/alpha
Relational algebra SQL 
Quel) 
Themes: 
Data redundancy Physical data independence 
Logical data independence 
High level language 
Why study ancient history? 
&#8220;Those that do not understand the mistakes of their ancestors will end up repeating them&#8221; 
Use  Zoo example (with one more kind of object) 
3 objects 
2 relationships 
Animals (Name, species, age, feeding_time) ------------------- -
|   lives_in | 
Cages (id, size)   &#61663;---------------------- |    cared_for_by 
| 
Keepers (Name, address)&lt;----------------------------------------- -
Each animal in ONE cage, multiple animals can share a cage 
Each animal cared for by ONE keeper, a keeper cares for multiple animals 
IMS  (IBM 1968) 
Segment types  (record types) 
Segments (instances) 
Schema (hierarchical collection of segment types &#8211; must be a tree) 
Possible schemas </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Whenever the logical schema changes &#8211; you need program maintenance &#8211; unknown 
complexity!!  In the worst case, toss everything; begin again!!
This was what was motivating Codd &#8211;
Codasyl (Committee on Data Systems Languages)
BF Goodrich (the guys without the blimp) built a prototype
Commercialized by Cullinet Corp. as IDS
Codasyl committee wrote standardization documents closely linked to IDS
Record types (like IMS)
Connected by named Sets (1::n relationships)
Arranged in a graph
Keepers Cages 
| | 
Cared_for_by | | Lives_in 
\/ \/ 
Animals 
Models network data directly.  Ought to be better than IMS. 
Record types 
1) hashed 
2) clustered with the owner record in some set 
Pointer spaghetti for set implementation 
If a record is not hashed, then it can only be accessed through set membership 
. 
Codasyl DML 
Find the cages that Sam enters 
Find Keepers (name = &#8216;Sam&#8217;) 
Until nomore 
Find next Animal in Cared_for_by 
Find parent in Lives_in 
IMS has current segment; current parent ( 2 pins) 
Codasyl has </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Efficiency 
Programmability of high level languages 
Cobol 
Won hands down by relational model. </text>
        </slide>
      </slides>
    </lecture>
    <videos>
      <video>
        <video_url/>
        <video_title/>
        <transcript>
          <slice>
            <text_slice/>
            <time_slice/>
          </slice>
        </transcript>
      </video>
    </videos>
  </lectures>
</doc>
