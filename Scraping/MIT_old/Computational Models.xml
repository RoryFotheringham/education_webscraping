<doc>
  <source>MIT</source>
  <date>28/01/2023</date>
  <course>
    <course_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/</course_url>
    <course_title>Computational Models of Discourse</course_title>
    <course_tags>
      <list>Engineering </list>
      <list>Computer Science </list>
      <list>Humanities </list>
    </course_tags>
  </course>
  <lectures>
    <lecture>
      <lecture_title>Rhetorical Structure Theory (PDF)
Guest Speaker: Florian Wolf</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec09/</lecture_pdf_url>
      <lectureno>9</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Accuracy 
&#8226;	Compared against manually constructed trees
&#8226;	Tested against human-constructed trees 
&#8226;	Automatically constructed trees exhibit high 
similarity with human-constructed trees 
&#8226;	However, see (Marcu&amp;Echihabi, 2002) CONTRAST vs ELABORATION: only 61 from 238 have a 
discourse marker (26%) 
Domain-independent Models of Text Structure	 14/16 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Domain-Independent Rhetorical Model
Model elements: &#8226; 
&#8211;	Binary Relations 
&#8211;	Compositionality Principle 
&#8226;	Requirements: 
&#8211;	Stability and Reproducibility of an Annotation 
Scheme 
&#8211;	Expressive Power of a Model 
Domain-independent Models of Text Structure	 4/16 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Automatic Computation
(Marcu, 1997; Marcu&amp;Echihabi, 2002) 
Surface cues for discourse relations: 
I like vegetables, but I hate tomatoes. 
Domain-independent Models of Text Structure 12/16 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Relations 
Relation Nucleus Satellite 
Background text whose understanding 
is being facilitated text whose understanding 
is being facilitated 
Elaboration basic information additional information 
Preparation text to be presented text which prepares the 
reader to expect and in&#173;
terpret the text to be pre&#173;
sented 
Domain-independent Models of Text Structure 10/16 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Other Words Also Count!
(Marcu&amp;Echihabi, 2002)
Surface cues for discourse relations:
I like vegetables, but I hate tomatoes. 
Domain-independent Models of Text Structure 15/16 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Is it Realistic 
Domain-independent Models of Text Structure 3/16 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Domain-Dependent Rhetorical Model 
Domain: Scienti&#64257;c Articles 
&#8226;	Human exhibit high agreement on the annotation 
scheme 
&#8226;	The scheme covers only a small fraction of discourse relations 
Domain-independent Models of Text Structure	 2/16 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Rhetorical Structure Theory
(Mann&amp;Thompson:1988, Matthessen&amp;Thompson:1988)
&#8226;	Developed in the framework of natural language 
generation 
&#8226;	Aims to describe &#8220;building blocks&#8221; of text structure
&#8211;	Nucleus vs Satellites 
&#8211;	Binary Relations between Discourse Units 
&#8226;	Compositionality principle de&#64257;ne how to build a
tree from binary relations
Domain-independent Models of Text Structure	 5/16 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
[No matter how much one wants to stay a non-smoker, A&#160;
], [the truth is that the pressure to smoke in junior high is 
Bgreater than it will be any other time of one&#8217;s life. ]. [We 
know that 3,000 teens start smoking each day , C][although 
it is a fact that 90% of them once thought that smoking was 
Dsomething that they&#8217;ll never do. ]&#160;
Domain-independent Models of Text Structure 6/16 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Method
&#8226;	Assume that certain markers unambiguously predict
discourse relations
&#8226;	Create Cartesian product of words located on two
sides of a discourse marker
&#8226;	For each pair of words, compute its likelihood to
predict a discourse relations
&#8226;	argmax rk&#160;P(rk|(s1,s2))&#160; =
argmax rk&#160;P((s1,s2)|rk)&#8727; P(rk), where
P((s1,s2)rk)=&#160;&#65533;
i,j&#8712;s1,s2&#160;P((w i,w j)rk)
 |	 |
Domain-independent Models of Text Structure	 16/16 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Domain-independent Models of Text
Structure
Regina Barzilay 
March 3, 2003 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Automatic Computation of RST Relations 
(Marcu, 1997) 
&#8226;	Aggregate discourse relations to a few stable
groups: (contrast, elaboration, condition,
cause-explanatuin-evidence)
&#8226;	Establish deterministic correspondence between cue 
phrases and discourse relations: 
&#8211;	{But, However }&#8594; Contrast 
&#8211;	{In addition, Moreover }&#8594; Elaboration 
Domain-independent Models of Text Structure	 13/16 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Compositionality
Whenever two large text spans are connected through a 
rhetorical relation, that rhetorical relation holds 
between the most important parts of the constituent 
spans. 
Marcu (1997): used constraint-satisfaction approach to 
build discourse trees given a set of binary relations 
Domain-independent Models of Text Structure 9/16 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Ambiguity
John can open the safe.
He knows the combination.
Domain-independent Models of Text Structure 11/16  
 
To see this image, go to 
http://images.google.com/images?q=yolady.gif</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
RST tree
JUSTIFICATION 
A B C D 
JUSTIFICATION CONCESSION 
Domain-independent Models of Text Structure 8/16 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Domain-Dependent Content Models 
&#8226; Capture topics and their distribution 
&#8226; Based on pattern matching techniques 
&#8211; Motifs of semantic units 
&#8211; Distributional model 
&#8226; Useful in generation and summarization 
Domain-independent Models of Text Structure 1/16 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Binary Relations 
&#8226; (JUSTIFICATION, A, B) 
&#8226; (JUSTIFICATION, D, B) 
&#8226; (EVIDENCE, C, B) 
&#8226; (CONCESSION, C, D) 
&#8226; (RESTATEMENT , D, A) 
Domain-independent Models of Text Structure 7/16 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Discourse Structure in Text Summarization; Alignment (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec10/</lecture_pdf_url>
      <lectureno>10</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Alignment in MT
&#8226;	Alignment task: Given bitext, identify units which 
are translations of each other. 
&#8226;	Units: paragraphs, sentences, phrases, words.
&#8226;	Usage: &#64257;rst step for full translation(Brown et al), 
lexicography(Dagan &amp; Church, Fung &amp; McKeown), 
aid for human transaltors(Shemtov), multi-lingual 
IR. 
Summarization	 17/39 </text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Content Structure Induction 
Automatically induced topic labeling via clustering
Lisbon has a mild and equable climate, with a mean annual temperature of 63 degree F (17 degree C). The 
proximity of the Atlantic and the frequency of sea fogs keep the atmosphere humid, and summers can be 
somewhat oppressive, although the city has been esteemed as a winter health resort since the 18th century . 
Average annual rainfall is 26.6 inches (666 millimetres). 
Jakarta is a tropical, humid city , with annual temperatures ranging between the extremes of 75 and 93 degree F (24 and 34 degree C) and a relative humidity between 75 and 85 percent. The average mean temperatures 
are 79 degree F (26 degree C) in January and 82 degree F (28 degree C) in October. The annual rainfall is 
more than 67 inches (1,700 mm). Temperatures are often modi&#64257;ed by sea winds. Jakarta, like any other large 
city , also has its share of air and noise pollution. 
Summarization 32/39 </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Domain-Dependent Structure-Based
Alignment
(Barzilay&amp;Elhadad, 2003) Assumption: Weak similarity 
function augmented with structural information 
Content Structure Induction &#8226; 
&#8226; Learning of Structural Mapping Rules 
&#8226; Macro Alignment 
&#8226; Micro Alignment 
Summarization 31/39 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Alignment Input
Amsterdam is the largest city in The Netherlands and the countrys economic center. It is the of&#64257;cial capital 
of The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams 
historic attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old 
houses. Amsterdam is also famous for its atmosphere of freedom and tolerance. 
City and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the capital and the principal commercial and &#64257;nancial centre of The Netherlands. To the scores of tourists who 
visit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the 
distinctive colour and &#64258;avour of its old sections, which have been so well preserved. However, visitors to the city also see a crowded metropolis beset by environmental pollution, traf&#64257;c congestion, and housing 
shortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a 
bygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and 
the atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing urban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of government, which is The Hague. The royal family , for example, is only occasionally in residence at the 
Royal Palace, on the square known as the Dam, in Amsterdam. 
Summarization 15/39 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Feature Selection
&#8220;Deep Features&#8221; 
Rhetorical structure based &#8226; 
&#8211;	RST (Marcu, 2000) 
&#8211;	Domain-dependent argumentative structure 
(Teufel&amp;Moens, 2000) 
&#8226; Content-based (Barzilay&amp;Lee, 2003) 
Around 10% improvement 
Summarization	 13/39 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Computational Geometry Methods 
discovered TPC 
undiscovered TPC noise 
main 
diagonal next 
TPC chain 
search 
frontier 
search 
frontier search 
rectangle 
previous chain 
Summarization 25/39 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Feature Selection 
Shallow Features: 
&#8226;	Locational Features (in the newspaper genre, the 
&#64257;rst paragraph is a summary) 
&#8226;	Presence of cue words (e.g., &#8220;in conclusion&#8221;) 
&#8226;	Sentence length 
&#8226;	Number of highly weighted words in a sentence 
Summarization	 8/39 </text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Micro-Alignment 
&#9127;&#160;
&#9130;s(i,&#160; j &#8722;1)&#8722; skip&#160; penalty&#160; &#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;s(i&#8722;1,j )&#8722; skip&#160; penalty&#160; &#9130;&#160;&#9130;&#160;&#9130;&#160;&#9128;&#160;s(i&#8722;1,j&#8722;1) +sim(i,&#160; j )&#160;s(i,&#160; j )=max&#160;&#9130;&#160;&#9130;&#160;s(i&#8722;1,j&#8722;2) +sim(i,&#160; j )+sim(i,&#160; j &#8722;1)
&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;s(i&#8722;2,j&#8722;1) +sim(i,&#160; j )+sim(i &#8722;1,j )
&#9130;&#160;&#9130;&#160;&#9129;&#160;s(i&#8722;2,j&#8722;2) +sim(i,&#160; j &#8722;1) +sim(i &#8722;1,j )&#160;
Summarization 36/39 </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Learning of Structural Mapping Rules
Classi&#64257;cation on cluster level
Features: words, cluster type
j2 j1 Corpus2 Corpus1 
i1 i2 
Par.2 Par.1 Par.3 Par.1 
Par.2 
Par.1 Text Text 
Text Text ClusterB
ClusterE
Cluster1 
Summarization 34/39 </text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Macro-Alignment
For unseens pair of texts applied a trained classi&#64257;er to 
generate possible mappings 
Text 1 Text 2 
Par. 2 
Cluster B 
Par. 2 Par. 7 
Cluster E Cluster 3 
Par. 13 
Cluster G 
Summarization 35/39 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Zipf Distribution
The product of the frequency of words (f) and their 
rank(r) is approximately constant: f&#160;&#8727; R&#160;=&#160;C&#160;(where C 
is around N/10) 
Freq 
Rank 100 200 300 400 500 600 700 
12345678 
Rank = order of words&#8217; frequency of occurrence 
Summarization 9/39 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Supervised Approaches 
++ 
++ 
+&#8722; 
&#8722;&#8722; 
&#8722; 
&#8722;&#8722;&#8722; 
Summarization 5/39 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
What is Summarizer 
&#8226;	Find important information in a text
&#8226;	Learn transformation rules based on training 
instances 
&#8226;	Extract certain facts from a text, and combine them into a text 
Summarization	 2/39 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Supervised Approaches 
Training Data 
Generation via 
Alignment Feature 
Selection Classification 
Summarization 6/39 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Matching Predicate 
&#8226;	Length similarity . (Gale &amp; Church, Brown et al)
&#8226;	Lexical similarity: 
&#8211;	Bilingual dictionary (Wu) 
&#8211;	Words with the same distribution. (Kay &amp; 
Roscheisen, Fung &amp; McKeown) 
&#8211;	Cognates (Simard et al, Church, Melamed) 
Summarization	 22/39 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Methods for Overall Alignment 
&#8226; Dynamic programming 
&#8226; Methods based on Computational Geometry 
&#8226; Signal processing Methods 
Summarization 23/39 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Weak Similarity Function 
Petersburg served as the capital of Russia for 200 &#183; 
years.(A) For two centuries Petersburg was the capital of the &#183; 
Russian Empire.
The city is also the country&#8217;s leading port and center
 &#183; 
of commerce. (B) And yet, as with so much of the city , the port facili- &#183; 
ties are old and inef&#64257;cient. 
Summarization 29/39 </text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Learning of Structural Mapping Rules 
j2 j1 Corpus2 Corpus1 
i1 i2 
Par.2 Par.1 Par.3 Par.1 
Par.2 
Par.1 Text Text 
Text Text ClusterB
ClusterE
Cluster1 
Summarization 33/39 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Alignment for Summarization 
&#8226; Always monolingual 
&#8226; Seems to be trivial (use word intersection!) 
Summarization 27/39 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Key Questions 
Content selection &#8226; 
&#8226;	Content organization and linguistic realization 
Evaluation&#8226; 
Summarization	 4/39 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Supervised Approaches
&#8226;	Alignment (trivial for extraction, hard for 
generation) 
Feature Selection &#8226; 
&#8226;	Classi&#64257;cation (standard classi&#64257;ers &#8212;</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Patterns of Mapping 
Summarization 30/39 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Length-based Alignment
Let D(i, j )be the lowest cost alignment between
sentences s1,...,s i&#160;and t1,...,t j.
Base: D(0,0)&#160; = 0.
&#9127;&#160;
&#9130;D(i, j&#8722; 1)&#160; + cost( 0:1 align&#160;&#966;, t j)&#160; &#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;D(i&#8722; 1,j ) +&#160; cost( 1:0 align&#160;si,&#966;)&#160; &#9130;&#160;&#9130;&#160;&#9130;&#160;&#9128;D(i&#8722; 1,j&#8722; 1)&#160; + cost(1:1 align&#160;si,tj)D(i, j )=m i n&#160;&#9130;&#160;&#9130;&#160;D(i&#8722; 1,j&#8722; 2)&#160; + cost(1:2 align&#160;si,tj&#8722;1,tj)&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;&#9130;&#160;D(i&#8722; 2,j&#8722; 1)&#160; + cost(2:1 align&#160;si&#8722;1,si,tj)&#160;&#9130;&#160;&#9130;&#160;&#9129;&#160;D(i&#8722; 2,j&#8722; 2)&#160; + cost(2:2 align&#160;si&#8722;1,si,tj&#8722;1,tj)&#160;
Summarization 19/39 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Computational Geometry Methods
(Melamed, 1997) Assumption: Distribution of &#8220;true points of
correspondence (TPC)&#8221; satis&#64257;es certain geometric properties
&#8226;	Generate all the matching points satisfying the matching
predicate (over-generation)
&#8226;	Find a subset of matching points that satis&#64257;es a pattern of
TPC:
&#8211;	Linearity 
&#8211;	Injectivity 
&#8211;	Low variance of slope 
Various heuristics are used to minimize the search space 
Summarization	 24/39 </text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Summarization Evaluation
&#8226;	Precision/Recall or their weighted version are used
&#8226;	As a baseline, people use a &#8220;lead&#8221; summary 
&#8226;	Human agreement is computed using Kappa 
&#8226;	When evaluation results matter, it is done manually 
(DUC competition) 
&#8211;	Provides large collection of human-generated 
summaries 
&#8211;	Outputs are evaluated manually 
Summarization	 38/39 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
TF*IDF
wik&#160;=&#160;Tfik&#160;&#8727; log(N/n k)&#160;
wik&#160;&#8212;T e r m k&#160;in document Di&#160;
Tfik&#160;&#8212; Frequency of term k&#160;in document Di&#160;
N&#160;&#8212; total number of documents in the collection C&#160;
nk&#160;&#8212; total number of documents in the collection C&#160;
that contain Tk&#160;
Summarization 12/39 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Alignment 
Champollion &#8217;1822 
Find pairs 
of corresponding ele&#173;ments 
Summarization 14/39  
 
 
 
To see this image, go to 
http://images.google.com/images?q= rosetta_stone.jpg&amp;imgsz=large</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Design Choices in Alignment 
Determined by a Corpus Type 
&#8226; Matching predicate 
&#8226; Search strategy 
Summarization 20/39 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Signal Processing Methods 
(Fung, 1995) 
Summarization 26/39 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Length-based Alignment
&#8226;	Matching Predicate: Long sentences will be 
translated as long sentences, short sentences 
translated as short sentences 
&#8226;	Method: Dynamic programming 
Summarization	 18/39 </text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Semantic-based Summarization
Assumption: In a limited domain, we know &#8220;what is 
important&#8221; (Radev&amp;McKeown, 1995, 
Elhadad&amp;McKeown, 2001) 
&#8226;	Use an information extraction system to select 
&#8220;important information&#8221; 
&#8226;	Use a semantics-to-text generation system to 
generate a new text 
Summarization	 39/39 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Alignment Output
Amsterdam is the largest city in The Netherlands and the countrys economic center. It is the of&#64257;cial capital 
of The Netherlands, though The Hague is the home of the government. Tourists come to see Amsterdams 
historic attractions and collections of great art. They admire the citys scenic canals, bridges, and stately old 
houses. Amsterdam is also famous for its atmosphere of freedom and tolerance. 
City and port, western Netherlands, located on the IJsselmeer and connected to the North Sea. It is the capital and the principal commercial and &#64257;nancial centre of The Netherlands. To the scores of tourists who 
visit each year, Amsterdam is known for its historical attractions, for its collections of great art, and for the 
distinctive colour and &#64258;avour of its old sections, which have been so well preserved. However, visitors to the city also see a crowded metropolis beset by environmental pollution, traf&#64257;c congestion, and housing 
shortages. It is easy to describe Amsterdam, which is more than 700 years old, as a living museum of a 
bygone age and to praise the eternal beauty of the centuries-old canals, the ancient patrician houses , and 
the atmosphere of freedom and tolerance, but the modern city is still working out solutions to the pressing urban problems that confront it. Amsterdam is the nominal capital of The Netherlands but not the seat of government, which is The Hague. The royal family , for example, is only occasionally in residence at the 
Royal Palace, on the square known as the Dam, in Amsterdam. 
Summarization 16/39 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Summarization 
Regina Barzilay 
March 8, 2003 </text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation 
Range Struct Cos. 
Prec. Rec. Prec. Rec. 
0%&#8211;40% 50% 25% 23% 15% 
40%&#8211;70% 85% 73% 66% 86% 
70%&#8211;100% 95% 95% 90% 95% 
Summarization 37/39 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Word Frequency vs Resolving Power
(from van Rijsbergen, 1979) The most frequent words 
are not the most descriptive 
freq Upper cutoff Lower Cutoff 
discriminative power of significant words 
significant words 
rank 
Summarization 10/39 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Corpus Type
&#8226;	Language Proximity (Monolingual vs Bilingual, 
technical vs lay) 
&#8226;	Content Proximity (comparable vs parallel) 
&#8226;	Matching Granularity (1:1 vs 1:5) 
Summarization	 21/39 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Assigning Weights
&#8226;	Raw frequencies (typically with the list of 
stop-words) 
&#8226;	TF*IDF &#8211; a way to deal with the problem of the Zipf distribution 
&#8211;	TF - Term frequency 
&#8211;	IDF - Inverse term frequency 
Summarization	 11/39 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
It is hard! 
&#8226; Insertions, deletions, reodering 
&#8226; Weak similarity function 
Summarization 28/39 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today 
&#8226; Summarization (content selection, evaluation) 
&#8226; Techniques: alignment, classi&#64257;cation, rewriting 
Summarization 1/39 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Types of Summarization 
&#8226;	Input: speech/text, single-/multi-document
&#8226;	Output: generic/query-oriented 
&#8226;	Approach: domain dependent/independent, 
extraction/generation 
Summarization	 3/39 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Topic Segmentation (cont.): Hierarchical Text Segmentation, Meeting Segmentation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec03/</lecture_pdf_url>
      <lectureno>3</lectureno>
      <slides>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Meeting Segmentation 
Motivation: Facilitate information Access &#8226; 
&#8226; Challenges: 
&#8211; High error rate in transcription 
&#8211; Multi-thread structure 
Topic Segmentation 14/23 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
HMM-based Segmentation: Decoding 
&#8226; Transitions are controlled by switch penalty 
&#8226; Segmentation via Viterbi-style decoding 
Topic Segmentation 11/23 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
TDT Performance 
Input Type CSeg&#160;for ABC 
ASR 0.1723 
Closed Captions 0.1515 
Transcripts 0.1356 Note the impact for 
ASR! 
Topic Segmentation 13/23 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today&#8217;s Topics 
&#8226; Hierarchical segmentation 
&#8226; HMM-based segmentation 
&#8226; Supervised segmentation 
Topic Segmentation 5/23 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Overlaps
&#8226; Average overlap rate within some window 
Little overlap in the beginning of segments 
Topic Segmentation 19/23 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;&#160;Examples of Derived Rules 
Condition Decision Conf. 
LC&#8804;0.67, CUE&#8805;1, 
OVR&#8804;1.20, SIL&#8804;3.42 yes 94.1 
LC&#8804;0.35, SIL &gt;3.42, 
OVR&#8804;4.55 yes 92.2 
CUE&#8805;1, ACT&gt;0.1768, 
OVR&#8804;1.20, LC&#8804;0.67 yes 91.6 
... 
default no 
Topic Segmentation 22/23 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Agglomerative Clustering
4
1 2 3 4 5
 5 
&#8226;	First, each data point is a singleton cluster
&#8226;	Next, closest points are merged until all points are 
combined 1 3 
2
Topic Segmentation	 6/23 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
More Results 
&#8226;	High sensitivity to change in parameter values
&#8226;	Thesaural information does not help 
Most of the mistakes are &#8220;close misses&#8221; &#8226; 
Topic Segmentation	 4/23 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Determination of Window Size 
Feature Tag Size(sec) Side 
Cue phrases CUE 5 both 
Silence (gaps) SIL 30 left 
Overlap OVR 30 right 
Speaker activity ACT 5 both 
Lexical cohesion LC 30 both 
Topic Segmentation 21/23 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Segmentation Algorithm 
&#8226; Preprocessing and Initial segmentation 
&#8226; Similarity Computation 
&#8226; Boundary Detection 
Topic Segmentation 2/23 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results 
Method Pk&#160; WD 
Feature-based 23.00 25.47 
Cohesion-based 31.91 35.88 
Topic Segmentation 23/23 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Broadcast News Segmentation 
Goal: divide news stream into stories &#8226; 
&#8226;	Assumption: news stories typically belong to one of 
several categories (sports, politics, ... ) 
Topic Segmentation	 9/23 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
TDT Segmentation Results
&#8226;	Data: 384 shows, 6,000 stories and 2.2 million 
words 
&#8226;	Sources: ABC, CNN, . . . 
TDT Evaluation Measure: &#8226; 
CSeg&#160; =&#160;&#945;&#160;&#8727; PMi s s&#160; +( 1&#8722; &#945;)&#8727; PF alseAlarm&#160;
Topic Segmentation	 12/23 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
HMM-based Segmentation: Construction 
van Mulbregt&amp;Carp&amp;Gillick&amp;Lowe&#8217;99: 
&#8226;	Each state of HMM represents a topic 
&#8226;	Topics are derived via story clustering 
&#8226;	Emission probabilities for a state are computed 
based on a unigram language model 
Topic Segmentation	 10/23 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Selected Cue Words 
OKAY 93.05 
shall 0.44 
anyway 0.43 
alright 0.64 
let&#8217;s 0.66 
good 0.81 
Topic Segmentation 17/23 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Silences
&#8226;	Pauses &#8212; speaker silence in the middle of her 
speech 
&#8226; Gap &#8212; silences not attributable to any party
Topic boundaries are typically preceeded by gaps
Topic Segmentation	 18/23 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation Results 
Methods Precision Recall 
Baseline 33% 0.44 0.37 
Baseline 41% 0.43 0.42 
Chains 0.64 0.58 
Blocks 0.66 0.61 
Judges 0.81 0.71 
Topic Segmentation 3/23 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
0 Speaker Change 
10
 20
 30
Topic Segmentation 20/23 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Topic Segmentation 1/23 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Cue Word Selection 
Automatic computation of cue words: 
&#8226;	Compute word probability to appear in boundary 
position 
&#8226;	Select words with the highest probability
Remove non-cues.
&#8226; 
Topic Segmentation	 16/23 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Algorithm for Feature Segmentation
Supervised ML 
(Galley&amp;McKeown&amp;Fosler-Lussier&amp;Jing&#8217;03) 
&#8226; Combines multiple knowledge source:
&#8211; cue phrases 
&#8211; silences
&#8211; overlaps
&#8211; speaker change 
&#8211; lexical cohesion 
&#8226; Uses probabilistic classi&#64257;er (decision tree) to 
combine them 
Topic Segmentation 15/23 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Topic Segmentation 
Regina Barzilay 
February 11, 2004 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Agglomerative Clustering
&#8226;	Complete-link &#8212; merge the two clusters whose 
merger has the smallest diameter 
&#8226;	Single-link &#8212; merge the two clusters whose two closest members have the smallest distance 
&#8226;	Average-link &#8212; merges in each iteration the pair of clusters with the highest cohesion. 
Topic Segmentation	 7/23 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Hierarchical Segmentation 
(Yaari, 1997) 
&#8226;	Partition the text into elementary segments 
&#8226;	While more than one segment left do 
&#8211;	Find closest adjacent segments s i,si+1(based on 
cosine measure) 
&#8211;	Merge si,si+1&#160;into one segment 
Topic Segmentation	 8/23 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Studies of Dialogues; Taxonomy of Speech Acts (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec14/</lecture_pdf_url>
      <lectureno>14</lectureno>
      <slides>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Searle&#8217;s Taxonomy
&#8226;	Expressives: expressing the psychological state of 
the speaker about a state of affairs ( thinking, 
apologizing, welcoming, deploring ) 
&#8226;	Declarations: bringing about a different state of the 
world via the utterance (I resign, You&#8217;re &#64257;red) 
Studies of Dialogs	 19/22 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Continuers
Devices the hearer uses to indicate that she believes she
understands what the speaker means (Clark&amp;Schaefer,
1989)
&#8226;	Continued attention: B shows she is continuing to
attend and therefore remains satis&#64257;ed with A&#8217;s
presentation
Relevant next contribution : B starts in on the next &#8226; 
relevant contribution 
&#8226; Acknowledgment: B nods or says a continuer like
uh-huh, yeah,o ra n assessment like that&#8217;s great
Studies of Dialogs	 11/22 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Transition-taking Rules 
Silence Duration 
A: Is there something bothering you or not? 
(1.0) 
A: Yes or no? 
(1.5) 
A: Eh? 
B: No. 
Studies of Dialogs 8/22 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Conversational Implicature
A1: And, what day in May did you want to travel?
C2: OK uh I need to be there for a meeting that&#8217;s from the 12th
to the 15th
A4: ...There&#8217;s three non-stops today. 
Studies of Dialogs 13/22 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Today 
&#8226;	Properties of human dialogs
&#8226;	Formalization of these properties required for 
system building 
&#8211;	Taxonomies of Speech Acts 
Studies of Dialogs	 2/22 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Turn Taking
People can quickly &#64257;gure out who should talk next, and 
when they should speak 
&#8226;	Little overlap (Levinson&#8217;1983 smaller than 5%)
&#8226;	The amount of time between two turns is low (less than few hundred milliseconds) 
Studies of Dialogs	 5/22 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Grounding
The speaker and the hearer must establish common 
ground (Stalnaker, 1978) 
A: ...&#160;returning on U.S. &#64258;ight one one eight 
B: Mm hmm 
Mm is a back-channel 
Studies of Dialogs 10/22 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Dialogue Example
C1: I need to travel in May.
A1: And, what day in May you want to travel?
C2: OK uh I need to be there from the 12th to the 15th
A2: And you&#8217;re &#64258;ying into what city?
C3: Seattle
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I don&#8217;t think there&#8217;s many options for non-stop
A4: Right. There&#8217;s three non-stops today.
C5: What are they?
Studies of Dialogs 4/22 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Continuers
&#8226;	Demonstration: B demonstrates all or part of what 
she has understood A to mean, for example by 
paraphrasing or reformulating A&#8217;s utterance, or by 
collaboratively completing A&#8217;s utterance 
A: OK I&#8217;ll take the 5ish &#64258;ight on the night before on the 11th 
B: On the 11th? 
&#8226;	Display: B displays verbatim all or part of A&#8217;s 
presentation 
Grounding is expressed in different modalities 
Note the impact of modality (face2face vs phone 
conversation) 
Studies of Dialogs	 12/22 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Conversational Implicature
&#8226;	The interpretation of an utterance relies on more 
than just the literal meaning of the sentence 
&#8226;	The speaker seems to expect the hearer to draw certain inferences 
Studies of Dialogs	 14/22 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Backward-looking Functions 
&#8226; ACCEPT: a claim made by a speaker 
&#8226; ANSWER: answering a question 
&#8226; UNDERSTANDING: whether speaker understands 
Studies of Dialogs 22/22 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Transition-taking Rules 
Conversation Analysis (1974) 
&#8226;	If during this turn the current speaker has selected A 
as the next speaker then A must speak next 
&#8226;	If the current speaker does not select the next speaker, any other speaker may take the next turn 
&#8226;	If no one else takes a turn, the current speaker may take the next turn 
Studies of Dialogs	 6/22 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;&#160;Grice Maxims
&#8226;	Maxim of Quantity: Be exactly as informative as 
required 
&#8211;	Make your contribution as informative as 
required (for the current purposes of exchange) 
&#8211;	Do not make your contribution more informative 
that is required 
&#8226;	Maxim of Quality: Try to make your contribution 
one that is true: 
&#8211;	Do not say what you believe to be false 
&#8211;	Do not say that for which you lack adequate 
&#65533;evidence 
Studies of Dialogs	 15/22 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Dialogue Act Markup
DAMSL (Dialogue Act Markup in Several Layers) 
(Allen&amp;Core&#8217;1997; Walker et al&#8217;1996; Carletta&#8217;1997) 
&#8226;	Driven by the needs of dialog system developers 
&#8226;	Contains several levels, including forward looking
function and backward Looking function
Hierarchical in structure &#8226; 
&#8226;	Domain-independent, but tailored towards task-oriented 
dialogs 
Studies of Dialogs	 20/22 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Studies of Dialogs 
Regina Barzilay 
March 31, 2004 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Grice Maxims 
Maxim of Relevance: Be relevant &#8226; 
&#8226; Maxim of Quality: Be perspicuous 
&#8211; Avoid obscurity of expression 
&#8211; Avoid ambiguity 
&#8211; Be brief (avoid unnecessary prolixity) 
&#8211; Be orderly 
Studies of Dialogs 16/22 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Searle&#8217;s Taxonomy
Searle (1975): Re&#64257;ned taxonomy of (illocutionary)
speech acts
&#8226;	Assertive: committing the speaker to somethings
being the case ( suggesting, swearing, boasting,
concluding)
&#8226;	Directives: attempts by the speaker to get the
addressee to so something (asking, ordering,
requesting, inviting, begging)
&#8226;	Commissives: committing the speaker to some
future course of actions ( promising, planning,
vowing, betting, opposing)
Studies of Dialogs	 18/22 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Dialogue Acts
&#8226;	Austin (1962): Utterance in a dialogue is a kind of 
action performed by the speaker 
&#8226;	Types of acts:
&#8211;	locutionary act: the utterance of a sentence 
with a particular meaning 
&#8211; illocutionary act: the act of asking, promising,
etc., in uttering a sentence (you can&#8217;t do that!)
&#8211;	perlocutionary act: the (often intentional) 
production of certain effects upon the feelings, 
thoughts, or actions of the addressee in uttering 
a sentence 
Studies of Dialogs	 17/22 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Utterance Segmentation 
&#8226; Cue words (well, and, so) 
&#8226; N-grams and POS sequences 
&#8226; Prosody (pitch, accent, pause duration) 
Studies of Dialogs 9/22 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
What Makes Dialogue Different? 
&#8226; Turn taking 
&#8226; Grounding 
&#8226; Conversational Implicature 
Studies of Dialogs 3/22 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Transition-taking Rules 
Mechanisms for the next speaker selection:
&#8226;	adjacency pairs (Schelgoff, 1968):
GREETING GREETING
 &#8594;
COMPLIMENT DOWNPLAYER
 &#8594;
REQUEST &#8594; GRANT
utterance boundaries
&#8226; 
Studies of Dialogs	 7/22 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Forward-looking Functions 
&#8226; STATEMENT: a claim made by a speaker 
&#8226; INFO-REQUEST: a question by the speaker 
&#8226; CHECK: question for con&#64257;rming information 
&#8226; OPENING: greeting 
Studies of Dialogs 21/22 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Dialogs 
What is a connection between the picture and our topic? 
Studies of Dialogs 1/22  
 
 
 
To see this image, go to 
http://images.google.com/images?q=false_maria.jpg</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Intentions and the Structure of Discourse (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec13/</lecture_pdf_url>
      <lectureno>13</lectureno>
      <slides>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Plan Representation 
REQUEST-INFO (A,C,F) 
Constraints: Agent(A) &#8743; Client (C)
Precondition: Know (C,I)
Effect: Know (A, I)
Body: B(C, W (A, Know (A, I)))
Intentions and The Structure of Discourse 24/25 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Intentions and The Structure of
Discourse
Regina Barzilay 
March 29, 2004 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Plan Representation 
STRIPS BOOK-FLIGHT (A,C,F) 
Constraints: Agent(A) &#8743; Flight (F) &#8743; Client (C)
Precondition: Know (A,departure-date(F)) &#8743; Know
(A,departure-time(F)) &#8743; Know (A, origin-city(F)) &#8743;
Know (A, destination-city(F)) &#8743; Has-Seats (F) &#8743; ...
Effect: Flight-Booked (A, C, F)
Body: Make-Reservation (A, F , C)
Intentions and The Structure of Discourse 23/25 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Intention Hierarchy
&#8226;	Dominance: I1 dominates I2, I3, I4, I5
I1&#160;(Intend C (Intend A (A &#64257;nd a &#64258;ight for C))) 
I3&#160;(Intend A (Intend C (Tell C A destination city))) 
&#8226;	Satisfaction-precedence: I2, I3 satisfaction-precedes I5 I
3&#160;(Intend A (Intend C (Tell C A destination city))) 
I5&#160;(Intent C (Intend A (A &#64257;nd a nonstop &#64258;ight for C))) 
Intentions and The Structure of Discourse	 19/25 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Patterns of Entity Distribution
Scholars 
&#8722;
&#8722;
&#8722;
&#8722;
&#8722;
S 
Balance
&#8722;&#8722;
&#8722;
&#8722; 
&#8722; 
Fate
&#8722;
&#8722;
&#8722;
&#8722; 
&#8722; 1 SSSXX 1 
Hearing
&#8722;&#8722; 2 2 
Spaniards
&#8722;&#8722;
&#8722;&#8722;
&#8722;&#8722; 
&#8722;&#8722;
&#8722;&#8722; 3 SXX O 3 
Thousands
4 4 
Judge
&#8722;&#8722; &#8722;
&#8722; &#8722; 
OO 
5 OX X 5 
Warrant
&#8722;&#8722; 
&#8722;&#8722; 
&#8722;&#8722; 6 6 
Extradition
&#8722;&#8722;&#8722;
&#8722; 
&#8722;&#8722;
&#8722; 
Arrest
&#8722;
&#8722; 
&#8722;
&#8722;
O 
Surgery
&#8722;
X
&#8722;
&#8722;
&#8722;
&#8722; 
October
&#8722;&#8722;
&#8722;
&#8722;&#8722; 
London
&#8722;&#8722;
&#8722;
&#8722;&#8722; 
Pinochet
S
&#8722;
SS
&#8722; 
Augusto
&#8722;&#8722;
&#8722;
&#8722;&#8722; 
Dictator
&#8722;&#8722;
&#8722;
&#8722;&#8722; 
Intentions and The Structure of Discourse 10/25 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Discourse Processing: Key Questions 
What individuates a discourse? &#8226; 
What makes it coherent? &#8226; 
Travelers left and entered our car at every stopping of 
the train. We began to recite our lessons. Similar facts 
were observed on the 23rd of July in the same year, in 
the Paci&#64257;c Ocean, by the Columbus, of the West India 
and Paci&#64257;c Steam Navigation Company . 
Intentions and The Structure of Discourse 1/25 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Intention Hierarchy 
DS1 
C1 DS2 DS3 DS4 DS 5 
A1&#8722;C1 A2&#8722;C2 A3 C4&#8722;C7 
Intentions and The Structure of Discourse 20/25 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Determining Intentional Structure 
Intentions and The Structure of Discourse 25/25 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Informational vs. Intentional Coherence
(Moore&amp;Pollack, 1992): informational and intentional
levels of discourse analysis cannot be separated
You&#8217;ll want to book your reservation before the end of 
the day . Proposition 143 goes into effect tomorrow. 
Intentional structure: convince the caller to book &#8226; 
her reservation until the end of the day 
&#8226;	Information structure: explanation relation between 
two sentences 
Intentions and The Structure of Discourse	 22/25 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Intentional Structure
Discourse Purpose(DP) is an underlying purpose that is 
held by the person who initiates discourse 
Discourse Segment Purpose(DSP) speci&#64257;es how this 
segment contributes to achieving the overall discourse 
purpose 
&#8226;	Assumption: one DP per discourse
&#8226;	No Taxonomy of Intentions (not the difference with 
the RST) 
Intentions and The Structure of Discourse	 13/25 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Two Views on Discourse 
Is the speaker in the loop? 
&#8226;	Informational Approach: Understanding Linguistic 
Structure is suf&#64257;cient for Discourse Processing 
&#8226;	Intentional Approach: Understanding Speaker
Intentions is required for Discourse Processing
Intentions and The Structure of Discourse	 2/25 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Intention Hierarchy
Understanding Structural Relations among relations is 
key! 
Dominance:&#8226; 
DSP 1&#160;dominates DSP 2&#160;if satisfying DSP 2&#160;is intended 
to provide part of the satisfaction of DSP 1&#160;
&#8226;	Satisfaction-precedence: 
DSP 1&#160;satisfaction-precedes DSP 2&#160;if DSP 1&#160;must be 
satis&#64257;ed before DSP 2&#160;
Intentions and The Structure of Discourse	 16/25 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Linguistic Structure 
Constituents:&#8226; 
&#8211;	Discourse segments 
&#8211;	Embedding relations that can hold between them 
&#8226;	Interaction between linguistic structure and the 
discourse utterances 
&#8211;	Linguistic expressions re&#64258;ect discourse structure
&#8211;	Discourse structure constraints the interpretation of expressions 
Intentions and The Structure of Discourse	 7/25 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Attentions, Intentions, and the Structure of
Discourse
Abstract Model of Discourse Structure as a composite of 
three interacting constituents: 
&#8226;	Linguistic Structure
Discourse Utterances
Intentional Structure &#8226; 
Intentions organized in hierarchical discourse structure 
Attentional Structure &#8226;
Dynamically-changing model of objects, properties and 
relations that are salient at each point of discourse 
Intentions and The Structure of Discourse	 6/25 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Coherent Discourse
&#8226;	Overall plan of the speaker ties intentions together
&#8226;	Interaction between the speaker&#8217;s plan and the 
hearer&#8217;s intention comprehension 
&#8211;	Intention determination &#8212; complete
speci&#64257;cation of what is intended by whom
&#8211;	Intention recognition &#8212; the processing that leads a discourse participant to identify what the 
intention is 
Intentions and The Structure of Discourse	 21/25 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
C1: I need to travel in May .
A1: And, what day in May you want to travel?
C2: OK uh I need to be there from the 12th to the 15th
A2: And you&#8217;re &#64258;ying into what city?
C3: Seattle
A3: And what time would you like to leave Pittsburgh?
C4: Uh hmm I don&#8217;t think there&#8217;s many options for non-stop
A4: Right. There&#8217;s three non-stops today .
C5: What are they?
A5: The &#64257;rst one departs PGH at 10:00 ...
C6: OK I&#8217;ll take the 5ish &#64258;ight on the night before on the 11th
A6: On the 11th? OK. Departs at 5:55 pm
C7: OK
Intentions and The Structure of Discourse 17/25 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Examples of Intention Types
&#8226; Intend that some agent intend to identify an object
Intend that Ruth intend to identify my dress. 
&#8226; Intend that some agent know some property of an
object Intend that Ruth know that my dress is red
Intentions and The Structure of Discourse 15/25 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Informational Approach
Understanding Linguistic Structure is suf&#64257;cient for 
Discourse Processing 
&#8226;	Lexical cohesion &#8212; patterns of sentence connectivity 
Rhetorical relations &#8212; content-based relations &#8226; 
between sentences 
Intentions and The Structure of Discourse	 3/25 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Informational Approach 
Observations: 
&#8226;	Amenable for computational approaches (esp. 
corpus-based techniques) 
&#8226;	Shown to be useful in some natural language processing tasks 
&#8226;	Independent of how humans process discourse 
&#8226;	Limited expressive and predictive power 
Intentions and The Structure of Discourse	 4/25 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Attentional Structure 
Abstraction of participants&#8217; focus of attention
&#8226;	Attentional Structure is modeled by focus spaces: 
objects and relations in focus of participants&#8217; 
attention 
&#8226;	Changes in Attentional Structure are modeled by a 
set of insertion and deletion rules 
Intentions and The Structure of Discourse	 9/25 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Linguistic Structure
&#8226;	(Para)-Linguistic expressions re&#64258;ect discourse 
structure 
&#8211;	Cue phrases (For example, In the &#64257;rst place)
&#8211;	Change in aspect and tense 
&#8211;	Change in intonation and gesture 
&#8226;	Discourse structure constraints the interpretation of expressions 
&#8211;	Pronoun resolution 
Intentions and The Structure of Discourse	 8/25 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Transitions
DISCOURSE SEGMENTS FOCUS SPACE STACK DOMINANCE HIERARCHY 
DS1 
DS2 
DS3 Properties 
objects relatetions 
Properties 
objects relations DS1 FS1 DS3 FS3 DSP1 DOMINATES DSP3 
DSP1 DOMINATES DSP2 
Intentions and The Structure of Discourse 12/25 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Intentional Approach
Understanding Speaker Intentions is central to 
Discourse Processing 
&#8220;Attention, Intentions, and the Structure of Discourse&#8221;(Grosz&amp;Sidner:1986) 
Utterances are considered as actions &#8226; 
&#8226;	The hearer&#8217;s understanding of the plan-based speaker intentions is the basis of discourse 
coherence 
Intentions and The Structure of Discourse	 5/25 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Transitions 
DISCOURSE SEGMENTS FOCUS SPACE STACK DOMINANCE HIERARCHY 
DS1 
DS2 
DS3 Properties 
objects relatetions 
Properties 
objects relations DS1 DS2 FS2 
FS1 DSP1 DOMINATES DSP2 
Intentions and The Structure of Discourse 11/25 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Examples of Intention Types
&#8226;	Intend that some agent intend to perform some 
physical task 
Intend that Ruth intend to &#64257;x the &#64258;at tire. 
&#8226;	Intend that some agent believe some fact 
Intend that Ruth believe that camp&#64257;re has started. 
&#8226;	Intend that some agent believe that one fact supports another 
Intend that Ruth believe the smell of smoke provides evidence that the camp&#64257;re is started. 
Intentions and The Structure of Discourse	 14/25 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Intention Hierarchy
I1&#160;(Intend C (Intend A (A &#64257;nd a &#64258;ight for C)))
I2&#160;(Intend A (Intend C (Tell C A departure date)))
I3&#160;(Intend A (Intend C (Tell C A destination city)))
I4&#160;(Intend A (Intend C (Tell C A departure time)))
I5&#160;(Intent C (Intend A (A &#64257;nd a nonstop &#64258;ight for C)))
Intentions and The Structure of Discourse 18/25 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Temporal Ordering in Discourse (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec11/</lecture_pdf_url>
      <lectureno>11</lectureno>
      <slides>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Parameter Estimation 
Decision tree on one-feature classi&#64257;ers &#8226; 
Standard feature selection &#8226; 
Temporal Relations in Discourse 23/24 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Ordering Events:Features 
&#8226;	Type of verb: reporting, stative, accomplishment
&#8226;	Tense and Aspect in the current clause, and shifts 
from previous clauses 
Presence of anchor and TIMEX &#8226; 
&#8226;	Syntactic features: type of conjunction, modi&#64257;ers 
and propositions 
Temporal Relations in Discourse	 15/24 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Resolving Temporal Expressions 
(Mani&amp;Wilson, 2001) 
&#8226;	Primary Goal: Disambiguation of speaker and
reference time dependent time markers
&#8220;now&#8221;, &#8220;today&#8221;, &#8220;next Tuesday&#8221;
&#8226;	Secondary Goal: Construction of event chronologies 
Temporal Relations in Discourse	 9/24 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Analysis
&#8226;	In some texts, timeline is a backbone of discourse 
organization 
&#8226;	(Newspaper) texts are rife with time switches
&#8226;	We can typically recreate a timeline of events
&#8226;	Temporal information is re&#64258;ected via tense, 
connectives, temporal markers 
Temporal Relations in Discourse	 2/24 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Algorithm for TIMEX tagging 
&#8226;	Identify TIMEX 
Resolve self-Contained &#8226; 
&#8226;	Propagate annotations based on context 
&#8211;	Track Reference Time based on Temporal Focus 
and Document creation date 
Temporal Relations in Discourse	 11/24 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Assigning Time Values(2)
Rule-based approach for propagation of TIMEX: 
Positional offsets from reference time: &#8220;next &#8226; 
month&#8221;, &#8220;this coming Thursday&#8221; 
&#8226;	Implicit offset based on verb tense: &#8220;The &#64257;rst 
shipment was loaded Thursday&#8221; 
&#8226; Use of lexical markers: &#8220;after&#8221;, &#8220;until&#8221; 
Accuracy: above 80% 
Temporal Relations in Discourse	 13/24 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Newspaper Text
NEW YORK (AP) - Martha Stewart thanked viewers of her television 
shows and readers of her magazines and websites for their support 
on Monday after meeting with a probation of&#64257;cer following her con&#173;
viction for lying about a stock sale. 
She made the remarks as she was getting into the front passenger 
seat of a sport utility vehicle outside a lower Manhattan courthouse, where she met with a probation of&#64257;cer for about an hour. &#8221;I want to thank my readers, my viewers and the Internet users,&#8221; Stewart 
said just before closing the door behind her. 
The probation meeting is the &#64257;rst step toward sentencing on 
June 17 . 
Temporal Relations in Discourse 1/24 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Temporal Relations in Discourse 
Regina Barzilay 
March 11, 2003 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Tense as Discourse Anaphora 
(Webber, 1988) 
John went into the &#64258;orist shop.
He had promised Mary some &#64258;owers.
She said she wouldn&#8217;t forget him if he forgot.
So he picked up three rose, two white ones, and one
pink.
Temporal Relations in Discourse 6/24 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results 
Tested on 2069 clauses 
ANCHORS REF MOVES 
MAJORITY 76.9%(AT) 65.75% (KEEP) 
ML 80.2% 71.8% 
Temporal Relations in Discourse 16/24 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Annotation Scheme
ISO standard: CC:YY :MM:DD:HH:XX:SS
Time points are treated as primitives (Bennett&amp;Partee,
1972)
&#8226;	&#8220;Tuesday , Novembers 2, 2002&#8221; &#8594; 20:00:11:02
&#8226;	&#8220;From May 1999 to June 1999&#8220; is represented as 
two points 
&#8226; &#8220; April is usually wet&#8221; is not marked as TIMEX 
Only 25% of clauses have time marker 
Temporal Relations in Discourse	 10/24 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Model Features:Lexico-Semantic 
&#8226; Verb Identity 
&#8226; Verb Class (15 classes of WordNet, 200 Levin) 
&#8226; Noun Identity 
&#8226; Noun Class (25 WordNEt classes) 
&#8226; Adjective 
Temporal Relations in Discourse 21/24 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Model 
t&#160;=argmax tjP(S M,tj,S S)=&#160;
=argmax tjP(S M)P(tj|SM)P(S SSM,tj)= |
=argmax tjP(tj|SM)P(S SSM,tj)= |
=argmax tjP(tj)P(S M|tj)P(S SSM,tj) |
Temporal Relations in Discourse 18/24 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today 
&#8226; Time marker disambiguation 
&#8226; Event ordering 
&#8211; Inter-sentential 
&#8211; Intra-sentential 
Temporal Relations in Discourse 4/24 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Reichenbach&#8217;s Topology of Tense 
Reichenbach (1947) 
&#8226; Point of speech (S) 
&#8226; Point of event (E) 
&#8226; Point of reference (R) 
John has climbed Everest. (E&#161;R=S)
John is in the class. (E=R=S)
John will climb Everest (S&lt;E=R)
John had climbed Everest (E&lt;R&lt;S)
John will have climbed Everest (S&lt;E&lt;R)
Temporal Relations in Discourse 5/24 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Processing Temporal Relations: Why?
&#8226;	Question-Answering: 
Did Stewart meet a probation of&#64257;cer before her sentence? 
&#8226;	Dialogue Systems: 
What &#64258;ights to NY do you have in a week from today? 
Information Extraction &#8226; 
John was killed yesterday 
&#8226;	Natural-Language Generation and Summarization 
&#8226;	Novelty detection 
Temporal Relations in Discourse	 3/24 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Temporal Processing: Knowledge Sources 
(Lascarides&amp;Asher, 1993) 
&#8226;	Tense and aspect 
Mike entered the room. He had drunk/was drinking the 
wine. 
&#8226;	Temporal Adverbials/Connectives 
A drunken man died in the central Philippines when he put a &#64257;recracker under his armpit. 
&#8226;	World Knowledge
Mike kissed the girl he met at a party .
Temporal Relations in Discourse	 7/24 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Ordering Events:Method 
n&#160;-class classi&#64257;cation 
&#8226; Anchor Relations: at, before, after, unde&#64257;ned 
&#8226; Reference Time moves: keep, revert, shift 
Temporal Relations in Discourse 14/24 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Assigning Time Values(1)
remove SEMI-TIMEX expressions, such as &#8220;today&#8221; in 
&#8220;USA TODAY&#8221; 
&#8226;	Method: Binary Classi&#64257;cation 
Features: &#8226; 
&#8211;	POS info: word before, word after 
&#8211;	Lexical info: presence of &#8220;said&#8221;, &#8220;will&#8221;, &#8220;year&#8221;, 
&#8220;most&#8221; 
&#8211;	Temporal markers: presence of a year or a day 
&#8211;	Context: whether appear in quotations 
Accuracy: 79.8%, Baseline 66.5% 
Temporal Relations in Discourse	 12/24 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Model Features:Others 
&#8226;	Syntactic Feature (complexity) 
&#8226;	Argument Feature (verb tendency to take objects, 
PPs, ...) 
Relative Position of S and M &#8226; 
Temporal Relations in Discourse	 22/24 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Studies of Tense 
(we will not cover today , but see the references)
&#8226;	Linguistic analysis of tense: 
(Kamp&amp;Reyle, 1993; Moens&amp;Steedman, 1988) 
Annotation Schemes: &#8226; 
(Setzer and Gaizauskas, 2001) 
Temporal Relations in Discourse	 8/24 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Results 
83,810 (80% training, 10% development/testing)
&#8226;	64% accuracy (note that some connectives are
interchangeable)
&#8226;	Best features: verb, verb classes, syntactic structure, 
position 
&#8226;	No impact: nouns, adjectives, temporal 
Temporal Relations in Discourse	 24/24 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Unsupervised Approach
(Lapata&amp;Lascarides, 2004)
Goal: Infer Sentence-internal Temporal Relations
Key Idea: Use connectives as predictors of relations
Leonard Shane, 65, held the post of president before
William Shane, 37, was elected to it last year.
The results were announced after the market closed.
Leaving the party , John walked home.
Temporal Relations in Discourse 17/24 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Model Features:Temporal
FINITE = {past, present }
NON-FINITE = {in&#64257;nitive, ing-form, en-form }
MODALITY = {NIL, future, ability , possibility , obligation
}
ASPECT = {imperfective, perfective, progressive }
VOICE = {active, passive } NEGATION = {af&#64257;rmative, 
negative } 
Temporal Relations in Discourse 20/24 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Model
Assuming conditional independence of subordinate from 
main clause: 
argmax tjP(tj)P(S M|tj)P(S Stj)|
Assuming feature independence: 
argmax tjP(tj)&#65533;
P(a(M,i )|tj)P(a(S,i)tj)|
i&#160;
Temporal Relations in Discourse 19/24 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>MIT Dialogue Systems (PDF) (Courtesy of Stephanie Seneff. Used with permission.)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec17/</lecture_pdf_url>
      <lectureno>17</lectureno>
      <slides>
        <slide>
          <slideno>68</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample Interaction in Restaurant Domain
&#8226;System knows NO restaurants by name upon start-up
&#8226;Two-pass processing recognizes &#8220;Royal East&#8221; in first query
&#8226;Level of detail in summaries dependent on data sets  
&#8226;Database retrievals license more restaurant names
&#8226;&#8220;Bollywood Caf&#233;&#8221; recognized in first pass
&#8226;This approach can in principal handle an unlimited number 
of restaurant names, worldwide
</text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTighter SR/NL Integration
&#8226;Natural language analysis can provide long distance 
constraints that n-grams cannot
&#8226;Examples:
&#8211; What is the flight serves dinner?
&#8211; What meals does flight two serve dinner?
&#8226;Question: How can we design systems that will take 
advantage of such constraints?
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Database
Server
UserDialogue Manager
Context
TrackingHubDialogue Control Table
User Model
. . .
Source: BOS
Class: Coach
. . .Dialogue State
Source: BOS
Destination: DFW
Day: Tomorrow
Time: 9 a.m.
I HAVE AN AMERICAN FLIGHT THAT 
LEAVES AT 9:30.  WOULD THAT WORK?An Illustrative Example
CAN YOU PROVIDE A DEPARTURE TIME 
OR AIRLINE PREFERENCE?I WANT TO GO TO DALLAS 
TOMORROWSource: BOS
DEPARTING 9 A.M.
9 A.M.DepartTime: 9 a.m.
SELECTED FLIGHTBOOK ITDestination: DFW
Day: TomorrowSource: BOS
Day: TomorrowDate: Jan 21, 2000
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample of Natural Language Understanding
show me flights from boston to denverflight destination sourcetopicdisplay object
predicatefull_parse
commandsentence
predicate
city city to from flight_listdestination source flightdisplaySome parse nodes 
carry semantic tags for
creating semantic frameClause: DISPLAY
Topic: FLIGHT
Predicate: SOURCE
Topic: CITY
Name: "Boston"
Predicate: DESTINATION
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: SOURCE
Topic: CITY
Name: " Boston "
Predicate: DESTINATION
Topic: CITY
Name: " Denver "
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSContext Free Rules for Example
sentence &#8594; full_parse [robust_parse]
full_parse &#8594; (command question statement &#8230;)
command &#8594; display object
object &#8594; [determiner]  topic [predicate] [predicate]
predicate &#8594; (source destination depart_time &#8230;)
source &#8594; from  (city airport)
destination &#8594; to (city airport)
display &#8594; show me
city &#8594; (boston dallas denver &#8230;)
determiner &#8594; (a the )
...
&#8226;Context free : left hand side of rule is single symbol
&#8226;brackets [ ]: optional
&#8226;Parentheses ( ): alternates .    
&#8226;Terminal words in italicsShow me flights from Boston to Denver
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>78</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Short Video Clip
&#8226;First two turns illustrate different summarizations of 
database results for two different cities         (automatically determined)
&#8226;Third turn shows multi-modal interaction               
(speech plus pen)
&#8226;In last turn, user refers to restaurant by name, but the 
name was unknown to the recognizer at the beginning 
of the dialogue (flexible vocabulary)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSComponents of a Conversational System
DISCOURSE 
CONTEXTDISCOURSE 
CONTEXTDIALOGUE
MANAGEMENTDIALOGUE
MANAGEMENTDATABASEGraphs
&amp; Tables
LANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGMeaning
RepresentationMeaning
Representation
MeaningLANGUAGE
GENERATIONLANGUAGE
GENERATIONSPEECH
SYNTHESISSPEECH
SYNTHESISSentence
SPEECH
RECOGNITIONSPEECH
RECOGNITION
Words
Intro || NLU || Dialogue || Eval uation || Portability || FutureSpeech</text>
        </slide>
        <slide>
          <slideno>65</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
What&#8217;s the phone number of Flora in Arlington ???? What&#8217;s the phone number of Flora in Arlington&#8226;Dynamically alter vocabulary within a single utterance
&#8220;What&#8217;s the phone number 
for Flora in Arlington.&#8221;Dynamic Vocabulary Understanding: II
Arlington Diner
Blue Plate Express
Tea Tray in the Sky
Asiana Grille
Bagels etc
Flora
&#8230;.HubNLGNLG
ASRASRContextContextTTSTTSDialogDialog
NLUNLUAudioAudioDBDB
&#8220;The telephone number 
for Flora is &#8230;&#8221;Clause: wh_question
Property: phoneTopic: restaurantName: ????
City: ArlingtonClause: wh_questionProperty: phoneTopic: restaurantName: Flora
City: Arlington</text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Choose AlternateHypothesis Selection Process
&#8226;Involves recognizer, parser, and dialogue manager
&#8226;Control specified in hub programCollect N-best 
Candidate FramesParse Probabilities
Select Preferred 
Hypothesis
Pragmatic 
EvaluationWord 
Graph
Word Confidence 
Scores
Dialogue Context 
Filter
Implicit 
ConfirmationExplicit 
ConfirmationParse Status
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>46</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
System RefinementLimited
NL
Capabilities
Data
Collection
(Wizard)
Performance
EvaluationExpanded
NL
CapabilitiesSpeech
Recognition
Data
Collection
(Wizard-less)System Development Cycle
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>47</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSData Collection
&#8226;System development is chicken &amp; egg problem
&#8226;Data collection has evolved considerably
&#8211; Wizard-based &#8594;system-based data collection
&#8211; Laboratory deployment &#8594;public deployment 
&#8211; 100s of users &#8594;thousands &#8594;millions
&#8226;Data from realusers solving realproblems accelerates 
technology development
&#8211; Significantly different from laboratory environment
&#8211; Highlights weaknesses, allows continuous evaluation&#8211; But, requires systems providing realinformation!
&#8226;Expanding corpora will require unsupervised training or 
adaptation to unlabelled data
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>51</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>60</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
&#8211;Vocabulary growth is unbounded across a wide variety of tasksLearning New Words
&#8211;Many new words are important content words (i.e., 75% nouns)
NAME
NOUN
VERB
ADJECTIVE
ADVERBConversational
Interfaces&#8226;Conversational interfaces must be able to learn new words
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Constraint
Coverage100%100%speechMultiple Roles for Natural Language 
Parsing in Spoken Language Context 
Understanding100%
textspoken
language
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
:clause requestkeypad &#198;KeypadDate
:week | :day | :reldate &#198;ResolveRelativeDate
!:destination &#198;NeedDestination
:clause book &amp; :numfound =1 &#198;AddFlightToItinerary
:nonstops &amp; :arrivaltime &#198;SpeakArrivalTimesRepresentative Entries from Flight Domain   
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>56</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Backing off to Suggestions
clause: locate
category: restaurant
cuisine: chinesecity: cambridge
street: mainMeaning 
Representation
DB
0 entries
12 entriescountry
state
city
neighborhood
street
restaurant
lat/lon
cuisinecountry
state
city
neighborhood
street
restaurant
lat/lon
cuisinecountry
state
city
neighborhood
street
restaurant
lat/lon
cuisineI don&#8217;t know of any Chinese 
restaurants on Main Street in 
Cambridge.  However, I know of 
12 Chinese restaurants in Harvard Square and Central 
Square.I&#8217;m looking for a Chinese restaurant 
on Main Street in Cambridge
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
&#8226;Simple, directed dialogue systems are being deployed 
commercially
&#8226;Application-specific, mixed-initiative spoken dialogue 
systems are emerging from universities and other research institutions
&#8226;Research on multi-modal mixed-initiative dialogue 
systems is beginningCurrent Landscape
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSWhat Makes Parsing Hard?
&#8226;Must realize high coverage of well-formed sentences 
within domain
&#8226;Should disallow ill-formed sentences, e.g.,
&#8211; the flight that arriving in the morning
&#8211; what restaurants do you know about any banks?
&#8226;Avoid parse ambiguity (redundant parses)
&#8226;Maintain efficiency
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
MIT&#8217;s TINA NL System
&#8226;TINA was designed for speech understanding
&#8211; Grammar rules intermix syntax and semantics  
&#8211; Probabilities are trained from user utterances&#8211; Parse tree converted to a semantic frame that encapsulates 
meaning
&#8226;TINA enhances its coverage through robust parsing
&#8211; &#8220;Full parse&#8221; hypotheses are preferred
&#8211; Backs off to parsing fragments and skipping unimportant words  
&#8211; Fragments are combined into a full semantic frame
&#8211; When all else fails, system ba cks off to phrase spotting
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>SPOKEN LANGUAGE SYSTEMS
MIT Computer Science and Artificial Intellligence Laboratory  Spoken Computer 
Conversational Systems 
Stephanie Seneff
CSAIL, MIT
April 12, 2004  </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
ESPRIT
SPEECHSome Speech-Related Government Programs
DARPA SC ARPA SURBBN, CMU, Lincoln 
SDC, SRI, ...  
HWIM, Harpy, Hearsay
DARPA SLSATT, BBN, CMU, CRIM, 
MIT, SRI, Unisys, ...ATIS, Banking, DART, 
OM, VOYAGER, ...
ESPRIT
SUNDIAL
CNET, CSELT, 
DaimlerBenz, LogicaAir and Train TravelLE
ARISE
CSELT, IRIT, KPN, 
LIMSI, U. Nijmegen..Train Travel1970 1990 1980 2000DARPA WSJ/BN D.C.ATT, BBN, CMU, CU, IBM, 
MIT, MITRE, SpeechWorks,SRI, +Affiliates, ...
Complex Travel
ESPRIT
MASK
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>75</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSpeech only &#8594;Multimodal Interactions
&#8226;Typing, pointing, clicking can augment/complement speech 
&#8226;A picture (or a map) is worth a thousand words?
LANGUAGE
UNDERSTANDINGLANGUAGE
UNDERSTANDINGmeaningSPEECH
RECOGNITIONSPEECH
RECOGNITION
GESTURE
RECOGNITIONGESTURE
RECOGNITIONHANDWRITING
RECOGNITIONHANDWRITING
RECOGNITION
MOUTH &amp; EYES
TRACKINGMOUTH &amp; EYES
TRACKING
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSpoken Language Understanding  
&#8226;Spoken input differs significantly from text
&#8211; False starts
&#8211; Filled pauses
&#8211; Agrammatical constructs&#8211; Recognition errors
&#8226;We need to design natural language components that can 
both constrain the recognizer's search space and respond 
appropriately even when the input speech is not fully 
understood
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Natural Language Processing Components
&#8226;Understanding:
&#8211; Parse input query into a meaning repr esentation, to be interpreted for 
appropriate action by application domain
&#8211; Select best candidate from proposed recognizer hypotheses
&#8226;Discourse Context Resolution
&#8211; Interpret each query in context of preceding dialogue
&#8226;Dialogue Management
&#8211; Plan course of action under both ex pected and unexpected conditions; 
compose response frames.
&#8226;Generation
&#8211; Paraphrase user queries into same or different language.
&#8211; Compose well-formed sentences to  speak the (sequence of) response 
frames prepared by the dialogue manager.
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>45</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
NL Re-SortN Complete
"sentence"
hypothesesparsable
sentences
SRbest 
scoring
hypothesisspeech
show me flights from boston to denver and
show me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onshow me flights from boston to denver andshow me flights from boston to denvershow me flights from boston to denver onshow me flight from boston to denver andshow me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver and
show me a flight from boston to denver
show me a flight from boston to denver onAnswerSR/NL Integration via N-Best Interface
&#8226;N-Best resorting has also been used as a mechanism for 
applying computationall y expensive constraints
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>66</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Dynamic Vocabulary Recognition
&#8226;Recognizer search space represented as a finite-state 
transducer containing static and dynamic components
&#8226;Dynamic word classes are pre-specified (e.g., CITY)
&#8226;New vocabulary words determined by dialogue (e.g., Nice)
&#8226;Graph splices determined by phonological constraints
&#8226;Phantom word-class marker not used for recognition
</text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Stochastic Approaches to 
Language Understanding
Semantic
ModelLexical
Model
what to say how to say itmeaning sentence
MS
&#8226;Choose among all possible meanings the one that 
maximizes:
( | )( )(| )()PS MPMPM SPS=
&#8226;HMM techniques have been used to determine the meaning 
of utterances (ATT, BBN, IBM. CU)
&#8226;Encouraging results have been achieved, but a large body of 
annotated data is needed for training
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Discourse and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>54</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Cuisine: 4 American, 2 Brazilian, 2 Indian, &#8230;
Price: 7 inexpensive, 4 medium, &#8230;
Street: 4 Hancock St., 2 Adams St., &#8230;Summarization
&#8226;Summarize information returned from database query
&#8211; Use clustering methods to dynamically determine response
&#8211; Guides user towards appropriat e disambiguation constraints
&#8220;Are there any 
restaurants in Quincy?&#8221;
Intro || NLU || Dialogue || Evaluation || Portability || Future&#8220;There are 14 restaurants.  Some of the 
options are American, Brazilian,
Indian, and Italian. None of them are 
expensive. Many of them are on 
Hancock Street, Adams Street, Billings 
Road, and Franklin Street. &#8221;</text>
        </slide>
        <slide>
          <slideno>71</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSResearch Issues: Dialogue Modeling
&#8226;Modeling human-human conversations
&#8211; Are human-human dialogues a good model for systems?
&#8211; If so, how do we structure our systems to enable the same kinds of 
interaction found in human-human conversations?
&#8226;Dialogue strategies
&#8211; When to use explicit vs.implicit vs.no confirmation?
&#8211; When to back off to alternatives such as typing or keypad entry?
&#8211; How to model help mechanisms to in form users of system capabilities?
&#8211; How to recognize and recover from errors?
&#8211; How to enable the above capabilities across diverse domains
&#8226;Producing and responding to back-channel
&#8211; Would likely have striking e ffect if properly implemented
&#8226;How can system learn user preferences through repeated 
interactions?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Incorporating Soft NL Constraints into 
Recognizer
&#8226;Class n-gram can be automatically derived from NL Grammar
to        idaho_falls &lt;cty&gt; on           may &lt;mth&gt; twenty_third &lt;dy&gt;SENTENCE
CLARIFIER
DESTINATION DATE
TO CITY_NAME MONTH ON DAY
CARDINAL_DATE
to           idaho falls               on             may       twenty thirdCITY_NAME MONTH
CARDINAL_DATE
&#8226;Developer identifies parse categories for class n-gram
&#8226;System tags words with associated class labels
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Human ComputerInitiative
&#8226;Human takes 
complete control
&#8226;Computer is totally 
passive&#8226;Human takes 
complete control
&#8226;Computer is totally 
passive
H:I want to visit my grandmother.&#8226;Computer maintains 
tight control
&#8226;Human is highly 
restricted&#8226;Computer maintains 
tight control
&#8226;Human is highly 
restricted
C:Please say the departure city.Dialogue Interaction Modes
&#8226;Conversational systems differ in the degree with which 
human or computer takes the initiative
Mixed Initiative
Dialogue
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Example of MIT&#8217;s Mercury 
Travel Planning System
&#8226;New user calling into Mercury flight planning system
&#8226;Illustrated technical issues:
&#8211;Back-off to directed dialogue when necessary (e.g., password)
&#8211;Understanding mid-stream corrections (e.g., &#8220;no Wednesday&#8221;)
&#8211;Soliciting necessary information from user
&#8211;Confirming understood concepts to user
&#8211;Summarizing multiple database results
&#8211;Allowing negotiation with user
&#8211;Articulating pertinent information
&#8211;Understanding fragments in  context (e.g., &#8220;4:45&#8221;)
&#8211;Quantifying user satisfaction (e.g., questionnaire)
Intro || NLU || Dialogue || Eval uation || Portability || Future
&#8211;Understanding relative dates (e .g., &#8220;the following Tuesday&#8221;)</text>
        </slide>
        <slide>
          <slideno>67</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Audio Clip: Rapid Development and 
Flexible Vocabulary
&#8226;Domain-independent dialogue manager
&#8211; Domain-specific information encoded in external tables
&#8226;No real user data available for training
&#8211; Generate thousands of utteranc es through dialogue simulation
&#8211; Train recognizer and NL com ponents on simulated utterances
&#8226;Responses tailored to properties of  retrieved 
database entries
&#8211; Enumerate short lists
&#8211; Provide succinct summary of long lists
&#8226;Recognizer vocabulary of restaurant names 
dynamically adjusted as dialogue unfolds</text>
        </slide>
        <slide>
          <slideno>61</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Acquiring New Words: Proper Names
&#8226;Initial research based on acquiring unknown user names
&#8211; User is asked to speak and spell their first and last names
&#8220;&lt;unknown&gt;  j o a n n e&#8221; Processed &#8220;jh ow ae n&#8220;Joanne   j o a n n e&#8221; Spoken
&#8226;Obtains both pronunciation and spelling of unknown word
&#8211;Integrated sound-to-letter constraint s reduce letter error rate by 44%
&#8226;Used in enrollment phase of a task delegation domain (Orion)
&#8211;New users can register over the telephone
&#8211;System automatically incorporates information for subsequent useJoanne  j o a n n e 
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSWhat Are Conversational Systems?
Systems that can communicate with users through a 
conversational paradigm, i.e., they can:
&#8211;Understand verbal input, using
*Speech recognition
*Language understanding (in context)
&#8211;Verbalize response, using
* Language generation
* Speech synthesis
&#8211; Engage in dialogue with a user during the interaction
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSGalaxy Communicator Architecture
Language
GenerationLanguage
Generation
Speech
RecognitionSpeech
RecognitionDiscourse
ResolutionDiscourse
ResolutionText-to-Speech
ConversionText-to-Speech
ConversionDialogue
ManagementDialogue
Management
Language
UnderstandingLanguage
UnderstandingApplication
Back-endApplication
Back-endAudio
ServerAudio
ServerAudio
ServerAudio
ServerI/O
ServersI/O
ServersApplication
Back-endApplication
Back-endApplication
Back-endApplication
Back-endHub
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMultiple Roles of Dialogue Modeling
&#8226;For each turn, prepare the system&#8217;s side of the 
conversation, including responses and clarifications
&#8226;Resolve ambiguities
&#8211; Ambiguous database retrieval (e.g. London, England or London, 
Kentucky)
&#8211; Pragmatic considerations (e.g ., too many flights to speak)
&#8226;Inform and guide user  
&#8211; Suggest subsequent sub-goals (e.g., what time?)
&#8211; Offer dialogue-context depen dent assistance upon request
&#8211; Provide plausible alternatives when requested information 
unavailable
&#8211; Initiate clarification sub-dialogues for confirmation
&#8226;Influence other system components
&#8211; Adjust language model due to dialogue context
&#8211; Update discourse context
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
We cannot expect any natural language system to be able 
to fully parse and understand all such sentencesWe cannot expect any natural language system to be able 
to fully parse and understand all such sentencesUsers can be very creative with 
language,  especially when frustrated
Examples from ATIS domain:
&#8226;I would like to find a flight from Pittsburgh to Boston on 
Wednesday and I have to be in Boston by one so I would like a flight out of here no later than 11 a.m.
&#8226;I'll repeat what I said before I&#8217;m on scenario 3 I would like a 
727 flight from Washington DC to Atlanta Georgia I would like 
it during the hours of from 9 a.m. till 2 p.m. if I can get a flight within that- that time frame and if .. I would like it for Friday
Intro || NLU || Dialogue || Evaluati on || Portability || Future&#8226;[laughter] [throat clear] Some database &lt;um&gt; I&#8217;m inquiring 
about a first class flight originating city Atlanta destination city Boston any class fare will be alright
</text>
        </slide>
        <slide>
          <slideno>57</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Simulated Dialogue InteractionSimulating the User 
I&#8217;m interested in a Chinese restaurant
What is the phone number of Royal EastStructured 
Database
Entities 
Properties 
Relationships 
Ontologies
Stylized 
Queries
Natural   
QueriesWeb 
sourceBoston Restaurant Guide
NAME  : Royal East  
PHONE: 617 222 4444CUISINE: Chinese
Restaurants SERVE CUISINERestaurants    IN         CITY
I want CUISINE ChineseTell me PHONE for NAME royal east
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>80</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMIT&#8217;s Discourse Module Internals
DISCOURSE
MODULEInput Frame 
Displayed List
Resolve 
Deixis
Incorporate 
Fragments
Interpreted FrameResolve 
PronounsResolve 
Definite NP
Fill 
Obligatory 
RolesUpdate 
History 
Elements
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>62</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Illustration of User Name Enrollment
&#8226;Enrollment dialogue has been simplified for illustrative 
purposes
&#8211; User prompted for name, ce ll phone number, and time zone
&#8226;If user confirms spellings of first and last names, 
vocabulary is automatically augmented to support them
&#8226;(Not illustrated) System backs off to keypad entry when 
spoken information incorrectly interpreted
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Typical Context Resolution Tasks
Input Semantic Frame
Resolve Deixis
Incorporate FragmentsResolve Pronouns
Inherit Predicates
Fill Obligatory Roles
Interpreted FrameUpdate History&#8220;What does this one serve?&#8221;
&#8220;What is their phone number?&#8221;
&#8220;Are there any on Main Street?&#8221;
&#8220;What about Mass Ave?&#8221;
&#8220;Give me directions from MIT.&#8221;&#8220;Show me restaurants in Cambridge.&#8221;
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>79</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Short Video Clip
Video Clip
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS The U.S. DARPA Communicator Program 
(1998-2003)
&#8226;Still focusing on flight scheduling domain
&#8211; Sites were tasked with finding their own realflight database
&#8226;Emphasis on common architecture with plug-and-play 
capabilities: Galaxy Communicator
&#8226;Generally much larger set of cities than in ATIS (&gt;500), 
and covering at least major airports world-wide
&#8226;Sites were free to organize dialogue interaction in any 
way they chose
&#8211; Encouraged mixed-initiative dialogue development
&#8226;Evaluation was conducted on per-site basis and 
depended critically on user exit polls
&#8211; Users were frequent traveler s booking their real travel 
arrangements
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>58</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>69</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Outline
&#8226;Introduction and historical context
&#8226;Speech understanding
&#8226;Context resolution and dialogue modeling
&#8226;Data collection and evaluation
&#8226;Rapid development of new domains 
&#8226;Flexibility and personalization
&#8226;Future research challenges  
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Typical Discourse Phenomena 
in Conversational Systems
&#8226;Deictic (verbal pointing) and anaphoric (e.g., pronominal) 
reference:
1. Show me the restaurants in Cambridge.
2. What is the phone number of the third one ?
3. How do I get there from the nearest subway stop?
&#8226;Ellipsis:
1. When does flight twenty two arrive in Dallas?
2. What is the departure time ()?
&#8226;Fragments:
1. What is the weather today in Denver?
2. How about Salt Lake City?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Word GraphFinal Hypothesis Selection Process
Parse 
Status
Confidence 
ScoresParse 
Scores
Discourse 
ContextPragmatic 
ConsiderationParsed Candidates
Final Selection
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>53</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Dialogue
ManagementDialogue
Management
Database
ServerDatabase
Server HubLanguage
GenerationLanguage
Generation
Speech
RecognitionSpeech
RecognitionContext
ResolutionContext
ResolutionSpeech
SynthesisSpeech
Synthesis
Language 
UnderstandingLanguage 
UnderstandingAudio
ServerAudio
ServerGALAXY 
Architecture HotelsHotels
Air TravelAir TravelGeneric Dialogue Modeling
&#8226;Dialogue management is abstracted into set of utility 
functions that serve as component building blocks  
&#8211; Allows extensive sharing of softw are among broad class of domains
&#8211; Will enable rapid portability to new domains 
SportsSportsCanonicalizerCanonicalizerGeneric 
Dialogue 
ManagerGeneric 
Dialogue 
Manager
SummarizerSummarizerWeatherWeather
Summarize 
database 
resultsPlan system 
responses
Regularize 
common 
concepts
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>74</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSMonolingual &#8594;Multilingual
&#8226;Language transparent design: 
&#8211; it is crucial that we seek solution s that will easily port to other 
languages besides English 
&#8226;Can we develop tools to automatically derive linguistic 
structure (e.g., parsing rules) from aligned corpora?
Mokusei
MuXing
Speech
UnderstandingSpeech
UnderstandingSpeech
GenerationSpeech
GenerationSpeech
GenerationSpeech
Generation
Common
Semantic
FrameCommon
Semantic
Frame
DATABASESpeech
UnderstandingSpeech
Understanding
Speech
GenerationSpeech
Generation
Speech
UnderstandingSpeech
UnderstandingCommon
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>55</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSExample Dialogue in Restaurant Domain
&#8220;Tell me about restaurants in Boston.&#8221;
&#8220;I have found 404 restaurants in Boston. They are located in Boston and South 
Boston.  There are 29 choices for cuisine.  I also have information about price 
range.&#8221;
&#8220;How about a high end restaurant in Back Bay?&#8221;
&#8220;I have found 10 high end restaurants in  Back Bay. Some of the options are 
Nouvelle Cuisine, American and Italian.&#8221;
&#8220;Italian&#8221;
&#8220;There are 2 high end Italian restaurants in Back Bay.  Cafe Louis on Berkeley 
Street and Via Matta in the Park Plaza.&#8221;
&#8220;What restaurants do you know in Somerville?&#8221;
&#8220;There are 35 restaurants in Somerville. There are 13 choices for cuisine. Many of 
them are on Broadway, Washington St., Elm St. and Holland St.&#8221;
&#8220;Holland Street&#8221;
&#8220;There are four restaurants in Somerville on Holland Street:  Orleans, Mr. Crepe, 
Johnny D's Uptown and Sabur.&#8221;
&#8220;What kind of food do they serve?&#8221;
&#8220;Orleans serves American food, Mr. Crepe serves bakery items, Johnny D's 
Uptown serves barbecue and Sabur serves Mediterranean food.&#8221;
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>77</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Conclusions
&#8226;Spoken dialogue systems are needed, due to
&#8211; Miniaturization of computers
&#8211; Increased connectivity&#8211; Human desire to communicate
&#8226;To be truly useful, these interfaces must behave naturally
&#8211; Embody linguistic competence, both input and output
&#8211; Help people solve real problems efficiently
&#8226;Conversational interfaces must be able to learn from user 
interaction and database content
&#8226;To achieve this flexibility requires progress in key areas:
&#8211; Response planning needs to be flexible and content-driven 
&#8211; New concepts must be acquire d naturally during interaction
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
&#8226;By introducing NL constraints early, one can potentially 
improve performance
NL Re-Sortparsable
sentences
SRbest 
scoring
hypothesisspeechbest
partial theory
next word
extensionsAlternatives to N-Best Interface
&#8211; can also reduce the need for a statistical language model, which may 
be hard to obtain for some applications
&#8226;However, NL parsing is generally very slow and memory 
intensive
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSAn Attractive Strategy
&#8226;Conduct R&amp;D of human language technologies within the
context of real (and useful) application domains
&#8211; Flight schedules and status, weat her, restaurant or hotel guide,
calendar management, city navigation,  traffic reports, sports updates, 
etc.
&#8226;Forces us to confront critical technical issues (e.g., error 
recovery, new word problem)
&#8226;Provides a rich and continuing source of useful data
&#8226;Demonstrates the usefulness of the technology
&#8226;Facilitates technology transfer
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
ESPRIT
SPEECHSome Speech-Related Government Programs
DARPA SC ARPA SURBBN, CMU, Lincoln 
SDC, SRI, ...  
HWIM, Harpy, Hearsay
DARPA SLSATT, BBN, CMU, CRIM, 
MIT, SRI, Unisys, ...ATIS, Banking, DART, 
OM, VOYAGER, ...
ESPRIT
SUNDIAL
CNET, CSELT, 
DaimlerBenz, LogicaAir and Train TravelLE
ARISE
CSELT, IRIT, KPN, 
LIMSI, U. Nijmegen..Train Travel1970 1990 1980 2000DARPA WSJ/BN D.C.ATT, BBN, CMU, CU, IBM, 
MIT, MITRE, SpeechWorks,SRI, +Affiliates, ...
Complex Travel
ESPRIT
MASK
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>48</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSData vs.Performance (Weather Domain)
&#8226;Longitudinal evaluations show improvements
&#8226;Collecting real data improves performance:
&#8211; Enables increased complexity and improved robustness for acoustic 
and language models
&#8211; Better match than laboratory recording conditions
&#8226;Users come in all kinds
Intro || NLU || Dialogue || Evaluation || Portability || Future051015202530354045
Apr May Jun Jul Aug Nov Apr Nov MayError Rate (%)
110100
Training Data (x1000)Word
DataWER
Data</text>
        </slide>
        <slide>
          <slideno>63</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSEnvisioned Future Extension
USER: Can you tell me the phone number of the  
Thaiku Restaurant in Seattle?
SYSTEM: I may not know the name of the restaurant.   
Can you spell it for me?
USER: T H A I K USYSTEM: The phone number of Thaiku is 206 706 7807
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSThe U.S. DARPA SLS Program (1990-1995)
&#8226;The Community adopted a common task (Air Travel 
Information Service, or ATIS) to spur technology 
development
&#8226;Users could verbally query a static database for air travel 
information
&#8211; 11 cities in North America (ATIS-2)
&#8211; Expanded to 46 cities in 1993 (ATIS-3)
&#8226;All systems could handle continuous speech from unknown 
speakers (~2,000 word vocabulary)
&#8226;Research driven by five annual common evaluations  
&#8211; CAS  evaluation methodology &#8211; heavy  dependence on strict manually 
provided &#8220;correct&#8221; answers
&#8211; restricted system design to passive mode of  interaction&#8211; shifted focus away from user interface
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>52</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
&#8226;To achieve this flexibility requires progress in many areas:
&#8211;Dialogue manager needs to be domain-independent
&#8211;Responses should reflect distributed properties of database retrievals 
&#8211;Systems must be able to acquire and use new words and concepts&#8220;Where is Atasca in 
Cambridge&#8221;&#8220;I&#8217;d like to 
hear some 
rock music&#8221;&#8226;Conversational interfaces will be much more effective if they 
can adapt to user requests and changing database contentFlexible Conversational Interaction
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
&#8230;&#8230;..
Customer: Yeah, [um] I'm looking for the Buford Cinema.
Agent: OK, and you're wanting to know what&#8216;s showing 
there or ... 
Customer: Yes, please. 
Agent: Are you looking for a particular movie?
Customer: [um] What's showing.
Agent: OK, one moment.
&#8230;&#8230;..
Agent: They're showing A Troll In Central Park .
Customer: No.
Agent: Frankenstein .
Customer: What time is that on?
Agent: Seven twenty and nine fifty.
Customer: OK, and the others?The Nature of Mixed Initiative Interactions
(A Human-Human Example)
disfluency
interruption, overlap
confirmation
clarification
back channel
inference
ellipsis
co-reference
complex quantifierclarification subdialogue
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>70</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSResearch Issues: Speech Understanding 
&#8226;We need to do more than just understand the words
&#8211; Confidence scoring (utterance &amp; word levels) 
&#8211; Utilizing timing information&#8211; Modeling non-speech events and disfluencies
&#8211; Out-of-vocabulary word detection &amp; addition
&#8226;Acquisition of linguistic knowledge
&#8211; Still don&#8217;t know how to rapidly deve lop effective language models that 
yield high coverage of within-domain linguistic space
&#8226;Robustness to environments and speakers
&#8211; Adaptation and personalization
&#8226;Other challenges:
&#8211; Detecting and utilizing non-linguis tic information such as speaker 
identity and emotional state
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Understanding Words in Context 
&#8226;Subtle differences in phrasing can lead to completely 
different interpretations
&#8211; Is there a six A.M. flight?
&#8211; Are there six A.A. flights?
&#8211; Is there a flight six?
&#8211; Is there a flight at six&#8220;six&#8221; could mean:
&#8211;A  t i m e&#8211; A count&#8211; A flight number 
&#8226;The possibility of recognition errors makes it hard to rely 
on features like the article &#8220;a&#8221; or the plurality of &#8220;flights.&#8221;
&#8226;Yet insufficient syntactic/semantic analysis can lead to 
gross misinterpretations
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>76</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Research Issues: Multimodal Interactions
&#8226;What is the unifying linguistic framework that can 
adequately describe multi-modal interactions?
&#8226;What is the optimal design for system configuration?
&#8211; E.g., timing constraints less stri ngent when signals are more robust
&#8226;What are the appropriate integration and delivery 
strategies?
&#8211; How are modalities affected by presence of alternative modes? 
* Graphical interface alters para meters of response decisions
*T e r s e  vs.verbose spoken responses depend on existence of 
ancillary graphics
&#8211; When to utilize which modality
&#8211; trans-modal interfaces (e.g ., read email over the phone)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>64</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSDynamic Vocabulary Understanding
Dynamically alter vocabulary based on dialogue context
&#8220;Tell me about restaurants 
in Arlington.&#8221;&#8220;There are 11 restaurants 
in Arlington.  Some of the 
options are&#8230;&#8221;
Arlington Diner
Blue Plate Express
Tea Tray in the Sky
Asiana Grille
Bagels etc
Flora
&#8230;.HubNLGNLG
ASRASRContextContextTTSTTSDialogDialog
NLUNLUAudioAudioDBDB
&#8220;What&#8217;s the phone 
number for Flora?&#8221;&#8220;The telephone number 
for Flora is &#8230;&#8221;</text>
        </slide>
        <slide>
          <slideno>50</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Some Numeric Evaluation Metrics 
in Flight Domain 
68.0%0.0%25.0%6.3%
66.2%16.9%4.2%12.7%&#8220;MIT&#8221; Data
(66 calls and 1,150 utterances)&#8220;NIST&#8221; Data
(55 calls and 826 utterances)
NIST MIT
94.1 89.7 Transcription Parse Coverage (%)10.6 11.0 Concept Error Rate (%)14.4 15.3 Word Error Rate (%)353.3 299.3 Task Completion Time (s.)Task Completion Rate
Itinerary + Pricing
Incomplete Itinerary No ItineraryComplete Itinerary
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>59</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Flexible Vocabulary 
&#8226;Impractical to support all proper names all the time
&#8211; Several hundred thousand hotel names in the U.S.
&#8211; Issues of recognition accuracy and computational load
&#8226;Solution is two-fold
&#8211; Support the ability for the user to explicitly enter names when 
appropriate
&#8211; Adapt the system&#8217;s working voc abulary to dynamically reflect 
information it presents to the user
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>72</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Rapid Development of Flexible Systems
A Challenge:
&#8211; Given an unstructured knowledge sour ce, how long would it take to 
create a dialogue system capable of providing access to that 
information space through natural spoken interaction?
&#8211; Which aspects of system developm ent consume the most resources?
&#8226;Speech Understanding:
&#8211; How to obtain language models ade quately capturing the linguistic 
space of the domain?
&#8211; How to exploit dialogue contex t to adjust the vocabulary and 
language models  
&#8226;Dialogue Management/Response Planning:
&#8211; How to efficiently encode all the appropriate system responses, 
including help mechanisms and error recovery?
&#8211; How to separate out domain-specific aspects so that new domains 
can leverage code devel oped for other applications?
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>73</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
The Role of Simulated Dialogues?
&#8226;We cannot rely solely on live human-computer dialogue 
to stress test our systems  
&#8226;Somewhat effective strategy:
&#8211; Batch mode reprocessing of previously recorded dialogues
&#8226;However, prior mixed initiative dialogues quickly 
become incoherent as systems evolve
&#8226;Proposal:  simulate user half of the conversation
&#8211; Randomly generate an appropriate re sponse in reaction to each 
system turn
&#8226;Simulated Input as text strings or spoken utterances
&#8211; selection from library of user utterances or 
&#8211; through speech synthesis
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTypical Steps in Transforming User Query
&#8226;Parsing 
&#8211; Establishes syntactic organization and 
semantic content 
&#8226;Generate Semantic Frame
&#8211; Produces meaning representation 
identifying relevant constituents and 
their relationships
&#8226;Incorporation of discourse context
&#8211; Deals with fragments, pronominal 
references, etc.
&#8226;Transformation to database query
&#8211; Produces SQL formatted string for 
database retrievalGenerate 
Frame
Incorporate 
Context
Produce     
DB QueryProduce 
Parse TreeRecognizer Hypotheses
Parse Tree
Semantic Frame
Frame in Context
SQL
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
Input Processing: Understanding
LANGUAGE
UNDERSTANDINGSemantic
RepresentationSPEECH
RECOGNITIONSpeech
Waveform
Sentence
Hypotheses
Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: "Boston"
Predicate: TO
Topic: CITY
Name: "Denver"Clause: DISPLAY
Topic: FLIGHT
Predicate: FROM
Topic: CITY
Name: " Boston "
Predicate: TO
Topic: CITY
Name: " Denver "FLIGHT
FLIGHTSDENVER SHOW TO BOSTON FROM ME
ONAND
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSSome other Spoken Dialogue Systems
&#8226;Canon TARSAN (Japanese)
&#8211;Info retrieval from CD-ROM
&#8226;InfoTalk (Cantonese)
&#8211;Transit fare
&#8226;KDD ACTIS (Japanese)
&#8211;Area-codes, country-codes 
and time-difference 
&#8226;NEC (Japanese)
&#8211;Ticket reservation
&#8226;NTT (Japanese)
&#8211;Directory assistance
&#8226;SpeechWorks (Chinese)
&#8211;Stock quotes
&#8226;Toshiba TOSBURG (Japanese)
&#8211;Fast food ordering&#8226;Canon TARSAN (Japanese)
&#8211;Info retrieval from CD-ROM
&#8226;InfoTalk (Cantonese)
&#8211;Transit fare
&#8226;KDD ACTIS (Japanese)
&#8211;Area-codes, country-codes 
and time-difference 
&#8226;NEC (Japanese)
&#8211;Ticket reservation
&#8226;NTT (Japanese)
&#8211;Directory assistance
&#8226;SpeechWorks (Chinese)
&#8211;Stock quotes
&#8226;Toshiba TOSBURG (Japanese)
&#8211;Fast food orderingAsia U.S.
&#8226;AT&amp;T How May I Help You?,...
&#8226;BBN Call Routing
&#8226;CMU Movieline, Travel,...
&#8226;Colorado U Travel
&#8226;IBM Mutual funds, Travel
&#8226;Lucent Movies, Call Routing,...
&#8226;MIT Jupiter, Voyager, Pegasus,..
&#8211;Weather, navigation, flight info
&#8226;Nuance Finance, Travel,&#8230;
&#8226;OGI CSLU Toolkit
&#8226;SpeechWorks Finance, Travel,...
&#8226;UC-Berkeley BERP
&#8211;Restaurant information
&#8226;U Rochester TRAINS
&#8211;Scheduling trains&#8226;AT&amp;T How May I Help You?,...
&#8226;BBN Call Routing
&#8226;CMU Movieline, Travel,...
&#8226;Colorado U Travel
&#8226;IBM Mutual funds, Travel
&#8226;Lucent Movies, Call Routing,...
&#8226;MIT Jupiter, Voyager, Pegasus,..
&#8211;Weather, navigation, flight info
&#8226;Nuance Finance, Travel,&#8230;
&#8226;OGI CSLU Toolkit
&#8226;SpeechWorks Finance, Travel,...
&#8226;UC-Berkeley BERP
&#8211;Restaurant information
&#8226;U Rochester TRAINS
&#8211;Scheduling trainsEurope
&#8226;CSELT (Italian)
&#8211;Train schedules
&#8226;KTH WAXHOLM (Swedish)
&#8211;Ferry schedule
&#8226;LIMSI (French)
&#8211;Flight/train schedules
&#8226;Nijmegen (Dutch)
&#8211;Train schedule
&#8226;Philips (Dutch,Fr.,German)
&#8211;Flight/Train schedules
&#8226;Vocalis VOCALIST (English)
&#8211;Flight schedules&#8226;CSELT (Italian)
&#8211;Train schedules
&#8226;KTH WAXHOLM (Swedish)
&#8211;Ferry schedule
&#8226;LIMSI (French)
&#8211;Flight/train schedules
&#8226;Nijmegen (Dutch)
&#8211;Train schedule
&#8226;Philips (Dutch,Fr.,German)
&#8211;Flight/Train schedules
&#8226;Vocalis VOCALIST (English)
&#8211;Flight schedules
&#8226;Large-scale deployment of some dialogue systems
&#8211; e.g., CSELT, Nuance, Philips, ScanSoft (SpeechWorks)
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSDialogue Management Strategies
&#8226;Directed dialogues can be implemented as a directed graph 
between dialogue states
&#8211; Connections between states are predefined
&#8211; User is guided through the graph by the machine&#8211; Directed dialogues have been succ essfully deployed commercially
&#8226;Mixed-initiative dialogues are possible when state transitions 
determined dynamically
&#8211; User has flexibility to specify constraints in any order
&#8211; System can &#8220;back off&#8221; to a directed  dialogue under certain circumstances
&#8211; Mixed-initiative dialogue systems are mainly research prototypes
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
The Premise:
Everybody
wants
InformationEverybody
wants
Information
Need new
interfaces
Speech is It!
For North America
CommerceNet
Research Center (1999)
Even when
they are
on the moveEven when
they are
on the move
The interface
must be
easy to useThe interface
must be
easy to useDevicesmust besmall
Intro || NLU || Dialogue || Eval uation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
U: I need a flight from Boston to San Francisco
C: Did you say Boston or Austin?U: Boston, MassachusettsC: What date will you be traveling?
U: Tomorrow
C: Hold on while I retrieve the flights for you
C: I have found 10 flights meeting your sp ecification.             
When would you like to leave?
U: In the morning.
C: Do you have a preferred airline?
U: United
C: I found two non-stop United fl ights leaving in the morning &#8230;Help the user narrow
down the choicesClarification
(insufficient info)Clarification
(recognition errors)
&#8226;Post-Retrieval: Multiple DB Retrievals =&gt; Unique ResponseStages of Dialogue Interaction
&#8226;Pre-Retrieval: Ambiguous Input =&gt; Unique Query to DB
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS Word Networks: 
Efficient Representation of N-best Lists
show me flights from boston to denver and
show me flights from boston to denvershow me flights from boston to denver on
show me flight from boston to denver and
show me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver andshow me a flight from boston to denvershow me a flight from boston to denver onshow me flights from boston to denver andshow me flights from boston to denvershow me flights from boston to denver on
show me flight from boston to denver and
show me flight from boston to denvershow me flight from boston to denver onshow me flights from boston to denver inshow me a flight from boston to denver andshow me a flight from boston to denvershow me a flight from boston to denver onanswer
If the parser can propose probabilities for next-word 
theories in the network, then it can be used to adjust 
theory scores in second pass through the network
show
ameflights
flight boston from denver toand
on
in# #
Intro || NLU || Dialogue || Evaluati on || Portability || Future</text>
        </slide>
        <slide>
          <slideno>49</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLS
0 1 02 03 04 05 06 07 0Entire SetIn Domain (ID)Male (ID)Female (ID)Child (ID)Non-native (ID)Out of DomainExpert
% Error RateSentence
Word
&#8226;Male ERs are better than females (1.5x) and children (2x)
&#8226;Strong foreign accents and out-of-domain queries are hard
&#8226;Experienced users are 5x better than novices
&#8226;Understanding error rate is consistently lower than SERASR Error Analysis (Weather Domain)
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>Computer Science and Artificial Intelligence LaboratorySLSTable-Driven Dialogue Control
&#8226;Set of operations perform specialized tasks
&#8226;Ordered set of rules specify active operations  
&#8226;Dynamic set of state variables drive rule execution
&#8226;A rule fires when conditions are met:
&#8211; Simple arithmetic, string, and Boolean tests on state variables
&#8226;Operations typically alter state variables
&#8226;Operations specify one of three possible moves :      
Continue, Stop, Start Over
&#8226;Several rules apply in a single turn
Intro || NLU || Dialogue || Evaluation || Portability || Future</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Plan Recognition (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec19/</lecture_pdf_url>
      <lectureno>19</lectureno>
      <slides>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>'
&amp;$
%Example
withoutplanrecognition withplanrecognition
1USER: "Let's workonemail." USER: "Let's work onemail."
2AGENT: "Youhave5unread messages." AGENT: "You have5unread messages."
3USER: "Let's readamessage."
4USER: &lt;Select message from Chuck&gt; USER: &lt;Select message from Chuck&gt;
5USER: "Let's readmessage initsownwindow."
6USER: &lt;Open message&gt; USER: &lt;Open message&gt;
7USER: "Let's dosomething using message."
8USER: &lt;Visit http://foo.bar.com&gt; USER: &lt;Visit http://foo.bar.com&gt;
9 ...long lunch... ...long lunch...
10USER: "What wasIdoing?" USER: "What wasIdoing?"
11AGENT: "Working onemail. AGENT: "Working onemail.
12 Reading message from Chuck. Reading message fromChuck.
13 Reading message initsownwindow. Reading message initsown...
14 Donesomething using message. Donesomething using message.
15 &lt;Visit http://foo.bar.com&gt; &lt;Visit http://foo.bar.com&gt;
16 Expecting toreact tomessage." Expecting toreact tomessage."
17USER: "Let's react tomessage."
18USER: "Let's sendemail."
19USER: &lt;Start newmessage&gt; USER: &lt;Start newmessage&gt;
20USER: "Let's fillinmessage."
21USER: &lt;Address message toDan&gt; USER: &lt;Address message toDan&gt;
22 AGENT: "Why yousending email toDan?
23 (a)reacting tomsgfromChuck
24 (b)asanewgoal"
25 USER: "I'm reacting tomsgfromChuck."
26 ...additional editing actions... ...additional editing actions...
27USER: "Done filling inmessage." USER: "Done filling inmessage."
28AGENT: "Let's send, cancel orsavemsg." AGENT: "Let's send, cancel orsave msg."
29USER: "Please savemessage." USER: "Please save message."
30AGENT: &lt;Save message&gt; AGENT: &lt;Save message&gt;
Key: "utterance" &lt;primitive action&gt;
DialogueSystems 16/16</text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Cohesion and Local Coherence: Lexical Chains, Centering Theory; Applications to Automated Essay Scoring (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec04/</lecture_pdf_url>
      <lectureno>4</lectureno>
      <slides>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Experimental Set-Up 
Data from (Britton&amp; Gulgoz&#8217;88) 
Source: text on the airwar in Vietnam from an Air &#8226; 
Force training textbook 
&#8226;	Various revision methods to improve text
readability:
&#8211;	Principled (based on propositional model) 
&#8211;	Heuristic (based on reader&#8217;s intuition) 
&#8211;	Readability (based on readability index) 
Lexical Cohesion and Coherence	 17/34 </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Center Shifting 
Shifting the center, if it is neither retained no continued 
&#8226; Cb(U n+1) &lt;&gt;&#160; Cb(U n) 
Lexical Cohesion and Coherence 31/34 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Text Cohesion
Cohesion captures devices that link sentences into a text 
Lexical cohesion &#8226; 
References &#8226; 
&#8226; Ellipsis 
&#8226; Conjunctions 
Lexical Cohesion and Coherence 7/34 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results 
Weighted No. Inference 
LSA word props Ef&#64257;ciency mult. 
Text coherence overlap recalled (props/min) choice 
Original 0.192 0.047 35.5 3.44 37.11 
Readability rev. 0.193 0.073 32.8 3.57 29.74 
Principled rev. 0.347 0.204 58.6 5.24 46.44 
Heuristic rev. 0.403 0.225 56.2 6.01 48.23 
Lexical Cohesion and Coherence 19/34 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Experimental Set-Up 
Data from (Britton&amp; Gulgoz&#8217;88) 
&#8226;	Evaluation: based on recall, ef&#64257;ciency recall and 
scores on a multiple choice 
&#8226;	Assessment: Principled and Heuristic is better than Readability and Original 
Lexical Cohesion and Coherence	 18/34 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Lexical Chains: Example 
Lexical Cohesion and Coherence 12/34 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Centering Theory: Basics 
&#8226;	Unit of analysis: centers 
&#8226;	&#8220;Af&#64257;liation&#8221; of a center: utterance (U) and discourse 
segment (DS) 
&#8226; Function of a center: to link between a given
utterance and other utterances in discourse
Lexical Cohesion and Coherence	 25/34 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Center Continuation
Continuation of the center from one utterance not only 
to the next, but also to subsequent utterances 
&#8226;	Cb(U n+1)=C b(U n) 
&#8226;	Cb(U n+1) is the most highly ranked element of
Cf(U n+1) (thus, likely to be C b(U n+2)
Lexical Cohesion and Coherence	 29/34 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Coherence in Automatically Generated Text
DUC results: most of automatic summaries exhibit &#8226; 
lack of coherence 
&#8226;	Is it possible to automatically compute text 
coherence? 
&#8211;	text representation 
&#8211;	inference procedure 
Lexical Cohesion and Coherence	 3/34 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Text Representation
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Lexical Cohesion and Coherence 4/34 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Understanding the Results
&#8226;	No signi&#64257;cant difference between LSA and the 
baseline model in this experiment 
&#8226;	Other experiments showed that LSA may perform better, but note need in parameter estimation 
&#8226;	Neither model is used for prediction 
Lexical Cohesion and Coherence	 20/34 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
&#65533;&#160;
&#65533;&#160; &#65533;&#160;Vector-Based Coherence Assessment
&#8226;	Each sentence is represented as a weighted vector of
its terms SENTENCE 1: 1000110
SENTENCE 2: 1 111001
&#8226;	Distance between two adjacent sentences is
measured using cosine
wy,b1wt,b2sim(b 1,b2)=&#65533;&#160;t&#160;
n&#160; 2 2w	 wt	 t,b1&#160; t=1&#160; t,b2&#160;
&#8226;	Lexical continuity is measured as average distance
between sentences in a paragraph
Lexical Cohesion and Coherence	 15/34 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Lexical Cohesion and Coherence 
Regina Barzilay 
February 17, 2004 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Phenomena to be Explained
Johh went to his favorite music 
store to buy a piano. 
He had frequented the store for 
many years. 
He was excited that he could &#64257;&#173;
nally buy a piano. 
He arrived just as the store was closing for the day. John went to his favorite music store to buy a piano. 
It was a store John had fre&#173;quented for many years. 
He was excited that he could &#64257;&#173;
nally buy a piano. 
It was closing just as John ar&#173;
rived. 
Lexical Cohesion and Coherence 22/34 </text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Application to Essay Grading
(Miltsakaki&amp;Kukich&#8217;00) 
Framework: GMAT e-rater &#8226;
&#8226;	Implementation: manual annotation of coreference 
information 
&#8226;	Grading: based on ratio of shifts 
&#8226;	Data: GMAT essays 
Lexical Cohesion and Coherence	 33/34 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today&#8217;s Topics 
&#8226; Two linguistic theories of text connectivity 
&#8211; Text Cohesion (Halliday&amp;Hasan&#8217;76) 
&#8211; Centering Theory (Grosz&amp;Joshi&amp;Weinstein&#8217;83) 
&#8226; Application to automatic essay scoring 
Lexical Cohesion and Coherence 5/34 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Centering Theory 
(Grozs&amp;Joshi&amp;Weinstein&#8217;95) 
&#8226;	Goal: to account for differences in perceived 
discourse 
Focus: local coherence &#8226; 
global vs immediate focusing in discourse (Grosz&#8217;77) 
&#8226;	Method: analysis of reference structure 
Lexical Cohesion and Coherence	 21/34 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example 
Halliday&amp;Hasan(1982) 
Time &#64258;ies.
- You can&#8217;t; they &#64258;y too quickly.
Find three cohesive ties! 
Lexical Cohesion and Coherence 8/34 </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Study results
&#8226;	Correlation between shifts and low grades 
(established using t-test) 
&#8226;	Improvement of score prediction in 57% 
Lexical Cohesion and Coherence	 34/34 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
John went to his favorite music store to buy a piano.
It was a store John had frequented for many years.
He was excited that he could &#64257;nally buy a piano.
It was closing just as John arrived.
Lexical Cohesion and Coherence 27/34 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Lexical Chains: Computation 
&#8220;Associanist text models&#8220; 
&#8226;	De&#64257;ne word similarity function 
&#8226;	De&#64257;ne &#8220;insertion con&#64258;ict&#8221; strategy (greedy vs. 
dynamic strategy) 
Lexical Cohesion and Coherence	 11/34 </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Center Retaining 
Retention of the center from one utterance to the next
&#8226;	Cb(U n+1)=C b(U n) 
&#8226;	Cb(U n+1) is not the most highly ranked element of 
Cf(U n+1) (thus, unlikely to be C b(U n+2) 
Lexical Cohesion and Coherence	 30/34 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Lexical Chains: Accuracy
Example: Entertainment-service 1 auto-maker 1 enterprise 1 
massachusetts-institute 1 technology-microsoft 1 microsoft 10 concern 
1 company 6 
&#8226;	The accuracy bounded by the quality of a lexical
resource
&#8226;	The need in disambiguation makes the task harder 
Disambiguation accuracy around 60% 
For more examples see: 
http://www.cs.columbia.edu/nlp/summarization-test/index.html 
Lexical Cohesion and Coherence	 13/34 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Analysis
&#8226;	The same content, different realization 
Variation in coherence arises from choice of &#8226; 
syntactic expressions and syntactic forms 
Lexical Cohesion and Coherence	 23/34 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Term similarity 
Latent Semantic Analysis (Deerwester&#8217;90)
&#8226;	Goal: identi&#64257;cation of semantically similar words 
birth, born, baby 
&#8226; Assumption: the context surrounding a given word
provides important information about its meaning
&#8226;	Method: Singular Vector Decomposition 
Lexical Cohesion and Coherence	 16/34 </text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Coherent Discourse 
Coherence is established via center continuation
John went to his favorite music 
store to buy a piano. 
He had frequented the store for 
many years. 
He was excited that he could &#64257;&#173;
nally buy a piano. 
He arrived just as the store was 
closing for the day. John went to his favorite music 
store to buy a piano. 
It was a store John had fre&#173;
quented for many years. 
He was excited that he could &#64257;&#173;nally buy a piano. 
It was closing just as John ar&#173;
rived. 
Lexical Cohesion and Coherence 32/34 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Constraints on Distribution of Centers 
&#8226;	Cf&#160;is determined only by U; 
&#8226;	Cf&#160;are partially ordered in terms of salience 
&#8226;	The most highly ranked element of C f&#160;(U n&#8722;1)i s
realized as C b&#160;(U n)
&#8226;	Syntax plays role in ambiguity resolution: subj &gt;
ind obj &gt;&#160;obj &gt;o t h e r s
&#8226;	Types of transitions: center continuation, center
retaining, center shifting
Lexical Cohesion and Coherence	 28/34 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Another Example
John really goofs sometimes.
Yesterday was a beautiful day and he was excited about
trying out his new sailboat.
He wanted Tony to join him on a sailing trip.
He called him at 6am.
He was sick and furious at being woken up so early.
Lexical Cohesion and Coherence 24/34 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Leftovers from Last Time 
Input Type CSeg&#160;for ABC 
ASR 0.1723 
Closed Captions 0.1515 
Transcripts 0.1356 
Note the impact for ASR! 
Lexical Cohesion and Coherence 1/34 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Text cohesion 
Hobbs&#8217; Example(1982) 
The concept of cohesion refers to relations of meaning that exist within the 
text, and that de&#64257;nes it as a text. Cohesion occurs where the interpretation of 
some element in the discourse dependent on that of another. 
Lexical Cohesion and Coherence 6/34 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Automatic Measurement of Text Coherence
&#8226;	Cohesive ties re&#64258;ect the degree of text coherence 
&#8226;	First attempts to (semi-) automate cohesion 
judgments rely on: 
&#8211;	propositional modeling of text structure (Kintsch&amp;van Dijk&#8217;78) 
time consuming and requires training 
&#8211;	readability measures (Flesch&#8217;48) 
weak correlation with comprehension measures 
Lexical Cohesion and Coherence	 14/34 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Lexical Chains: Applications 
Summarization&#8226; 
&#8226; Segmentation 
&#8226; Malapropism Detection
Information Retrieval
&#8226; 
Lexical Cohesion and Coherence 10/34 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Center Typology 
&#8226;	Types: 
&#8211; Forward-looking Centers C f&#160;(U, DS) 
&#8211; Backward-looking Centers C b&#160;(U, DS) 
&#8226;	Connection: C b&#160;(U n) connects with one of C f&#160;
(U n&#8722;1) 
Lexical Cohesion and Coherence	 26/34 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Lexical Chains: Example 
1. There was once a little girl and a little boy and a dog 
2. And the sailor was their daddy 
3. And the little doggy was white 
4. And they like the little doggy 
5. And they stroke it 
6. And they fed it 
7. And they ran away 
8 . A n dt h e nd a d d y h a dt og oo nas h i p 
9. And the children misssed &#8217;em 
10. And they began to cry 
Lexical Cohesion and Coherence 9/34 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Lack of Coherence 
Hobbs&#8217; Example(1982) 
When Teddy Kennedy paid a courtesy call on Ronald Reagan recently, he 
made only one Cabinet suggestion. Western surveillance satellites con&#64257;rmed 
huge Soviet troop concentrations virtually encircling Poland. 
Lexical Cohesion and Coherence 2/34 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Introduction (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec01/</lecture_pdf_url>
      <lectureno>1</lectureno>
      <slides>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Text Grammars
Assumption: existence of text grammar in limited 
domains (analogous to sentence grammar) 
&#8226; Tale grammar (31 terminals) V .Propp(1920s) 
&#8226; Scienti&#64257;c articles: introduction, conclusions, ... . 
Computational Models of Discourse: Overview 19/25 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example: Information Retrieval 
Show me the p roof of Cantor theorem 
Computational Models of Discourse: Overview 6/25 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Does it work?
A day after a suicide bomber killed 10 people in a terror attack on a Jerusalem 
bus, Israeli forces conducted operations Friday in the West Bank and Gaza, 
killing three Palestinians the Israeli army had identi&#64257;ed as terrorists. The 
leader of Hamas said Friday that his group is making every effort to seize 
Israeli soldiers as bargaining chips for the release of Palestinians in Israeli 
jails. At least 45 people were wounded in the terror attack, which Israeli of&#173;
&#64257;cials said proved the need for what Israel calls a security fence intended to block terrorists from entering the country. 
Computational Models of Discourse: Overview 25/25 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Types of Discourse 
&#8226; Monologue (narrative, lecture) 
&#8226; Human-human dialog 
&#8226; Human-machine dialogue 
Computational Models of Discourse: Overview 23/25 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Inference in Discourse Processing
&#8226;	There are several possible ways to interpret an 
utterance in context 
&#8226;	We need to &#64257;nd the most likely interpretation
&#8226;	Discourse model provides a computational 
framework for this search 
Computational Models of Discourse: Overview	 11/25 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Computational Approaches
&#8226;	Rule-based approaches: manually encode all the 
required domain and common knowledge 
&#8226;	Machine-learning approaches: learn all the required knowledge from a corpus 
&#8211;	Supervised classi&#64257;cation 
&#8211;	Hidden-Markov Models 
&#8211;	Clustering 
&#8211;	Reinforcement learning 
Computational Models of Discourse: Overview	 21/25 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Rhetorical Structure Theory
Assumption: Clauses in well-formed text are related via 
prede&#64257;ned rhetorical relations 
Evidence: a claim information intended to &#8226; &#8594;
increase the readers&#8217; belief in the claim 
...&#8226; 
Computational Models of Discourse: Overview 17/25 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
What is Discourse 
Example by Charles Fillmore: 
Please use the toilets, not the pool. 
The pool for members only. 
Computational Models of Discourse: Overview 9/25 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Models of Discourse Structure
&#8226;	Investigation of lexical connectivity patterns as the 
re&#64258;ection of discourse structure 
&#8226;	Speci&#64257;cation of a small set of rhetorical relation among discourse segments 
&#8226;	Adaption of the notion of grammar
&#8226; Examination of intentions and relations among
them as the foundation of discourse structure
Computational Models of Discourse: Overview	 13/25 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Discourse Exhibits Structure!
&#8226;	Discourse can be partition into segments, which can 
be connected in a limited number of ways 
&#8226;	Speakers use linguistic devices to make this 
structure explicit 
cue phrases, intonation, gesture 
&#8226;	Listeners comprehend discourse by recognizing this 
structure 
&#8211;	Kintsch, 1974: experiments with recall 
&#8211;	Haviland&amp;Clark, 1974: reading time for given/new information 
Computational Models of Discourse: Overview	 12/25 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Why NLP?
Lots of information is in natural language format 
Documents&#8226; 
News broadcasts &#8226; 
User Utterances &#8226; 
Lots of users want to communicate in natural language.
&#8220;DO what I mean!&#8221; &#8226; 
&#8220;Now we are betting the company on these natural
interface technologies&#8221; Bill Gates, 1997 
Computational Models of Discourse: Overview 3/25 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example: Question-Answering(1) 
Q: Who is the president of First Union Corp? 
First Union Corp is continuing to wrestle with sever prob&#173;
lems. According to industry insiders at Pine Webber, their 
president, John R. Georgius, is planning to retire soon. 
Computational Models of Discourse: Overview 5/25 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example: User Interfaces
SHRDLU (Winograd, 1972): language interface for 
block manipulation 
Person: PICK UP A BIG RED BLOCK.
Computer: OK. (does it)
Person: PUT IT NEAR THE PYRAMID.
Computational Models of Discourse: Overview 4/25 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Does it work? 
&#8226;	Summarization: F-scores 70% (DUC 2003)
&#8226;	Anaphora resolution: F-scores 60-70%
(Ng&amp;Cardie:2002)
&#8226;	RST parsing: 47% (compare with th accuracy of syntactic parsers &#8212; high 80th!) 
Discourse processing is hard! 
Computational Models of Discourse: Overview	 24/25 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Cohesion
Assumption: Well-formed text exhibits strong lexical 
connectivity via use of: 
&#8226;	Repetitions
&#8226;	Synonyms
Coreference
&#8226; 
Computational Models of Discourse: Overview	 15/25 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example 
1. A: I&#8217;m going camping next week. Do you have a two person tent I could borrow? 
2. B: Sure. I have a two-person backpacking tent. 
3. A: The last trip I was on there was a huge storm. 
4. A: It poured for two hours. 
5. A: I had a tent, but I got soaked anyway. 
6. B: What kind of tent was it? 
7. A: A tube tent. 
8. B: Tube tents don&#8217;t stand up well in a real storm. 
9. A: True. 
10.B: Where are you going on this trip? 
11.A: Up in the Minarets. 
12.B: Do you need any other equipment? 
13.A: No. 
14.B: Okay. I&#8217;ll bring the tent tomorrow. 
Computational Models of Discourse: Overview 14/25 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Discourse Phenomena
&#8226; a word, phrase, and utterance whose interpretation 
is shaped by the discourse or dialogue context 
John arrived at an oasis. He saw the camels around the
water hole ...
John arrived at an oasis. He left the camels around the
water hole ...
&#8226; a sequence of utterances whose interpretation is 
more than sum of its component parts 
Computational Models of Discourse: Overview 10/25 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Computational Models of Discourse:
Overview
Regina Barzilay 
February 4, 2004 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Rhetorical Structure Theory 
3. The last trip I was on there was a huge storm. 
4. It poured for two hours. 
5. I had a tent, but I got soaked anyway. 
result 
3 4 5 evidence 
Computational Models of Discourse: Overview 18/25 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Plan for Today
Overview of the course:
&#8226;	Discourse processing: motivation and background 
Discourse theories &#8226; 
&#8226;	Applications 
Administration: 
&#8226;	Requirements 
Computational Models of Discourse: Overview	 1/25 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Natural Language Processing
Goal: Computers using natural language as input or 
output 
language languag e 
understanding computer 
generation 
NLU example: convert an utterance into a sequence of computer instructions 
NLG example: produce a summary of a patient&#8217;s record 
Computational Models of Discourse: Overview 2/25 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Applications 
Summarization&#8226; 
&#8226; Anaphora resolution 
&#8226; Essay grading 
&#8226; Segmentation 
&#8226; Information ordering 
&#8226; Dailog processing 
...&#8226; 
Computational Models of Discourse: Overview 22/25 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Cohesion 
1. A: I&#8217;m going camping next week. Do you have a two person tent I could borrow? 
2. B: Sure. I have a two-person backpacking tent. 
3. A: The last trip I was on there was a huge storm. 
4. A: It poured for two hours. 
5. A: I had a tent, but I got soaked anyway. 
6. B: What kind of tent was it? 
7. A: A tube tent. 
8. B: Tube tents don&#8217;t stand up well in a real storm. 
9. A: True. 
10.B: Where are you going on this trip? 
11.A: Up in the Minarets. 
12.B: Do you need any other equipment? 
13.A: No. 
14.B: Okay. I&#8217;ll bring the tent tomorrow. 
Computational Models of Discourse: Overview 16/25 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example: Text Generation
(Duboue&amp;McKeown, 2003): verbalization of semantic 
data from movie databases 
Actor, born Thomas Connery on August 25, 1930, in Ed&#173;
inburgh, Scotland. He has a brother, Neil, born in 1938. 
Connery dropped out of school to join the British Navy. 
Connery is best known for his portrayal of the British spy, 
James Bond, in the 1960s. 
Computational Models of Discourse: Overview 7/25 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Challenges
Sentences cannot be processed in isolation 
Coreference&#8226; 
&#8226; Ordering
&#8226; Segmentation 
We need to model text and dialog structure 
Computational Models of Discourse: Overview 8/25 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Intention-based Approaches 
Dialogue as collaborative activity: 
&#8226; Intention of A: to get a tent 
&#8226; To achieve this goal, A: 
&#8211; Requests a tent from B 
&#8211; Convinces B in the importance of this request 
Computational Models of Discourse: Overview 20/25 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Automatic Reference Resolution (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec05/</lecture_pdf_url>
      <lectureno>5</lectureno>
      <slides>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Clustering for Coreference 
(Cardie&amp;Wagstaff:1999) 
&#8226;	Each group of coreferent noun phrases de&#64257;nes an 
equivalence class 
&#8226;	Distance measure incorporates &#8220;linguistic intuition&#8221; 
about similarity of noun phrases 
&#8226;	Hard constraints enforce clustering construction 
Reference Resolution	 26/?? </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Generic Algorithm 
Identi&#64257;cation of Discourse Entities
&#8226; 
Identify nouns and pronouns in text
Characterization of Discourse Entities
&#8226;
Compute for each discourse entity NP i&#160;a set of values from
{Ki1,...,k im} from mknowledge sources
&#8226;	Anaphoricity Determination
Eliminate non-anaphoric expressions to cut search space
Generation of Candidate Antecedents &#8226; 
Compute for each anaphoric NP j&#160;a list of candidate antecedents 
Cj&#160;
Reference Resolution	 18/?? </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Algorithm
1. Remove potential referents that do not agree in 
number or gender with the pronoun 
2. Remove potential referents that do not pass 
intrasentetial syntactic coreference constraints 
3. Update the total salience value of the referent
4. Select the referent with the highest value 
Accuracy on unseen data: 86% 
Reference Resolution 25/?? </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Syntactic Constraints
&#8226;	Gender Agreement 
John has an Acura. It is attractive. 
&#8226;	Syntactic Agreement 
John bought himself a new Acura. 
John bought him a new Acura. 
Reference Resolution	 13/?? </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Preferences in Pronoun Interpretation
&#8226;	Recency: Entities introduced in recent utterances 
are more salient than those introduced further back 
John has an Integra. Bill has a Legend. Mary likes to drive it. 
&#8226;	Repeated mention: Entities that have been focus on in the prior discourse are more likely to continue to be focused on in subsequent discourse 
John needed a car to get his new job. He decided that he wanted 
something sporty . Bill went to the Acura dealership with him. He 
bought an Integra. 
Reference Resolution	 15/?? </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Salience Factors 
Sentence Recency 100 
Subject Emphasis 80 
Existential Emphasis 70 
Accusative 50 
Indirect Object 40 
Non-adverbial Emphasis 50 
Head-noun Emphasis 80 
Reference Resolution 23/?? </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Instance Representation
Based noun phrases (automatically computed) are
represented with 11 features:
Individual Words &#8226; 
Head Word &#8226; 
Position &#8226; 
&#8226; Pronoun type (nominative, accusative) 
&#8226; Semantic Class: Time, City , Animal, Human, Object (WordNet) 
&#8226; Gender (WordNet, speci&#64257;ed list) 
&#8226; Animacy (based on WordNet) 
Reference Resolution 27/?? </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Syntactic Constraints on Coreference 
&#8226; Number Agreement 
* John has a new Acura. They are red.
John has three New Acuras. It is red.
&#8226; Person and Case Agreement
* John and Mary have Acuras. We love them. 
You and I have Acuras. We love them. 
Reference Resolution 12/?? </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Pronouns
Stronger constrains on using pronouns than on noun 
phrase references. 
&#8226; Require a high degree of activation from a referent 
&#8226; Have short activation span 
a. John went to Bob&#8217;s party , and parked next to a Acura Integra. 
b. He went inside and talked to Bob for more than an hour. 
a. Bob told him that he recently got engaged. 
b. ??He also said that he bought it yesterday . 
Reference Resolution 9/?? </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Supervised Learning
(Soon et al.,2001) 
Decision Tree Induction &#8226; 
&#8226;	Shallow feature representation (12 features):
&#8226;	&#8220;corrective&#8221; clustering 
&#8226;	Signi&#64257;cant performance gain over rule-based 
algorithms 
Reference Resolution	 31/?? </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today&#8217;s Topics 
Motivation&#8226; 
&#8226; Types of referential expressions 
&#8226; Syntactic and semantic constraints on coreference 
&#8226; Preferences in coreference interpretation2 
&#8226; Algorithm&#8217;s for coreference resolution 
Reference Resolution 4/?? </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
When something goes wrong
In the past decade almost all Islamic revivalist movements 
have been labeled fundamentalists, whether they be of extremist or moderate origin. The widespread impact of the term is obvious from the following quotation from one 
of the most in&#64258;uential Encyclopedias under the title 
&#8216;Fundamentalist&#8217;: &#8220;The term fundamentalist has. . . been used to describe members of militant Islamic groups.&#8221; Why would the media use this speci&#64257;c word, so often with relation to Muslims? Most of them are radical Baptist, 
Lutheran and Presbyterian groups. 
Reference Resolution 6/?? </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Troublemakers
&#8226;	Inferrables: inferential relation to an evoked entity
I almost bought an Acura today , but a door had a dent and the
engine seemed noisy .
Discontinuous Sets: refer to entities that do not &#8226; 
form a set in a text
John has an Acura, and Mary has a Mazda. They drive them all
the time.
&#8226;	Generics: refer to general set of entities (in contrast
to a speci&#64257;c set mentioned in text)
I saw no less than six Acuras today . They are the coolest cars.
Reference Resolution	 11/?? </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Results
MUC-6 (30 documents): Recall 48.8*%, Precision
57.4%, F-measure 52.8%
Baseline: 34.6%, 69.3%, 46.1% Types of Mistakes:
&#8226;	Parsing mistakes
&#8226;	Coarse entity representation and mistakes in feature computation 
&#8226;	Greedy nature of the algorithm 
Reference Resolution	 30/?? </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Demonstratives and One Anaphora
&#8226;	Demonstratives (this, that) capture spatial proximity 
I like this one, better than that 
&#8226;	One Anaphora evokes a new entity into the 
discourse whose description is dependent of this new entity 
I saw no less that 6 Acuras today . Now I was one. 
Reference Resolution	 10/?? </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Syntactic Factors
subject &gt;&#160;existential predicate nominal &gt;&#160;object &gt;
indirect object &gt;&#160;demarcated adverbial PP
1. An Acura Integra is parked on the lot. (subject) 
2. There is an Acura Integra parked in the lot. 
3 . ... 
4. Inside his Acura Integra, John kissed Mary . (demarcated
adverbial PP)
Penalty for non-head occurrences
Score for equivalence classes
Reference Resolution 24/?? </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Semantic Constraints
&#8226; Selectional restrictions of the verb on its arguments
(1) John parked his Acura in the garage. He had driven it around 
for hours. 
(2) John parked his Acura in the garage. It is incredibly messy , 
with old bike and car parts lying around everywhere. 
(3) John parked his Acura in downtown Beverly Hills. It is
incredibly messy , with old bike and car parts lying around
everywhere.
Reference Resolution 14/?? </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Clustering Algorithm 
&#8226;	Initialization: every noun is a singleton
&#8226;	From right to left, compare each noun to all
proceeding clusters
&#8226;	Combine &#8220;close enough&#8221; clusters unless there exist any incompatible NP 
Example: The chairman spoke with Ms. White. He ... 
Reference Resolution	 29/?? </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Reference Resolution 
Regina Barzilay 
February 18, 2004 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Reference Resolution
&#8226;	Task: determine which noun phrases refer to each 
real-world entity mentioned in a document 
&#8226;	Goal: partition noun phrases in a text into coreference equivalence classes, with one cluster for each set of coreferent NPs 
&#8226;	Difference between anaphora and coreference
In the previous example: {Mrs. J2ones, she, she, Mrs. Jones }, 
{my brother, my brother }, {my brother&#8217;s wife } 
Reference Resolution	 3/?? </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Types of referential expressions: Nouns
Inde&#64257;nite Noun Phrases: &#8226; 
I saw an Acura Integra today .
Some Acura Integras were being unloaded.
I saw this awesome Acura Integra today .
De&#64257;nite Noun Phrases &#8226; 
I saw an Acura Integra today . The Integra was white and needed
to be washed.
The fastest car in the Indianapolis 500 was an Integra.
Reference Resolution 8/?? </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Preferences in Pronoun Interpretation 
Verb Semantics: emphasis on one of verb&#8217;s arguments 
&#8226;	&#8220;implicit causality&#8221; of a verb causes change in
salience of verb arguments
John telephoned Bill. He lost the pamphlet on Acuras.
John criticized Bill. He lost the pamphlet on Acuras.
&#8226;	thematic roles (Goal, Source) cause change in 
salience of verb arguments 
John seized the Acura pamphlet from Bill. He loves reading 
about cars. 
John passed the Acura pamphlet to Bill. He loves reading about 
cars. 
Reference Resolution	 17/?? </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Motivation 
Information extraction &#8226; 
&#8226;	Question-Answering 
Machine-Translation &#8226;
pronoun in the Malay language is translated by its antecedent 
(Mitkov, 1999) 
Summarization&#8226; 
Reference Resolution	 5/?? </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Discourse Model Update 
(Lappin&amp;Leass, 1994) 
&#8226;	Add every new discourse entity to discourse model 
&#8226;	Update its value based on salience factors 
&#8226;	Cut in half recency values when process new entity 
(recency enforcement) 
Reference Resolution	 22/?? </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Reference Resolution: Trends
&#8226;	Knowledge-Rich Approaches vs Knowledge-Lean 
Approaches 
&#8226;	Semi-automatic Fully-Automatic Preprocessing 
&#8226;	Small-scale vs Large-Scale Evaluation 
Reference Resolution	 20/?? </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Generic Algorithm(cont.)
&#8226;	Filtering 
Remove all the members of C j&#160;that violate reference constraints 
&#8226;	Scoring/Ranking 
Order the candidates based on preferences and soft constraints 
&#8226;	Searching/Clustering 
Clustering of instances with the same antecedent 
Reference Resolution	 19/?? </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Reference Resolution: Example 
The Salesgirl (Burns and Allen) 
Gracie: And then Mr. and Mrs. Jones were having matrimonal
trouble, and my brother was hired to watch Mrs. Jones.
George: Well, I am imagine she was a very attractive woman.
Gracie: She was, and my brother watched her day and night for
six month.
George: Well, what happened?
Gracie: She &#64257;nally got a divorce.
George: Mrs. Jones?
Gracie: No, my brother&#8217;s wife.
Reference Resolution 1/?? </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Adding Linguistic Knowledge
Rich Linguistic representation for learning (Ng&amp;Cardie 
2002) 
53 features &#8226; 
manual feature selection &#8226; 
&#8226;	signi&#64257;cant gain in performance over (Soon et al., 2001) 
Reference Resolution	 32/?? </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Knowledge-Lean Multi-strategy Approach 
(Lappin&amp;Leass, 1994) 
&#8226;	Integrates the effects of the recency and
syntactically-based preferences
&#8226;	Doesn&#8217;t rely on semantic or pragmatic knowledge
&#8226;	Follows greedy strategy 
&#8226;	Two stages: discourse model update and pronoun resolution 
Reference Resolution	 21/?? </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Preferences in Pronoun Interpretation
&#8226;	Grammatical Role: Hierarchy of candidate entities 
based on their grammatical role 
John went to the Acura dealership with Bill. He bought an 
Integra. 
Bill went to the Acura dealership with John. He bought an 
Integra. 
Parallelism: &#8226; 
Mary went with Sue to the Acura dealership. Sally went with her to the Mazda dealership. 
Reference Resolution	 16/?? </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Distance Metric 
dist(NP i,NP j)=&#65533;
wf&#160;&#8727; incomp f(NP i,NP j)&#160;
f&#160;
Reference Resolution 28/?? </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
When something goes wrong
Why would the media use this speci&#64257;c word, so often with 
relation to Muslims? 
Before the term fundamentalist was branded for Muslims, 
it was, and still is, being used by certain Christian denominations. Most of them are radical Baptist, 
Lutheran and Presbyterian groups. 
Reference Resolution 7/?? </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Generation of Referring Expressions (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec07/</lecture_pdf_url>
      <lectureno>7</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;	 &#65533;
Lexicalization + Syntactic Realization
&#8226; This stage determines the particular words to be
used to express domain concepts and relations.
&#8226;	Constraints: discourse focus, style constraints,
syntactic environment.
&#8226; Implementation: decision tree.
Example: predicate: win(X, Y)
Verb: X defeated Y, Y was defeated X, X won in the game against X, X won the game 
Noun:: victory of X over Y, victory of X, defeat of Y 
&#65533;&#160; &#65533;&#160;
Generation of Referring Expression 10/34 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;&#160;Example
Many years ago, there was an Emperor, who was so excessively 
fond of new clothes, that he spent all his money in dress. He did 
not care to go either to the theatre or the chase, except for the 
opportunities then afforded him for displaying his new clothes. He had a different suit for each hour of the day; and as of any 
other king or emperor, one is accustomed to say, &#8221;he is sitting 
in council,&#8221; it was always said of him, &#8221;The Emperor is sitting in his wardrobe.&#8221; One day, two rogues, calling themselves weavers, 
made their appearance. They gave out that they knew how to 
weave stuffs of the most beautiful colors and elaborate patterns, the clothes manufactured from which should have the wonder&#173;
ful property of remaining invisible to everyone who was extraor&#173;
dinarily simple in character. &#8221;These must, indeed, be splendid clothes!&#8221; thought the Emperor. 
Generation of Referring Expression 29/34 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
Algorithm: Representation 
(Reiter&amp;Dale,1992) 
&#8226; Input is organized in (attribute, value) pairs 
&#8226; Type is one of the attributes 
&#8226; Attributes are organized in taxonomy 
Object 1: (type, chihuahua), (size, small), (color, black). 
Object 2: (type, chihuahua), (size, large), (color, white). 
Object 3: (type, cat), (size, small), (color, black). 
&#65533;&#160; &#65533;&#160;
Generation of Referring Expression 18/34 </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation
Rewriting rules based on HMM improve the 
performance of summarization system: 
Preferences: 89% rewrite, 9% original, 2% no 
preference 
Generation of Referring Expression 34/34 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Sentence Planning
&#8226;	A one-to-one mapping from predicates to messages 
is bad. 
&#8226;	Sentence planning groups information into
sentences (aggregation).
Before aggregation: Shaquille O&#8217;Neal scored 37
points. The game was on Friday night.
Orlando Magic defeated Toronto Raptors.
Raptors lost seven games in a row.
After aggregation: Shaquille O&#8217;Neal scored 37
points Friday night powering the Orlando Magic
to a 101 89 victory over the Toronto Raptors,
losers of seven in a row.
Generation of Referring Expression	 9/34 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Generation of Referring Expression 
Regina Barzilay 
February 25, 2004 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Realization task 
Insert function words. &#8226; 
&#8226; Choose correct speci&#64257;cation of content words.
Order words. &#8226; 
FUF/SURGE input for the sentence &#8220; John likes Mary 
now.&#8221; 
((cat clause)
(proc ((type mental) (tense present) (lex "like")))
(partic ((processor ((cat proper) (lex "John")))
(phenomenon ((cat proper) (lex "Mary")))))
(circum ((time ((cat adv) (lex "now"))))))
Generation of Referring Expression 11/34 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Unsupervised Referent Selection
Statistical Model of Syntactic Realization 
(Nenkova&amp;McKeown, 2003) 
Hypothesis: There is a regularity in lexical realization of referent chain 
Generation of Referring Expression 28/34 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Impact of Conversational Implicature
Redundant Information causes violations of 
Conversation Implicature (Grice, 1975) 
Sit by the table.
Sit by the brown wood table.
Generation of Referring Expression 13/34 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Results of Transcript Analysis
&#8226;	Preference for adjectives that communicate size, 
shape or color? 
the black dog vs the male dog 
&#8226;	The use of speci&#64257;c head nouns depends on audience expertise 
the small dog vs the chihuahua 
&#8226;	Preference for relative adjectives in speech, and for absolute adjectives in writing 
the small dog vs the one foot high dog 
Generation of Referring Expression	 17/34 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Algorithm
&#8226;	Check Success: see if the contracted description 
picks up one entity from the context 
&#8226;	Choose Property: determine which properties of the referent would rule out the largest number of 
entities 
&#8226;	Extend Description: add the chosen properties to 
the description being constructed and remove 
relevant entities from the discourse. 
Generation of Referring Expression	 19/34 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
NLG System Architecture 
Content Determination &#8226; 
&#8226;	Discourse Planning 
&#8226;	Sentence Aggregation 
Lexicalization&#8226; 
&#8226;	Syntactic and morphological realization 
Generation of Referring Expression	 6/34 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example: Summary of basketball games
scoring((Shaquille, O&#8217;Neal), 37)
time(Friday, night)
team((Shaquille, O&#8217;Neal), (Orlando, Magic))
win(Orlando, Magic), (Toronto, Raptors)
score(101,89)
... 
Generation of Referring Expression 4/34 </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Types of Referents
&#8226;	Is the target named entity the head of the phrase or 
not? 
&#8226;	If it is the head what kind of pre- and post- modi&#64257;er 
dies it have? 
How was the name itself realized in the NP? &#8226; 
Generation of Referring Expression	 31/34 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Why to Use NLG?
&#8226;	Important information is stored in ways which are 
not comprehensible to the end users &#8212; databases, 
expert system, log &#64257;les. 
&#8226;	NLG systems can present this information to users in 
an accessible way. 
&#8226;	Data presented in textual form can be searched by 
IR systems. 
Generation of Referring Expression	 3/34 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
What is NLG?
&#8226;	Program which produces texts in natural language.
&#8226;	Input: some underlying non-linguistic 
representation of information. 
&#8226;	Output: documents, reports, help messages and other types of texts. 
&#8226;	Knowledge sources required: knowledge of 
language and of the domain. 
Generation of Referring Expression	 2/34 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Key Idea
&#8226;	Semantic constraints imposed on lexical choice are 
re&#64258;ected in contextual indicators 
&#8226;	This correlation can be learned automatically from a 
large collections of texts, given a feature vector and 
a referent 
Generation of Referring Expression	 22/34 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Experimental Results 
&#8226;	Training: 10, 353 Testing: 1,511 
&#8226;	Precision: 88.87%, Recall 63.39% 
&#8226;	Steep learning curve 
(500: 64.29%, 2.86% &#8594; 50,000: 88.87%, 63.39%) 
&#8226;	Positive impact of WordNet extension &#8212; 10% 
increase on average 
Generation of Referring Expression	 26/34 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Transcript Analysis
(Reiter&amp;Dale,1992) (&#8220;assembly task&#8221; dialog) 
Which attribute should be used? &#8226; 
gender vs color vs shape 
&#8226;	Is it preferable to use modi&#64257;er or to use a more 
speci&#64257;c head noun? 
the small dog vs the chihuahua 
&#8226;	Should relative or absolute adjectives be used? 
the small dog vs the one foot high dog 
Generation of Referring Expression	 16/34 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Rule Examples
IF in&#64258;ation IN CONTEXT , THEN &#8220;politician&#8221;
IF detective IN PROFILE AND agency in CONTEXT ,
THEN &#8220;policeman&#8221;
IF celine IN CONTEXT , THEN &#8220;north american&#8221;
Generation of Referring Expression 27/34 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Inappropriate Modi&#64257;ers
Overly speci&#64257;c or unexpected classi&#64257;ers violates 
principles of Conversation Implicature 
Look at the dog. 
Look at the pitt bull. 
Generation of Referring Expression 14/34 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Supervised Approach to Referent Selection
Goal: Select the best entity description in a given corpus 
(Radev, 1998) 
Elections (1996): &#8220;Bill Clinton, the democratic presiden&#173;
tial candidate&#8221;
False bomb alert in Little Rock, Ark (1997): &#8220;Bill Clinton,
an Arkansas native&#8221;
Generation of Referring Expression 21/34 </text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
HHM Construction
&#8226;	Each state of HMM corresponds to one syntactic 
realization 
&#8226;	Transitions are estimated based on corpus counts (anaphoric expressions are not resolved!) 
Generation of Referring Expression	 32/34 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Features
&#8226;	Context &#8212; a bag of words surrounding the entity
&#8226;	Length of the article &#8212; an integer 
&#8226;	Name of the entity &#8212; e.g., &#8220;Bill Clinton&#8221; 
&#8226;	Pro&#64257;le &#8212; set of all the descriptions 
Wordnet &#8212; WordNet extension for Pro&#64257;le members &#8226; 
Generation of Referring Expression	 25/34 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Discourse Planning
&#8226;	Text have an underlying structure in which parts are 
related together. 
&#8226;	Rhetorical relationships. 
&#8226;	Conceptual grouping. 
Generation of Referring Expression	 8/34 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Entity Pro&#64257;le
Collection of entity descriptions (automatically
constructed)
Example: Pro&#64257;le of Ung Huot
a senior member, Cambodia&#8217;s, Cambodian foreign minister, co-
premier, &#64257;rst prime minister, foreign minister, MR., new co-
premier, new &#64257;rst prime minister, newly-appointed prime minis&#173;
ter, premier 
Generation of Referring Expression 23/34 </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Target NP Features 
Premodi&#64257;ers:&#8226; 
&#8211;	Titles &#8220;President George W . Bush&#8221; 
&#8211;	Name-external modi&#64257;ers &#8220;Irish Flutist James 
Galway&#8221; 
Postmodi&#64257;ers: &#8226; 
&#8211;	Apposition 
&#8211;	Relative Clause 
&#8211;	Prepositional Phrase Modi&#64257;ciation 
Generation of Referring Expression	 30/34 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Distributional Properties of Pro&#64257;le 
&#8226; 11, 504 entities from 178 MB of newswire 
&#8226; 9, 053 have a single description 
&#8226; 2, 451 very from 2 to 24 descriptions 
Generation of Referring Expression 24/34 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Content determination
&#8226;	Input: Knowledge base.
Schemata and Inference mechanism.
&#8226;
&#8226; Output: predicates to be conveyed in the text. 
Example: 
Game statistics: win(Orlando, Magic), (Toronto, Raptors) 
Player&#8217;s records: team((Shaquille, O&#8217;Neal), (Orlando, Magic)) 
Team&#8217;s record: lost(7,(Toronto, Raptors)) 
Generation of Referring Expression	 7/34 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Text-to-Text Generation 
&#8226;	Input: text (lack of semantic information)
&#8226;	Applications: summarization, question-answering, 
machine translation 
Generation of Referring Expression	 20/34 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Psychological Data
(Levelt, 1989) 
People do include unnecessary modi&#64257;ers in referring 
expressions 
a white bird, a black cup and a white cup 
&#8226; Incremental Processing Helps in Understanding 
&#8226; Redundancy Helps in Understanding 
Generation of Referring Expression 15/34 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Basics
&#8226;	A noun phrase is considered to be a referring
expression iff its only communicative purpose is to
identify an object too the hearer
Context is the set of entities that the hearer is &#8226; 
currently assumed to be attending to 
&#8226;	Referring expression satis&#64257;es the referential
communicative goal if it is a distinguishing
description in the given context
Example: small black dog, large white dog, small black
cat
Generation of Referring Expression	 12/34 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Today&#8217;s Topics 
&#8226;	Overview of Natural Language Generation
&#8226;	Psychological Evidence regarding Generation of 
Referring Expressions 
&#8226;	Selection of Referring Expressions 
&#8211;	Symbolic Approaches (Dale&amp;Reiter) 
&#8211;	Corpus-Based Approaches (Radev,
Nenkova&amp;McKeown)
Generation of Referring Expression	 1/34 </text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Fragment of HMM 
Modi&#64257;cation No Modi&#64257;cation 
Initial 0.76 0.24 
Modi&#64257;cation 0.44 0.56 
No modi&#64257;cation 0.24 0.75 
Generation of Referring Expression 33/34 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Output
Orlando, FL &#8212; Shaquille O&#8217;Neal scored 37 points Friday 
night powering the Orlando Magic to a 101 89 victory 
over the Toronto Raptors, losers of seven in a row. 
Generation of Referring Expression 5/34 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Discourse in Psycholinguistics (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec21/</lecture_pdf_url>
      <lectureno>21</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text></text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Learning Dialogue Strategies via Reinforcement Learning (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec16/</lecture_pdf_url>
      <lectureno>16</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text></text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Topic Segmentation: Agreement, Evaluation, Automatic Text Segmentation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec02/</lecture_pdf_url>
      <lectureno>2</lectureno>
      <slides>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Segmentation: Agreement
Percent agreement &#8212; ratio between observed 
agreements and possible agreements 
A BC
&#8722; &#8722; &#8722;
&#8722; &#8722; &#8722;
+&#8722;&#8722; 
&#8722;++ 
&#8722;&#8722;&#8722; 
+++ 
&#8722; &#8722; &#8722; 
&#8722;&#8722;&#8722; 
22
=&#160; 91%8&#8727; 3&#160;
Topic Segmentation 4/34 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Flow model of discourse 
Chafe&#8217;76: 
&#8220;Our data ... suggest that as a speaker moves from 
focus to focus (or from thought to thought) there 
are certain points at which they may be a more or 
less radical change in space, time, character con&#173;&#64257;guration, event structure, or even world ... At 
points where all these change in a maximal way, 
an episode boundary is strongly present.&#8221; 
Topic Segmentation 11/34 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation Measures 
Boundary Non-boundary 
Alg. Boundary a b 
Alg. Non-boundary c d 
aRecall a+c&#160;
aPrecision
a+b&#160;
b+cError a+b +c+d&#160;
Topic Segmentation 7/34 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
What is Segmentation?
Segmentation: determining the positions at which topics 
change in a stream of text or speech. 
SEGMENT 1: OKAY
tsk There&#8217;s a farmer,
he looks like ay uh Chicano American,
he is picking pears.
A-nd u-m he&#8217;s just picking them,
he comes off the ladder,
a-nd he- u-h puts his pears into the basket.
SEGMENT 2: U-h a number of people are going by,
and one of them is um I don&#8217;t know,
I can&#8217;t remember the &#64257;rst ...t h e &#64257; r s t p e r s o n t h a t g o e s b y
Topic Segmentation 1/34 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example 
Stargazers Text(from Hearst, 1994) 
&#8226; Intro - the search for life in space 
&#8226; The moon&#8217;s chemical composition 
&#8226; How early proximity of the moon shaped it 
&#8226; How the moon helped the life evolve on earth 
&#8226; Improbability of the earth-moon system 
Topic Segmentation 12/34 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Text Segmentation 
Hearst&#8217;94 
&#8226;	Goal: divide text into coherent segments 
&#8226;	Main Idea: change in lexical connectivity patterns 
signals topic change 
&#8211;	Linguistic Theory: Text Cohesion 
Topic Segmentation	 9/34 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Skorochodko&#8217;s Text Types
Chaine d 
Ringed 
Monolit h 
Piecewi se 
Topic Segmentation 10/34 </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
0 Speaker Change 
10
 20
 30
Topic Segmentation 31/34 </text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Determination of Window Size 
Feature Tag Size(sec) Side 
Cue phrases CUE 5 both 
Silence (gaps) SIL 30 left 
Overlap OVR 30 right 
Speaker activity ACT 5 both 
Lexical cohesion LC 30 both 
Topic Segmentation 32/34 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Motivation 
Information Retrieval &#8226; 
Summarization&#8226; 
&#8226;	Question-Answering 
&#8226;	Word-sense disambiguation and anaphora 
resolution 
Topic Segmentation	 2/34 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results on Agreement 
Grosz&amp;Hirschbergberg&#8217;92 newspaper text 74-95% 
Hearst&#8217;93 expository text 80% 
Passanneau&amp;Litman&#8217;93 monologues 82-92% 
Topic Segmentation 5/34 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Similarity Computation: Representation 
Vector-Space Representation 
SENTENCE 1: I like apples 
SENTENCE 2: Apples are good for you 
Vocabulary Apples Are For Good I Like you
Sentence 1&#160; 1 0 0 0 1 1 0
Sentence 2&#160; 1 1 1 1 0 0 1
Topic Segmentation 16/34 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Algorithm for Feature Segmentation
Supervised ML 
(Galley&amp;McKeown&amp;Fosler-Lussier&amp;Jing&#8217;03) 
&#8226; Combines multiple knowledge source:
&#8211; cue phrases 
&#8211; silences
&#8211; overlaps
&#8211; speaker change 
&#8211; lexical cohesion 
&#8226; Uses probabilistic classi&#64257;er (decision tree) to 
combine them 
Topic Segmentation 26/34 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Simple Algorithm 
Passanneau&amp;Litman&#8217;93 
Recall Precision Error 
Cue 72% 15% 50% 
Pause 92% 18% 49% 
Humans 74% 55% 11% 
Topic Segmentation 8/34 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Today&#8217;s Topics
&#8226;	Human Agreement on Segmentation and Evaluation
&#8226;	Segmentation Algorithms: 
&#8211;	Features: word distribution, cue words, speaker, 
change,...&#160;
&#8211;	Methods: classi&#64257;cation, clustering, HMMs, ...
&#8226;	Segmentation for different genres: text, meetings, broadcasts, 
Topic Segmentation	 3/34 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Segmentation Evaluation
Comparison with human-annotated 
segments(Hearst&#8217;94): 
&#8226;	13 articles (1800 and 2500 words)
&#8226;	7 judges 
&#8226;	boundary if three judges agree on the same segmentation point 
Topic Segmentation	 21/34 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Cochran&#8217;s Test
Estimate the null hypothesis that the number of subjects 
assigning a boundary at any position is randomly 
distributed 
Topic Segmentation 6/34 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Preprocessing and Initial Segmentation 
Tokenization &#8226; 
&#8226; Morphological analysis 
&#8226; Token-sequence division 
Topic Segmentation 15/34 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Silences
&#8226;	Pauses &#8212; speaker silence in the middle of her 
speech 
&#8226; Gap &#8212; silences not attributable to any party
Topic boundaries are typically preceeded by gaps
Topic Segmentation	 29/34 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
More Results 
&#8226;	High sensitivity to change in parameter values
&#8226;	Thesaural information does not help 
Most of the mistakes are &#8220;close misses&#8221; &#8226; 
Topic Segmentation	 24/34 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Meeting Segmentation 
Motivation: Facilitate information Access &#8226; 
&#8226; Challenges: 
&#8211; High error rate in transcription 
&#8211; Multi-thread structure 
Topic Segmentation 25/34 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
&#65533;&#160;
&#65533;
Similarity Computation: Cosine Measure
Cosine of angle between two vectors in n-dimensional
space
tsim (b1,b2)=&#65533;&#160;wy,b1wt,b2&#160;
n&#160; 2 2&#160;&#65533;wt&#160; t,b1&#160; t=1&#160;wt,b2&#160;
SENTENCE 1: 1000110
SENTENCE 2: 1111001
sim(S 1,S2)=
1&#8727;0+0&#8727;1+0&#8727;1+0&#8727;1+1&#8727;0+1&#8727;0+0&#8727;1
 &#8730;
(12+02+02+02+12+12+02)&#8727;(12+12+12+12+02+02+12&#160;=0.26
Topic Segmentation 17/34 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Boundary Detection
Based on changes in sequence of similarity scores: 
Depth Scores: relative depth (in comparison to the 
closest maximum) 
Number of segments: s&#160;&#8722; &#963;/2&#160;
Topic Segmentation 20/34 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Cue Word Selection 
Automatic computation of cue words: 
&#8226;	Compute word probability to appear in boundary 
position 
&#8226;	Select words with the highest probability
Remove non-cues.
&#8226; 
Topic Segmentation	 27/34 </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Overlaps
&#8226; Average overlap rate within some window 
Little overlap in the beginning of segments 
Topic Segmentation 30/34 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Agreement on Segmentation 
Topic Segmentation 22/34 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation Results 
Methods Precision Recall 
Baseline 33% 0.44 0.37 
Baseline 41% 0.43 0.42 
Chains 0.64 0.58 
Blocks 0.66 0.61 
Judges 0.81 0.71 
Topic Segmentation 23/34 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Selected Cue Words 
OKAY 93.05 
shall 0.44 
anyway 0.43 
alright 0.64 
let&#8217;s 0.66 
good 0.81 
Topic Segmentation 28/34 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Topic Segmentation 
Regina Barzilay 
February 8, 2004 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
14 form 1 111 1 1 1 1 1 1 1 1 1 1 |
 8 scientist 11 1 1 1 1 1 1 |
 5 space 11 1 1 1 |
25 star 1 1 11 22 111112 1 1 1 11 1111 1 |
 5 binary 11 1 1 1|
 4 trinary 1 1 1 1|
 8 astronomer 1 1 1 1 1 1 1 1 |
 7 orbit 1 1 12 1 1 |
 6 pull 2 1 1 1 1 |
16 planet 1 1 11 1 1 21 11111 1 1|
 7 galaxy 1 1 1 11 1 1|
 4 lunar 1 1 1 1 |
19 life 1  1  1 1 11 1 11 1 1 1 1 1 111 1 1 |
27 moon 13 1111 1 1 22 21 21 21 11 1 |
 3 move 1 1 1 |
 7 continent 2 1 1 2 1 |
 3 shoreline 12 |
 6 time 1 1  1  1 1 1 |
 3 water 11 1 |
 6 say 1 1 1 11 1 |
 3 species 1  1  1 |
-------------------------------------------------------------------------------------------------------------+
Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95|
-------------------------------------------------------------------------------------------------------------+
Topic Segmentation 13/34 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Segmentation Algorithm 
&#8226; Preprocessing and Initial segmentation 
&#8226; Similarity Computation 
&#8226; Boundary Detection 
Topic Segmentation 14/34 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Similarity Computation: Output 
0.22 
0.33 
Topic Segmentation 18/34 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Gap Plot 
1
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
20 40 60 80 100 120 140 160 180 200 220 240 26 0
Topic Segmentation 19/34 </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results 
Method Pk&#160; WD 
Feature-based 23.00 25.47 
Cohesion-based 31.91 35.88 
Topic Segmentation 34/34 </text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;&#160;Examples of Derived Rules 
Condition Decision Conf. 
LC&#8804;0.67, CUE&#8805;1, 
OVR&#8804;1.20, SIL&#8804;3.42 yes 94.1 
LC&#8804;0.35, SIL &gt;3.42, 
OVR&#8804;4.55 yes 92.2 
CUE&#8805;1, ACT&gt;0.1768, 
OVR&#8804;1.20, LC&#8804;0.67 yes 91.6 
... 
default no 
Topic Segmentation 33/34 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Domain-dependent Models of Text Structure (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec08/</lecture_pdf_url>
      <lectureno>8</lectureno>
      <slides>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Features 
Position &#8226; 
Verb Tense and Voice &#8226; 
&#8226; History 
&#8226; Lexical Features (&#8220;other researchers claim that&#8221;) 
Domain-dependent Text Structures 15/44 </text>
        </slide>
        <slide>
          <slideno>34</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Application: Information Ordering 
&#8226;	Input: set of sentences 
&#8226;	Applications: 
&#8211;	Text summarization 
&#8211;	Natural Language Generation 
&#8226;	Goal: Recover most likely sequences 
&#8220;get marry&#8221; prior to &#8220;give birth&#8221; (in some domains) 
Domain-dependent Text Structures	 34/44 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Pattern Detection
Analogous to motif detection 
T1:A B C D FAABFD 
T2: F CABDD FF 
&#8226; Scanning 
&#8226; Generalizing 
&#8226; Filtering 
Domain-dependent Text Structures 20/44 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Approach
&#8226;	Goal: Rhetorical segmentation with labeling 
Annotation Scheme: &#8226; 
&#8211;	Own work: aim, own, textual 
&#8211;	Background 
&#8211;	Other Work: contrast, basis, other 
&#8226;	Implementation: Classi&#64257;cation 
Domain-dependent Text Structures	 12/44 </text>
        </slide>
        <slide>
          <slideno>39</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Baselines for Ordering 
&#8226;	&#8220;Straw&#8221; baseline: Bigram Language model
&#8226;	&#8220;State-of-the-art&#8221; baseline: (Lapata:2003)
&#8211;	represent a sentence using lexico-syntactic 
features 
&#8211;	compute pairwise ordering preferences 
&#8211;	&#64257;nd optimally global order 
Domain-dependent Text Structures	 39/44 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Domain-dependent Text Structures 
Regina Barzilay 
March 1, 2003 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Argumentative Zoning
BACKGROUND 
Many of the recent advances in Question Answering have followed 
from the insight that systems can bene&#64257;t from by exploiting the 
redundancy ...&#160;
OTHER WORK 
Brill et al. (2001) describe using the vast amount of data available on 
the WWW to achieve impressive performance ...&#160;
WEAKNESS 
The Web, while nearly in&#64257;nite in content, is not a complete repository 
of useful information ...&#160;
OWN CONTRIBUTION 
In order to combat these inadequacies, we propose a strategy in which 
in information is extracted from ...&#160;
Domain-dependent Text Structures 10/44 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation
Pattern con&#64257;dence: 84.62%
Constraint accuracy: 89.45%
Domain-dependent Text Structures 22/44 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Similarity in Domain Texts
TOKYO (AP) A moderately strong earthquake with a preliminary magni&#173;
tude reading of 5.1 rattled northern Japan early Wednesday , the Central Meteorological Agency said. There were no immediate reports of casual&#173;
ties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers 
(36 miles) beneath the Paci&#64257;c Ocean near the northern tip of the main island of Honshu
. . . . 
ATHENS, Greece (Ap) A strong earthquake shook the Aegean Sea island of Crete on Sunday but caused no injuries or damage. The quake had 
a preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT) 
on the sea &#64258;oor 70 kilometers (44 miles) south of the Cretan port of 
Chania. The Athens seismological institute said the temblor&#8217;s epicenter 
was located 380 k ilometers (238 miles) south of the capital
. . . . 
Domain-dependent Text Structures 25/44 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Content-based Structure
&#8226;	Describe the strength and the impact of an 
earthquake 
&#8226;	Specify its magnitude 
&#8226;	Specify its location 
...&#8226; 
Domain-dependent Text Structures	 3/44 </text>
        </slide>
        <slide>
          <slideno>33</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Viterbi re-estimation 
Goal: incorporate ordering information 
&#8226; Decode the training data with Viterbi decoding 
&#8226; Use the new clustering as the input to the parameter 
estimation procedure 
Domain-dependent Text Structures 33/44 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Analogy with Syntax 
Domain-independent Theory of Sentence Structure
&#8226; Fixed set of word categories (nouns, verbs, ...) 
&#8226; Fixed set of relations (subject, object, ...) 
P(&#8220; A is sentence this weird&#8221;) 
Domain-dependent Text Structures 5/44 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Supervised Content Modeling 
(Duboue&amp; McKeown, 2001) 
&#8226;	Goal: Find types of semantic information 
characteristic to a domain and ordering constraints 
on their presentation 
&#8226;	Approach: &#64257;nd patterns in a set of transcripts
manually annotated with semantic units
Domain: Patients records &#8226; 
Domain-dependent Text Structures	 17/44 </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
From Clusters to States 
&#8226; Each large cluster constitutes a state 
&#8226; Agglomerate small clusters into an &#8220;insert&#8221; state 
Domain-dependent Text Structures 30/44 </text>
        </slide>
        <slide>
          <slideno>38</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Evaluation: Data 
Domain Average 
Length Vocabulary 
Size Token/ 
type 
Earthquake 10.4 1182 13.158 
Clashes 14 1302 4.464 
Drugs 10.3 1566 4.098 
Finance 13.7 1378 12.821 
Accidents 11.5 2003 5.556 
Domain-dependent Text Structures 38/44 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
What is Text? 
A product of cohesive ties (cohesion)
ATHENS, Greece (Ap) A strong earthquake shook the 
Aegean Sea island of Crete on Sunday but caused no in&#173;
juries or damage. The quake had a preliminary magni&#173;
tude of 5.2 and occurred at 5:28 am (0328 GMT) on the 
sea &#64258;oor 70 kilometers (44 miles) south of the Cretan 
port of Chania. The Athens seismological institute said the temblor&#8217;s epicenter was located 380 kilometers (238 
miles) south of the capital. No injuries or damage were 
reported. 
Domain-dependent Text Structures 1/44 </text>
        </slide>
        <slide>
          <slideno>31</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Estimating Emission Probabilities
State si&#160;emission probability: 
n&#160;psi&#160;(w0,&#160; ...,&#160; w n)=&#160;/producttext
j=0&#160;psi&#160;(w jwj&#8722;1) |
Estimation for a &#8220;normal&#8221; state: &#8226; 
def&#160;fci(ww/prime)+&#160;&#948;1&#160;psi(w/prime|w) =&#160;fci&#160;(w)+&#160;&#948;1|V,&#160;| 
Estimation for the &#8220;insertion&#8221; state: &#8226; 
def&#160; 1&#8722; max i&lt;m&#160;psi&#160;(w/primew)&#160;. psm&#160;(w/prime|w) =&#160;/summationtext&#160;
u&#8712;V&#160;(1&#160;&#8722; max i&lt;m&#160;psi&#160;|
(u w ))|
Domain-dependent Text Structures 31/44 </text>
        </slide>
        <slide>
          <slideno>37</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Summarization: Algorithm
Input: source text
Training data: parallel corpus of summaries and source
texts (aligned)
&#8226;	Employ Viterbi on source texts and summaries
&#8226;	Compute state likelihood to generate summary
sentences:
p(s&#160;&#8712; summar&#160;y s&#160;&#8712; source )=summar&#160;y&#160; count(s)
|	source&#160; count(s)&#160;,&#160;
&#8226;	Given a new text, decode it and extract sentences
corresponding to &#8220;summary&#8221; states
Domain-dependent Text Structures	 37/44 </text>
        </slide>
        <slide>
          <slideno>44</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Summarization: Learning Curve 
90 Summarization Accuracy 85 
80 
75 
70 
65 
60 hmm-based 
word+loc 
lead 
10 15 20 25 30 
Summary/source training set size 
Domain-dependent Text Structures 44/44 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Two Approaches to Text Structure 
&#8226; Domain-dependent models (Today) 
&#8211; Content-based models 
&#8211; Rhetorical models 
&#8226; Domain-independent models 
&#8211; Rhetorical Structure Theory (Next Class) 
Domain-dependent Text Structures 6/44 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Semantic Sequence
age, gender, pmh, pmh, pmh, pmh, med-preop,
med-preop, med-preop, drip-preop, med-preop,
ekg-preop, echo-preop, hct-preop, procedure, . . .
Domain-dependent Text Structures 19/44 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Examples 
Category Realization 
Aim We have proposed a method of clustering words 
based on large corpus data 
Textual Section 2 describes three parsers which are ...&#160;
Contrast However, no method for extracting the relation&#173;
ship from super&#64257;cial linguistic expressions was 
described in their paper. 
Domain-dependent Text Structures 13/44 </text>
        </slide>
        <slide>
          <slideno>36</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Application: Summarization
&#8226;	Domain-dependent summarization:
(Radev&amp;McKeown:1998)
&#8211;	specify types of important information
(manually)
&#8211;	use information extraction to identify this
information (automatically)
&#8226;	Domain-independent summarization: (Kupiec et
al:1995)
&#8211;	represent a sentence using shallow features 
&#8211;	use a classi&#64257;er 
Domain-dependent Text Structures	 36/44 </text>
        </slide>
        <slide>
          <slideno>40</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;&#160;Results: Ordering 
Domain Algorithm Prediction 
Accuracy Rank &#964;&#160;
Content 72% 2.67 0.81 
Earthquake Lapata &#8217;03 24% (N/A) 0.48 
Bigram 4% 485.16 0.27 
Content 48% 3.05 0.64 
Clashes Lapata &#8217;03 27% (N/A) 0.41 
Bigram 12% 635.15 0.25 
Content 38% 15.38 0.45 
Drugs Lapata &#8217;03 27% (N/A) 0.49 
Bigram 11% 712.03 0.24 
Content 96% 0.05 0.98 
Finance Lapata &#8217;03 17% (N/A) 0.44 
Bigram 66% 7.44 0.74 
Content 41% 10.96 0.44 
Accidents Lapata &#8217;03 10% (N/A) 0.07 
Bigram 2% 973.75 0.19 
Domain-dependent Text Structures 40/44 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Computing Content Model 
Implementation: Hidden Markov Model 
&#8226; States represent topics 
&#8226; State-transitions represent ordering constraints 
Rescue Strength Location History 
efforts Casualties 
Domain-dependent Text Structures 27/44 </text>
        </slide>
        <slide>
          <slideno>43</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results: Summarization 
Summarizer Extraction accuracy 
Content-based 88% 
Sentence classi&#64257;er 76% 
(words + location) 
Leading n&#160;sentences 69% 
Domain-dependent Text Structures 43/44 </text>
        </slide>
        <slide>
          <slideno>32</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Estimating Transition Probabilities
3/6 
1/5 3/4 
ci,cj)+&#948; 2&#160;p(s j|si)=g(
g(ci)+&#948; 2m&#160;
g(ci,cj)is a number of adjacent sentences ( ci, cj) 
g(ci)is a number of sentences in ci&#160;
Domain-dependent Text Structures 32/44 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Today: Domain-Speci&#64257;c Models 
Rhetorical Models: &#8226; 
&#8211;	Argumentative Zoning of Scienti&#64257;c Articles 
(Teufel, 1999) 
Content-based Models: &#8226; 
&#8211;	Supervised (Duboue&amp;McKeown, 2001) 
&#8211;	Unsupervised (Barzilay&amp;Lee, 2004) 
Domain-dependent Text Structures	 8/44 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Initial Topic Induction
Agglomerative clustering with cosine similarity measure
(Iyer&amp;Ostendorf:1996,Florian&amp;Yarowsky:1999, Barzilay&amp;Elhadad:2003) 
The Athens seismological institute said the temblor&#8217;s epicenter was lo&#173;
cated 380 kilometers (238 miles) south of the capital. 
Seismologists in Pakistan&#8217;s Northwest Frontier Province said the temblor&#8217;s 
epicenter was about 250 kilometers (155 miles) north of the provincial capital Peshawar. 
The temblor was centered 60 kilometers (35 miles) northwest of the provincial capital of Kunming, about 2,200 kilometers (1,300 miles) southwest of Beijing, a bureau seismologist said. 
Domain-dependent Text Structures 29/44 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Rhetorical Structure 
elaboration 
contrast 
S S2 S 
1 3
Domain-dependent Text Structures 4/44 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Argumentative Zoning
Many of the recent advances in Question Answering have
followed from the insight that systems can bene&#64257;t from
by exploiting the redundancy in large corpora.
Brill et al. (2001) describe using the vast amount of
data available on the WWW to achieve impressive per&#173; 
formance ...
The Web, while nearly in&#64257;nite in content, is not a com&#173; 
plete repository of useful information ...
In order to combat these inadequacies, we propose a
strategy in which in information is extracted from ...
Domain-dependent Text Structures 9/44 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Motivation 
Summarization&#8226; 
Extract a representative subsequence from a set of
sentences
&#8226;	Question-Answering Find an answer to a question in natural language 
&#8226;	Text Ordering 
Order a set of information-bearing items into a coherent 
text 
Machine Translation &#8226; 
Find the best translation taking context into account 
Domain-dependent Text Structures	 7/44 </text>
        </slide>
        <slide>
          <slideno>42</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Ordering: Learning Curve 
100
Ordering Accuracy 90
80
70
60
50
40
30
20
10
0
earthquake 
finance 
clashes 
accidents 
drugs 
0 20 40 60 80 100
Training set size
Domain-dependent Text Structures 42/44 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Kappa Statistics
(Siegal&amp;Castellan, 1998; Carletta, 1999)
Kappa controls agreement P(A)for chance agreement
P(E)
P(A)&#8722; p(E )K&#160;=&#160;1&#8722; p(E )&#160;
Kappa from Argumentative Zoning: 
&#8226; Stability: 0.83 
&#8226; Reproducibility: 0.79 
Domain-dependent Text Structures 14/44 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Similarity in Domain Texts
TOKYO (AP) A moderately strong earthquake with a preliminary magni&#173;
tude reading of 5.1 rattled northern Japan early Wednesday , the Central Meteorological Agency said. There were no immediate reports of casual&#173;
ties or damage. The quake struck at 6:06 am (2106 GMT) 60 kilometers 
(36 miles) beneath the Paci&#64257;c Ocean near the northern tip of the main island of Honshu
. . . . 
ATHENS, Greece (AP) A strong earthquake shook the Aegean Sea island of Crete on Sunday but caused no injuries or damage. The quake had 
a preliminary magnitude of 5.2 and occurred at 5:28 am (0328 GMT) 
on the sea &#64258;oor 70 kilometers (44 miles) south of the Cretan port of 
Chania. The Athens seismological institute said the temblor&#8217;s epicenter 
was located 380 k ilometers (238 miles) south of the capital. No injuries or damage were reported. 
Domain-dependent Text Structures 24/44 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;
&#65533;&#65533;
&#65533;Example of Learned Pattern
intraop-problems
intraop-problems&#9127;
&#9130;&#9130;&#9130;&#9128;
&#9130;&#9130;&#9130;&#9129;operation 11.11%
drip 33.33%
intraop-problems 33.33%
total-meds-anesthetics 22.22%&#9131;
&#9130;&#9130;&#9130;&#9132;
&#9130;&#9130;&#9130;&#9133;
drip
Domain-dependent Text Structures 21/44</text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Narrative Grammars
&#8226;	Propp (1928): fairy tales follow a &#8220;story grammar&#8221;
&#8226;	Barlett (1932): formulaic text structure facilities 
reader&#8217;s comprehension 
&#8226;	Wray (2002): texts in multiple domains exhibit 
signi&#64257;cant structural similarity 
Domain-dependent Text Structures	 26/44 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Model Construction 
&#8226;	Initial topic induction 
&#8226;	Determining states, emission and transition 
probabilities 
Viterbi re-estimation &#8226; 
Domain-dependent Text Structures	 28/44 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Motivation
&#8226;	Scienti&#64257;c articles exhibit (consistent across
domains) similarity in structure
&#8211;	BACKGROUND 
&#8211;	OWN CONTRIBUTION 
&#8211;	RELATION TO OTHER WORK 
&#8226;	Automatic structure analysis can bene&#64257;t: 
&#8211;	Q&amp;A 
&#8211;	summarization 
&#8211;	citation analysis 
Domain-dependent Text Structures	 11/44 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Annotated Transcript 
He is 58-year-old male . History is signi&#64257;cant for Hodgkin&#8217;s disease , 
age gender pmh 
t r e a t e d w i t h ...t o h i s n e c k , b a c k a n d c h e s t . Hyperspadias , BPH , 
pmh pmh 
hiatal hernia and proliferative lymph edema in his right arm .N o I V &#8217; s 
pmh pmh 
or blood pressure down in the left arm. Medications &#8212; Inderal , Lopid , 
med-preop med-preop 
Pepcid , nitroglycerine and heparin . EKG has PAC&#8217;s . ... 
med-preop drip-preop med-preop ekg-preop 
Domain-dependent Text Structures 18/44 </text>
        </slide>
        <slide>
          <slideno>35</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Information Ordering: Algorithm 
Input: set of sentences 
&#8226;	Produce all permutations of the set 
Rank them based on the content model &#8226; 
Domain-dependent Text Structures	 35/44 </text>
        </slide>
        <slide>
          <slideno>41</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Baselines for Summarization 
&#8226;	&#8220;Straw&#8221; baseline: n&#160;leading sentences 
&#8226;	&#8220;State-of-the-art&#8221; Kupiec-style classi&#64257;er: 
&#8211;	Sentence representation: lexical features and 
location 
&#8211;	Classi&#64257;er: BoosTexter 
Domain-dependent Text Structures	 41/44 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Results 
&#8226; Classi&#64257;cation accuracy is above 70% 
&#8226; Zoning improves classi&#64257;cation 
Domain-dependent Text Structures 16/44 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Content Models 
(Barzilay&amp;Lee, 2004) 
&#8226;	Content models represent topics and their ordering 
in text. 
Domain: newspaper articles on earthquake 
Topics: &#8220;strength&#8221;, &#8220;location&#8221;, &#8220;casualties&#8221;, . . . 
Order: &#8220;casualties&#8221; prior to &#8220;rescue efforts&#8221; 
&#8226; Assumption: Patterns in content organization are 
recurrent 
Domain-dependent Text Structures	 23/44 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
What is Text? 
A product of structural relations (coherence)
S1: A strong earthquake shook the Aegean Sea island of 
Crete on Sunday 
S2: but caused no injuries or damage. 
S3: The quake had a preliminary magnitude of 5.2 
Domain-dependent Text Structures 2/44 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Automatic Interpretation of Dialogue Acts (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec15/</lecture_pdf_url>
      <lectureno>15</lectureno>
      <slides>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Searle&#8217;s Inference Chain
1. X asked me a question about whether I have the
ability to give a list of &#64258;ights
2. I assume that X is being cooperative in the
conversation and that his utterance therefore has
some plan
3. X knows that I have the ability to give such a list ,
and therefore is no alternative reason why X should
have purely theoretical interest in my list-giving
ability
4. Therefore X&#8217;s utterance has some illocutionary
point. What can it be?
Automatic Interpretation of Dialogue Acts 10/26 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Searle&#8217;s Inference Chain
5. A preparatory condition for a directive is that the 
hearer have the ability to perform the directed 
action 
6. Therefore X has asked me a question about my 
preparedness for the action of giving X a list of &#64258;ights 
7. Furthermore, X and I are in a conversational 
situation in which giving &#64258;ights is expected and 
common activity 
Automatic Interpretation of Dialogue Acts 11/26 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;&#160;INFORM De&#64257;nition
Grice: A speaker informs the hearer of something
merely by causing the hearer to believe that the speaker
wants them to know something
INFORM (S,H,P) 
Constraints: Speaker (S) &#8743; Hearer (H) &#8743; Preposi&#173;
tion (P)
Precondition: Know (S,P) &#8743; W(S, INFORM (S, H,
P))
Effect: Know (H, P)
Body: B (H, W (S, Know (H, P)))
Automatic Interpretation of Dialogue Acts 17/26 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Automatic Interpretation of
Dialogue Acts
Regina Barzilay 
April 4, 2004 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Two Solutions
&#8226;	Shallow Processing: act classi&#64257;cation based on 
lexical, prosodic and structural cues 
&#8226;	Deep Understanding: inference-based interpretation 
Automatic Interpretation of Dialogue Acts	 7/26 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
DAMSL 
&#8226; STATEMENT: a claim made by a speaker 
&#8226; INFO-REQUEST: a question by the speaker 
&#8226; CHECK: question for con&#64257;rming information 
&#8226; ACCEPT: a claim made by a speaker 
&#8226; ANSWER: answering a question 
&#8226; UNDERSTANDING: whether speaker understands 
Automatic Interpretation of Dialogue Acts 2/26 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Plan Inference Rules (PI)
&#8226;	(PI. AE) Action-Effect Rule: For all agents S and H,
if Y is an effect of action X and if H believes that S
wants X to be done, then it is plausible that H
believes that S wants Y to obtain
&#8226;	(PI. PA) Precondition-Action Rule: For all agents S
and H, if X is a precondition of actions Y and if H
believes S wants X to obtain, then it is plausible that
H believes that S wants Y to be done
&#8226;	(PI. BA) Body-Action Rule: For all agents S and H,
if X is part of the body of Y and if H believes that S
wants X done, then it is plausible that H believes
that S wants Y done
Automatic Interpretation of Dialogue Acts	 20/26 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Pros/Cons of Inference Approaches 
Automatic Interpretation of Dialogue Acts 23/26 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today 
Automatic Interpretation of Dialogue Acts 
&#8226; Plan-Inferential Interpretation 
&#8226; Cue-based Interpretation 
Automatic Interpretation of Dialogue Acts 3/26 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Plan-Inferential Interpretation
(Gordon&amp;Lakoff, 1971; Searle, 1975): A speaker could
mention or question various properties of the desired 
activity to make an indirect request
&#8226;	The speaker can mention the hearer&#8217;s ability to
perform the action
Can you give me the list of the &#64258;ights from Atlanta to Boston? 
Would you be able to put me on the &#64258;ight with Delta? 
&#8226;	The speaker can mention speaker&#8217;s wish or desire
about the activity
I want to &#64258;y from Boston to San Francisco
I would like to stop somewhere else in between
Automatic Interpretation of Dialogue Acts	 8/26 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
YES-NO-QUESTION: Will breakfast be served on 
USAir 1557? 
STATEMENT: I don&#8217;t care about lunch 
COMMAND: Show me &#64258;ights from Milwaukee to 
Orlando on Thursday night 
Automatic Interpretation of Dialogue Acts 4/26 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Plan Inference Rules (PI)
&#8226;	(PI. KP) Know-Desire Rule: For all agents S and H,
if H believes S wants to KNOWIF(P), then H
believes S wants P to be true:
plausible&#160;B(H, W (S, KNOW IF (S, P )))&#160; =&#160;&#8658;
B(H, W (S, P ))
&#8226;	(EI. 1) Extended Inference Rule: B(H, W (S))&#160;can
be added to any plan inference rules: 
plausible&#160;B(H, W (S, Y ))&#160;is a PI If	B (H, W (S, X ))&#160; =&#160;&#8658;
rule, then 
plausible&#160;B(H, W (S, B (H, (W (S, X )))))&#160; =&#160; &#8658;
B(H, W (S, B (H, (W (S, Y )))))
Automatic Interpretation of Dialogue Acts	 21/26 </text>
        </slide>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Cue-based Interpretation 
Microgrammar of dialogue (Goodwin, 1996) 
1. Words and Collocations (please for REQUEST)
2. Prosody (rising pitch for YES-NO-QUESTIONS
3. Conversational Structure (yeah after PROPOSAL 
likely to be an agreement; after AGREEMENT likely 
to be a backchannel) 
Automatic Interpretation of Dialogue Acts 24/26 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
8. Therefore, in the absence of any other plausible act,
X is probably requesting me to give him a list of 
&#64258;ights 
Automatic Interpretation of Dialogue Acts 12/26 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
BDI Models
Belief, Desire and Intention Models (Cohen &amp; Perrault, 
1979; Perrault &amp; Allen, 1980; Allen, 1995) 
&#8226;	Used for generation and understanding 
Based on formalization of Searle&#8217;s Inference Chains &#8226; 
via planning techniques 
Automatic Interpretation of Dialogue Acts	 13/26 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Surface Acts
&#8226;	&#8220;Surface-level acts&#8221; correspond to the literal
meaning of the imperative, interrogative and
declarative structures:
S.REQUEST (S,H,ACT)
&#8211;	B (H, W(S, ACT(H)))
&#8226;	They trigger the start of the hearer&#8217;s inference chain
(by matching the body of a request)
Speaker: S. REQUEST (S, H, InformIf (H, S,
CanDo(H,Give (H, S, LIST)))))
Hearer: REQUEST (H, S, GIVE,(H, S, LIST))
Automatic Interpretation of Dialogue Acts	 19/26 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
REQUEST De&#64257;nition 
REQUEST (S,H,ACT) 
Constraints: Speaker (S) &#8743; Hearer (H) &#8743; ACT (A)
Preconditions: W(S, ACT(H))
Effect: W (H, ACT (H))
Body: B (H, W (S, ACT (H)))
Automatic Interpretation of Dialogue Acts 18/26 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Example of Inference 
Can you give me a list of &#64258;ights from Atlanta?
1. S.REQUESTS
(S,H,InformIf(H,S,CanDo(H,Give(H,S,LIST))))
2.	PI.AE: 
B(H,W(S,InformIf(H,S,CanDo(H,Give(H,S,LIST))))) 
3.	PI.AE/EI: B(H,W(S,KnowIF(H,S,CanDo(H,Give(H,S,LIST))))) 
4.	PI.KP/EI: B(H,W(S,CanDo(H,Give(H,S,LIST)))) 
5.	PI.KP/EI: B(H,W(S, Give(H,S,LIST))) 
6.	PI.BA: REQUEST(H,S,GIVE(H,S,LIST)) 
Automatic Interpretation of Dialogue Acts	 22/26 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
BDI Axiomatization 
Action Schemas (similar to STRIPS) 
&#8226;	Constraints: Variable types 
&#8226;	Preconditions: Conditions that must already be 
true to successfully perform the action 
&#8226;	Effects: A set of partially ordered goal states that 
must be achieved in performing the actions 
&#8226;	Body: A set of partially ordered goal states that 
must be achieved in performing the action 
Automatic Interpretation of Dialogue Acts	 15/26 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example of Action Schema 
BOOK-FLIGHT (A,C,F) 
Constraints: Agent(A) &#8743; Flight (F) &#8743; Client (C)
Precondition: Know (A,departure-date(F)) &#8743; Know
(A,departure-time(F)) &#8743; Know (A, origin-city(F)) &#8743;
Know (A, destination-city(F)) &#8743; Has-Seats (F) &#8743; ...
Effect: Flight-Booked (A, C, F)
Body: Make-Reservation (A, F , C)
Automatic Interpretation of Dialogue Acts 16/26 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Last Time
DAMSL (Dialogue Act Markup in Several Layers) 
(Allen&amp;Core&#8217;1997; Walker et al&#8217;1996; Carletta&#8217;1997) 
&#8226;	Driven by the needs of dialog system developers 
&#8226;	Contains several levels, including forward looking
function and backward Looking function
Hierarchical in structure &#8226; 
&#8226;	Domain-independent, but tailored towards task-oriented 
dialogs 
Automatic Interpretation of Dialogue Acts	 1/26 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Plan-Inferential Interpretation
&#8226;	The speaker can mention the hearer&#8217;s doing the 
action 
Would you please repeat this information?
Will you tell me the departure time on American &#64258;ight?
&#8226;	The speaker can question the speaker&#8217;s having 
permission to receive results of the action 
May I get lunch instead of breakfast?
Could I have a listing of &#64258;ights leaving Boston?
Automatic Interpretation of Dialogue Acts	 9/26 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Incorporating Dialogue Structure 
E&#160;- observable evidence consisting of prosodic features
(F) and lexical features (W) 
D&#160;={d1,d2,...,d N} 
P(D)P(E|D)D&#8727; =argmax DP(d|E)=argmax D&#160; =&#160;P(E)&#160;
argmax DP(D)P(E|D)&#160;
P(E|D)=P (F D )P(W D )= | |
argmax DP(D)P(F|D)P(W D )|
Automatic Interpretation of Dialogue Acts 26/26 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Dialogue MicroGrammar
(Sumh&amp;Weibel, 1994; Mast et al, 1996; Stolcke et al, 
2001) 
d&#8727; =argmax dP(d|W)=argmax dP(d)P (W d)|
Example: Ngrams with high predictive power for reformulation: 
so you, you mean, so they, so it&#8217;s 
Automatic Interpretation of Dialogue Acts 25/26 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Interpretation is NOT Easy!
The mapping between surface form and speech act is 
not obvious and not one-to-one 
QUESTION vs REQUEST: Can you give me a list of
&#64258;ights from Atlanta to Boston?
QUESTION vs STATEMENT:...&#160; And you said you
want to travel next week?
Automatic Interpretation of Dialogue Acts 6/26 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
BDI Basics 
Belief:&#8226; 
B(S, P )
B(A, P )&#8743; B(A, Q )&#8594; B(A, P&#160; &#8743; Q)
Know&#8226; 
KNOW (S, P )&#8801; P&#160;&#8743; B(S, P )&#160;
Know Whether &#8226; 
KNOWIF (S, P )&#8801;
KNOW (S, P )&#8744; KNOW (S,&#172;P)
Want &#8226; 
W(S, P ), W (S, S (ACT (H))&#160;
Automatic Interpretation of Dialogue Acts 14/26 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Interpretation is Easy! 
YES-NO-QUESTIONS have aux-inversion
STATEMENTS have declarative syntax (no aux-
inversion)
COMMANDS have imperative syntax (commands
with no syntactic subject)
Automatic Interpretation of Dialogue Acts 5/26 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Automatic Reference Resolution and Reference Generation (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec06/</lecture_pdf_url>
      <lectureno>6</lectureno>
      <slides>
        <slide>
          <slideno>24</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Features 
F-measure: 
&#8226; Fem&amp;Masc Pronoun: 17.4% baseline, 17.25% 
&#8226; Third Person Neuter Pronoun: 14.68%, 19.26% 
&#8226; Third Person Plural: 28.30%, 28.70% 
Reference Resolution 24/30 </text>
        </slide>
        <slide>
          <slideno>30</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Statistical Generation 
&#8226; (Radev,1998): classi&#64257;cation-based 
&#8226; (Nenkova&amp;McKeown,2003): HMM-based 
Reference Resolution 30/30 </text>
        </slide>
        <slide>
          <slideno>29</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Algorithm
&#8226;	Check Success: see if the contracted description 
picks up one entity from the context 
&#8226;	Choose Property: determine which properties of the referent would rule out the largest number of 
entities 
&#8226;	Extend Description: add the chosen properties to 
the description being constructed and remove 
relevant entities from the discourse. 
Reference Resolution	 29/30 </text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Announcements
3/3 &#8212; &#64257;rst part of the projects 
Example topics 
&#8226;	Segmentation 
Identi&#64257;cation of discourse structure &#8226; 
Summarization&#8226; 
&#8226;	Anaphora resolution 
&#8226;	Cue phrase selection 
Reference Resolution	 1/30 </text>
        </slide>
        <slide>
          <slideno>14</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Abstract Referents
(Webber, 1988)
(A0) Each Fall, penguins migrate to Fiji.
(A1) That&#8217;s where they wait out the winter.
(A2) That&#8217;s when it&#8217;s cold even for them.
(A3) That&#8217;s why I&#8217;m going there next month.
(A4) It happens just before the eggs hutch.
Reference Resolution 14/30 </text>
        </slide>
        <slide>
          <slideno>10</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Co-training for Coreference
Coreference does not support natural split of features
Algorithm for feature splitting
&#8226;	Train a classi&#64257;er on each feature separately
&#8226;	Select the best feature and assign it to the &#64257;rst view,
and the second best feature assign to the second
view
&#8226;	Iterate over the remaining feature, and add them to
one of the views
Separate training for each reference type (personal 
pronouns, possessives,... )
Reference Resolution	 10/30 </text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Reference Resolution 
Regina Barzilay 
February 23, 2004 </text>
        </slide>
        <slide>
          <slideno>18</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Types of Speech Acts 
Tell, Request, Wh-Questions, YN-Question, Con&#64257;rm 
(1) The highway is closed (Tell) 
(2) Is the highway closed? (Y/N Question) 
(3) That&#8217;s right. 
(4) Why is the highway closed? (WH-Q) 
(5) *That&#8217;s right. 
Reference Resolution 18/30 </text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Reference Resolution
Captain Farragut was a good seaman, worthy of the 
frigate he commanded. His vessel and he were one. He 
was the soul of it. 
&#8226; Coreference resolution: {the frigate, his vessel, it }
&#8226; Anaphora resolution: {his vessel, it } 
Coreference is a harder task! 
Reference Resolution 2/30 </text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Features (Soon et al, 2001) 
&#8226;	distance in sentences between anaphora and antecedent? 
&#8226;	antecedent in a pronoun? 
&#8226;	weak string identity between anaphora and antecedent? 
&#8226;	anaphora is a de&#64257;nite noun phrase? 
&#8226;	anaphora is a demonstrative pronoun? 
&#8226;	number agreement between anaphora and antecedent 
&#8226;	semantic class agreement anaphora and antecedent 
&#8226;	gender agreement between anaphora and antecedent 
&#8226;	anaphora and antecedent are both proper names? 
an alias feature &#8226; 
&#8226;	an appositive feature 
Reference Resolution	 4/30 </text>
        </slide>
        <slide>
          <slideno>20</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example
Engine 1 goes to Avon to get the oranges.
(TELL (MOVE :theme x :dest y :reason (LOAD :theme w)))
(the x (refers-to x ENG1))
(the y (refers-to y AVON))
(the w (refers-to w ORANGES))
So it&#8217;ll get there at 3 p.m.
(ARRIVE :theme x :dest: y :time z)
&#8220;get there&#8221; requires MOVABLE-OBJECT
Reference Resolution 20/30 </text>
        </slide>
        <slide>
          <slideno>23</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Features
Features induced for spoken dialogue: ante-exp-type [type
of antecedent (NP ,S ,V P ) ]
ana-np-pref [preference for NP arguments]
mdist-3mf3p [ the number of NP-markables between anaphora
and potential antecedent ]
ante-t&#64257;df [the relative importance of the expression in the
dialogues]
average-ic [ information content: neg. log of the total
frequency of the word divided by number of words ]
Reference Resolution 23/30 </text>
        </slide>
        <slide>
          <slideno>15</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Abstract Referents
&#8226;	Webber (1990): each discourse unit produces a 
pseudo discourse entity &#8212; &#8220;proxy for its 
propositional content&#8221; 
&#8226;	Abstract Pronoun interpretation: requires 
presentation of fact referents 
&#8226;	Walker&amp;Whittaker (1990): in problem-solving 
dialogs, people refer to aspects of the solution that 
were not explicitly mentioned 
(Byron, 2002) 
A1 Send engine to Elmira. 
A2 That&#8217;s six hours. 
Reference Resolution	 15/30 </text>
        </slide>
        <slide>
          <slideno>12</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Anaphora In Spoken Dialogue 
Differences between spoken and written text 
&#8226;	High frequency of anaphora 
&#8226;	Presence of &#8220;Vague anaphora&#8221;
(Eckert&amp;Strube&#8217;2000) 33%
Presence of non-NP-antecedents &#8226; 
(Byron&amp;Allen&#8217;1998) TRAINS93: 50% 
(Eckert&amp;Strube&#8217;2000) SwitchBoard: 22% 
&#8226;	Presence of repairs, dis&#64258;uences, abandoned 
utterances and so on... 
Reference Resolution	 12/30 </text>
        </slide>
        <slide>
          <slideno>21</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Evaluation
10 dialogues, 557 utterances, 180 test pronouns 
Salience-based resolution: 37% &#8226; 
&#8226;	Adding Semantic constraints: 43%
&#8226;	Adding Abstract referents: 67%
&#8220;Smart&#8221; Search order: 72%
&#8226; 
&#8226;	Domain Independent Semantics: 51% 
Reference Resolution	 21/30 </text>
        </slide>
        <slide>
          <slideno>22</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Knowledge-Lean Approach 
(Strube&amp;Muller&#8217;2003) 
&#8226;	Switchboard: 3275 sentences, 1771 turns, 16601 
markables 
&#8226; Data annotated with dis&#64258;uency information
&#8220;Problematic&#8221; utterances were discarded
&#8226;
&#8226;	Approach: ML combines standard features with 
dialogue speci&#64257;c features 
Reference Resolution	 22/30 </text>
        </slide>
        <slide>
          <slideno>27</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example 
U1: Lyn&#8217;s mother is a gardener. U2: Craige likes her. 
Reference Resolution 27/30 </text>
        </slide>
        <slide>
          <slideno>28</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Anaphora Generation 
(Reiter&amp;Dale&#8217;1995) 
&#8226;	Application: Lexical choice for generation 
Framework:&#8226;
Context Set C=&#160;a1,a2,...,a n
Properties: pk1,pk2,...,p km
&#8226; Goal: Distinguish Referent from the Rest 
Reference Resolution	 28/30 </text>
        </slide>
        <slide>
          <slideno>11</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Results 
Improvements for some types of references
&#8226;	De&#64257;nite noun phrases: from 19% to 28% (2000
training instances)
&#8226;	No improvements for possessives, proper names and
possessive pronouns
Study of learning curves
&#8226; Personal and possessive pronoun can be trained
from very small training data (100 instances)
&#8226;	Other types of references require large amounts of
training data
Reference Resolution	 11/30 </text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Observations 
(Ng&amp;Cardie&#8217;2002) 
0,76,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y, 
Y,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-. 0,75,83,C,D,C,D,D,D,C,D,I,I,C,I,I,C,N,N,D,C,D,D,N,N,N,N,N,C,Y, Y,D,D,D,C,0,D,D,D,D,D,D,C,1,D,D,C,Y,Y,D,D,D,20,20,D,D,+. 0,74,83,C,D,C,D,D,D,D,D,I,I,C,I,I,D,N,N,D,C,D,D,N,N,N,N,N,C,Y, 
Y,D,D,D,C,0,D,D,D,D,D,D,D,1,D,D,C,N,Y,D,D,D,20,20,D,D,-. 
Reference Resolution 5/30 </text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Observations
&#8226;	Feature selection plays an important role in
classi&#64257;cation accuracy: MUC-6 62.6% (Soon et al.,
2001) &#8594; Ng&amp;Cardie, 2002) 69.1%
&#8226;	Clustering operates over the results of hard
clustering, which may negatively in&#64258;uence the &#64257;nal
results
&#8226;	Machine learning techniques rely on large amounts
of annotated data: 30 texts
&#8226;	All the methods are developed on the same corpus
of newspaper articles
Reference Resolution	 7/30 </text>
        </slide>
        <slide>
          <slideno>13</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Example of Dialog
A1: ..[ he]i&#8217;s nine months old ...
A2: ..[ He] i&#160;likes to dig around a little bit.
A3: ..[ His mother] i&#160;mother comes in and says, why
did you let [ him] i&#160;[plays in the dirt] j.
A4: I guess [[he]i&#8217;s enjoying himself] k.
B5: [That] k&#8217;s right.
B6: [It]j&#8217;s healthy ...
Reference Resolution 13/30 </text>
        </slide>
        <slide>
          <slideno>25</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Observations 
&#8226; Coreference for speech processing is hard!
&#8226; New features for dialogue are required
Prosodic featires seems to be useful
&#8226; 
Reference Resolution 25/30 </text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Last Time
&#8226;	Symbolic Multi-Strategy Anaphora Resolution 
(Lappin&amp;Leass, 1994) 
&#8226;	Clustering-based Coreference Resolution (Cardie&amp;Wagstaff, 1999) 
&#8226;	Supervised ML Coreference Resolution +&#160; Clustering 
(Soon et al, 2001), (Ng&amp;Cardie, 2002) 
Reference Resolution	 3/30 </text>
        </slide>
        <slide>
          <slideno>9</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Co-training 
(Blum&amp;Mitchell, 1998) 
1. Given a small amount of training data, train two
classi&#64257;ers based on orthogonal set of features
2. Add to training set n&#160;instances on which both
classi&#64257;ers agree
3. Retrain both classi&#64257;ers on the extended set 
4. Return to step 2 
Reference Resolution 9/30 </text>
        </slide>
        <slide>
          <slideno>16</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Symbolic Approach 
Pronominal Anaphora Resolution (Byron, 2002) 
&#8226; Mentioned Entities &#8212; referents nouns phrases 
Activated Entities &#8212; entire sentences and nominals &#8226; 
&#8226; Discourse Entity attributes: 
&#8211; Input: The surface linguistic constituent 
&#8211; Type: ENGINE, PERSON, ... 
&#8211; Composition: hetero- or homogeneous 
&#8211; Speci&#64257;city: individual or kind 
Reference Resolution 16/30 </text>
        </slide>
        <slide>
          <slideno>26</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Hobbs&#8217; Algorithm 
Task: Pronoun resolution &#8226; 
&#8226; Features: Fully Syntactic 
&#8226; Accuracy: 82% 
Reference Resolution 26/30 </text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Today 
&#8226; Minimizing amounts of training data: 
&#8211; Co-training 
&#8211; Weakly-supervised learning 
&#8226; Hobbs&#8217; algorithm 
&#8226; Anaphora resolution in dialogs 
Reference Resolution 8/30 </text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text>&#65533;&#160; &#65533;
&#65533;&#160; &#65533;
Classi&#64257;cation Rules 
+ 786 59 IF SOON-WORDS-STR = C 
+ 73 10 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM &lt;=&#160; 1P R O &#173;
RESOLVE = C ANIMACY = C 
+ 40 8 IF WNCLASS = C CONSTRAINTS = D PARANUM &lt;=&#160; 0 PRO-RESOLVE = C 
+ 16 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM &lt;=&#160; 1 BOTH-IN-QUOTES = I 
APPOSITIVE = C 
+ 17 0 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C PARANUM &lt;=&#160; 1 
BPRONOUN-1 = Y AGREEMENT = C CONSTRAINTS = C BOTH-PRONOUNS = C 
+ 38 24 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C SENTNUM &lt;=&#160; 2B O T H &#173;
PRONOUNS = D AGREEMENT = C SUBJECT-2 = Y 
+ 36 8 IF WNCLASS = C PROPER-NOUN = D NUMBERS = C BOTH-PROPER-NOUNS = 
C 
+ 11 0 IF WNCLASS = C CONSTRAINTS = D SENTNUM &lt;=&#160; 3 SUBJECT-1 = Y SUBJECT&#173;
2 = Y SUBCLASS = D IN-QUOTE-2 = N BOTH-DEFINITES = I 
Reference Resolution 6/30 </text>
        </slide>
        <slide>
          <slideno>19</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Semantic Constraints 
&#8220;Heavily-typed&#8221; system 
&#8226;	Verb Senses (selectional restrictions) 
&#8220;Load them into the boxcar&#8221; ( them has to be 
CARGO) 
Predicate NPs &#8226; 
&#8220;That&#8217;s a good route &#8220; ( that has to be a ROUTE) 
&#8226;	Predicate Adjectives
&#8220;It&#8217;s right&#8221; (it has to be a proposition)
Reference Resolution	 19/30 </text>
        </slide>
        <slide>
          <slideno>17</slideno>
          <text>&#65533;	 &#65533;
&#65533;	 &#65533;
Activated Entities 
Generation of Multiple Proxies 
&#8226;	To load the boxcars/Loading them takes an hour 
(in&#64257;nitive or gerund phrase) 
&#8226;	I think he that he&#8217;s an alien (the entire clause) 
&#8226;	I think that he&#8217;s an alien (sentential) 
&#8226;	If he&#8217;s an alien (Subordinate clause) 
Reference Resolution	 17/30 </text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Evaluation for Discourse Processing (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec20/</lecture_pdf_url>
      <lectureno>20</lectureno>
      <slides>
        <slide>
          <slideno>5</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>8</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>2</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text></text>
        </slide>
      </slides>
    </lecture>
    <lecture>
      <lecture_title>Dialogue Systems (PDF)</lecture_title>
      <lecture_pdf_url>https://ocw.mit.edu/courses/6-892-computational-models-of-discourse-spring-2004/resources/lec18/</lecture_pdf_url>
      <lectureno>18</lectureno>
      <slides>
        <slide>
          <slideno>2</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>6</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>0</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>1</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>5</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>4</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>7</slideno>
          <text></text>
        </slide>
        <slide>
          <slideno>3</slideno>
          <text></text>
        </slide>
      </slides>
    </lecture>
    <videos>
      <video>
        <video_url/>
        <video_title/>
        <transcript>
          <slice>
            <text_slice/>
            <time_slice/>
          </slice>
        </transcript>
      </video>
    </videos>
  </lectures>
</doc>
